{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"colab":{"name":"Training.ipynb","provenance":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"JNI_CLbCVJqo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":139},"executionInfo":{"status":"ok","timestamp":1594548572240,"user_tz":-120,"elapsed":28088,"user":{"displayName":"Giacomo","photoUrl":"https://lh6.googleusercontent.com/-_yNWKXs4tL8/AAAAAAAAAAI/AAAAAAAAv5Y/ThWC96YwxpY/s64/photo.jpg","userId":"11330407691056598140"}},"outputId":"f6b8b5e7-cb0c-45d8-df82-484eaf0aabb0"},"source":["%tensorflow_version 1.x\n","import tensorflow as tf\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n","Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lJLlRpGgU77w","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1594548572241,"user_tz":-120,"elapsed":27915,"user":{"displayName":"Giacomo","photoUrl":"https://lh6.googleusercontent.com/-_yNWKXs4tL8/AAAAAAAAAAI/AAAAAAAAv5Y/ThWC96YwxpY/s64/photo.jpg","userId":"11330407691056598140"}},"outputId":"1e9c769d-0007-417f-dc39-755cca639b48"},"source":["try:\n","    import keras\n","except:\n","    !pip install keras"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"XCYfaVGeU774","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":71},"executionInfo":{"status":"ok","timestamp":1594548575806,"user_tz":-120,"elapsed":31347,"user":{"displayName":"Giacomo","photoUrl":"https://lh6.googleusercontent.com/-_yNWKXs4tL8/AAAAAAAAAAI/AAAAAAAAv5Y/ThWC96YwxpY/s64/photo.jpg","userId":"11330407691056598140"}},"outputId":"bafc77a7-7a3c-4398-8d7f-23c26a2f5766"},"source":["from pathlib import Path\n","import os\n","os.chdir('/content/drive/My Drive/haikurnn-master/haikurnn-master/notebooks/models/v1')\n","tf_session = tf.Session()\n","from keras import backend as K\n","K.set_session(tf_session)\n","import math\n","from keras.callbacks import ModelCheckpoint,  CSVLogger\n","from keras.layers import Add, Dense, Input, LSTM\n","from keras.models import Model\n","from keras.preprocessing.text import Tokenizer\n","from keras.utils import np_utils\n","import requests\n","import numpy as np\n","import pandas as pd\n","from sklearn.externals import joblib\n","\n","# Local library with model definitions for training and generating\n","from models import create_training_model"],"execution_count":5,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n","  warnings.warn(msg, category=FutureWarning)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"JaeVQmCAU779","colab_type":"text"},"source":["# Load Input"]},{"cell_type":"code","metadata":{"id":"72OQufHqU77-","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594548611944,"user_tz":-120,"elapsed":923,"user":{"displayName":"Giacomo","photoUrl":"https://lh6.googleusercontent.com/-_yNWKXs4tL8/AAAAAAAAAAI/AAAAAAAAv5Y/ThWC96YwxpY/s64/photo.jpg","userId":"11330407691056598140"}}},"source":["# Settings\n","\n","# Percent of samples to use for training, might be necessary if you're running out of memory\n","sample_size = 1\n","\n","# The latent dimension of the LSTM\n","latent_dim = 4096\n","\n","# Number of epochs to train for\n","epochs = 40\n","\n","root_path = Path('../../..')\n","input_path = root_path / 'input'\n","poem_path = input_path / 'poems'\n","haiku_path = poem_path / 'haikus.csv'\n","\n","name = 'all_data_test_2'\n","output_dir = Path('output_%s' % name)\n","output_dir.mkdir()"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"D0DFCwgua_8u","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594548613144,"user_tz":-120,"elapsed":449,"user":{"displayName":"Giacomo","photoUrl":"https://lh6.googleusercontent.com/-_yNWKXs4tL8/AAAAAAAAAAI/AAAAAAAAv5Y/ThWC96YwxpY/s64/photo.jpg","userId":"11330407691056598140"}}},"source":["f1 = open(\"/content/drive/My Drive/haikurnn-master/haikurnn-master/notebooks/models/v1/first.txt\",\"r\")\n","f2 = open(\"/content/drive/My Drive/haikurnn-master/haikurnn-master/notebooks/models/v1/second.txt\",\"r\")\n","f3 = open(\"/content/drive/My Drive/haikurnn-master/haikurnn-master/notebooks/models/v1/third.txt\",\"r\")"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"mVwEQT0hdmic","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594548614074,"user_tz":-120,"elapsed":680,"user":{"displayName":"Giacomo","photoUrl":"https://lh6.googleusercontent.com/-_yNWKXs4tL8/AAAAAAAAAAI/AAAAAAAAv5Y/ThWC96YwxpY/s64/photo.jpg","userId":"11330407691056598140"}}},"source":["first = f1.read().split(sep='\\n')\n","second = f2.read().split(sep='\\n')\n","third = f3.read().split(sep='\\n')"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"Uu60ZWhAfuDf","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594548615007,"user_tz":-120,"elapsed":1112,"user":{"displayName":"Giacomo","photoUrl":"https://lh6.googleusercontent.com/-_yNWKXs4tL8/AAAAAAAAAAI/AAAAAAAAv5Y/ThWC96YwxpY/s64/photo.jpg","userId":"11330407691056598140"}}},"source":["for x in first:\n","  if len(x) < 2:\n","    first.remove(x)\n","for x in second:\n","  if len(x) < 2:\n","    second.remove(x)\n","for x in third:\n","  if len(x) < 2:\n","    third.remove(x)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"sHHZ-gejgc3T","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1594548617626,"user_tz":-120,"elapsed":906,"user":{"displayName":"Giacomo","photoUrl":"https://lh6.googleusercontent.com/-_yNWKXs4tL8/AAAAAAAAAAI/AAAAAAAAv5Y/ThWC96YwxpY/s64/photo.jpg","userId":"11330407691056598140"}},"outputId":"b6aa44ad-840e-4e0f-9703-25eddcc45c39"},"source":["print(len(first), len(second), len(third))"],"execution_count":14,"outputs":[{"output_type":"stream","text":["0 3810 3810\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RLQKh4KNDPGu","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594548878460,"user_tz":-120,"elapsed":3810,"user":{"displayName":"Giacomo","photoUrl":"https://lh6.googleusercontent.com/-_yNWKXs4tL8/AAAAAAAAAAI/AAAAAAAAv5Y/ThWC96YwxpY/s64/photo.jpg","userId":"11330407691056598140"}}},"source":["def get_hyp_lm_tercets(tercets):\n","    new_tercets = []\n","    for tercet in tercets:\n","        new_tercets.append([])\n","        for verse in tercet:\n","            new_tercets[-1].append([])\n","            for hyp_w in verse:\n","                new_tercets[-1][-1].extend(hyp_w)\n","                new_tercets[-1][-1].append('<SEP>')\n","            new_tercets[-1][-1] = new_tercets[-1][-1][:-1]\n","\n","    return new_tercets\n","\n","def is_vowel(c):\n","    return c in 'aeiouAEIOUàìíèéùúüòï'\n","\n","def unsplittable_cons():\n","    u_cons = []\n","    for c1 in ('b', 'c', 'd', 'f', 'g', 'p', 't', 'v'):\n","        for c2 in ('l', 'r'):\n","            u_cons.append(c1 + c2)\n","\n","    others = ['gn', 'gh', 'ch']\n","    u_cons.extend(others)\n","    return u_cons\n","\n","\n","def are_cons_to_split(c1, c2):\n","    to_split = ('cq', 'cn', 'lm', 'rc', 'bd', 'mb', 'mn', 'ld', 'ng', 'nd', 'tm', 'nv', 'nc', 'ft', 'nf', 'gm', 'fm', 'rv', 'fp')\n","    return (c1 + c2) in to_split or (not is_vowel(c1) and (c1 == c2)) or ((c1 + c2) not in unsplittable_cons()) and (\n","        (not is_vowel(c1)) and (not is_vowel(c2)) and c1 != 's')\n","\n","\n","def is_diphthong(c1, c2):\n","    return (c1 + c2) in ('ia', 'ie', 'io', 'iu', 'ua', 'ue', 'uo', 'ui', 'ai', 'ei', 'oi', 'ui', 'au', 'eu', 'ïe', 'iú', 'iù')\n","\n","\n","def is_triphthong(c1, c2, c3):\n","    return (c1 + c2 + c3) in ('iai', 'iei', 'uoi', 'uai', 'uei', 'iuo')\n","\n","\n","def is_toned_vowel(c):\n","    return c in 'àìèéùòï'\n","\n","def has_vowels(sy):\n","    for c in sy:\n","        if is_vowel(c):\n","            return True\n","    return False\n","\n","\n","def hyphenation(word):\n","    \"\"\"\n","    Split word in syllables\n","    :param word: input string\n","    :return: a list containing syllables of the word\n","    \"\"\"\n","    if not word or word == '':\n","        return []\n","    # elif len(word) == 3 and (is_vowel(word[1]) and is_vowel(word[2]) and not is_toned_vowel(word[2]) and (\n","    #     not is_diphthong(word[1], word[2]) or (word[1] == 'i'))):\n","    elif len(word) == 3 and (is_vowel(word[1]) and is_vowel(word[2]) and not is_toned_vowel(word[2]) and (\n","        not is_diphthong(word[1], word[2]))):\n","        return [word[:2]] + [word[2]]\n","    elif len(word) == 3 and is_vowel(word[0]) and not is_vowel(word[1]) and is_vowel(word[2]):\n","        return [word[:2]] + [word[2]]\n","    elif len(word) == 3:\n","        return [word]\n","\n","    syllables = []\n","    is_done = False\n","    count = 0\n","    while not is_done and count <= len(word) - 1:\n","        syllables.append('')\n","        c = word[count]\n","        while not is_vowel(c) and count < len(word) - 1:\n","            syllables[-1] = syllables[-1] + c\n","            count += 1\n","            c = word[count]\n","\n","        syllables[-1] = syllables[-1] + word[count]\n","\n","        if count == len(word) - 1:\n","            is_done = True\n","        else:\n","            count += 1\n","\n","            if count < len(word) and not is_vowel(word[count]):\n","                if count == len(word) - 1:\n","                    syllables[-1] += word[count]\n","                    count += 1\n","                elif count + 1 < len(word) and are_cons_to_split(word[count], word[count + 1]):\n","                    syllables[-1] += word[count]\n","                    count += 1\n","                elif count + 2 < len(word) and not is_vowel(word[count + 1]) and not is_vowel(word[count + 2]) and word[\n","                    count] != 's':\n","                    syllables[-1] += word[count]\n","                    count += 1\n","            elif count < len(word):\n","                if count + 1 < len(word) and is_triphthong(word[count - 1], word[count], word[count + 1]):\n","                    syllables[-1] += word[count] + word[count + 1]\n","                    count += 2\n","                elif is_diphthong(word[count - 1], word[count]):\n","                    syllables[-1] += word[count]\n","                    count += 1\n","\n","                if count + 1 < len(word) and are_cons_to_split(word[count], word[count + 1]):\n","                    syllables[-1] += word[count]\n","                    count += 1\n","\n","            else:\n","                is_done = True\n","\n","    if not has_vowels(syllables[-1]) and len(syllables) > 1:\n","        syllables[-2] = syllables[-2] + syllables[-1]\n","        syllables = syllables[:-1]\n","\n","    return syllables\n","\n","\n","\n","def get_dc_hyphenation(canti):\n","    hyp_canti, hyp_tokens = [], []\n","    for canto in canti:\n","        hyp_canti.append([])\n","        for verso in canto:\n","            syllables = seq_hyphentation(verso)\n","            hyp_canti[-1].append(syllables)\n","            for syllable in syllables:\n","                hyp_tokens.extend(syllable)\n","\n","    return hyp_canti, hyp_tokens\n","\n","\n","def seq_hyphentation(words):\n","    \"\"\"\n","    Converts words in a list of strings into lists of syllables\n","    :param words: a list of words (strings)\n","    :return: a list of lists containing word syllables\n","    \"\"\"\n","    return [hyphenation(w) for w in words]\n","\n","\n","def get_dc_cantos(filename, encoding=None):\n","    # raw_data = read_words(filename=filename)\n","    cantos, words, raw = [], [], []\n","    with open(filename, \"r\", encoding=encoding) as f:\n","        for line in f:\n","            sentence = line.strip()\n","            sentence = str.replace(sentence, \"\\.\", \" \\. \")\n","            sentence = str.replace(sentence, \"[\", '')\n","            sentence = str.replace(sentence, \"]\", '')\n","            sentence = str.replace(sentence, \"-\", '')\n","            sentence = str.replace(sentence, \";\", \" ; \")\n","            sentence = str.replace(sentence, \",\", \" , \")\n","            # sentence = str.replace(sentence, \" \\'\", '')\n","            sentence = str.replace(sentence, \"\\'\", ' \\' ')\n","            if len(sentence) > 1:\n","                # sentence = sentence.translate(string.punctuation)\n","                tokenized_sentence = nl.word_tokenize(sentence)\n","                # tokenized_sentence = sentence.split()\n","                tokenized_sentence = [w.lower() for w in tokenized_sentence if len(w) > 0]\n","                tokenized_sentence = [w for w in tokenized_sentence if \",\" not in w]\n","                tokenized_sentence = [w for w in tokenized_sentence if \".\" not in w]\n","                tokenized_sentence = [w for w in tokenized_sentence if \":\" not in w]\n","                tokenized_sentence = [w for w in tokenized_sentence if \";\" not in w]\n","                tokenized_sentence = [w for w in tokenized_sentence if \"«\" not in w]\n","                tokenized_sentence = [w for w in tokenized_sentence if \"»\" not in w]\n","                # ts = []\n","                ts = tokenized_sentence\n","                # [ts.extend(re.split(\"(\\')\", e)) for e in tokenized_sentence]\n","                tokenized_sentence = [w for w in ts if len(w) > 0]\n","\n","                if len(tokenized_sentence) == 2:\n","                    cantos.append([])\n","                    raw.append([])\n","                elif len(tokenized_sentence) > 2:\n","                    raw[-1].append(sentence)\n","                    cantos[-1].append(tokenized_sentence)\n","                    words.extend(tokenized_sentence)\n","    print(cantos)\n","    return cantos, words, raw\n","\n","\n","def create_tercets(cantos):\n","    tercets = []\n","    for i,canto in enumerate(cantos):\n","        for v,verse in enumerate(canto):\n","            if v%3 == 0:\n","                tercets.append([])\n","\n","            tercets[-1].append(verse)\n","        tercets = tercets[:-1]  # removes the last malformed tercets (only 2 verses)\n","\n","    return tercets\n","\n","def pad_list(l, pad_token, max_l_size, keep_lasts=False, pad_right=True):\n","    \"\"\"\n","    Adds a padding token to a list\n","    inputs:\n","    :param l: input list to pad.\n","    :param pad_token: value to add as padding.\n","    :param max_l_size: length of the new padded list to return,\n","    it truncates lists longer that 'max_l_size' without adding\n","    padding values.\n","    :param keep_lasts: If True, preserves the max_l_size last elements\n","    of a sequence (by keeping the same order).  E.g.:\n","    if keep_lasts is True and max_l_size=3 [1,2,3,4] becomes [2,3,4].\n","\n","\n","    :return: the list padded or truncated.\n","    \"\"\"\n","    to_pad = []\n","    max_l = min(max_l_size, len(l))  # maximum len\n","    l_init = len(l) - max_l if len(l) > max_l and keep_lasts else 0  # initial position where to sample from the list\n","    l_end = len(l) if len(l) > max_l and keep_lasts else max_l\n","    for i in range(l_init, l_end):\n","        to_pad.append(l[i])\n","\n","    # for j in range(len(l), max_l_size):\n","    #     to_pad.append(pad_token)\n","    pad_tokens = [pad_token] * (max_l_size-len(l))\n","    padded_l = to_pad + pad_tokens if pad_right else pad_tokens + to_pad\n","\n","    return padded_l\n","\n","\n","def save_data(data, file):\n","    with open(file, 'wb') as output:\n","        pickle.dump(data, output, pickle.HIGHEST_PROTOCOL)\n","\n","def load_data(file):\n","    with open(file, 'rb') as obj:\n","        return pickle.load(obj)\n","\n","def print_and_write(file, s):\n","    print(s)\n","    file.write(s)\n","\n","\n","class Vocabulary(object):\n","    def __init__(self, vocab_size=None):\n","        self.dictionary = dict()\n","        self.rev_dictionary = dict()\n","        self.count = []\n","        self.special_tokens = []\n","        self.vocab_size = vocab_size\n","\n","    def build_vocabulary_from_counts(self, count, special_tokens=[]):\n","        \"\"\"\n","        Sets all the attributes of the Vocabulary object.\n","        :param count: a list of lists as follows: [['token', number_of_occurrences],...]\n","        :param special_tokens: a list of strings. E.g. ['<EOS>', '<PAD>',...]\n","        :return: None\n","        \"\"\"\n","\n","        dictionary = dict()\n","        for word, _ in count:\n","            dictionary[word] = len(dictionary)\n","\n","        # adding eventual special tokens to the dictionary (e.g. <EOS>,<PAD> etc..)\n","        d = len(dictionary)\n","        for i, token in enumerate(special_tokens):\n","            dictionary[token] = d + i\n","\n","        self.count = count\n","        self.dictionary = dictionary\n","        self.rev_dictionary = dict(zip(self.dictionary.values(), self.dictionary.keys()))\n","        self.special_tokens = special_tokens\n","        self.vocab_size = len(dictionary)\n","\n","    def build_vocabulary_from_tokens(self, tokens, vocabulary_size=None, special_tokens=[]):\n","        \"\"\"\n","        Given a list of tokens, it sets the Vocabulary object attributes by constructing\n","        a dictionary mapping each token to a unique id.\n","        :param tokens: a list of strings.\n","         E.g. [\"the\", \"cat\", \"is\", ... \".\", \"the\", \"house\" ,\"is\" ...].\n","         NB: Here you should put all your token instances of the corpus.\n","        :param vocabulary_size: The number of elements of your vocabulary. If there are more\n","        than 'vocabulary_size' elements on tokens, it considers only the 'vocabulary_size'\n","        most frequent ones.\n","        :param special_tokens: Optional. A list of strings. Useful to add special tokens in vocabulary.\n","        If you don't have any, keep it empty.\n","        :return: None\n","        \"\"\"\n","\n","        vocabulary_size = vocabulary_size if vocabulary_size is not None else self.vocab_size\n","        vocabulary_size = vocabulary_size - (len(special_tokens) + 1) if vocabulary_size else None\n","        # counts occurrences of each token\n","        count = [['<UNK>', -1]]\n","        count.extend(collections.Counter(tokens).most_common(vocabulary_size))  # takes only the most frequent ones, if size is None takes them all\n","        self.build_vocabulary_from_counts(count, special_tokens)  # actually build the vocabulary\n","        self._set_unk_count(tokens)  # set the number of OOV instances\n","\n","    @staticmethod\n","    def merge_vocabulary(vocab0, vocab1, vocabulary_size=-1):\n","        \"\"\"\n","        Merge two Vocabulary objects into a new one.\n","        :param vocab0: first Vocabulary object\n","        :param vocab1: second Vocabulary object\n","        :param vocabulary_size: parameter to decide the merged vocabulary size.\n","        With default value -1, all the words of both vocabularies are preserved.\n","        When set to 0, the size of the vocabulary is set to the size of vocab0,\n","        when set to 1 it is kept the size of vocab1.\n","        :return: a new vocabulary\n","        \"\"\"\n","        # get size of the new vocabulary\n","        vocab_size = vocab0.vocab_size + vocab1.vocab_size if vocabulary_size == -1 else vocabulary_size\n","        merged_special_tokens = list(set(vocab0.special_tokens) | set(vocab1.special_tokens))\n","\n","        # merge the counts from the two vocabularies and then selects the most_common tokens\n","        merged_counts = collections.Counter(dict(vocab0.count)) + collections.Counter(dict(vocab1.count))\n","        merged_counts = merged_counts.most_common(vocab_size)\n","        count = [['<UNK>', -1]]\n","        count.extend(merged_counts)\n","\n","        # create the new vocabulary\n","        merged_vocab = Vocabulary(vocab_size)\n","        merged_vocab.build_vocabulary_from_counts(count, merged_special_tokens)\n","        return merged_vocab\n","\n","    @staticmethod\n","    def merge_vocabularies(vocab_list, vocab_size=None):\n","        \"\"\"\n","        Join a list of vocabularies into a new one.\n","        :param vocab_list: a list of Vocabulary objects\n","        :param vocab_size: the maximum size of the merged vocabulary.\n","        :return: a vocabulary merging them all.\n","        \"\"\"\n","        vocab_size = vocab_size if vocab_size else sum([v.vocab_size for v in vocab_list])\n","        merged_vocab = Vocabulary(vocab_size)\n","        for voc in vocab_list:\n","            merged_vocab = Vocabulary.merge_vocabulary(merged_vocab, voc, vocab_size)\n","        return merged_vocab\n","\n","    def string2id(self, dataset):\n","        \"\"\"\n","        Converts a dataset of strings into a dataset of ids according to the object dictionary.\n","        :param dataset: any string-based dataset with any nested lists.\n","        :return: a new dataset, with the same shape of dataset, where each string is mapped into its\n","        corresponding id associated in the dictionary (0 for unknown tokens).\n","        \"\"\"\n","\n","        def _recursive_call(items):\n","            new_items = []\n","            for item in items:\n","                if isinstance(item, str) or isinstance(item, int) or isinstance(item, float):\n","                    new_items.append(self.word2id(item))\n","                else:\n","                    new_items.append(_recursive_call(item))\n","            return new_items\n","\n","        return _recursive_call(dataset)\n","\n","    def id2string(self, dataset):\n","        \"\"\"\n","        Converts a dataset of integer ids into a dataset of string according to the reverse dictionary.\n","        :param dataset: any int-based dataset with any nested lists. Allowed types are int, np.int32, np.int64.\n","        :return: a new dataset, with the same shape of dataset, where each token is mapped into its\n","        corresponding string associated in the reverse dictionary.\n","        \"\"\"\n","        def _recursive_call(items):\n","            new_items = []\n","            for item in items:\n","                if isinstance(item, int) or isinstance(item, np.int) or isinstance(item, np.int32) or isinstance(item, np.int64):\n","                    new_items.append(self.id2word(item))\n","                else:\n","                    new_items.append(_recursive_call(item))\n","            return new_items\n","\n","        return _recursive_call(dataset)\n","\n","    def word2id(self, item):\n","        \"\"\"\n","        Maps a string token to its corresponding id.\n","        :param item: a string.\n","        :return: If the token belongs to the vocabulary, it returns an integer id > 0, otherwise\n","        it returns the value associated to the unknown symbol, that is typically 0.\n","        \"\"\"\n","        return self.dictionary[item] if item in self.dictionary else self.dictionary['<UNK>']\n","\n","    def id2word(self, token_id):\n","        \"\"\"\n","        Maps an integer token to its corresponding string.\n","        :param token_id: an integer.\n","        :return: If the id belongs to the vocabulary, it returns the string\n","        associated to it, otherwise it returns the string associated\n","        to the unknown symbol, that is '<UNK>'.\n","        \"\"\"\n","\n","        return self.rev_dictionary[token_id] if token_id in self.rev_dictionary else self.rev_dictionary[self.dictionary['<UNK>']]\n","\n","    def get_unk_count(self):\n","        return self.count[0][1]\n","\n","    def _set_unk_count(self, tokens):\n","        \"\"\"\n","        Sets the number of OOV instances in the tokens provided\n","        :param tokens: a list of tokens\n","        :return: None\n","        \"\"\"\n","        data = list()\n","        unk_count = 0\n","        for word in tokens:\n","            if word in self.dictionary:\n","                index = self.dictionary[word]\n","            else:\n","                index = 0  # dictionary['<UNK>']\n","                unk_count += 1\n","            data.append(index)\n","        self.count[0][1] = unk_count\n","\n","    def add_element(self, name, is_special_token=False):\n","        if name not in self.dictionary:\n","            self.vocab_size += 1\n","            self.dictionary[name] = self.vocab_size\n","            self.rev_dictionary[self.vocab_size] = name\n","\n","            if is_special_token:\n","                self.special_tokens = list(self.special_tokens)\n","                self.special_tokens.append(name)\n","\n","            self.count.append([name, 1])\n","\n","    def set_vocabulary(self, dictionary, rev_dictionary, special_tokens, vocab_size):\n","        self.dictionary = dictionary,\n","        self.rev_dictionary = rev_dictionary\n","        self.special_tokens = special_tokens\n","        self.vocab_size = vocab_size\n","\n","    @staticmethod\n","    def load_vocabulary(filename):\n","        return load_data(filename)\n","\n","    def save_vocabulary(self, filename):\n","        save_data(self, filename)\n","\n","class SyLMDataset(object):\n","    def __init__(self, config, sy_vocab=None):\n","        self.config = config\n","        self.vocabulary = sy_vocab\n","\n","        self.raw_train_x = []\n","        self.raw_val_x = []\n","        self.raw_test_x = []\n","        self.raw_x = []\n","\n","        self.train_x, self.train_y = [], []\n","        self.val_x, self.val_y = [], []\n","        self.test_x, self.test_y = [], []\n","        self.x, self.y = [], []\n","\n","    def initialize(self, sess):\n","        pass\n","\n","    def load(self, sources):\n","        \"\"\"\n","        Extract raw texts form sources and gather them all together.\n","        :param sources: a string or an iterable of strings containing the file(s)\n","        to process in order to build the dataset.\n","        :return: a list of raw strings.\n","        \"\"\"\n","        return NotImplementedError\n","\n","    def build(self, sources, split_size=0.8):\n","        \"\"\"\n","        :param sources: a string or an iterable of strings containing the file(s)\n","        to process in order to build the dataset.\n","        :param split_size: the size to split the dataset, set >=1.0 to not split.\n","        \"\"\"\n","\n","        raw_x = self.load(sources)\n","        # raw_x = self.tokenize([self.preprocess(ex) for ex in raw_x])  # fixme\n","        # splitting data\n","        print(raw_x)\n","        self.raw_x = raw_x\n","        if split_size < 1.0:\n","            self.raw_train_x, self.raw_test_x = self.split(self.raw_x, train_size=split_size)\n","            self.raw_train_x, self.raw_val_x = self.split(self.raw_train_x, train_size=split_size)\n","        else:\n","            self.raw_train_x = self.raw_x\n","\n","        if self.vocabulary is None:\n","            # creates vocabulary\n","            tokens = [item for sublist in self.raw_train_x for item in sublist]  # get tokens\n","            special_tokens = (\"<GO>\", \"<PAD>\", \"<SEP>\", \"<EOS>\", \"<EOV>\")\n","            self._create_vocab(tokens, special_tokens=special_tokens)\n","\n","        # creates x,y for train\n","        self.train_x = self._build_dataset(self.raw_train_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n","        self.train_y = self._build_dataset(self.raw_train_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n","\n","        # creates x,y for validation\n","        self.val_x = self._build_dataset(self.raw_val_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n","        self.val_y = self._build_dataset(self.raw_val_x, insert_go=False, max_len=self.config.sentence_max_len, shuffle=False)\n","\n","        # creates x,y for validation\n","        self.test_x = self._build_dataset(self.raw_test_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n","        self.test_y = self._build_dataset(self.raw_test_x, insert_go=False, max_len=self.config.sentence_max_len, shuffle=False)\n","\n","    def _create_vocab(self, tokens, special_tokens=(\"<PAD>\", \"<GO>\", \"<SEP>\", \"<EOV>\", \"<EOS>\")):\n","        \"\"\"\n","        Create the vocabulary. Special tokens can be added to the tokens obtained from\n","        the corpus.\n","        :param tokens: a list of all the tokens in the corpus. Each token is a string.\n","        :param special_tokens: a list of strings.\n","        \"\"\"\n","        vocab = Vocabulary(vocab_size=self.config.input_vocab_size)\n","        vocab.build_vocabulary_from_tokens(tokens, special_tokens=special_tokens)\n","        self.vocabulary = vocab\n","\n","    @staticmethod\n","    def split(raw_data, train_size=0.8):\n","        size = math.floor(len(raw_data)*train_size)\n","        return raw_data[:size], raw_data[size:]\n","\n","    @staticmethod\n","    def preprocess(txt):\n","        return txt\n","\n","    @staticmethod\n","    def shuffle(x):\n","        return random.sample(x, len(x))\n","\n","    @staticmethod\n","    def tokenize(txt):\n","        return txt\n","\n","    def _build_dataset(self, raw_data, max_len=100, insert_go=True, keep_lasts=False, pad_right=True, shuffle=True):\n","        \"\"\"\n","        Converts all the tokens in e1_raw_data by mapping each token with its corresponding\n","        value in the dictionary. In case of token not in the dictionary, they are assigned to\n","        a specific id. Each sequence is padded up to the seq_max_len setup in the config.\n","\n","        :param raw_data: list of sequences, each sequence is a list of tokens (strings).\n","        :param max_len: max length of a sequence, crop longer and pad smaller ones.\n","        :param insert_go: True to insert <GO>, False otherwise.\n","        :param keep_lasts: True to truncate initial elements of a sequence.\n","        :param pad_right: pad to the right (default value True), otherwise pads to left.\n","        :param shuffle: Optional. If True data are shuffled.\n","        :return: A list of sequences where each token in each sequence is an int id.\n","        \"\"\"\n","        dataset = []\n","        for sentence in raw_data:\n","            sentence_ids = [self.vocabulary.word2id(\"<GO>\")] if insert_go else []\n","            sentence_ids.extend([self.vocabulary.word2id(w) for w in sentence])\n","            sentence_ids.append(self.vocabulary.word2id(\"<EOS>\"))\n","            #sentence_ids = pad_list(sentence_ids, self.vocabulary.word2id(\"<PAD>\"), max_len, keep_lasts=keep_lasts, pad_right=pad_right)\n","\n","            dataset.append(sentence_ids)\n","\n","        if shuffle:\n","            return random.sample(dataset, len(dataset))\n","        else:\n","            return dataset\n","\n","    def get_batches(self, batch_size=32):\n","        \"\"\"\n","        Iterator over the training set. Useful method to run experiments.\n","        :param batch_size: size of the mini_batch\n","        :return: input and target.\n","        \"\"\"\n","        x, y = self.train_x, self.train_y\n","        \n","        i = random.randint(0, batch_size)\n","        batches = []\n","        eov = self.vocabulary.word2id(\"<EOV>\")\n","        # prepare batches\n","        while i < len(x):\n","            j = 0\n","            batch_x, batch_y = [], []\n","            while j < batch_size and i+j<len(x):\n","                for c in x[i+j]:\n","                  batch_x.append(c)\n","                batch_x.append(eov)\n","                for c in y[i+j]:\n","                  batch_y.append(c)\n","                batch_y.append(eov)\n","                j += 1\n","            i += batch_size\n","\n","            batches.append((batch_x, batch_y))\n","\n","        # shuffle\n","        random.shuffle(batches)\n","\n","        # supply\n","        i = 0\n","        while i < len(batches):\n","            yield batches[i][0], batches[i][1]\n","            i += 1\n","\n","class DanteSyLMDataset(SyLMDataset):\n","    def __init__(self, config, sy_vocab=None):\n","        \"\"\"\n","        Class to create a dataset from Dante Alighieri's Divine Comedy.\n","        :param config: a Config object\n","        :param sy_vocab: (optional) a Vocabulary object where tokens of the dictionary\n","        are syllables. If None, the vocabulary is create automatically from the source.\n","        \"\"\"\n","        super().__init__(config, sy_vocab)\n","\n","    def load(self, sources):\n","        \"\"\"\n","        Load examples from dataset\n","        :param sources: data filepath.\n","        :return:\n","        \"\"\"\n","        canti, _, raw = get_dc_cantos(filename=sources)  # get raw data from file\n","        canti, tokens = get_dc_hyphenation(canti)  # converts each\n","        print(canti)\n","        tercets = create_tercets(canti)\n","        tercets = get_hyp_lm_tercets(tercets)\n","        print(tercets)\n","        x = []\n","        for tercet in tercets:\n","            x.append([])\n","            print(tercet)\n","            for verse in tercet:\n","                x[-1].extend(verse)\n","                x[-1].append(\"<EOV>\")\n","        print(x)\n","        #x = self.shuffle(x)\n","        return x\n","\n","def seq2str(seq):\n","    def output2string(batch, rev_vocabulary, special_tokens, end_of_tokens):\n","        to_print = ''\n","        for token in batch:\n","            if token in special_tokens:\n","                to_print += ' '\n","            elif end_of_tokens and token in end_of_tokens:\n","                to_print += '\\n'\n","            elif token in rev_vocabulary:\n","                to_print += rev_vocabulary[token]\n","            else:\n","                to_print += '<UNK>'\n","        return to_print\n","\n","    return output2string(seq, poetry_sy_lm_dataset.vocabulary.rev_dictionary,\n","      special_tokens=[poetry_sy_lm_dataset.vocabulary.word2id(\"<PAD>\"), 0, poetry_sy_lm_dataset.vocabulary.word2id(\"<SEP>\"),\n","                      poetry_sy_lm_dataset.vocabulary.word2id(\"<GO>\"), poetry_sy_lm_dataset.vocabulary.word2id(\"<EOS>\")],\n","      end_of_tokens=[poetry_sy_lm_dataset.vocabulary.word2id(\"<EOV>\")])\n","\n","class cnfg:\n","  vocab_size = 1884\n","  input_vocab_size = 1884\n","  emb_size = 300\n","  sentence_max_len = 75"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"2gfRlX36DVnT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":578},"executionInfo":{"status":"error","timestamp":1594548878461,"user_tz":-120,"elapsed":3798,"user":{"displayName":"Giacomo","photoUrl":"https://lh6.googleusercontent.com/-_yNWKXs4tL8/AAAAAAAAAAI/AAAAAAAAv5Y/ThWC96YwxpY/s64/photo.jpg","userId":"11330407691056598140"}},"outputId":"5490135c-7269-4ec1-b19e-9727f6a1476e"},"source":["import collections\n","import random\n","import nltk as nl\n","nl.download('punkt')\n","config = cnfg()\n","poetry_sy_lm_dataset = DanteSyLMDataset(config, sy_vocab=None)\n","\n","data_path = os.path.join(os.getcwd(), \"first.txt\") \n","poetry_sy_lm_dataset.build(data_path, split_size=0.9)  # actual creation of  vocabulary (if not provided) and dataset\n","print(\"Train size: \" + str(len(poetry_sy_lm_dataset.train_y)))\n","print(\"Val size: \" + str(len(poetry_sy_lm_dataset.val_y)))\n","print(\"Test size: \" + str(len(poetry_sy_lm_dataset.test_y)))\n","first = [b for b in poetry_sy_lm_dataset.get_batches(20)]\n","\n","poetry_sy_lm_dataset1 = DanteSyLMDataset(config, sy_vocab=poetry_sy_lm_dataset.vocabulary)\n","\n","data_path = os.path.join(os.getcwd(), \"second.txt\")  # dataset location, here just the name of the source file\n","poetry_sy_lm_dataset.build(data_path, split_size=0.9)  # actual creation of  vocabulary (if not provided) and dataset\n","print(\"Train size: \" + str(len(poetry_sy_lm_dataset.train_y)))\n","print(\"Val size: \" + str(len(poetry_sy_lm_dataset.val_y)))\n","print(\"Test size: \" + str(len(poetry_sy_lm_dataset.test_y)))\n","second = [b for b in poetry_sy_lm_dataset.get_batches(20)]\n","\n","poetry_sy_lm_dataset2 = DanteSyLMDataset(config, sy_vocab=poetry_sy_lm_dataset1.vocabulary)\n","\n","data_path = os.path.join(os.getcwd(), \"third.txt\")  # dataset location, here just the name of the source file\n","poetry_sy_lm_dataset.build(data_path, split_size=0.9)  # actual creation of  vocabulary (if not provided) and dataset\n","print(\"Train size: \" + str(len(poetry_sy_lm_dataset.train_y)))\n","print(\"Val size: \" + str(len(poetry_sy_lm_dataset.val_y)))\n","print(\"Test size: \" + str(len(poetry_sy_lm_dataset.test_y)))\n","third = [b for b in poetry_sy_lm_dataset.get_batches(20)]"],"execution_count":26,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[]\n","[]\n","[]\n","[]\n","[]\n","Train size: 0\n","Val size: 0\n","Test size: 0\n"],"name":"stdout"},{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-65362899a137>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"second.txt\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# dataset location, here just the name of the source file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mpoetry_sy_lm_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# actual creation of  vocabulary (if not provided) and dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train size: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoetry_sy_lm_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Val size: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoetry_sy_lm_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-25-477b25736254>\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, sources, split_size)\u001b[0m\n\u001b[1;32m    470\u001b[0m         \"\"\"\n\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m         \u001b[0mraw_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msources\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m         \u001b[0;31m# raw_x = self.tokenize([self.preprocess(ex) for ex in raw_x])  # fixme\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;31m# splitting data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-25-477b25736254>\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, sources)\u001b[0m\n\u001b[1;32m    607\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m         \"\"\"\n\u001b[0;32m--> 609\u001b[0;31m         \u001b[0mcanti\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dc_cantos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msources\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# get raw data from file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    610\u001b[0m         \u001b[0mcanti\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dc_hyphenation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcanti\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# converts each\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcanti\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-25-477b25736254>\u001b[0m in \u001b[0;36mget_dc_cantos\u001b[0;34m(filename, encoding)\u001b[0m\n\u001b[1;32m    176\u001b[0m                     \u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_sentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                     \u001b[0mraw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m                     \u001b[0mcantos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                     \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"]}]},{"cell_type":"code","metadata":{"id":"8bKrvRAafVCq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":589},"executionInfo":{"status":"ok","timestamp":1594485606715,"user_tz":-120,"elapsed":8888,"user":{"displayName":"Giacomo","photoUrl":"https://lh6.googleusercontent.com/-_yNWKXs4tL8/AAAAAAAAAAI/AAAAAAAAv5Y/ThWC96YwxpY/s64/photo.jpg","userId":"11330407691056598140"}},"outputId":"f55bde54-8212-4722-e15f-976766f32c3d"},"source":["df = pd.DataFrame()\n","df['0'] = first\n","df['1'] = second\n","df['2'] = third\n","df['0_syllables'] = [11 for x in range(3810)]\n","df['1_syllables'] = [11 for x in range(3810)]\n","df['2_syllables'] = [11 for x in range(3810)]\n","df"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>0_syllables</th>\n","      <th>1_syllables</th>\n","      <th>2_syllables</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>in tutte tue question certo mi piaci</td>\n","      <td>rispuose ma ' l bollor dell ' acqua rossa</td>\n","      <td>dovea ben solver l ' una che tu faci</td>\n","      <td>11</td>\n","      <td>11</td>\n","      <td>11</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>letè vedrai ma fuor di questa fossa</td>\n","      <td>là dove vanno l ' anime a lavarsi</td>\n","      <td>quando la colpa pentuta è rimossa</td>\n","      <td>11</td>\n","      <td>11</td>\n","      <td>11</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>poi disse omai è tempo da scostarsi</td>\n","      <td>dal bosco fa che di retro a me vegne</td>\n","      <td>li margini fan via che non son arsi</td>\n","      <td>11</td>\n","      <td>11</td>\n","      <td>11</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>ora cen porta l ' un de ' duri margini</td>\n","      <td>e ' l fummo del ruscel di sopra aduggia</td>\n","      <td>sí che dal foco salva l ' acqua e li argini</td>\n","      <td>11</td>\n","      <td>11</td>\n","      <td>11</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>quale i fiamminghi tra guizzante e bruggia</td>\n","      <td>temendo il fiotto che ' nver lor s ' avventa</td>\n","      <td>fanno lo schermo perché ' l mar si fuggia</td>\n","      <td>11</td>\n","      <td>11</td>\n","      <td>11</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>3805</th>\n","      <td>dentro dal ciel della divina pace</td>\n","      <td>si gira un corpo nella cui virtute</td>\n","      <td>l ' esser di tutto suo contento giace</td>\n","      <td>11</td>\n","      <td>11</td>\n","      <td>11</td>\n","    </tr>\n","    <tr>\n","      <th>3806</th>\n","      <td>lo ciel seguente c ' ha tante vedute</td>\n","      <td>quell ' esser parte per diverse essenze</td>\n","      <td>da lui distinte e da lui contenute</td>\n","      <td>11</td>\n","      <td>11</td>\n","      <td>11</td>\n","    </tr>\n","    <tr>\n","      <th>3807</th>\n","      <td>li altri giron per varie differenze</td>\n","      <td>le distinzion che dentro da sé hanno</td>\n","      <td>dispongono a lor fini e lor semenze</td>\n","      <td>11</td>\n","      <td>11</td>\n","      <td>11</td>\n","    </tr>\n","    <tr>\n","      <th>3808</th>\n","      <td>questi organi del mondo cosí vanno</td>\n","      <td>come tu vedi omai di grado in grado</td>\n","      <td>che di su prendono e di sotto fanno</td>\n","      <td>11</td>\n","      <td>11</td>\n","      <td>11</td>\n","    </tr>\n","    <tr>\n","      <th>3809</th>\n","      <td>riguarda bene omai sí com ' io vado</td>\n","      <td>per questo loco al vero che disiri</td>\n","      <td>sí che poi sappi sol tener lo guado</td>\n","      <td>11</td>\n","      <td>11</td>\n","      <td>11</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3810 rows × 6 columns</p>\n","</div>"],"text/plain":["                                                0  ... 2_syllables\n","0            in tutte tue question certo mi piaci  ...          11\n","1             letè vedrai ma fuor di questa fossa  ...          11\n","2             poi disse omai è tempo da scostarsi  ...          11\n","3          ora cen porta l ' un de ' duri margini  ...          11\n","4      quale i fiamminghi tra guizzante e bruggia  ...          11\n","...                                           ...  ...         ...\n","3805            dentro dal ciel della divina pace  ...          11\n","3806         lo ciel seguente c ' ha tante vedute  ...          11\n","3807          li altri giron per varie differenze  ...          11\n","3808           questi organi del mondo cosí vanno  ...          11\n","3809          riguarda bene omai sí com ' io vado  ...          11\n","\n","[3810 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"mckVDu6ZU78J","colab_type":"text"},"source":["# Format Input for Training"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"UlnJ6ipGU78O","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":589},"executionInfo":{"status":"ok","timestamp":1594485606717,"user_tz":-120,"elapsed":8877,"user":{"displayName":"Giacomo","photoUrl":"https://lh6.googleusercontent.com/-_yNWKXs4tL8/AAAAAAAAAAI/AAAAAAAAv5Y/ThWC96YwxpY/s64/photo.jpg","userId":"11330407691056598140"}},"outputId":"68df43cc-197a-437c-82f4-d3cbe83391ca"},"source":["# Drop samples that are longer that the 99th percentile of length\n","\n","max_line_length = int(max([df['%s' % i].str.len().quantile(.99) for i in range(3)]))\n","df = df[\n","    (df['0'].str.len() <= max_line_length) & \n","    (df['1'].str.len() <= max_line_length) & \n","    (df['2'].str.len() <= max_line_length)\n","].copy()\n","df"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>0_syllables</th>\n","      <th>1_syllables</th>\n","      <th>2_syllables</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>in tutte tue question certo mi piaci</td>\n","      <td>rispuose ma ' l bollor dell ' acqua rossa</td>\n","      <td>dovea ben solver l ' una che tu faci</td>\n","      <td>11</td>\n","      <td>11</td>\n","      <td>11</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>letè vedrai ma fuor di questa fossa</td>\n","      <td>là dove vanno l ' anime a lavarsi</td>\n","      <td>quando la colpa pentuta è rimossa</td>\n","      <td>11</td>\n","      <td>11</td>\n","      <td>11</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>poi disse omai è tempo da scostarsi</td>\n","      <td>dal bosco fa che di retro a me vegne</td>\n","      <td>li margini fan via che non son arsi</td>\n","      <td>11</td>\n","      <td>11</td>\n","      <td>11</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>ora cen porta l ' un de ' duri margini</td>\n","      <td>e ' l fummo del ruscel di sopra aduggia</td>\n","      <td>sí che dal foco salva l ' acqua e li argini</td>\n","      <td>11</td>\n","      <td>11</td>\n","      <td>11</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>quale i fiamminghi tra guizzante e bruggia</td>\n","      <td>temendo il fiotto che ' nver lor s ' avventa</td>\n","      <td>fanno lo schermo perché ' l mar si fuggia</td>\n","      <td>11</td>\n","      <td>11</td>\n","      <td>11</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>3805</th>\n","      <td>dentro dal ciel della divina pace</td>\n","      <td>si gira un corpo nella cui virtute</td>\n","      <td>l ' esser di tutto suo contento giace</td>\n","      <td>11</td>\n","      <td>11</td>\n","      <td>11</td>\n","    </tr>\n","    <tr>\n","      <th>3806</th>\n","      <td>lo ciel seguente c ' ha tante vedute</td>\n","      <td>quell ' esser parte per diverse essenze</td>\n","      <td>da lui distinte e da lui contenute</td>\n","      <td>11</td>\n","      <td>11</td>\n","      <td>11</td>\n","    </tr>\n","    <tr>\n","      <th>3807</th>\n","      <td>li altri giron per varie differenze</td>\n","      <td>le distinzion che dentro da sé hanno</td>\n","      <td>dispongono a lor fini e lor semenze</td>\n","      <td>11</td>\n","      <td>11</td>\n","      <td>11</td>\n","    </tr>\n","    <tr>\n","      <th>3808</th>\n","      <td>questi organi del mondo cosí vanno</td>\n","      <td>come tu vedi omai di grado in grado</td>\n","      <td>che di su prendono e di sotto fanno</td>\n","      <td>11</td>\n","      <td>11</td>\n","      <td>11</td>\n","    </tr>\n","    <tr>\n","      <th>3809</th>\n","      <td>riguarda bene omai sí com ' io vado</td>\n","      <td>per questo loco al vero che disiri</td>\n","      <td>sí che poi sappi sol tener lo guado</td>\n","      <td>11</td>\n","      <td>11</td>\n","      <td>11</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3739 rows × 6 columns</p>\n","</div>"],"text/plain":["                                                0  ... 2_syllables\n","0            in tutte tue question certo mi piaci  ...          11\n","1             letè vedrai ma fuor di questa fossa  ...          11\n","2             poi disse omai è tempo da scostarsi  ...          11\n","3          ora cen porta l ' un de ' duri margini  ...          11\n","4      quale i fiamminghi tra guizzante e bruggia  ...          11\n","...                                           ...  ...         ...\n","3805            dentro dal ciel della divina pace  ...          11\n","3806         lo ciel seguente c ' ha tante vedute  ...          11\n","3807          li altri giron per varie differenze  ...          11\n","3808           questi organi del mondo cosí vanno  ...          11\n","3809          riguarda bene omai sí com ' io vado  ...          11\n","\n","[3739 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"5MGD8cfvU78S","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1594485606718,"user_tz":-120,"elapsed":8864,"user":{"displayName":"Giacomo","photoUrl":"https://lh6.googleusercontent.com/-_yNWKXs4tL8/AAAAAAAAAAI/AAAAAAAAv5Y/ThWC96YwxpY/s64/photo.jpg","userId":"11330407691056598140"}},"outputId":"7ce038c1-d46e-489c-afcf-36bb38c89827"},"source":["# Pad the lines to the max line length with new lines\n","for i in range(3):\n","    # For input, duplicate the first character\n","    # TODO - Why?\n","    df['%s_in' % i] = (df[str(i)].str[0] + df[str(i)]).str.pad(max_line_length+2, 'right', '\\n')\n","    \n","    # \n","    #df['%s_out' % i] = df[str(i)].str.pad(max_line_len, 'right', '\\n') + ('\\n' if i == 2 else df[str(i+1)].str[0])\n","    \n","    # TODO - trying to add the next line's first character before the line breaks\n","    if i == 2: # If it's the last line\n","        df['%s_out' % i] = df[str(i)].str.pad(max_line_length+2, 'right', '\\n')\n","    else: \n","        # If it's the first or second line, add the first character of the next line to the end of this line.\n","        # This helps with training so that the next RNN has a better chance of getting the first character right.\n","        df['%s_out' % i] = (df[str(i)] + '\\n' + df[str(i+1)].str[0]).str.pad(max_line_length+2, 'right', '\\n')\n","    \n","max_line_length += 2\n","\n","df"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>0_syllables</th>\n","      <th>1_syllables</th>\n","      <th>2_syllables</th>\n","      <th>0_in</th>\n","      <th>0_out</th>\n","      <th>1_in</th>\n","      <th>1_out</th>\n","      <th>2_in</th>\n","      <th>2_out</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>in tutte tue question certo mi piaci</td>\n","      <td>rispuose ma ' l bollor dell ' acqua rossa</td>\n","      <td>dovea ben solver l ' una che tu faci</td>\n","      <td>11</td>\n","      <td>11</td>\n","      <td>11</td>\n","      <td>in tutte tue question certo mi piaci\\n\\n\\n\\n...</td>\n","      <td>in tutte tue question certo mi piaci\\nr\\n\\n\\n...</td>\n","      <td>rrispuose ma ' l bollor dell ' acqua rossa\\n\\n...</td>\n","      <td>rispuose ma ' l bollor dell ' acqua rossa\\nd\\n...</td>\n","      <td>ddovea ben solver l ' una che tu faci\\n\\n\\n\\n\\...</td>\n","      <td>dovea ben solver l ' una che tu faci\\n\\n\\n\\n\\n...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>letè vedrai ma fuor di questa fossa</td>\n","      <td>là dove vanno l ' anime a lavarsi</td>\n","      <td>quando la colpa pentuta è rimossa</td>\n","      <td>11</td>\n","      <td>11</td>\n","      <td>11</td>\n","      <td>letè vedrai ma fuor di questa fossa\\n\\n\\n\\n\\...</td>\n","      <td>letè vedrai ma fuor di questa fossa\\nl\\n\\n\\n\\...</td>\n","      <td>llà dove vanno l ' anime a lavarsi\\n\\n\\n\\n\\n\\n...</td>\n","      <td>là dove vanno l ' anime a lavarsi\\nq\\n\\n\\n\\n\\n...</td>\n","      <td>qquando la colpa pentuta è rimossa\\n\\n\\n\\n\\n\\n...</td>\n","      <td>quando la colpa pentuta è rimossa\\n\\n\\n\\n\\n\\n\\...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>poi disse omai è tempo da scostarsi</td>\n","      <td>dal bosco fa che di retro a me vegne</td>\n","      <td>li margini fan via che non son arsi</td>\n","      <td>11</td>\n","      <td>11</td>\n","      <td>11</td>\n","      <td>poi disse omai è tempo da scostarsi\\n\\n\\n\\n\\...</td>\n","      <td>poi disse omai è tempo da scostarsi\\nd\\n\\n\\n\\...</td>\n","      <td>ddal bosco fa che di retro a me vegne\\n\\n\\n\\n\\...</td>\n","      <td>dal bosco fa che di retro a me vegne\\nl\\n\\n\\n\\...</td>\n","      <td>lli margini fan via che non son arsi\\n\\n\\n\\n\\n...</td>\n","      <td>li margini fan via che non son arsi\\n\\n\\n\\n\\n\\...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>ora cen porta l ' un de ' duri margini</td>\n","      <td>e ' l fummo del ruscel di sopra aduggia</td>\n","      <td>sí che dal foco salva l ' acqua e li argini</td>\n","      <td>11</td>\n","      <td>11</td>\n","      <td>11</td>\n","      <td>ora cen porta l ' un de ' duri margini\\n\\n\\n...</td>\n","      <td>ora cen porta l ' un de ' duri margini\\ne\\n\\n...</td>\n","      <td>ee ' l fummo del ruscel di sopra aduggia\\n\\n\\n...</td>\n","      <td>e ' l fummo del ruscel di sopra aduggia\\ns\\n\\n...</td>\n","      <td>ssí che dal foco salva l ' acqua e li argini\\n...</td>\n","      <td>sí che dal foco salva l ' acqua e li argini\\n\\...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>quale i fiamminghi tra guizzante e bruggia</td>\n","      <td>temendo il fiotto che ' nver lor s ' avventa</td>\n","      <td>fanno lo schermo perché ' l mar si fuggia</td>\n","      <td>11</td>\n","      <td>11</td>\n","      <td>11</td>\n","      <td>quale i fiamminghi tra guizzante e bruggia\\n...</td>\n","      <td>quale i fiamminghi tra guizzante e bruggia\\nt...</td>\n","      <td>ttemendo il fiotto che ' nver lor s ' avventa\\...</td>\n","      <td>temendo il fiotto che ' nver lor s ' avventa\\n...</td>\n","      <td>ffanno lo schermo perché ' l mar si fuggia\\n\\n...</td>\n","      <td>fanno lo schermo perché ' l mar si fuggia\\n\\n\\...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>3805</th>\n","      <td>dentro dal ciel della divina pace</td>\n","      <td>si gira un corpo nella cui virtute</td>\n","      <td>l ' esser di tutto suo contento giace</td>\n","      <td>11</td>\n","      <td>11</td>\n","      <td>11</td>\n","      <td>dentro dal ciel della divina pace\\n\\n\\n\\n\\n\\...</td>\n","      <td>dentro dal ciel della divina pace\\ns\\n\\n\\n\\n\\...</td>\n","      <td>ssi gira un corpo nella cui virtute\\n\\n\\n\\n\\n\\...</td>\n","      <td>si gira un corpo nella cui virtute\\nl\\n\\n\\n\\n\\...</td>\n","      <td>ll ' esser di tutto suo contento giace\\n\\n\\n\\n...</td>\n","      <td>l ' esser di tutto suo contento giace\\n\\n\\n\\n\\...</td>\n","    </tr>\n","    <tr>\n","      <th>3806</th>\n","      <td>lo ciel seguente c ' ha tante vedute</td>\n","      <td>quell ' esser parte per diverse essenze</td>\n","      <td>da lui distinte e da lui contenute</td>\n","      <td>11</td>\n","      <td>11</td>\n","      <td>11</td>\n","      <td>lo ciel seguente c ' ha tante vedute\\n\\n\\n\\n...</td>\n","      <td>lo ciel seguente c ' ha tante vedute\\nq\\n\\n\\n...</td>\n","      <td>qquell ' esser parte per diverse essenze\\n\\n\\n...</td>\n","      <td>quell ' esser parte per diverse essenze\\nd\\n\\n...</td>\n","      <td>dda lui distinte e da lui contenute\\n\\n\\n\\n\\n\\...</td>\n","      <td>da lui distinte e da lui contenute\\n\\n\\n\\n\\n\\n...</td>\n","    </tr>\n","    <tr>\n","      <th>3807</th>\n","      <td>li altri giron per varie differenze</td>\n","      <td>le distinzion che dentro da sé hanno</td>\n","      <td>dispongono a lor fini e lor semenze</td>\n","      <td>11</td>\n","      <td>11</td>\n","      <td>11</td>\n","      <td>li altri giron per varie differenze\\n\\n\\n\\n\\...</td>\n","      <td>li altri giron per varie differenze\\nl\\n\\n\\n\\...</td>\n","      <td>lle distinzion che dentro da sé hanno\\n\\n\\n\\n\\...</td>\n","      <td>le distinzion che dentro da sé hanno\\nd\\n\\n\\n\\...</td>\n","      <td>ddispongono a lor fini e lor semenze\\n\\n\\n\\n\\n...</td>\n","      <td>dispongono a lor fini e lor semenze\\n\\n\\n\\n\\n\\...</td>\n","    </tr>\n","    <tr>\n","      <th>3808</th>\n","      <td>questi organi del mondo cosí vanno</td>\n","      <td>come tu vedi omai di grado in grado</td>\n","      <td>che di su prendono e di sotto fanno</td>\n","      <td>11</td>\n","      <td>11</td>\n","      <td>11</td>\n","      <td>questi organi del mondo cosí vanno\\n\\n\\n\\n\\n...</td>\n","      <td>questi organi del mondo cosí vanno\\nc\\n\\n\\n\\n...</td>\n","      <td>ccome tu vedi omai di grado in grado\\n\\n\\n\\n\\n...</td>\n","      <td>come tu vedi omai di grado in grado\\nc\\n\\n\\n\\n...</td>\n","      <td>cche di su prendono e di sotto fanno\\n\\n\\n\\n\\n...</td>\n","      <td>che di su prendono e di sotto fanno\\n\\n\\n\\n\\n\\...</td>\n","    </tr>\n","    <tr>\n","      <th>3809</th>\n","      <td>riguarda bene omai sí com ' io vado</td>\n","      <td>per questo loco al vero che disiri</td>\n","      <td>sí che poi sappi sol tener lo guado</td>\n","      <td>11</td>\n","      <td>11</td>\n","      <td>11</td>\n","      <td>riguarda bene omai sí com ' io vado\\n\\n\\n\\n\\...</td>\n","      <td>riguarda bene omai sí com ' io vado\\np\\n\\n\\n\\...</td>\n","      <td>pper questo loco al vero che disiri\\n\\n\\n\\n\\n\\...</td>\n","      <td>per questo loco al vero che disiri\\ns\\n\\n\\n\\n\\...</td>\n","      <td>ssí che poi sappi sol tener lo guado\\n\\n\\n\\n\\n...</td>\n","      <td>sí che poi sappi sol tener lo guado\\n\\n\\n\\n\\n\\...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3739 rows × 12 columns</p>\n","</div>"],"text/plain":["                                                0  ...                                              2_out\n","0            in tutte tue question certo mi piaci  ...  dovea ben solver l ' una che tu faci\\n\\n\\n\\n\\n...\n","1             letè vedrai ma fuor di questa fossa  ...  quando la colpa pentuta è rimossa\\n\\n\\n\\n\\n\\n\\...\n","2             poi disse omai è tempo da scostarsi  ...  li margini fan via che non son arsi\\n\\n\\n\\n\\n\\...\n","3          ora cen porta l ' un de ' duri margini  ...  sí che dal foco salva l ' acqua e li argini\\n\\...\n","4      quale i fiamminghi tra guizzante e bruggia  ...  fanno lo schermo perché ' l mar si fuggia\\n\\n\\...\n","...                                           ...  ...                                                ...\n","3805            dentro dal ciel della divina pace  ...  l ' esser di tutto suo contento giace\\n\\n\\n\\n\\...\n","3806         lo ciel seguente c ' ha tante vedute  ...  da lui distinte e da lui contenute\\n\\n\\n\\n\\n\\n...\n","3807          li altri giron per varie differenze  ...  dispongono a lor fini e lor semenze\\n\\n\\n\\n\\n\\...\n","3808           questi organi del mondo cosí vanno  ...  che di su prendono e di sotto fanno\\n\\n\\n\\n\\n\\...\n","3809          riguarda bene omai sí com ' io vado  ...  sí che poi sappi sol tener lo guado\\n\\n\\n\\n\\n\\...\n","\n","[3739 rows x 12 columns]"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"saTxe6d8U78X","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594489534329,"user_tz":-120,"elapsed":1322,"user":{"displayName":"Giacomo","photoUrl":"https://lh6.googleusercontent.com/-_yNWKXs4tL8/AAAAAAAAAAI/AAAAAAAAv5Y/ThWC96YwxpY/s64/photo.jpg","userId":"11330407691056598140"}}},"source":["inputs = df[['0_in', '1_in', '2_in']].values\n","\n","tokenizer = Tokenizer(filters='', char_level=True)\n","tokenizer.fit_on_texts(inputs.flatten())\n","n_tokens = len(tokenizer.word_counts) + 1\n","\n","# X is the input for each line in sequences of one-hot-encoded values\n","X = np_utils.to_categorical([\n","    tokenizer.texts_to_sequences(inputs[:,i]) for i in range(3)\n","], num_classes=n_tokens)\n","\n","outputs = df[['0_out', '1_out', '2_out']].values\n","\n","# Y is the output for each line in sequences of one-hot-encoded values\n","Y = np_utils.to_categorical([\n","    tokenizer.texts_to_sequences(outputs[:,i]) for i in range(3)\n","], num_classes=n_tokens)\n","\n","# X_syllables is the count of syllables for each line\n","X_syllables = df[['0_syllables', '1_syllables', '2_syllables']].values"],"execution_count":46,"outputs":[]},{"cell_type":"code","metadata":{"id":"5WJscJRsU78b","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1594488460846,"user_tz":-120,"elapsed":1777,"user":{"displayName":"Giacomo","photoUrl":"https://lh6.googleusercontent.com/-_yNWKXs4tL8/AAAAAAAAAAI/AAAAAAAAv5Y/ThWC96YwxpY/s64/photo.jpg","userId":"11330407691056598140"}},"outputId":"ed10f319-21dc-4dc0-a5a8-4b81166d971f"},"source":["joblib.dump([latent_dim, n_tokens, max_line_length, tokenizer], str(output_dir / 'metadata.pkl'))"],"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['output_all_data_test_2/metadata.pkl']"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"markdown","metadata":{"id":"3mSx66cMU78e","colab_type":"text"},"source":["# Training Model"]},{"cell_type":"code","metadata":{"id":"5fnlhgPMVeAs","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594485607123,"user_tz":-120,"elapsed":9226,"user":{"displayName":"Giacomo","photoUrl":"https://lh6.googleusercontent.com/-_yNWKXs4tL8/AAAAAAAAAAI/AAAAAAAAv5Y/ThWC96YwxpY/s64/photo.jpg","userId":"11330407691056598140"}}},"source":["from keras.layers import Add, Dense, Input, LSTM\n","from keras.models import Model\n","from keras.utils import np_utils\n","import numpy as np"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"OnkCcwCBVbvD","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594485607124,"user_tz":-120,"elapsed":9215,"user":{"displayName":"Giacomo","photoUrl":"https://lh6.googleusercontent.com/-_yNWKXs4tL8/AAAAAAAAAAI/AAAAAAAAv5Y/ThWC96YwxpY/s64/photo.jpg","userId":"11330407691056598140"}}},"source":["class TrainingLine:\n","    def __init__(self, name, previous_line, lstm, n_tokens):\n","        self.char_input = Input(shape=(None, n_tokens), name='char_input_%s' % name)\n","\n","        self.syllable_input = Input(shape=(1,), name='syllable_input_%s' % name)\n","        self.syllable_dense = Dense(lstm.units, activation='relu', name='syllable_dense_%s' % name)\n","        self.syllable_dense_output = self.syllable_dense(self.syllable_input)\n","\n","        #self.lstm = LSTM(latent_dim, return_state=True, return_sequences=True, name='lstm_%s' % name)\n","\n","        if previous_line:\n","            initial_state = [\n","                Add(name='add_h_%s' % name)([\n","                    previous_line.lstm_h,\n","                    self.syllable_dense_output\n","                ]),\n","                Add(name='add_c_%s' % name)([\n","                    previous_line.lstm_c,\n","                    self.syllable_dense_output\n","                ])\n","            ]\n","        else:\n","            initial_state = [self.syllable_dense_output, self.syllable_dense_output]\n","\n","        self.lstm_out, self.lstm_h, self.lstm_c = lstm(self.char_input, initial_state=initial_state)\n","\n","        self.output_dense = Dense(n_tokens, activation='softmax', name='output_%s' % name)\n","        self.output = self.output_dense(self.lstm_out)\n","\n","def custom_loss(line1, line2, line3):\n","    loss = 0\n","    if line1[-1] == line3[-1]:\n","        loss += .5\n","    else:\n","        loss -= .5\n","    return loss\n","\n","\n","def create_training_model(latent_dim, n_tokens):\n","    lstm = LSTM(latent_dim, return_state=True, return_sequences=True, name='lstm')\n","    lines = []\n","    inputs = []\n","    outputs = []\n","\n","    for i in range(3):\n","        previous_line = lines[-1] if lines else None\n","        lines.append(TrainingLine('line_%s' % i, previous_line, lstm, n_tokens))\n","        inputs += [lines[-1].char_input, lines[-1].syllable_input]\n","        outputs.append(lines[-1].output)\n","\n","    training_model = Model(inputs, outputs)\n","    training_model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n","\n","    return training_model, lstm, lines, inputs, outputs\n"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"nIsXFtc5U78f","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1594487922242,"user_tz":-120,"elapsed":2324322,"user":{"displayName":"Giacomo","photoUrl":"https://lh6.googleusercontent.com/-_yNWKXs4tL8/AAAAAAAAAAI/AAAAAAAAv5Y/ThWC96YwxpY/s64/photo.jpg","userId":"11330407691056598140"}},"outputId":"c5671070-002d-4fbe-f32a-d5e901780a34"},"source":["training_model, lstm, lines, inputs, outputs = create_training_model(latent_dim, n_tokens)\n","\n","filepath = str(output_dir / (\"%s-{epoch:02d}-{loss:.2f}-{val_loss:.2f}.hdf5\" % latent_dim))\n","checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n","\n","csv_logger = CSVLogger(str(output_dir / 'training_log.csv'), append=True, separator=',')\n","\n","callbacks_list = [checkpoint, csv_logger]\n","\n","training_model.fit([\n","    X[0], X_syllables[:,0], \n","    X[1], X_syllables[:,1], \n","    X[2], X_syllables[:,2]\n","], [Y[0], Y[1], Y[2]], batch_size=64, epochs=epochs, validation_split=.05, callbacks=callbacks_list)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","Train on 3552 samples, validate on 187 samples\n","Epoch 1/40\n","3552/3552 [==============================] - 59s 17ms/step - loss: 8.7228 - output_line_0_loss: 2.9427 - output_line_1_loss: 2.9311 - output_line_2_loss: 2.8393 - val_loss: 8.0130 - val_output_line_0_loss: 2.7742 - val_output_line_1_loss: 2.6469 - val_output_line_2_loss: 2.5916\n","\n","Epoch 00001: loss improved from inf to 8.72276, saving model to output_all_data_test_2/2048-01-8.72-8.01.hdf5\n","Epoch 2/40\n","3552/3552 [==============================] - 56s 16ms/step - loss: 7.5021 - output_line_0_loss: 2.4302 - output_line_1_loss: 2.5625 - output_line_2_loss: 2.5051 - val_loss: 6.8102 - val_output_line_0_loss: 2.2969 - val_output_line_1_loss: 2.2922 - val_output_line_2_loss: 2.2206\n","\n","Epoch 00002: loss improved from 8.72276 to 7.50212, saving model to output_all_data_test_2/2048-02-7.50-6.81.hdf5\n","Epoch 3/40\n","3552/3552 [==============================] - 56s 16ms/step - loss: 6.7233 - output_line_0_loss: 2.2914 - output_line_1_loss: 2.2795 - output_line_2_loss: 2.2173 - val_loss: 10.0982 - val_output_line_0_loss: 4.0461 - val_output_line_1_loss: 2.9446 - val_output_line_2_loss: 3.1068\n","\n","Epoch 00003: loss improved from 7.50212 to 6.72328, saving model to output_all_data_test_2/2048-03-6.72-10.10.hdf5\n","Epoch 4/40\n","3552/3552 [==============================] - 56s 16ms/step - loss: 6.1037 - output_line_0_loss: 2.0783 - output_line_1_loss: 2.0366 - output_line_2_loss: 1.9842 - val_loss: 5.5312 - val_output_line_0_loss: 1.8587 - val_output_line_1_loss: 1.8687 - val_output_line_2_loss: 1.8027\n","\n","Epoch 00004: loss improved from 6.72328 to 6.10372, saving model to output_all_data_test_2/2048-04-6.10-5.53.hdf5\n","Epoch 5/40\n","3552/3552 [==============================] - 56s 16ms/step - loss: 5.4044 - output_line_0_loss: 1.8021 - output_line_1_loss: 1.8300 - output_line_2_loss: 1.7696 - val_loss: 5.0651 - val_output_line_0_loss: 1.6966 - val_output_line_1_loss: 1.7130 - val_output_line_2_loss: 1.6544\n","\n","Epoch 00005: loss improved from 6.10372 to 5.40443, saving model to output_all_data_test_2/2048-05-5.40-5.07.hdf5\n","Epoch 6/40\n","3552/3552 [==============================] - 56s 16ms/step - loss: 4.9945 - output_line_0_loss: 1.6704 - output_line_1_loss: 1.6880 - output_line_2_loss: 1.6339 - val_loss: 4.7608 - val_output_line_0_loss: 1.5950 - val_output_line_1_loss: 1.6149 - val_output_line_2_loss: 1.5499\n","\n","Epoch 00006: loss improved from 5.40443 to 4.99447, saving model to output_all_data_test_2/2048-06-4.99-4.76.hdf5\n","Epoch 7/40\n","3552/3552 [==============================] - 56s 16ms/step - loss: 4.6605 - output_line_0_loss: 1.5687 - output_line_1_loss: 1.5727 - output_line_2_loss: 1.5171 - val_loss: 4.5838 - val_output_line_0_loss: 1.5617 - val_output_line_1_loss: 1.5353 - val_output_line_2_loss: 1.4851\n","\n","Epoch 00007: loss improved from 4.99447 to 4.66049, saving model to output_all_data_test_2/2048-07-4.66-4.58.hdf5\n","Epoch 8/40\n","3552/3552 [==============================] - 56s 16ms/step - loss: 4.3980 - output_line_0_loss: 1.4859 - output_line_1_loss: 1.4802 - output_line_2_loss: 1.4309 - val_loss: 4.3840 - val_output_line_0_loss: 1.4786 - val_output_line_1_loss: 1.4818 - val_output_line_2_loss: 1.4221\n","\n","Epoch 00008: loss improved from 4.66049 to 4.39802, saving model to output_all_data_test_2/2048-08-4.40-4.38.hdf5\n","Epoch 9/40\n","3552/3552 [==============================] - 56s 16ms/step - loss: 4.1730 - output_line_0_loss: 1.4157 - output_line_1_loss: 1.4026 - output_line_2_loss: 1.3534 - val_loss: 4.1930 - val_output_line_0_loss: 1.4097 - val_output_line_1_loss: 1.4176 - val_output_line_2_loss: 1.3637\n","\n","Epoch 00009: loss improved from 4.39802 to 4.17304, saving model to output_all_data_test_2/2048-09-4.17-4.19.hdf5\n","Epoch 10/40\n","3552/3552 [==============================] - 56s 16ms/step - loss: 3.9718 - output_line_0_loss: 1.3520 - output_line_1_loss: 1.3342 - output_line_2_loss: 1.2845 - val_loss: 4.0492 - val_output_line_0_loss: 1.3738 - val_output_line_1_loss: 1.3584 - val_output_line_2_loss: 1.3153\n","\n","Epoch 00010: loss improved from 4.17304 to 3.97183, saving model to output_all_data_test_2/2048-10-3.97-4.05.hdf5\n","Epoch 11/40\n","3552/3552 [==============================] - 56s 16ms/step - loss: 3.7918 - output_line_0_loss: 1.2923 - output_line_1_loss: 1.2717 - output_line_2_loss: 1.2276 - val_loss: 3.9299 - val_output_line_0_loss: 1.3231 - val_output_line_1_loss: 1.3279 - val_output_line_2_loss: 1.2766\n","\n","Epoch 00011: loss improved from 3.97183 to 3.79177, saving model to output_all_data_test_2/2048-11-3.79-3.93.hdf5\n","Epoch 12/40\n","3552/3552 [==============================] - 56s 16ms/step - loss: 3.6191 - output_line_0_loss: 1.2344 - output_line_1_loss: 1.2130 - output_line_2_loss: 1.1718 - val_loss: 3.8336 - val_output_line_0_loss: 1.3039 - val_output_line_1_loss: 1.2808 - val_output_line_2_loss: 1.2466\n","\n","Epoch 00012: loss improved from 3.79177 to 3.61907, saving model to output_all_data_test_2/2048-12-3.62-3.83.hdf5\n","Epoch 13/40\n","3552/3552 [==============================] - 56s 16ms/step - loss: 3.4466 - output_line_0_loss: 1.1786 - output_line_1_loss: 1.1543 - output_line_2_loss: 1.1145 - val_loss: 3.8000 - val_output_line_0_loss: 1.2855 - val_output_line_1_loss: 1.2573 - val_output_line_2_loss: 1.2550\n","\n","Epoch 00013: loss improved from 3.61907 to 3.44659, saving model to output_all_data_test_2/2048-13-3.45-3.80.hdf5\n","Epoch 14/40\n","3552/3552 [==============================] - 56s 16ms/step - loss: 3.2665 - output_line_0_loss: 1.1149 - output_line_1_loss: 1.0927 - output_line_2_loss: 1.0580 - val_loss: 3.7098 - val_output_line_0_loss: 1.2501 - val_output_line_1_loss: 1.2441 - val_output_line_2_loss: 1.2133\n","\n","Epoch 00014: loss improved from 3.44659 to 3.26651, saving model to output_all_data_test_2/2048-14-3.27-3.71.hdf5\n","Epoch 15/40\n","3552/3552 [==============================] - 56s 16ms/step - loss: 3.0629 - output_line_0_loss: 1.0454 - output_line_1_loss: 1.0255 - output_line_2_loss: 0.9924 - val_loss: 3.6912 - val_output_line_0_loss: 1.2520 - val_output_line_1_loss: 1.2335 - val_output_line_2_loss: 1.2033\n","\n","Epoch 00015: loss improved from 3.26651 to 3.06285, saving model to output_all_data_test_2/2048-15-3.06-3.69.hdf5\n","Epoch 16/40\n","3552/3552 [==============================] - 56s 16ms/step - loss: 2.8327 - output_line_0_loss: 0.9685 - output_line_1_loss: 0.9477 - output_line_2_loss: 0.9172 - val_loss: 3.7768 - val_output_line_0_loss: 1.2786 - val_output_line_1_loss: 1.2575 - val_output_line_2_loss: 1.2383\n","\n","Epoch 00016: loss improved from 3.06285 to 2.83271, saving model to output_all_data_test_2/2048-16-2.83-3.78.hdf5\n","Epoch 17/40\n","3552/3552 [==============================] - 56s 16ms/step - loss: 2.5686 - output_line_0_loss: 0.8776 - output_line_1_loss: 0.8583 - output_line_2_loss: 0.8332 - val_loss: 3.8423 - val_output_line_0_loss: 1.2991 - val_output_line_1_loss: 1.2803 - val_output_line_2_loss: 1.2599\n","\n","Epoch 00017: loss improved from 2.83271 to 2.56862, saving model to output_all_data_test_2/2048-17-2.57-3.84.hdf5\n","Epoch 18/40\n","3552/3552 [==============================] - 56s 16ms/step - loss: 2.2819 - output_line_0_loss: 0.7813 - output_line_1_loss: 0.7610 - output_line_2_loss: 0.7392 - val_loss: 3.9666 - val_output_line_0_loss: 1.3457 - val_output_line_1_loss: 1.3223 - val_output_line_2_loss: 1.2957\n","\n","Epoch 00018: loss improved from 2.56862 to 2.28191, saving model to output_all_data_test_2/2048-18-2.28-3.97.hdf5\n","Epoch 19/40\n","3552/3552 [==============================] - 56s 16ms/step - loss: 1.9847 - output_line_0_loss: 0.6831 - output_line_1_loss: 0.6593 - output_line_2_loss: 0.6429 - val_loss: 4.1195 - val_output_line_0_loss: 1.3876 - val_output_line_1_loss: 1.3723 - val_output_line_2_loss: 1.3562\n","\n","Epoch 00019: loss improved from 2.28191 to 1.98470, saving model to output_all_data_test_2/2048-19-1.98-4.12.hdf5\n","Epoch 20/40\n","3552/3552 [==============================] - 56s 16ms/step - loss: 1.6901 - output_line_0_loss: 0.5894 - output_line_1_loss: 0.5546 - output_line_2_loss: 0.5465 - val_loss: 4.2624 - val_output_line_0_loss: 1.4240 - val_output_line_1_loss: 1.4341 - val_output_line_2_loss: 1.4008\n","\n","Epoch 00020: loss improved from 1.98470 to 1.69013, saving model to output_all_data_test_2/2048-20-1.69-4.26.hdf5\n","Epoch 21/40\n","3552/3552 [==============================] - 56s 16ms/step - loss: 1.4153 - output_line_0_loss: 0.5024 - output_line_1_loss: 0.4608 - output_line_2_loss: 0.4528 - val_loss: 4.5318 - val_output_line_0_loss: 1.5076 - val_output_line_1_loss: 1.5178 - val_output_line_2_loss: 1.5034\n","\n","Epoch 00021: loss improved from 1.69013 to 1.41526, saving model to output_all_data_test_2/2048-21-1.42-4.53.hdf5\n","Epoch 22/40\n","3552/3552 [==============================] - 56s 16ms/step - loss: 1.1787 - output_line_0_loss: 0.4318 - output_line_1_loss: 0.3748 - output_line_2_loss: 0.3729 - val_loss: 4.7123 - val_output_line_0_loss: 1.5718 - val_output_line_1_loss: 1.5843 - val_output_line_2_loss: 1.5535\n","\n","Epoch 00022: loss improved from 1.41526 to 1.17874, saving model to output_all_data_test_2/2048-22-1.18-4.71.hdf5\n","Epoch 23/40\n","3552/3552 [==============================] - 56s 16ms/step - loss: 0.9853 - output_line_0_loss: 0.3776 - output_line_1_loss: 0.3047 - output_line_2_loss: 0.3037 - val_loss: 4.9431 - val_output_line_0_loss: 1.6375 - val_output_line_1_loss: 1.6594 - val_output_line_2_loss: 1.6428\n","\n","Epoch 00023: loss improved from 1.17874 to 0.98528, saving model to output_all_data_test_2/2048-23-0.99-4.94.hdf5\n","Epoch 24/40\n","3552/3552 [==============================] - 56s 16ms/step - loss: 0.8301 - output_line_0_loss: 0.3381 - output_line_1_loss: 0.2460 - output_line_2_loss: 0.2464 - val_loss: 5.2111 - val_output_line_0_loss: 1.7038 - val_output_line_1_loss: 1.7660 - val_output_line_2_loss: 1.7382\n","\n","Epoch 00024: loss improved from 0.98528 to 0.83010, saving model to output_all_data_test_2/2048-24-0.83-5.21.hdf5\n","Epoch 25/40\n","3552/3552 [==============================] - 56s 16ms/step - loss: 0.7187 - output_line_0_loss: 0.3139 - output_line_1_loss: 0.2026 - output_line_2_loss: 0.2022 - val_loss: 5.3710 - val_output_line_0_loss: 1.7429 - val_output_line_1_loss: 1.8167 - val_output_line_2_loss: 1.8083\n","\n","Epoch 00025: loss improved from 0.83010 to 0.71871, saving model to output_all_data_test_2/2048-25-0.72-5.37.hdf5\n","Epoch 26/40\n","3552/3552 [==============================] - 56s 16ms/step - loss: 0.6273 - output_line_0_loss: 0.2964 - output_line_1_loss: 0.1652 - output_line_2_loss: 0.1662 - val_loss: 5.6092 - val_output_line_0_loss: 1.7979 - val_output_line_1_loss: 1.9097 - val_output_line_2_loss: 1.8982\n","\n","Epoch 00026: loss improved from 0.71871 to 0.62726, saving model to output_all_data_test_2/2048-26-0.63-5.61.hdf5\n","Epoch 27/40\n","3552/3552 [==============================] - 56s 16ms/step - loss: 0.5536 - output_line_0_loss: 0.2864 - output_line_1_loss: 0.1333 - output_line_2_loss: 0.1342 - val_loss: 5.7776 - val_output_line_0_loss: 1.8223 - val_output_line_1_loss: 1.9768 - val_output_line_2_loss: 1.9755\n","\n","Epoch 00027: loss improved from 0.62726 to 0.55359, saving model to output_all_data_test_2/2048-27-0.55-5.78.hdf5\n","Epoch 28/40\n","3552/3552 [==============================] - 56s 16ms/step - loss: 0.4937 - output_line_0_loss: 0.2762 - output_line_1_loss: 0.1091 - output_line_2_loss: 0.1088 - val_loss: 5.9317 - val_output_line_0_loss: 1.8545 - val_output_line_1_loss: 2.0405 - val_output_line_2_loss: 2.0334\n","\n","Epoch 00028: loss improved from 0.55359 to 0.49365, saving model to output_all_data_test_2/2048-28-0.49-5.93.hdf5\n","Epoch 29/40\n","3552/3552 [==============================] - 56s 16ms/step - loss: 0.4430 - output_line_0_loss: 0.2695 - output_line_1_loss: 0.0862 - output_line_2_loss: 0.0877 - val_loss: 6.0795 - val_output_line_0_loss: 1.8752 - val_output_line_1_loss: 2.1219 - val_output_line_2_loss: 2.0790\n","\n","Epoch 00029: loss improved from 0.49365 to 0.44298, saving model to output_all_data_test_2/2048-29-0.44-6.08.hdf5\n","Epoch 30/40\n","3552/3552 [==============================] - 56s 16ms/step - loss: 0.4077 - output_line_0_loss: 0.2657 - output_line_1_loss: 0.0704 - output_line_2_loss: 0.0718 - val_loss: 6.2167 - val_output_line_0_loss: 1.8988 - val_output_line_1_loss: 2.1720 - val_output_line_2_loss: 2.1429\n","\n","Epoch 00030: loss improved from 0.44298 to 0.40775, saving model to output_all_data_test_2/2048-30-0.41-6.22.hdf5\n","Epoch 31/40\n","3552/3552 [==============================] - 56s 16ms/step - loss: 0.3767 - output_line_0_loss: 0.2614 - output_line_1_loss: 0.0572 - output_line_2_loss: 0.0583 - val_loss: 6.4346 - val_output_line_0_loss: 1.9294 - val_output_line_1_loss: 2.2549 - val_output_line_2_loss: 2.2462\n","\n","Epoch 00031: loss improved from 0.40775 to 0.37666, saving model to output_all_data_test_2/2048-31-0.38-6.43.hdf5\n","Epoch 32/40\n","3552/3552 [==============================] - 56s 16ms/step - loss: 0.3532 - output_line_0_loss: 0.2578 - output_line_1_loss: 0.0478 - output_line_2_loss: 0.0479 - val_loss: 6.5246 - val_output_line_0_loss: 1.9557 - val_output_line_1_loss: 2.3092 - val_output_line_2_loss: 2.2567\n","\n","Epoch 00032: loss improved from 0.37666 to 0.35319, saving model to output_all_data_test_2/2048-32-0.35-6.52.hdf5\n","Epoch 33/40\n","3552/3552 [==============================] - 56s 16ms/step - loss: 0.3371 - output_line_0_loss: 0.2543 - output_line_1_loss: 0.0416 - output_line_2_loss: 0.0416 - val_loss: 6.6298 - val_output_line_0_loss: 1.9564 - val_output_line_1_loss: 2.3321 - val_output_line_2_loss: 2.3378\n","\n","Epoch 00033: loss improved from 0.35319 to 0.33713, saving model to output_all_data_test_2/2048-33-0.34-6.63.hdf5\n","Epoch 34/40\n","3552/3552 [==============================] - 56s 16ms/step - loss: 0.3249 - output_line_0_loss: 0.2510 - output_line_1_loss: 0.0370 - output_line_2_loss: 0.0372 - val_loss: 6.6938 - val_output_line_0_loss: 1.9732 - val_output_line_1_loss: 2.3654 - val_output_line_2_loss: 2.3514\n","\n","Epoch 00034: loss improved from 0.33713 to 0.32492, saving model to output_all_data_test_2/2048-34-0.32-6.69.hdf5\n","Epoch 35/40\n","3552/3552 [==============================] - 56s 16ms/step - loss: 0.3143 - output_line_0_loss: 0.2477 - output_line_1_loss: 0.0339 - output_line_2_loss: 0.0330 - val_loss: 6.8367 - val_output_line_0_loss: 1.9693 - val_output_line_1_loss: 2.4393 - val_output_line_2_loss: 2.4246\n","\n","Epoch 00035: loss improved from 0.32492 to 0.31430, saving model to output_all_data_test_2/2048-35-0.31-6.84.hdf5\n","Epoch 36/40\n","3552/3552 [==============================] - 57s 16ms/step - loss: 0.3082 - output_line_0_loss: 0.2462 - output_line_1_loss: 0.0309 - output_line_2_loss: 0.0312 - val_loss: 6.8618 - val_output_line_0_loss: 1.9737 - val_output_line_1_loss: 2.4719 - val_output_line_2_loss: 2.4134\n","\n","Epoch 00036: loss improved from 0.31430 to 0.30824, saving model to output_all_data_test_2/2048-36-0.31-6.86.hdf5\n","Epoch 37/40\n","3552/3552 [==============================] - 57s 16ms/step - loss: 0.3013 - output_line_0_loss: 0.2438 - output_line_1_loss: 0.0284 - output_line_2_loss: 0.0292 - val_loss: 6.9552 - val_output_line_0_loss: 1.9895 - val_output_line_1_loss: 2.4786 - val_output_line_2_loss: 2.4824\n","\n","Epoch 00037: loss improved from 0.30824 to 0.30131, saving model to output_all_data_test_2/2048-37-0.30-6.96.hdf5\n","Epoch 38/40\n","3552/3552 [==============================] - 57s 16ms/step - loss: 0.2948 - output_line_0_loss: 0.2415 - output_line_1_loss: 0.0265 - output_line_2_loss: 0.0268 - val_loss: 7.0561 - val_output_line_0_loss: 2.0150 - val_output_line_1_loss: 2.5251 - val_output_line_2_loss: 2.5133\n","\n","Epoch 00038: loss improved from 0.30131 to 0.29479, saving model to output_all_data_test_2/2048-38-0.29-7.06.hdf5\n","Epoch 39/40\n","3552/3552 [==============================] - 57s 16ms/step - loss: 0.2905 - output_line_0_loss: 0.2394 - output_line_1_loss: 0.0256 - output_line_2_loss: 0.0257 - val_loss: 7.1062 - val_output_line_0_loss: 2.0142 - val_output_line_1_loss: 2.5599 - val_output_line_2_loss: 2.5295\n","\n","Epoch 00039: loss improved from 0.29479 to 0.29049, saving model to output_all_data_test_2/2048-39-0.29-7.11.hdf5\n","Epoch 40/40\n","3552/3552 [==============================] - 57s 16ms/step - loss: 0.2858 - output_line_0_loss: 0.2370 - output_line_1_loss: 0.0245 - output_line_2_loss: 0.0243 - val_loss: 7.1497 - val_output_line_0_loss: 1.9948 - val_output_line_1_loss: 2.5879 - val_output_line_2_loss: 2.5636\n","\n","Epoch 00040: loss improved from 0.29049 to 0.28575, saving model to output_all_data_test_2/2048-40-0.29-7.15.hdf5\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.callbacks.History at 0x7ff5f5c5cbe0>"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"YRhPskjPU78i","colab_type":"text"},"source":["# Test Model"]},{"cell_type":"code","metadata":{"id":"QGBBkPQaPcvb","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594489872413,"user_tz":-120,"elapsed":1091,"user":{"displayName":"Giacomo","photoUrl":"https://lh6.googleusercontent.com/-_yNWKXs4tL8/AAAAAAAAAAI/AAAAAAAAv5Y/ThWC96YwxpY/s64/photo.jpg","userId":"11330407691056598140"}}},"source":["class GeneratorLine:\n","    def __init__(self, name, training_line, lstm, n_tokens):\n","        self.char_input = Input(shape=(None, n_tokens), name='char_input_%s' % name)\n","\n","        self.syllable_input = Input(shape=(1,), name='syllable_input_%s' % name)\n","        self.syllable_dense = Dense(lstm.units, activation='relu', name='syllable_dense_%s' % name)\n","        self.syllable_dense_output = self.syllable_dense(self.syllable_input)\n","\n","        self.h_input = Input(shape=(lstm.units,), name='h_input_%s' % name)\n","        self.c_input = Input(shape=(lstm.units,), name='c_input_%s' % name)\n","        initial_state = [self.h_input, self.c_input]\n","\n","        self.lstm = lstm\n","\n","        self.lstm_out, self.lstm_h, self.lstm_c = self.lstm(self.char_input, initial_state=initial_state)\n","\n","        self.output_dense = Dense(n_tokens, activation='softmax', name='output_%s' % name)\n","        self.output = self.output_dense(self.lstm_out)\n","\n","        self.syllable_dense.set_weights(training_line.syllable_dense.get_weights())\n","        #self.lstm.set_weights(lstm.get_weights())\n","        self.output_dense.set_weights(training_line.output_dense.get_weights())\n","\n","def sample(preds, temperature=1.0):\n","    # helper function to sample an index from a probability array\n","    # From https://github.com/llSourcell/keras_explained/blob/master/gentext.py\n","    preds = np.asarray(preds).astype('float64')\n","    preds = np.log(preds) / temperature\n","    exp_preds = np.exp(preds)\n","    preds = exp_preds / np.sum(exp_preds)\n","    probas = np.random.multinomial(1, preds, 1)\n","    return np.argmax(probas)\n","\n","class Generator1:\n","    def __init__(self, lstm, lines, tf_session, tokenizer, n_tokens, max_line_length):\n","        self.tf_session = tf_session\n","        self.tokenizer = tokenizer\n","        self.n_tokens = n_tokens\n","        self.max_line_length = max_line_length\n","\n","        self.lstm = LSTM(\n","            lstm.units, return_state=True, return_sequences=True,\n","            name='generator_lstm'\n","        )\n","        self.lines = [\n","            GeneratorLine(\n","                'generator_line_%s' % i,\n","                lines[i], self.lstm, self.n_tokens\n","            ) for i in range(3)\n","        ]\n","        self.lstm.set_weights(lstm.get_weights())\n","\n","    def generate_haiku(self, syllables=[11, 11, 11], temperature=.7, first_char=None):\n","        output = []\n","        h = None\n","        c = None\n","\n","        if first_char is None:\n","            first_char = chr(int(np.random.randint(ord('a'), ord('z')+1)))\n","\n","        next_char = self.tokenizer.texts_to_sequences(first_char)[0][0]\n","\n","        for i in range(3):\n","            line = self.lines[i]\n","            s = self.tf_session.run(\n","                line.syllable_dense_output,\n","                feed_dict={\n","                    line.syllable_input: [[syllables[i]]]\n","                }\n","            )\n","\n","            if h is None:\n","                h = s\n","                c = s\n","            else:\n","                h = h + s\n","                c = c + s\n","\n","            line_output = [next_char]\n","            tokens = self.n_tokens\n","            end = False\n","            next_char = None\n","            for i in range(self.max_line_length):\n","                char, h, c = self.tf_session.run(\n","                    [line.output, line.lstm_h, line.lstm_c],\n","                    feed_dict={\n","                        line.char_input: [[\n","                            np_utils.to_categorical(\n","                                line_output[-1],\n","                                num_classes=self.n_tokens\n","                            )\n","                        ]],\n","                        line.h_input: h,\n","                        line.c_input: c\n","                    }\n","                )\n","\n","                char = sample(char[0,0], temperature)\n","                if char == 1 and not end:\n","                    end = True\n","                if char != 1 and end:\n","                    next_char = char\n","                    char = 1\n","                line_output.append(char)\n","            \n","            cleaned_text = self.tokenizer.sequences_to_texts([\n","                line_output\n","            ])[0].strip()[1:].replace(\n","                '   ', '\\n'\n","            ).replace(' ', '').replace('\\n', ' ')\n","\n","            print(cleaned_text)\n","            output.append(cleaned_text)\n","        print('\\n')\n","        return output"],"execution_count":60,"outputs":[]},{"cell_type":"code","metadata":{"id":"3wSPfkJyU78j","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594489901007,"user_tz":-120,"elapsed":29472,"user":{"displayName":"Giacomo","photoUrl":"https://lh6.googleusercontent.com/-_yNWKXs4tL8/AAAAAAAAAAI/AAAAAAAAv5Y/ThWC96YwxpY/s64/photo.jpg","userId":"11330407691056598140"}}},"source":["generator = Generator1(lstm, lines, tf_session, tokenizer, n_tokens, max_line_length)"],"execution_count":61,"outputs":[]},{"cell_type":"code","metadata":{"id":"2pnP7ymtU78o","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":554},"executionInfo":{"status":"error","timestamp":1594490750843,"user_tz":-120,"elapsed":2206,"user":{"displayName":"Giacomo","photoUrl":"https://lh6.googleusercontent.com/-_yNWKXs4tL8/AAAAAAAAAAI/AAAAAAAAv5Y/ThWC96YwxpY/s64/photo.jpg","userId":"11330407691056598140"}},"outputId":"fb0be1e9-6997-4552-cebd-266a95846728"},"source":["for boh in range(30):\n","  generator.generate_haiku()\n"],"execution_count":67,"outputs":[{"output_type":"stream","text":[" sí che ' l pregno aere in acqua si converse\n","la pioggoa cappel du questo menso\n","di muïato na or di sta cotti ! '\n","\n","\n"," elli avean cappe con cappucci bassi\n","dinanzi alli occhi fatte della taglia\n","che in clugní per li monaci fassi\n","\n","\n"," l ' altro che già uscí preso di nave\n","veggio vender sua figlia e paggionte\n","come giusto pente av ' era punto\n","\n","\n"],"name":"stdout"},{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-67-d59c70ece224>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mboh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_haiku\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-60-9bf7acad3193>\u001b[0m in \u001b[0;36mgenerate_haiku\u001b[0;34m(self, syllables, temperature, first_char)\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mfirst_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'z'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mnext_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_char\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"]}]},{"cell_type":"code","metadata":{"id":"SwkfcGVxL6B8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1594489850155,"user_tz":-120,"elapsed":29249,"user":{"displayName":"Giacomo","photoUrl":"https://lh6.googleusercontent.com/-_yNWKXs4tL8/AAAAAAAAAAI/AAAAAAAAv5Y/ThWC96YwxpY/s64/photo.jpg","userId":"11330407691056598140"}},"outputId":"cd7bdabf-4f3e-4a12-9c1e-2057283fc0e2"},"source":["print(giac[0])"],"execution_count":59,"outputs":[{"output_type":"stream","text":[" l ' amor ch ' ad esso troppo s ' abbandona\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cWYo7AYeV1nl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":935},"executionInfo":{"status":"ok","timestamp":1594487957555,"user_tz":-120,"elapsed":2359581,"user":{"displayName":"Giacomo","photoUrl":"https://lh6.googleusercontent.com/-_yNWKXs4tL8/AAAAAAAAAAI/AAAAAAAAv5Y/ThWC96YwxpY/s64/photo.jpg","userId":"11330407691056598140"}},"outputId":"5ce0e437-ae66-45c4-dd57-81e16e0246f5"},"source":["training_model.summary()"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","syllable_input_line_0 (InputLay (None, 1)            0                                            \n","__________________________________________________________________________________________________\n","char_input_line_0 (InputLayer)  (None, None, 43)     0                                            \n","__________________________________________________________________________________________________\n","syllable_dense_line_0 (Dense)   (None, 2048)         4096        syllable_input_line_0[0][0]      \n","__________________________________________________________________________________________________\n","syllable_input_line_1 (InputLay (None, 1)            0                                            \n","__________________________________________________________________________________________________\n","lstm (LSTM)                     [(None, None, 2048), 17137664    char_input_line_0[0][0]          \n","                                                                 syllable_dense_line_0[0][0]      \n","                                                                 syllable_dense_line_0[0][0]      \n","                                                                 char_input_line_1[0][0]          \n","                                                                 add_h_line_1[0][0]               \n","                                                                 add_c_line_1[0][0]               \n","                                                                 char_input_line_2[0][0]          \n","                                                                 add_h_line_2[0][0]               \n","                                                                 add_c_line_2[0][0]               \n","__________________________________________________________________________________________________\n","syllable_dense_line_1 (Dense)   (None, 2048)         4096        syllable_input_line_1[0][0]      \n","__________________________________________________________________________________________________\n","char_input_line_1 (InputLayer)  (None, None, 43)     0                                            \n","__________________________________________________________________________________________________\n","add_h_line_1 (Add)              (None, 2048)         0           lstm[0][1]                       \n","                                                                 syllable_dense_line_1[0][0]      \n","__________________________________________________________________________________________________\n","add_c_line_1 (Add)              (None, 2048)         0           lstm[0][2]                       \n","                                                                 syllable_dense_line_1[0][0]      \n","__________________________________________________________________________________________________\n","syllable_input_line_2 (InputLay (None, 1)            0                                            \n","__________________________________________________________________________________________________\n","syllable_dense_line_2 (Dense)   (None, 2048)         4096        syllable_input_line_2[0][0]      \n","__________________________________________________________________________________________________\n","char_input_line_2 (InputLayer)  (None, None, 43)     0                                            \n","__________________________________________________________________________________________________\n","add_h_line_2 (Add)              (None, 2048)         0           lstm[1][1]                       \n","                                                                 syllable_dense_line_2[0][0]      \n","__________________________________________________________________________________________________\n","add_c_line_2 (Add)              (None, 2048)         0           lstm[1][2]                       \n","                                                                 syllable_dense_line_2[0][0]      \n","__________________________________________________________________________________________________\n","output_line_0 (Dense)           (None, None, 43)     88107       lstm[0][0]                       \n","__________________________________________________________________________________________________\n","output_line_1 (Dense)           (None, None, 43)     88107       lstm[1][0]                       \n","__________________________________________________________________________________________________\n","output_line_2 (Dense)           (None, None, 43)     88107       lstm[2][0]                       \n","==================================================================================================\n","Total params: 17,414,273\n","Trainable params: 17,414,273\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"c5eixLLynDWN","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594487957556,"user_tz":-120,"elapsed":2359578,"user":{"displayName":"Giacomo","photoUrl":"https://lh6.googleusercontent.com/-_yNWKXs4tL8/AAAAAAAAAAI/AAAAAAAAv5Y/ThWC96YwxpY/s64/photo.jpg","userId":"11330407691056598140"}}},"source":[" "],"execution_count":21,"outputs":[]}]}