{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "RNN_text_generator_00.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R8FnhxiWY_Y1",
        "outputId": "7bec9da0-427c-4047-de18-d7b6ed529dec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import time\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import requests\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, GRU, Conv2D, Flatten, MaxPool2D, Input\n",
        "from tensorflow.keras.activations import elu, relu, softmax, sigmoid\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fe1V83bpI8Tf",
        "colab_type": "text"
      },
      "source": [
        "## **Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V5YiWU0daYkT",
        "outputId": "da795fa5-6386-4ce9-f6a5-ef5de87e3ca7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Read the Divina Commedia\n",
        "url = \"https://raw.githubusercontent.com/DanieleVeri/deep_comedy/feature/GANs/divina_commedia.txt\"\n",
        "response = requests.get(url)\n",
        "\n",
        "divina_commedia = response.text\n",
        "\n",
        "# Replace rare characters\n",
        "divina_commedia = divina_commedia.replace(\"ä\", \"a\")\n",
        "divina_commedia = divina_commedia.replace(\"é\", \"è\")\n",
        "divina_commedia = divina_commedia.replace(\"ë\", \"è\")\n",
        "divina_commedia = divina_commedia.replace(\"Ë\", \"E\")\n",
        "divina_commedia = divina_commedia.replace(\"ï\", \"i\")\n",
        "divina_commedia = divina_commedia.replace(\"Ï\", \"I\")\n",
        "divina_commedia = divina_commedia.replace(\"ó\", \"ò\")\n",
        "divina_commedia = divina_commedia.replace(\"ö\", \"o\")\n",
        "divina_commedia = divina_commedia.replace(\"ü\", \"u\")\n",
        "divina_commedia = divina_commedia.replace(\"(\", \"-\")\n",
        "divina_commedia = divina_commedia.replace(\")\", \"-\")\n",
        "divina_commedia = divina_commedia.replace(\"[\", \"\")\n",
        "divina_commedia = divina_commedia.replace(\"]\", \"\")\n",
        "divina_commedia = re.sub(r'[0-9]+', '', divina_commedia)\n",
        "divina_commedia = divina_commedia.replace(\" \\n\", \"\\n\")\n",
        "\n",
        "# Store unique characters into a dict with numerical encoding\n",
        "unique_chars = list(set(divina_commedia))\n",
        "unique_chars.sort()  # to make sure you get the same encoding at each run\n",
        "\n",
        "# Store them in a dict, associated with a numerical index\n",
        "char2idx = { char[1]: char[0] for char in enumerate(unique_chars) }\n",
        "\n",
        "def numerical_encoding(text, char_dict):\n",
        "    \"\"\" Text to list of chars, to np.array of numerical idx \"\"\"\n",
        "    chars_list = [ char for char in text ]\n",
        "    chars_list = [ char_dict[char] for char in chars_list ]\n",
        "    chars_list = np.array(chars_list)\n",
        "    return chars_list\n",
        "\n",
        "encoded_text = numerical_encoding(divina_commedia, char2idx)\n",
        "\n",
        "def get_text_matrix(sequence, len_input):\n",
        "    X = np.empty((len(sequence)-len_input, len_input))\n",
        "    for i in range(X.shape[0]):\n",
        "        X[i,:] = sequence[i : i+len_input]\n",
        "    return X\n",
        "\n",
        "text_matrix = get_text_matrix(encoded_text, 100)\n",
        "\n",
        "# 2D formatted text\n",
        "canti = divina_commedia.split(\"\\n\\n\")\n",
        "\n",
        "num_lines = 0\n",
        "max_len = 0\n",
        "for c in canti:\n",
        "  lines = c.split('\\n')\n",
        "  num_lines += len(lines)\n",
        "  for l in lines:\n",
        "    if len(l) > max_len:\n",
        "      max_len = len(l)\n",
        "\n",
        "formatted = np.zeros((num_lines, max_len), dtype=int)\n",
        "\n",
        "rhymes = []\n",
        "num_lines = 0\n",
        "for i in canti:\n",
        "  lines = i.split('\\n')\n",
        "  for j in lines:\n",
        "    rhymes.append(j.split(' ')[-1])\n",
        "    encoded = numerical_encoding(j, char2idx)\n",
        "    left_padding = np.zeros(max_len-len(encoded))\n",
        "    formatted[num_lines, :] = np.concatenate((left_padding, encoded))\n",
        "    num_lines += 1\n",
        "\n",
        "print(text_matrix.shape)\n",
        "print(formatted.shape)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(534352, 100)\n",
            "(14234, 54)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8w9Pm22BI3yD",
        "colab_type": "text"
      },
      "source": [
        "## **LSTM training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mKysyBfpAtUS",
        "jupyter": {
          "outputs_hidden": true
        },
        "colab": {}
      },
      "source": [
        "# size of vocabulary\n",
        "vocab_size = len(char2idx)\n",
        "# size of mini batches during training\n",
        "batch_size = 100\n",
        "# size of training subset at each epoch\n",
        "subset_size = batch_size * 100\n",
        "# vector size of char embeddings\n",
        "embedding_size = 250\n",
        "len_input = 1000   # 200\n",
        "hidden_size = 250  # for Dense() layers\n",
        "\n",
        "n_epochs = 200\n",
        "learning_rate = 0.0001\n",
        "\n",
        "RNN = Sequential([\n",
        "    Embedding(vocab_size, embedding_size,\n",
        "              batch_input_shape=(batch_size, None)),\n",
        "    \n",
        "    LSTM(len_input, return_sequences = True),\n",
        "    \n",
        "    Dense(hidden_size, activation = relu), \n",
        "    \n",
        "    Dense(vocab_size)\n",
        "])\n",
        "RNN.summary()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
        "\n",
        "# This is an Autograph function\n",
        "# its decorator makes it a TF op - i.e. much faster\n",
        "@tf.function\n",
        "def train_on_batch(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        current_loss = tf.reduce_mean(\n",
        "            tf.keras.losses.sparse_categorical_crossentropy(\n",
        "                y, RNN(x), from_logits = True))\n",
        "    gradients = tape.gradient(current_loss, RNN.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, RNN.trainable_variables))\n",
        "    return current_loss\n",
        "\n",
        "loss_history = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    start = time.time()\n",
        "    print(epoch)\n",
        "    \n",
        "    # Take subsets of train and target\n",
        "    sample = np.random.randint(0, text_matrix.shape[0]-1, subset_size)\n",
        "    sample_train = text_matrix[ sample , : ]\n",
        "    sample_target = text_matrix[ sample+1 , : ]\n",
        "    \n",
        "    for iteration in range(sample_train.shape[0] // batch_size):\n",
        "        take = iteration * batch_size\n",
        "        x = sample_train[ take:take+batch_size , : ]\n",
        "        y = sample_target[ take:take+batch_size , : ]\n",
        "\n",
        "        current_loss = train_on_batch(x, y)\n",
        "        loss_history.append(current_loss)\n",
        "    \n",
        "    print(\"{}.  \\t  Loss: {}  \\t  Time: {}ss\".format(\n",
        "        epoch+1, current_loss.numpy(), round(time.time()-start, 2)))\n",
        "    \n",
        "\n",
        "    \n",
        "plt.plot(loss_history)\n",
        "plt.title(\"Training Loss\")\n",
        "plt.show()\n",
        "\n",
        "RNN.save(\"/text_generator_RNN_00.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0qjtiycI02D",
        "colab_type": "text"
      },
      "source": [
        "## **GANs training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXOZdOmyH7OM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# size of vocabulary\n",
        "vocab_size = len(char2idx)\n",
        "\n",
        "n_epochs = 200\n",
        "learning_rate = 0.0001\n",
        "\n",
        "disc = Sequential([\n",
        "    Input(shape=(54, 54, 1)),\n",
        "    Conv2D(4, (3, 3), activation = relu),\n",
        "    #MaxPool2D(pool_size=(2,2)),\n",
        "    Flatten(), \n",
        "    Dense(1, activation = sigmoid)\n",
        "])\n",
        "disc.summary()\n",
        "\n",
        "gen = Sequential([\n",
        "    Input(shape=(54, 54, 1)),\n",
        "    Conv2D(4, (3, 3), activation = relu),\n",
        "    #MaxPool2D(pool_size=(2,2)),\n",
        "    Flatten(), \n",
        "    Dense(1, activation = sigmoid)\n",
        "])\n",
        "gen.summary()\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gawSIEf_Ix31",
        "colab_type": "text"
      },
      "source": [
        "## **LSTM generation:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vE2hYSqAAtkn",
        "jupyter": {
          "source_hidden": true
        },
        "outputId": "a35b653e-ea16-47a3-d553-4c60110a4660",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 880
        }
      },
      "source": [
        "generator = Sequential([\n",
        "    Embedding(vocab_size, embedding_size,\n",
        "              batch_input_shape=(1, None)),\n",
        "    \n",
        "    LSTM(len_input, return_sequences = True, stateful=True),\n",
        "    \n",
        "    Dense(hidden_size, activation = relu), \n",
        "    \n",
        "    Dense(vocab_size)\n",
        "])\n",
        "\n",
        "generator.summary()\n",
        "\n",
        "# Import trained weights from RNN to generator\n",
        "generator.set_weights(RNN.get_weights())\n",
        "\n",
        "def generate_text(start_string, num_generate = 1000, temperature = 1.0):\n",
        "    \n",
        "    # Vectorize input string\n",
        "    input_eval = [char2idx[s] for s in start_string]  \n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "    \n",
        "    text_generated = [] # List to append predicted chars \n",
        "    \n",
        "    idx2char = { v: k for k, v in char2idx.items() }  # invert char-index mapping\n",
        "    \n",
        "    generator.reset_states()\n",
        "    \n",
        "    for i in range(num_generate):\n",
        "        predictions = generator(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        \n",
        "        # sample next char based on distribution and temperature\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "        \n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "        \n",
        "    return (start_string + ''.join(text_generated))\n",
        "\n",
        "init=\"\"\"\n",
        "Nel mezzo del cammin di nostra vita\n",
        "mi ritrovai per una selva oscura,\n",
        "chè la diritta via era smarrita.\n",
        "\"\"\"\n",
        "print(generate_text(init))\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (1, None, 250)            15500     \n",
            "_________________________________________________________________\n",
            "gru_2 (GRU)                  (1, None, 1000)           3756000   \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (1, None, 250)            250250    \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (1, None, 62)             15562     \n",
            "=================================================================\n",
            "Total params: 4,037,312\n",
            "Trainable params: 4,037,312\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\n",
            "Nel mezzo del cammin di nostra vita\n",
            "mi ritrovai per una selva oscura,\n",
            "chè la diritta via era smarrita.\n",
            "  Ahi giustizia tutto mi rispuose;\n",
            "però è là giù porgoglio soverchia,\n",
            "ond'era in molin che 'n sè a ver de li avanza.\n",
            "  Virgilio inverso me così scintilla\".\n",
            "  E io: \"Se fuoro aperte aparti.\n",
            "  Cred'ire a mio vi fu bagli,\n",
            "ben fuomo sì, che non hai fummo proè,\n",
            "s'alcuna pien d'ogne altro bando.\n",
            "  Appresso un'altra con manife atte;\n",
            "  e sì come visie;\n",
            "  d'inquarvammo per la via di salvas letaggio,\n",
            "chi va di luce in luce vien quella clogge.\n",
            "  Or discendiamo a Virgilio congiunto;\n",
            "  e si rammona ciascun si picca!\n",
            "  Cenare, avanti dipartiro.\n",
            "  L'essercito di Fiesole, e fui pavrebbe sparte,\n",
            "e l'altro scese in lor più ch'altri 'l Manto:\n",
            "Gaullibile fier le terpenti d'Itani,\n",
            "  ditemi chi voi sì conventi e per senno\n",
            "li vostre condizion di qua dal rio.\n",
            "  Tutto che trova attivo quivi, tinopesta,\n",
            "  quando noi fummo ghiotto veni bene,\n",
            "conteneggiava e riguardar pasciuto,\n",
            "dir c'inavor, sì volsi dier loco;\n",
            "chè per te non vera, ed è non fera,\n",
            "  ristolilla, sostener lo sacrosorti,\n",
            "avore sbadenti: 'D'un'alto e \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}