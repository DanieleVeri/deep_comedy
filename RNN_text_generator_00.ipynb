{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "RNN_text_generator_00.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R8FnhxiWY_Y1",
        "outputId": "7f79cbfc-8f80-4a4e-8ee1-5588ff80e967",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import time\n",
        "import re\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import requests\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Reshape, BatchNormalization, Dense, Dropout,       # General\n",
        "    Embedding, LSTM, Dense, GRU,                              # RNN\n",
        "    Conv2D, Conv2DTranspose, LeakyReLU, MaxPool2D, Flatten    # CNN\n",
        ")\n",
        "from tensorflow.keras.activations import elu, relu, softmax, sigmoid\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fe1V83bpI8Tf",
        "colab_type": "text"
      },
      "source": [
        "## **Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V5YiWU0daYkT",
        "outputId": "c4c45fc3-729e-45aa-9462-fd2cfc005e14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "source": [
        "# Read the Divina Commedia\n",
        "#url = \"https://raw.githubusercontent.com/DanieleVeri/deep_comedy/feature/GANs/divina_commedia.txt\"\n",
        "url = \"https://raw.githubusercontent.com/DanieleVeri/deep_comedy/master/divina_commedia_sillabe.txt\"\n",
        "response = requests.get(url)\n",
        "\n",
        "divina_commedia = response.text\n",
        "\n",
        "# Replace rare characters\n",
        "divina_commedia = divina_commedia.replace(\"ä\", \"a\")\n",
        "divina_commedia = divina_commedia.replace(\"é\", \"è\")\n",
        "divina_commedia = divina_commedia.replace(\"ë\", \"è\")\n",
        "divina_commedia = divina_commedia.replace(\"Ë\", \"E\")\n",
        "divina_commedia = divina_commedia.replace(\"ï\", \"i\")\n",
        "divina_commedia = divina_commedia.replace(\"Ï\", \"I\")\n",
        "divina_commedia = divina_commedia.replace(\"ó\", \"ò\")\n",
        "divina_commedia = divina_commedia.replace(\"ö\", \"o\")\n",
        "divina_commedia = divina_commedia.replace(\"ü\", \"u\")\n",
        "divina_commedia = divina_commedia.replace(\"(\", \"-\")\n",
        "divina_commedia = divina_commedia.replace(\")\", \"-\")\n",
        "divina_commedia = divina_commedia.replace(\"[\", \"\")\n",
        "divina_commedia = divina_commedia.replace(\"]\", \"\")\n",
        "divina_commedia = re.sub(r'[0-9]+', '', divina_commedia)\n",
        "divina_commedia = divina_commedia.replace(\" \\n\", \"\\n\")\n",
        "\n",
        "unique_chars = list(set(divina_commedia))\n",
        "unique_chars.sort()  # to make sure you get the same encoding at each run\n",
        "char2idx = { char[1]: char[0] for char in enumerate(unique_chars) }\n",
        "\n",
        "def numerical_encoding(text, char_dict):\n",
        "    \"\"\" Text to list of chars, to np.array of numerical idx \"\"\"\n",
        "    chars_list = [ char for char in text ]\n",
        "    chars_list = [ char_dict[char] for char in chars_list ]\n",
        "    chars_list = np.array(chars_list)\n",
        "    return chars_list\n",
        "\n",
        "encoded_text = numerical_encoding(divina_commedia, char2idx)\n",
        "\n",
        "def get_text_matrix(sequence, len_input):\n",
        "    X = np.empty((len(sequence)-len_input, len_input))\n",
        "    for i in range(X.shape[0]):\n",
        "        X[i,:] = sequence[i : i+len_input]\n",
        "    return X\n",
        "\n",
        "text_matrix = get_text_matrix(encoded_text, 100)\n",
        "\n",
        "# 2D formatted text\n",
        "canti = divina_commedia.split(\"\\n\\n\")\n",
        "\n",
        "num_lines = 0\n",
        "max_len = 0\n",
        "for c in canti:\n",
        "  lines = c.split('\\n')\n",
        "  num_lines += len(lines)\n",
        "  for l in lines:\n",
        "    if len(l) > max_len:\n",
        "      max_len = len(l)\n",
        "\n",
        "formatted = np.zeros((num_lines, max_len+2), dtype=int)\n",
        "\n",
        "rhymes = []\n",
        "num_lines = 0\n",
        "for i in canti:\n",
        "  lines = i.split('\\n')\n",
        "  for j in lines:\n",
        "    rhymes.append(j.split(' ')[-1])\n",
        "    encoded = numerical_encoding(j+'\\n', char2idx)\n",
        "    left_padding = np.ones(max_len-len(encoded)+2)\n",
        "    formatted[num_lines, :] = np.concatenate((left_padding, encoded))\n",
        "    num_lines += 1\n",
        "\n",
        "# word voc\n",
        "'''\n",
        "for c in canti:\n",
        "  lines = c.split('\\n')\n",
        "  for l in lines:\n",
        "'''\n",
        "sub = re.sub(r'[\\n\"\\',.;:()]', ' ', divina_commedia.lower())\n",
        "unique_words = list(set(sub.split(' ')))\n",
        "unique_words.sort()\n",
        "word2idx = { w[1]: w[0] for w in enumerate(unique_words) }\n",
        "\n",
        "# syllabe\n",
        "div_ls = set()\n",
        "aa = divina_commedia.split('\\n')\n",
        "for x in aa:\n",
        "  y = x.split(sep='-')\n",
        "  for t in y:\n",
        "    div_ls.add(t) \n",
        "\n",
        "uniq_sylla = list(div_ls)\n",
        "uniq_sylla.sort()  \n",
        "sylla2idx = { s[1]: s[0] for s in enumerate(uniq_sylla) }\n",
        "\n",
        "formatted_syll = np.zeros((num_lines, 18), dtype=int)\n",
        "num_lines = 0\n",
        "for i in canti:\n",
        "  lines = i.split('\\n')\n",
        "  for j in lines:\n",
        "    s = j.split('-')\n",
        "    yo = []\n",
        "    for cc in s:\n",
        "      yo.append(sylla2idx[cc])\n",
        "    yo.append(0)\n",
        "    left_padding = np.ones(18-len(yo))\n",
        "    formatted_syll[num_lines, :] = np.concatenate((left_padding, yo))\n",
        "    num_lines += 1\n",
        "\n",
        "print(text_matrix.shape)\n",
        "print(formatted.shape)\n",
        "#print(word2idx)\n",
        "\n",
        "print(formatted_syll[0:9, :])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(571011, 100)\n",
            "(14234, 54)\n",
            "[[   1    1    1    1    1    1  932  857 1840  304  121  869  315  965\n",
            "  1531 1763 1600    0]\n",
            " [   1    1    1    1    1  862 1194 1686 1740 1034 1727  906 1364 1736\n",
            "  1001 1348 1162    0]\n",
            " [   1    1    1    1    1    1  190  740  315 1218 1600 1764  394 1162\n",
            "  1420 1194 1600    0]\n",
            " [   1    1    1    1  702 1141 1649    1  330 1139  394 1162 1851  232\n",
            "  1268  371 1162    0]\n",
            " [   1    1    1    1  394 1495 1364 1736 1364 1739  553  394    1 1477\n",
            "   394  467 1616    0]\n",
            " [   1    1    1    1    1    1    1  156  932 1033 1405 1194  965 1736\n",
            "   740 1025 1162    0]\n",
            " [   1    1    1    1    1 1609 1718    1  826 1162  156 1076  232 1851\n",
            "  1063  881 1616    0]\n",
            " [   1    1    1    1    1    1    1  826 1034 1669 1611  304   32  168\n",
            "  1763 1686 1740    0]\n",
            " [   1    1    1    1    1    1  315 1264  298  747 1672  232 1359  168\n",
            "  1762 1340 1616    0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8w9Pm22BI3yD",
        "colab_type": "text"
      },
      "source": [
        "## **LSTM training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mKysyBfpAtUS",
        "jupyter": {
          "outputs_hidden": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0b21287e-0761-4208-98e6-e5b22c26c6c8"
      },
      "source": [
        "# size of vocabulary\n",
        "vocab_size = len(char2idx)\n",
        "\n",
        "# size of mini batches during training\n",
        "batch_size = 100\n",
        "# size of training subset at each epoch\n",
        "subset_size = batch_size * 100\n",
        "# vector size of char embeddings\n",
        "embedding_size = 250\n",
        "len_input = 1000   # 200\n",
        "hidden_size = 250  # for Dense() layers\n",
        "\n",
        "n_epochs = 20\n",
        "learning_rate = 0.0001\n",
        "\n",
        "RNN = Sequential([\n",
        "    Embedding(vocab_size, embedding_size,\n",
        "              batch_input_shape=(batch_size, None)),\n",
        "    \n",
        "    LSTM(len_input, return_sequences = True),\n",
        "    \n",
        "    Dense(hidden_size, activation = relu), \n",
        "    \n",
        "    Dense(vocab_size)\n",
        "])\n",
        "RNN.summary()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
        "\n",
        "# This is an Autograph function\n",
        "# its decorator makes it a TF op - i.e. much faster\n",
        "@tf.function\n",
        "def train_on_batch(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        current_loss = tf.reduce_mean(\n",
        "            tf.keras.losses.sparse_categorical_crossentropy(\n",
        "                y, RNN(x), from_logits = True))\n",
        "    gradients = tape.gradient(current_loss, RNN.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, RNN.trainable_variables))\n",
        "    return current_loss\n",
        "\n",
        "loss_history = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    start = time.time()\n",
        "    print(epoch)\n",
        "    \n",
        "    # Take subsets of train and target\n",
        "    sample = np.random.randint(0, text_matrix.shape[0]-1, subset_size)\n",
        "    sample_train = text_matrix[ sample , : ]\n",
        "    sample_target = text_matrix[ sample+1 , : ]\n",
        "    \n",
        "    for iteration in range(sample_train.shape[0] // batch_size):\n",
        "        take = iteration * batch_size\n",
        "        x = sample_train[ take:take+batch_size , : ]\n",
        "        y = sample_target[ take:take+batch_size , : ]\n",
        "\n",
        "        current_loss = train_on_batch(x, y)\n",
        "        loss_history.append(current_loss)\n",
        "    \n",
        "    print(\"{}.  \\t  Loss: {}  \\t  Time: {}ss\".format(\n",
        "        epoch+1, current_loss.numpy(), round(time.time()-start, 2)))\n",
        "    \n",
        "    \n",
        "plt.plot(loss_history)\n",
        "plt.title(\"Training Loss\")\n",
        "plt.show()\n",
        "\n",
        "RNN.save(\"/text_generator_RNN_00.h5\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (100, None, 250)          465500    \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (100, None, 1000)         5004000   \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (100, None, 250)          250250    \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (100, None, 1862)         467362    \n",
            "=================================================================\n",
            "Total params: 6,187,112\n",
            "Trainable params: 6,187,112\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "0\n",
            "1.  \t  Loss: 4.414053440093994  \t  Time: 4.41ss\n",
            "1\n",
            "2.  \t  Loss: 4.031836986541748  \t  Time: 3.22ss\n",
            "2\n",
            "3.  \t  Loss: 3.845198631286621  \t  Time: 3.23ss\n",
            "3\n",
            "4.  \t  Loss: 3.7512950897216797  \t  Time: 3.17ss\n",
            "4\n",
            "5.  \t  Loss: 3.6767420768737793  \t  Time: 3.17ss\n",
            "5\n",
            "6.  \t  Loss: 3.6771035194396973  \t  Time: 3.15ss\n",
            "6\n",
            "7.  \t  Loss: 3.7167606353759766  \t  Time: 3.15ss\n",
            "7\n",
            "8.  \t  Loss: 3.731139898300171  \t  Time: 3.19ss\n",
            "8\n",
            "9.  \t  Loss: 3.729219913482666  \t  Time: 3.19ss\n",
            "9\n",
            "10.  \t  Loss: 3.7092299461364746  \t  Time: 3.18ss\n",
            "10\n",
            "11.  \t  Loss: 3.6214067935943604  \t  Time: 3.16ss\n",
            "11\n",
            "12.  \t  Loss: 3.6767594814300537  \t  Time: 3.15ss\n",
            "12\n",
            "13.  \t  Loss: 3.604966878890991  \t  Time: 3.15ss\n",
            "13\n",
            "14.  \t  Loss: 3.7796974182128906  \t  Time: 3.16ss\n",
            "14\n",
            "15.  \t  Loss: 3.7009732723236084  \t  Time: 3.18ss\n",
            "15\n",
            "16.  \t  Loss: 3.7058398723602295  \t  Time: 3.15ss\n",
            "16\n",
            "17.  \t  Loss: 3.6643707752227783  \t  Time: 3.23ss\n",
            "17\n",
            "18.  \t  Loss: 3.6582672595977783  \t  Time: 3.24ss\n",
            "18\n",
            "19.  \t  Loss: 3.6814913749694824  \t  Time: 3.26ss\n",
            "19\n",
            "20.  \t  Loss: 3.7581396102905273  \t  Time: 3.24ss\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0qjtiycI02D",
        "colab_type": "text"
      },
      "source": [
        "## **GANs training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXOZdOmyH7OM",
        "colab_type": "code",
        "outputId": "dad913ab-1f7a-41cf-df7c-3d58aa531a53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# size of vocabulary\n",
        "vocab_size = len(char2idx)\n",
        "normalized = (formatted - vocab_size/2)/(vocab_size / 2)\n",
        "\n",
        "discriminator = Sequential()\n",
        "discriminator.add(Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[52, 52, 1]))\n",
        "discriminator.add(LeakyReLU())\n",
        "discriminator.add(MaxPool2D((2,2)))\n",
        "discriminator.add(Dropout(0.3))\n",
        "discriminator.add(Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
        "discriminator.add(LeakyReLU())\n",
        "discriminator.add(Dropout(0.3))\n",
        "discriminator.add(Flatten())\n",
        "discriminator.add(Dense(1))\n",
        "\n",
        "discriminator.summary()\n",
        "\n",
        "generator = Sequential()\n",
        "generator.add(Dense(13*13*256, use_bias=False, input_shape=(100,)))\n",
        "generator.add(BatchNormalization())\n",
        "generator.add(LeakyReLU())\n",
        "generator.add(Reshape((13, 13, 256)))\n",
        "assert generator.output_shape == (None, 13, 13, 256) # Note: None is the batch size\n",
        "generator.add(Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
        "assert generator.output_shape == (None, 13, 13, 128)\n",
        "generator.add(BatchNormalization())\n",
        "generator.add(LeakyReLU())\n",
        "generator.add(Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
        "assert generator.output_shape == (None, 26, 26, 64)\n",
        "generator.add(BatchNormalization())\n",
        "generator.add(LeakyReLU())\n",
        "generator.add(Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
        "assert generator.output_shape == (None, 52, 52, 1)\n",
        "\n",
        "generator.summary()\n",
        "\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "\n",
        "noise_dim = 100\n",
        "num_examples_to_generate = 16\n",
        "BATCH_SIZE = 256\n",
        "EPOCHS = 1000\n",
        "\n",
        "# We will reuse this seed overtime (so it's easier)\n",
        "# to visualize progress in the animated GIF)\n",
        "seed = tf.random.normal([num_examples_to_generate, noise_dim])\n",
        "@tf.function\n",
        "def train_step(images):\n",
        "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "      generated_images = generator(noise, training=True)\n",
        "      real_output = discriminator(images, training=True)\n",
        "      fake_output = discriminator(generated_images, training=True)\n",
        "      gen_loss = generator_loss(fake_output)\n",
        "      disc_loss = discriminator_loss(real_output, fake_output)\n",
        "    \n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "\n",
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    for image_batch in dataset:\n",
        "      train_step(image_batch)\n",
        "\n",
        "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "\n",
        "  # Generate after the final epoch\n",
        "  #display.clear_output(wait=True)\n",
        "  generate_and_save_images(generator, epochs, seed)\n",
        "\n",
        "\n",
        "idx2char = { v: k for k, v in char2idx.items() } \n",
        "\n",
        "def generate_and_save_images(model, epoch, test_input):\n",
        "  # Notice `training` is set to False.\n",
        "  # This is so all layers run in inference mode (batchnorm).\n",
        "  predictions = model(test_input, training=False)\n",
        "\n",
        "  fig = plt.figure(figsize=(20,20))\n",
        "\n",
        "  for i in range(predictions.shape[0]):\n",
        "      plt.subplot(4, 4, i+1)\n",
        "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "      plt.axis('off')\n",
        "      \n",
        "      res = []\n",
        "      for a in range(52):\n",
        "        line = []\n",
        "        for b in range(52):\n",
        "          discretized = math.floor(predictions[i, a, b, 0] * vocab_size/2 + vocab_size/2)\n",
        "          line.append(idx2char[discretized])\n",
        "        res.append(''.join(line))\n",
        "      print(\"=============\")\n",
        "      print(res)\n",
        "      print(\"=============\\n\\n\")\n",
        "      \n",
        "  plt.show()\n",
        "\n",
        "      \n",
        "splitted = np.split(normalized[0:normalized.shape[0]-normalized.shape[0]%52], normalized.shape[0] // 52)\n",
        "splitted = np.array(splitted).reshape(len(splitted), 52, 52, 1)\n",
        "print(splitted.shape)\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(splitted).shuffle(60000).batch(BATCH_SIZE)\n",
        "train(train_dataset, EPOCHS)\n",
        "\n",
        "plt.imshow(normalized[0:53,:], cmap='gray')\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 64)        1664      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)      (None, 26, 26, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 7, 7, 128)         204928    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 6273      \n",
            "=================================================================\n",
            "Total params: 212,865\n",
            "Trainable params: 212,865\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 43264)             4326400   \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 43264)             173056    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 43264)             0         \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 13, 13, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose (Conv2DTran (None, 13, 13, 128)       819200    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 13, 13, 128)       512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 13, 13, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTr (None, 26, 26, 64)        204800    \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 26, 26, 64)        256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 26, 26, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTr (None, 52, 52, 1)         1600      \n",
            "=================================================================\n",
            "Total params: 5,525,824\n",
            "Trainable params: 5,438,912\n",
            "Non-trainable params: 86,912\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-986e6dda56aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0msplitted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalized\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnormalized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnormalized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m52\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m52\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m \u001b[0msplitted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplitted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplitted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m52\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m52\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplitted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplitted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m60000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 255528 into shape (273,52,52,1)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gawSIEf_Ix31",
        "colab_type": "text"
      },
      "source": [
        "## **LSTM generation:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vE2hYSqAAtkn",
        "jupyter": {
          "source_hidden": true
        },
        "outputId": "a35b653e-ea16-47a3-d553-4c60110a4660",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 880
        }
      },
      "source": [
        "generator = Sequential([\n",
        "    Embedding(vocab_size, embedding_size,\n",
        "              batch_input_shape=(1, None)),\n",
        "    \n",
        "    LSTM(len_input, return_sequences = True, stateful=True),\n",
        "    \n",
        "    Dense(hidden_size, activation = relu), \n",
        "    \n",
        "    Dense(vocab_size)\n",
        "])\n",
        "\n",
        "generator.summary()\n",
        "\n",
        "# Import trained weights from RNN to generator\n",
        "generator.set_weights(RNN.get_weights())\n",
        "\n",
        "def generate_text(start_string, num_generate = 1000, temperature = 1.0):\n",
        "    \n",
        "    # Vectorize input string\n",
        "    input_eval = [char2idx[s] for s in start_string]  \n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "    \n",
        "    text_generated = [] # List to append predicted chars \n",
        "    \n",
        "    idx2char = { v: k for k, v in char2idx.items() }  # invert char-index mapping\n",
        "    \n",
        "    generator.reset_states()\n",
        "    \n",
        "    for i in range(num_generate):\n",
        "        predictions = generator(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        \n",
        "        # sample next char based on distribution and temperature\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "        \n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "        \n",
        "    return (start_string + ''.join(text_generated))\n",
        "\n",
        "init=\"\"\"\n",
        "Nel mezzo del cammin di nostra vita\n",
        "mi ritrovai per una selva oscura,\n",
        "chè la diritta via era smarrita.\n",
        "\"\"\"\n",
        "print(generate_text(init))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (1, None, 250)            15500     \n",
            "_________________________________________________________________\n",
            "gru_2 (GRU)                  (1, None, 1000)           3756000   \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (1, None, 250)            250250    \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (1, None, 62)             15562     \n",
            "=================================================================\n",
            "Total params: 4,037,312\n",
            "Trainable params: 4,037,312\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\n",
            "Nel mezzo del cammin di nostra vita\n",
            "mi ritrovai per una selva oscura,\n",
            "chè la diritta via era smarrita.\n",
            "  Ahi giustizia tutto mi rispuose;\n",
            "però è là giù porgoglio soverchia,\n",
            "ond'era in molin che 'n sè a ver de li avanza.\n",
            "  Virgilio inverso me così scintilla\".\n",
            "  E io: \"Se fuoro aperte aparti.\n",
            "  Cred'ire a mio vi fu bagli,\n",
            "ben fuomo sì, che non hai fummo proè,\n",
            "s'alcuna pien d'ogne altro bando.\n",
            "  Appresso un'altra con manife atte;\n",
            "  e sì come visie;\n",
            "  d'inquarvammo per la via di salvas letaggio,\n",
            "chi va di luce in luce vien quella clogge.\n",
            "  Or discendiamo a Virgilio congiunto;\n",
            "  e si rammona ciascun si picca!\n",
            "  Cenare, avanti dipartiro.\n",
            "  L'essercito di Fiesole, e fui pavrebbe sparte,\n",
            "e l'altro scese in lor più ch'altri 'l Manto:\n",
            "Gaullibile fier le terpenti d'Itani,\n",
            "  ditemi chi voi sì conventi e per senno\n",
            "li vostre condizion di qua dal rio.\n",
            "  Tutto che trova attivo quivi, tinopesta,\n",
            "  quando noi fummo ghiotto veni bene,\n",
            "conteneggiava e riguardar pasciuto,\n",
            "dir c'inavor, sì volsi dier loco;\n",
            "chè per te non vera, ed è non fera,\n",
            "  ristolilla, sostener lo sacrosorti,\n",
            "avore sbadenti: 'D'un'alto e \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}