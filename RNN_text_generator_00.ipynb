{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "RNN_text_generator_00.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R8FnhxiWY_Y1",
        "outputId": "9bcf6f16-4d3f-4ac3-fe64-86202aa52c15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import time\n",
        "import re\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import requests\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Reshape, BatchNormalization, Dense, Dropout,       # General\n",
        "    Embedding, LSTM, Dense, GRU,                              # RNN\n",
        "    Conv2D, Conv2DTranspose, LeakyReLU, MaxPool2D, Flatten    # CNN\n",
        ")\n",
        "from tensorflow.keras.activations import elu, relu, softmax, sigmoid\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fe1V83bpI8Tf",
        "colab_type": "text"
      },
      "source": [
        "## **Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V5YiWU0daYkT",
        "outputId": "eb63787e-6ae1-4238-ea9f-95ad4a945cf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Read the Divina Commedia\n",
        "url = \"https://raw.githubusercontent.com/DanieleVeri/deep_comedy/feature/GANs/divina_commedia.txt\"\n",
        "response = requests.get(url)\n",
        "\n",
        "divina_commedia = response.text\n",
        "\n",
        "# Replace rare characters\n",
        "divina_commedia = divina_commedia.replace(\"ä\", \"a\")\n",
        "divina_commedia = divina_commedia.replace(\"é\", \"è\")\n",
        "divina_commedia = divina_commedia.replace(\"ë\", \"è\")\n",
        "divina_commedia = divina_commedia.replace(\"Ë\", \"E\")\n",
        "divina_commedia = divina_commedia.replace(\"ï\", \"i\")\n",
        "divina_commedia = divina_commedia.replace(\"Ï\", \"I\")\n",
        "divina_commedia = divina_commedia.replace(\"ó\", \"ò\")\n",
        "divina_commedia = divina_commedia.replace(\"ö\", \"o\")\n",
        "divina_commedia = divina_commedia.replace(\"ü\", \"u\")\n",
        "divina_commedia = divina_commedia.replace(\"(\", \"-\")\n",
        "divina_commedia = divina_commedia.replace(\")\", \"-\")\n",
        "divina_commedia = divina_commedia.replace(\"[\", \"\")\n",
        "divina_commedia = divina_commedia.replace(\"]\", \"\")\n",
        "divina_commedia = re.sub(r'[0-9]+', '', divina_commedia)\n",
        "divina_commedia = divina_commedia.replace(\" \\n\", \"\\n\")\n",
        "\n",
        "# Store unique characters into a dict with numerical encoding\n",
        "unique_chars = list(set(divina_commedia))\n",
        "unique_chars.sort()  # to make sure you get the same encoding at each run\n",
        "\n",
        "# Store them in a dict, associated with a numerical index\n",
        "char2idx = { char[1]: char[0] for char in enumerate(unique_chars) }\n",
        "\n",
        "def numerical_encoding(text, char_dict):\n",
        "    \"\"\" Text to list of chars, to np.array of numerical idx \"\"\"\n",
        "    chars_list = [ char for char in text ]\n",
        "    chars_list = [ char_dict[char] for char in chars_list ]\n",
        "    chars_list = np.array(chars_list)\n",
        "    return chars_list\n",
        "\n",
        "encoded_text = numerical_encoding(divina_commedia, char2idx)\n",
        "\n",
        "def get_text_matrix(sequence, len_input):\n",
        "    X = np.empty((len(sequence)-len_input, len_input))\n",
        "    for i in range(X.shape[0]):\n",
        "        X[i,:] = sequence[i : i+len_input]\n",
        "    return X\n",
        "\n",
        "text_matrix = get_text_matrix(encoded_text, 100)\n",
        "\n",
        "# 2D formatted text\n",
        "canti = divina_commedia.split(\"\\n\\n\")\n",
        "\n",
        "num_lines = 0\n",
        "max_len = 0\n",
        "for c in canti:\n",
        "  lines = c.split('\\n')\n",
        "  num_lines += len(lines)\n",
        "  for l in lines:\n",
        "    if len(l) > max_len:\n",
        "      max_len = len(l)\n",
        "\n",
        "formatted = np.zeros((num_lines, max_len+2), dtype=int)\n",
        "\n",
        "rhymes = []\n",
        "num_lines = 0\n",
        "for i in canti:\n",
        "  lines = i.split('\\n')\n",
        "  for j in lines:\n",
        "    rhymes.append(j.split(' ')[-1])\n",
        "    encoded = numerical_encoding(j+'\\n', char2idx)\n",
        "    left_padding = np.ones(max_len-len(encoded)+2)\n",
        "    formatted[num_lines, :] = np.concatenate((left_padding, encoded))\n",
        "    num_lines += 1\n",
        "\n",
        "print(text_matrix.shape)\n",
        "print(formatted.shape)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(533803, 100)\n",
            "(14234, 52)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8w9Pm22BI3yD",
        "colab_type": "text"
      },
      "source": [
        "## **LSTM training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mKysyBfpAtUS",
        "jupyter": {
          "outputs_hidden": true
        },
        "colab": {}
      },
      "source": [
        "# size of vocabulary\n",
        "vocab_size = len(char2idx)\n",
        "# size of mini batches during training\n",
        "batch_size = 100\n",
        "# size of training subset at each epoch\n",
        "subset_size = batch_size * 100\n",
        "# vector size of char embeddings\n",
        "embedding_size = 250\n",
        "len_input = 1000   # 200\n",
        "hidden_size = 250  # for Dense() layers\n",
        "\n",
        "n_epochs = 200\n",
        "learning_rate = 0.0001\n",
        "\n",
        "RNN = Sequential([\n",
        "    Embedding(vocab_size, embedding_size,\n",
        "              batch_input_shape=(batch_size, None)),\n",
        "    \n",
        "    LSTM(len_input, return_sequences = True),\n",
        "    \n",
        "    Dense(hidden_size, activation = relu), \n",
        "    \n",
        "    Dense(vocab_size)\n",
        "])\n",
        "RNN.summary()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
        "\n",
        "# This is an Autograph function\n",
        "# its decorator makes it a TF op - i.e. much faster\n",
        "@tf.function\n",
        "def train_on_batch(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        current_loss = tf.reduce_mean(\n",
        "            tf.keras.losses.sparse_categorical_crossentropy(\n",
        "                y, RNN(x), from_logits = True))\n",
        "    gradients = tape.gradient(current_loss, RNN.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, RNN.trainable_variables))\n",
        "    return current_loss\n",
        "\n",
        "loss_history = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    start = time.time()\n",
        "    print(epoch)\n",
        "    \n",
        "    # Take subsets of train and target\n",
        "    sample = np.random.randint(0, text_matrix.shape[0]-1, subset_size)\n",
        "    sample_train = text_matrix[ sample , : ]\n",
        "    sample_target = text_matrix[ sample+1 , : ]\n",
        "    \n",
        "    for iteration in range(sample_train.shape[0] // batch_size):\n",
        "        take = iteration * batch_size\n",
        "        x = sample_train[ take:take+batch_size , : ]\n",
        "        y = sample_target[ take:take+batch_size , : ]\n",
        "\n",
        "        current_loss = train_on_batch(x, y)\n",
        "        loss_history.append(current_loss)\n",
        "    \n",
        "    print(\"{}.  \\t  Loss: {}  \\t  Time: {}ss\".format(\n",
        "        epoch+1, current_loss.numpy(), round(time.time()-start, 2)))\n",
        "    \n",
        "    \n",
        "plt.plot(loss_history)\n",
        "plt.title(\"Training Loss\")\n",
        "plt.show()\n",
        "\n",
        "RNN.save(\"/text_generator_RNN_00.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0qjtiycI02D",
        "colab_type": "text"
      },
      "source": [
        "## **GANs training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXOZdOmyH7OM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# size of vocabulary\n",
        "vocab_size = len(char2idx)\n",
        "normalized = (formatted - vocab_size/2)/(vocab_size / 2)\n",
        "\n",
        "discriminator = Sequential()\n",
        "discriminator.add(Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[52, 52, 1]))\n",
        "discriminator.add(LeakyReLU())\n",
        "discriminator.add(Dropout(0.3))\n",
        "discriminator.add(Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
        "discriminator.add(LeakyReLU())\n",
        "discriminator.add(Dropout(0.3))\n",
        "discriminator.add(Flatten())\n",
        "discriminator.add(Dense(1))\n",
        "\n",
        "discriminator.summary()\n",
        "\n",
        "generator = Sequential()\n",
        "generator.add(Dense(13*13*256, use_bias=False, input_shape=(100,)))\n",
        "generator.add(BatchNormalization())\n",
        "generator.add(LeakyReLU())\n",
        "generator.add(Reshape((13, 13, 256)))\n",
        "assert generator.output_shape == (None, 13, 13, 256) # Note: None is the batch size\n",
        "generator.add(Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
        "assert generator.output_shape == (None, 13, 13, 128)\n",
        "generator.add(BatchNormalization())\n",
        "generator.add(LeakyReLU())\n",
        "generator.add(Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
        "assert generator.output_shape == (None, 26, 26, 64)\n",
        "generator.add(BatchNormalization())\n",
        "generator.add(LeakyReLU())\n",
        "generator.add(Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
        "assert generator.output_shape == (None, 52, 52, 1)\n",
        "\n",
        "generator.summary()\n",
        "\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "\n",
        "noise_dim = 100\n",
        "num_examples_to_generate = 16\n",
        "BATCH_SIZE = 256\n",
        "EPOCHS = 3000\n",
        "\n",
        "# We will reuse this seed overtime (so it's easier)\n",
        "# to visualize progress in the animated GIF)\n",
        "seed = tf.random.normal([num_examples_to_generate, noise_dim])\n",
        "@tf.function\n",
        "def train_step(images):\n",
        "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "      generated_images = generator(noise, training=True)\n",
        "      real_output = discriminator(images, training=True)\n",
        "      fake_output = discriminator(generated_images, training=True)\n",
        "      gen_loss = generator_loss(fake_output)\n",
        "      disc_loss = discriminator_loss(real_output, fake_output)\n",
        "    \n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "\n",
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    for image_batch in dataset:\n",
        "      train_step(image_batch)\n",
        "\n",
        "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "\n",
        "  # Generate after the final epoch\n",
        "  #display.clear_output(wait=True)\n",
        "  generate_and_save_images(generator, epochs, seed)\n",
        "\n",
        "\n",
        "idx2char = { v: k for k, v in char2idx.items() } \n",
        "\n",
        "def generate_and_save_images(model, epoch, test_input):\n",
        "  # Notice `training` is set to False.\n",
        "  # This is so all layers run in inference mode (batchnorm).\n",
        "  predictions = model(test_input, training=False)\n",
        "\n",
        "  fig = plt.figure(figsize=(20,20))\n",
        "\n",
        "  for i in range(predictions.shape[0]):\n",
        "      plt.subplot(4, 4, i+1)\n",
        "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "      plt.axis('off')\n",
        "      '''\n",
        "      res = []\n",
        "      for a in range(52):\n",
        "        line = []\n",
        "        for b in range(52):\n",
        "          print(predictions[i, a, b, 0])\n",
        "          line.append(idx2char[predictions[i, a, b, 0]])\n",
        "        res.append(''.join(line))\n",
        "      print(\"=============\" + i)\n",
        "      print(res)\n",
        "      print(\"=============\\n\\n\")\n",
        "      '''\n",
        "  plt.show()\n",
        "\n",
        "      \n",
        "splitted = np.split(normalized[0:normalized.shape[0]-normalized.shape[0]%52], normalized.shape[0] // 52)\n",
        "splitted = np.array(splitted).reshape(len(splitted), 52, 52, 1)\n",
        "print(splitted.shape)\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(splitted).shuffle(60000).batch(BATCH_SIZE)\n",
        "train(train_dataset, EPOCHS)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "'''\n",
        "plt.imshow(normalized[0:53,:], cmap='gray')\n",
        "plt.show()\n",
        "noise = tf.random.normal([1, 100])\n",
        "generated_image = gen(noise, training=False)\n",
        "plt.imshow(generated_image[0, :, :, 0], cmap='gray')\n",
        "'''\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gawSIEf_Ix31",
        "colab_type": "text"
      },
      "source": [
        "## **LSTM generation:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vE2hYSqAAtkn",
        "jupyter": {
          "source_hidden": true
        },
        "outputId": "a35b653e-ea16-47a3-d553-4c60110a4660",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 880
        }
      },
      "source": [
        "generator = Sequential([\n",
        "    Embedding(vocab_size, embedding_size,\n",
        "              batch_input_shape=(1, None)),\n",
        "    \n",
        "    LSTM(len_input, return_sequences = True, stateful=True),\n",
        "    \n",
        "    Dense(hidden_size, activation = relu), \n",
        "    \n",
        "    Dense(vocab_size)\n",
        "])\n",
        "\n",
        "generator.summary()\n",
        "\n",
        "# Import trained weights from RNN to generator\n",
        "generator.set_weights(RNN.get_weights())\n",
        "\n",
        "def generate_text(start_string, num_generate = 1000, temperature = 1.0):\n",
        "    \n",
        "    # Vectorize input string\n",
        "    input_eval = [char2idx[s] for s in start_string]  \n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "    \n",
        "    text_generated = [] # List to append predicted chars \n",
        "    \n",
        "    idx2char = { v: k for k, v in char2idx.items() }  # invert char-index mapping\n",
        "    \n",
        "    generator.reset_states()\n",
        "    \n",
        "    for i in range(num_generate):\n",
        "        predictions = generator(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        \n",
        "        # sample next char based on distribution and temperature\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "        \n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "        \n",
        "    return (start_string + ''.join(text_generated))\n",
        "\n",
        "init=\"\"\"\n",
        "Nel mezzo del cammin di nostra vita\n",
        "mi ritrovai per una selva oscura,\n",
        "chè la diritta via era smarrita.\n",
        "\"\"\"\n",
        "print(generate_text(init))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (1, None, 250)            15500     \n",
            "_________________________________________________________________\n",
            "gru_2 (GRU)                  (1, None, 1000)           3756000   \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (1, None, 250)            250250    \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (1, None, 62)             15562     \n",
            "=================================================================\n",
            "Total params: 4,037,312\n",
            "Trainable params: 4,037,312\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\n",
            "Nel mezzo del cammin di nostra vita\n",
            "mi ritrovai per una selva oscura,\n",
            "chè la diritta via era smarrita.\n",
            "  Ahi giustizia tutto mi rispuose;\n",
            "però è là giù porgoglio soverchia,\n",
            "ond'era in molin che 'n sè a ver de li avanza.\n",
            "  Virgilio inverso me così scintilla\".\n",
            "  E io: \"Se fuoro aperte aparti.\n",
            "  Cred'ire a mio vi fu bagli,\n",
            "ben fuomo sì, che non hai fummo proè,\n",
            "s'alcuna pien d'ogne altro bando.\n",
            "  Appresso un'altra con manife atte;\n",
            "  e sì come visie;\n",
            "  d'inquarvammo per la via di salvas letaggio,\n",
            "chi va di luce in luce vien quella clogge.\n",
            "  Or discendiamo a Virgilio congiunto;\n",
            "  e si rammona ciascun si picca!\n",
            "  Cenare, avanti dipartiro.\n",
            "  L'essercito di Fiesole, e fui pavrebbe sparte,\n",
            "e l'altro scese in lor più ch'altri 'l Manto:\n",
            "Gaullibile fier le terpenti d'Itani,\n",
            "  ditemi chi voi sì conventi e per senno\n",
            "li vostre condizion di qua dal rio.\n",
            "  Tutto che trova attivo quivi, tinopesta,\n",
            "  quando noi fummo ghiotto veni bene,\n",
            "conteneggiava e riguardar pasciuto,\n",
            "dir c'inavor, sì volsi dier loco;\n",
            "chè per te non vera, ed è non fera,\n",
            "  ristolilla, sostener lo sacrosorti,\n",
            "avore sbadenti: 'D'un'alto e \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}