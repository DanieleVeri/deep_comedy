{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOFS4Em9YEMLFkWBx5pADdp"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHRo2NsK-Rgd",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "e8faa3b1-8f99-4f56-e97d-d56dec1aadd6"
      },
      "source": [
        "#@title Import & seed\n",
        "import time\n",
        "import re\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import requests\n",
        "import collections\n",
        "import pickle\n",
        "import copy, random\n",
        "import nltk as nl\n",
        "nl.download('punkt')\n",
        "from itertools import zip_longest\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Reshape, BatchNormalization, Dense, Dropout, concatenate,\n",
        "    Embedding, LSTM, Dense, GRU, Bidirectional, Add\n",
        ")\n",
        "from tensorflow.keras.activations import elu, relu, softmax, sigmoid\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "np.random.seed(1234)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLZ0LreHzhhr",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 583
        },
        "outputId": "8e8eb2d2-0653-4904-96c3-298f4e883f94"
      },
      "source": [
        "#@title Setup wandb\n",
        "!pip install wandb\n",
        "!wandb login f57cb185d23a8b60d349a4ea02278a6eee82550a\n",
        "import wandb\n",
        "wandb.init(project=\"deep_comedy\", name=\"daniele-3\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.6/dist-packages (0.9.4)\n",
            "Requirement already satisfied: sentry-sdk>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (0.16.4)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (5.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.8.1)\n",
            "Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.5.4)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.1.7)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.0.1)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: gql==0.2.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (0.2.0)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: watchdog>=0.8.3 in /usr/local/lib/python3.6/dist-packages (from wandb) (0.10.3)\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.352.0)\n",
            "Requirement already satisfied: urllib3>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from sentry-sdk>=0.4.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from sentry-sdk>=0.4.0->wandb) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.6/dist-packages (from GitPython>=1.0.0->wandb) (4.0.5)\n",
            "Requirement already satisfied: graphql-core<2,>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from gql==0.2.0->wandb) (1.1)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.6/dist-packages (from gql==0.2.0->wandb) (2.3)\n",
            "Requirement already satisfied: pathtools>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from watchdog>=0.8.3->wandb) (0.1.2)\n",
            "Requirement already satisfied: smmap<4,>=3.0.1 in /usr/local/lib/python3.6/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (3.0.4)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[32mSuccessfully logged in to Weights & Biases!\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/veri/deep_comedy\" target=\"_blank\">https://app.wandb.ai/veri/deep_comedy</a><br/>\n",
              "                Run page: <a href=\"https://app.wandb.ai/veri/deep_comedy/runs/2q55qdo0\" target=\"_blank\">https://app.wandb.ai/veri/deep_comedy/runs/2q55qdo0</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "W&B Run: https://app.wandb.ai/veri/deep_comedy/runs/2q55qdo0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onTHkWW_YOHp",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Model\n",
        "\n",
        "vocab_size = 1797\n",
        "wandb.config.num_layers = 4\n",
        "wandb.config.d_model = 256\n",
        "wandb.config.dff = 1024\n",
        "wandb.config.num_heads = 4\n",
        "input_vocab_size = vocab_size\n",
        "target_vocab_size = vocab_size\n",
        "wandb.config.dropout = 0.1\n",
        "batch_len = 304\n",
        "EPOCHS = 100\n",
        "\n",
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "    return pos * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                            np.arange(d_model)[np.newaxis, :],\n",
        "                            d_model)\n",
        "    # apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    # apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "def create_padding_mask(seq):   \n",
        "    seq = tf.cast(tf.math.equal(seq, pad), tf.float32)\n",
        "    # add extra dimensions to add the padding\n",
        "    # to the attention logits.\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
        "\n",
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask  # (seq_len, seq_len)\n",
        "\n",
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    \"\"\"Calculate the attention weights.\n",
        "    q, k, v must have matching leading dimensions.\n",
        "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "    The mask has different shapes depending on its type(padding or look ahead) \n",
        "    but it must be broadcastable for addition.\n",
        "    \n",
        "    Args:\n",
        "        q: query shape == (..., seq_len_q, depth)\n",
        "        k: key shape == (..., seq_len_k, depth)\n",
        "        v: value shape == (..., seq_len_v, depth_v)\n",
        "        mask: Float tensor with shape broadcastable \n",
        "            to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "        \n",
        "    Returns:\n",
        "        output, attention_weights\n",
        "    \"\"\"\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "    # scale matmul_qk\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "    # add the mask to the scaled tensor.\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)  \n",
        "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "    # add up to 1.\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "    return output, attention_weights\n",
        "\n",
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "    ])\n",
        "\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        assert d_model % self.num_heads == 0\n",
        "        self.depth = d_model // self.num_heads\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "            \n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"Split the last dimension into (num_heads, depth).\n",
        "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "        \n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "        \n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "        \n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "        \n",
        "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "        concat_attention = tf.reshape(scaled_attention, \n",
        "                                    (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "        return output, attention_weights\n",
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "    def __call__(self, x, training, mask):\n",
        "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "        \n",
        "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "        \n",
        "        return out2\n",
        "\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "    \n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "        \n",
        "    def __call__(self, x, enc_output, training, \n",
        "            look_ahead_mask, padding_mask):\n",
        "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "        \n",
        "        attn2, attn_weights_block2 = self.mha2(\n",
        "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "        \n",
        "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
        "        \n",
        "        return out3, attn_weights_block1, attn_weights_block2\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "                maximum_position_encoding, rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)   ########## rm ??????\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
        "                                                self.d_model)\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
        "                        for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "            \n",
        "    def __call__(self, x, training, mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        # adding embedding and position encoding.\n",
        "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "        x = self.dropout(x, training=training)\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "        return x  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "                maximum_position_encoding, rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
        "                        for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "    def __call__(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "        x = self.dropout(x, training=training)\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                                look_ahead_mask, padding_mask)\n",
        "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "        \n",
        "        # x.shape == (batch_size, target_seq_len, d_model)\n",
        "        return x, attention_weights\n",
        "\n",
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
        "                target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
        "                            input_vocab_size, pe_input, rate)\n",
        "\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
        "                            target_vocab_size, pe_target, rate)\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "        \n",
        "    def __call__(self, inp, tar, training, enc_padding_mask, \n",
        "            look_ahead_mask, dec_padding_mask):\n",
        "\n",
        "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
        "        \n",
        "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "        dec_output, attention_weights = self.decoder(\n",
        "            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "        \n",
        "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "        \n",
        "        return final_output, attention_weights\n",
        "\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "        \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "        \n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "learning_rate = CustomSchedule(wandb.config.d_model)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
        "                                     epsilon=1e-9)\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "    name='train_accuracy')\n",
        "\n",
        "val_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "    name='train_accuracy')\n",
        "\n",
        "transformer = Transformer(wandb.config.num_layers, wandb.config.d_model, \n",
        "                          wandb.config.num_heads, wandb.config.dff,\n",
        "                          input_vocab_size, target_vocab_size, \n",
        "                          pe_input=input_vocab_size, \n",
        "                          pe_target=target_vocab_size,\n",
        "                          rate=wandb.config.dropout)\n",
        "def create_masks(inp, tar):\n",
        "    # Encoder padding mask\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "    \n",
        "    # Used in the 2nd attention block in the decoder.\n",
        "    # This padding mask is used to mask the encoder outputs.\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "    \n",
        "    # Used in the 1st attention block in the decoder.\n",
        "    # It is used to pad and mask future tokens in the input received by \n",
        "    # the decoder.\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "    \n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask\n",
        "\n",
        "checkpoint_path = \"./checkpoints/train\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "# if a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print ('Latest checkpoint restored!!')\n",
        "\n",
        "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
        "# execution. The function specializes to the precise shape of the argument\n",
        "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
        "# batch sizes (the last batch is smaller), use input_signature to specify\n",
        "# more generic shapes.\n",
        "\n",
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, batch_len), dtype=tf.int64),\n",
        "    tf.TensorSpec(shape=(None, batch_len), dtype=tf.int64),\n",
        "]\n",
        "\n",
        "@tf.function()#(input_signature=train_step_signature)\n",
        "def train_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _ = transformer(inp, tar_inp, \n",
        "                                    True, \n",
        "                                    enc_padding_mask, \n",
        "                                    combined_mask, \n",
        "                                    dec_padding_mask)\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "        gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "    \n",
        "    train_loss(loss)\n",
        "    train_accuracy(tar_real, predictions)\n",
        "\n",
        "@tf.function()#(input_signature=train_step_signature)\n",
        "def val_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "    \n",
        "    predictions, _ = transformer(inp, tar_inp, \n",
        "                                False, \n",
        "                                enc_padding_mask, \n",
        "                                combined_mask, \n",
        "                                dec_padding_mask)\n",
        "    loss = loss_function(tar_real, predictions)\n",
        "    \n",
        "    val_loss(loss)\n",
        "    val_accuracy(tar_real, predictions)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GI14h2PZX70w",
        "colab_type": "code",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "outputId": "9138eec2-7faa-4bd5-fbdc-3ce655dbffd9"
      },
      "source": [
        "#@title Preprocessing\n",
        "\n",
        "def get_hyp_lm_tercets(tercets):\n",
        "    new_tercets = []\n",
        "    for tercet in tercets:\n",
        "        new_tercets.append([])\n",
        "        for verse in tercet:\n",
        "            new_tercets[-1].append([])\n",
        "            for hyp_w in verse:\n",
        "                new_tercets[-1][-1].extend(hyp_w)\n",
        "                new_tercets[-1][-1].append('<SEP>')\n",
        "            new_tercets[-1][-1] = new_tercets[-1][-1][:-1]\n",
        "\n",
        "    return new_tercets\n",
        "\n",
        "def is_vowel(c):\n",
        "    return c in 'aeiouAEIOUàìíèéùúüòï'\n",
        "\n",
        "def unsplittable_cons():\n",
        "    u_cons = []\n",
        "    for c1 in ('b', 'c', 'd', 'f', 'g', 'p', 't', 'v'):\n",
        "        for c2 in ('l', 'r'):\n",
        "            u_cons.append(c1 + c2)\n",
        "\n",
        "    others = ['gn', 'gh', 'ch']\n",
        "    u_cons.extend(others)\n",
        "    return u_cons\n",
        "\n",
        "\n",
        "def are_cons_to_split(c1, c2):\n",
        "    to_split = ('cq', 'cn', 'lm', 'rc', 'bd', 'mb', 'mn', 'ld', 'ng', 'nd', 'tm', 'nv', 'nc', 'ft', 'nf', 'gm', 'fm', 'rv', 'fp')\n",
        "    return (c1 + c2) in to_split or (not is_vowel(c1) and (c1 == c2)) or ((c1 + c2) not in unsplittable_cons()) and (\n",
        "        (not is_vowel(c1)) and (not is_vowel(c2)) and c1 != 's')\n",
        "\n",
        "\n",
        "def is_diphthong(c1, c2):\n",
        "    return (c1 + c2) in ('ia', 'ie', 'io', 'iu', 'ua', 'ue', 'uo', 'ui', 'ai', 'ei', 'oi', 'ui', 'au', 'eu', 'ïe', 'iú', 'iù')\n",
        "\n",
        "\n",
        "def is_triphthong(c1, c2, c3):\n",
        "    return (c1 + c2 + c3) in ('iai', 'iei', 'uoi', 'uai', 'uei', 'iuo')\n",
        "\n",
        "\n",
        "def is_toned_vowel(c):\n",
        "    return c in 'àìèéùòï'\n",
        "\n",
        "def has_vowels(sy):\n",
        "    for c in sy:\n",
        "        if is_vowel(c):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def hyphenation(word):\n",
        "    \"\"\"\n",
        "    Split word in syllables\n",
        "    :param word: input string\n",
        "    :return: a list containing syllables of the word\n",
        "    \"\"\"\n",
        "    if not word or word == '':\n",
        "        return []\n",
        "    # elif len(word) == 3 and (is_vowel(word[1]) and is_vowel(word[2]) and not is_toned_vowel(word[2]) and (\n",
        "    #     not is_diphthong(word[1], word[2]) or (word[1] == 'i'))):\n",
        "    elif len(word) == 3 and (is_vowel(word[1]) and is_vowel(word[2]) and not is_toned_vowel(word[2]) and (\n",
        "        not is_diphthong(word[1], word[2]))):\n",
        "        return [word[:2]] + [word[2]]\n",
        "    elif len(word) == 3 and is_vowel(word[0]) and not is_vowel(word[1]) and is_vowel(word[2]):\n",
        "        return [word[:2]] + [word[2]]\n",
        "    elif len(word) == 3:\n",
        "        return [word]\n",
        "\n",
        "    syllables = []\n",
        "    is_done = False\n",
        "    count = 0\n",
        "    while not is_done and count <= len(word) - 1:\n",
        "        syllables.append('')\n",
        "        c = word[count]\n",
        "        while not is_vowel(c) and count < len(word) - 1:\n",
        "            syllables[-1] = syllables[-1] + c\n",
        "            count += 1\n",
        "            c = word[count]\n",
        "\n",
        "        syllables[-1] = syllables[-1] + word[count]\n",
        "\n",
        "        if count == len(word) - 1:\n",
        "            is_done = True\n",
        "        else:\n",
        "            count += 1\n",
        "\n",
        "            if count < len(word) and not is_vowel(word[count]):\n",
        "                if count == len(word) - 1:\n",
        "                    syllables[-1] += word[count]\n",
        "                    count += 1\n",
        "                elif count + 1 < len(word) and are_cons_to_split(word[count], word[count + 1]):\n",
        "                    syllables[-1] += word[count]\n",
        "                    count += 1\n",
        "                elif count + 2 < len(word) and not is_vowel(word[count + 1]) and not is_vowel(word[count + 2]) and word[\n",
        "                    count] != 's':\n",
        "                    syllables[-1] += word[count]\n",
        "                    count += 1\n",
        "            elif count < len(word):\n",
        "                if count + 1 < len(word) and is_triphthong(word[count - 1], word[count], word[count + 1]):\n",
        "                    syllables[-1] += word[count] + word[count + 1]\n",
        "                    count += 2\n",
        "                elif is_diphthong(word[count - 1], word[count]):\n",
        "                    syllables[-1] += word[count]\n",
        "                    count += 1\n",
        "\n",
        "                if count + 1 < len(word) and are_cons_to_split(word[count], word[count + 1]):\n",
        "                    syllables[-1] += word[count]\n",
        "                    count += 1\n",
        "\n",
        "            else:\n",
        "                is_done = True\n",
        "\n",
        "    if not has_vowels(syllables[-1]) and len(syllables) > 1:\n",
        "        syllables[-2] = syllables[-2] + syllables[-1]\n",
        "        syllables = syllables[:-1]\n",
        "\n",
        "    return syllables\n",
        "\n",
        "\n",
        "\n",
        "def get_dc_hyphenation(canti):\n",
        "    hyp_canti, hyp_tokens = [], []\n",
        "    for canto in canti:\n",
        "        hyp_canti.append([])\n",
        "        for verso in canto:\n",
        "            syllables = seq_hyphentation(verso)\n",
        "            hyp_canti[-1].append(syllables)\n",
        "            for syllable in syllables:\n",
        "                hyp_tokens.extend(syllable)\n",
        "\n",
        "    return hyp_canti, hyp_tokens\n",
        "\n",
        "\n",
        "def seq_hyphentation(words):\n",
        "    \"\"\"\n",
        "    Converts words in a list of strings into lists of syllables\n",
        "    :param words: a list of words (strings)\n",
        "    :return: a list of lists containing word syllables\n",
        "    \"\"\"\n",
        "    return [hyphenation(w) for w in words]\n",
        "\n",
        "\n",
        "def get_dc_cantos(filename, encoding=None):\n",
        "    # raw_data = read_words(filename=filename)\n",
        "    cantos, words, raw = [], [], []\n",
        "    with open(filename, \"r\", encoding=encoding) as f:\n",
        "        for line in f:\n",
        "            sentence = line.strip()\n",
        "            sentence = str.replace(sentence, \"\\.\", \" \\. \")\n",
        "            sentence = str.replace(sentence, \"[\", '')\n",
        "            sentence = str.replace(sentence, \"]\", '')\n",
        "            sentence = str.replace(sentence, \"-\", '')\n",
        "            sentence = str.replace(sentence, \";\", \" ; \")\n",
        "            sentence = str.replace(sentence, \",\", \" , \")\n",
        "            # sentence = str.replace(sentence, \" \\'\", '')\n",
        "            sentence = str.replace(sentence, \"\\'\", ' \\' ')\n",
        "            if len(sentence) > 1:\n",
        "                # sentence = sentence.translate(string.punctuation)\n",
        "                tokenized_sentence = nl.word_tokenize(sentence)\n",
        "                # tokenized_sentence = sentence.split()\n",
        "                tokenized_sentence = [w.lower() for w in tokenized_sentence if len(w) > 0]\n",
        "                tokenized_sentence = [w for w in tokenized_sentence if \",\" not in w]\n",
        "                tokenized_sentence = [w for w in tokenized_sentence if \".\" not in w]\n",
        "                tokenized_sentence = [w for w in tokenized_sentence if \":\" not in w]\n",
        "                tokenized_sentence = [w for w in tokenized_sentence if \";\" not in w]\n",
        "                tokenized_sentence = [w for w in tokenized_sentence if \"«\" not in w]\n",
        "                tokenized_sentence = [w for w in tokenized_sentence if \"»\" not in w]\n",
        "                # ts = []\n",
        "                ts = tokenized_sentence\n",
        "                # [ts.extend(re.split(\"(\\')\", e)) for e in tokenized_sentence]\n",
        "                tokenized_sentence = [w for w in ts if len(w) > 0]\n",
        "\n",
        "                if len(tokenized_sentence) == 2:\n",
        "                    cantos.append([])\n",
        "                    raw.append([])\n",
        "                elif len(tokenized_sentence) > 2:\n",
        "                    raw[-1].append(sentence)\n",
        "                    cantos[-1].append(tokenized_sentence)\n",
        "                    words.extend(tokenized_sentence)\n",
        "\n",
        "    return cantos, words, raw\n",
        "\n",
        "\n",
        "def create_tercets(cantos):\n",
        "    tercets = []\n",
        "    for i,canto in enumerate(cantos):\n",
        "        for v,verse in enumerate(canto):\n",
        "            if v%3 == 0:\n",
        "                tercets.append([])\n",
        "\n",
        "            tercets[-1].append(verse)\n",
        "        tercets = tercets[:-1]  # removes the last malformed tercets (only 2 verses)\n",
        "\n",
        "    return tercets\n",
        "\n",
        "def pad_list(l, pad_token, max_l_size, keep_lasts=False, pad_right=True):\n",
        "    \"\"\"\n",
        "    Adds a padding token to a list\n",
        "    inputs:\n",
        "    :param l: input list to pad.\n",
        "    :param pad_token: value to add as padding.\n",
        "    :param max_l_size: length of the new padded list to return,\n",
        "    it truncates lists longer that 'max_l_size' without adding\n",
        "    padding values.\n",
        "    :param keep_lasts: If True, preserves the max_l_size last elements\n",
        "    of a sequence (by keeping the same order).  E.g.:\n",
        "    if keep_lasts is True and max_l_size=3 [1,2,3,4] becomes [2,3,4].\n",
        "\n",
        "\n",
        "    :return: the list padded or truncated.\n",
        "    \"\"\"\n",
        "    to_pad = []\n",
        "    max_l = min(max_l_size, len(l))  # maximum len\n",
        "    l_init = len(l) - max_l if len(l) > max_l and keep_lasts else 0  # initial position where to sample from the list\n",
        "    l_end = len(l) if len(l) > max_l and keep_lasts else max_l\n",
        "    for i in range(l_init, l_end):\n",
        "        to_pad.append(l[i])\n",
        "\n",
        "    # for j in range(len(l), max_l_size):\n",
        "    #     to_pad.append(pad_token)\n",
        "    pad_tokens = [pad_token] * (max_l_size-len(l))\n",
        "    padded_l = to_pad + pad_tokens if pad_right else pad_tokens + to_pad\n",
        "\n",
        "    return padded_l\n",
        "\n",
        "\n",
        "def save_data(data, file):\n",
        "    with open(file, 'wb') as output:\n",
        "        pickle.dump(data, output, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def load_data(file):\n",
        "    with open(file, 'rb') as obj:\n",
        "        return pickle.load(obj)\n",
        "\n",
        "def print_and_write(file, s):\n",
        "    print(s)\n",
        "    file.write(s)\n",
        "\n",
        "\n",
        "class Vocabulary(object):\n",
        "    def __init__(self, vocab_size=None):\n",
        "        self.dictionary = dict()\n",
        "        self.rev_dictionary = dict()\n",
        "        self.count = []\n",
        "        self.special_tokens = []\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    def build_vocabulary_from_counts(self, count, special_tokens=[]):\n",
        "        \"\"\"\n",
        "        Sets all the attributes of the Vocabulary object.\n",
        "        :param count: a list of lists as follows: [['token', number_of_occurrences],...]\n",
        "        :param special_tokens: a list of strings. E.g. ['<EOS>', '<PAD>',...]\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "\n",
        "        dictionary = dict()\n",
        "        for word, _ in count:\n",
        "            dictionary[word] = len(dictionary)\n",
        "\n",
        "        # adding eventual special tokens to the dictionary (e.g. <EOS>,<PAD> etc..)\n",
        "        d = len(dictionary)\n",
        "        for i, token in enumerate(special_tokens):\n",
        "            dictionary[token] = d + i\n",
        "\n",
        "        self.count = count\n",
        "        self.dictionary = dictionary\n",
        "        self.rev_dictionary = dict(zip(self.dictionary.values(), self.dictionary.keys()))\n",
        "        self.special_tokens = special_tokens\n",
        "        self.vocab_size = len(dictionary)\n",
        "\n",
        "    def build_vocabulary_from_tokens(self, tokens, vocabulary_size=None, special_tokens=[]):\n",
        "        \"\"\"\n",
        "        Given a list of tokens, it sets the Vocabulary object attributes by constructing\n",
        "        a dictionary mapping each token to a unique id.\n",
        "        :param tokens: a list of strings.\n",
        "         E.g. [\"the\", \"cat\", \"is\", ... \".\", \"the\", \"house\" ,\"is\" ...].\n",
        "         NB: Here you should put all your token instances of the corpus.\n",
        "        :param vocabulary_size: The number of elements of your vocabulary. If there are more\n",
        "        than 'vocabulary_size' elements on tokens, it considers only the 'vocabulary_size'\n",
        "        most frequent ones.\n",
        "        :param special_tokens: Optional. A list of strings. Useful to add special tokens in vocabulary.\n",
        "        If you don't have any, keep it empty.\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "\n",
        "        vocabulary_size = vocabulary_size if vocabulary_size is not None else self.vocab_size\n",
        "        vocabulary_size = vocabulary_size - (len(special_tokens) + 1) if vocabulary_size else None\n",
        "        # counts occurrences of each token\n",
        "        count = [['<UNK>', -1]]\n",
        "        count.extend(collections.Counter(tokens).most_common(vocabulary_size))  # takes only the most frequent ones, if size is None takes them all\n",
        "        self.build_vocabulary_from_counts(count, special_tokens)  # actually build the vocabulary\n",
        "        self._set_unk_count(tokens)  # set the number of OOV instances\n",
        "\n",
        "    @staticmethod\n",
        "    def merge_vocabulary(vocab0, vocab1, vocabulary_size=-1):\n",
        "        \"\"\"\n",
        "        Merge two Vocabulary objects into a new one.\n",
        "        :param vocab0: first Vocabulary object\n",
        "        :param vocab1: second Vocabulary object\n",
        "        :param vocabulary_size: parameter to decide the merged vocabulary size.\n",
        "        With default value -1, all the words of both vocabularies are preserved.\n",
        "        When set to 0, the size of the vocabulary is set to the size of vocab0,\n",
        "        when set to 1 it is kept the size of vocab1.\n",
        "        :return: a new vocabulary\n",
        "        \"\"\"\n",
        "        # get size of the new vocabulary\n",
        "        vocab_size = vocab0.vocab_size + vocab1.vocab_size if vocabulary_size == -1 else vocabulary_size\n",
        "        merged_special_tokens = list(set(vocab0.special_tokens) | set(vocab1.special_tokens))\n",
        "\n",
        "        # merge the counts from the two vocabularies and then selects the most_common tokens\n",
        "        merged_counts = collections.Counter(dict(vocab0.count)) + collections.Counter(dict(vocab1.count))\n",
        "        merged_counts = merged_counts.most_common(vocab_size)\n",
        "        count = [['<UNK>', -1]]\n",
        "        count.extend(merged_counts)\n",
        "\n",
        "        # create the new vocabulary\n",
        "        merged_vocab = Vocabulary(vocab_size)\n",
        "        merged_vocab.build_vocabulary_from_counts(count, merged_special_tokens)\n",
        "        return merged_vocab\n",
        "\n",
        "    @staticmethod\n",
        "    def merge_vocabularies(vocab_list, vocab_size=None):\n",
        "        \"\"\"\n",
        "        Join a list of vocabularies into a new one.\n",
        "        :param vocab_list: a list of Vocabulary objects\n",
        "        :param vocab_size: the maximum size of the merged vocabulary.\n",
        "        :return: a vocabulary merging them all.\n",
        "        \"\"\"\n",
        "        vocab_size = vocab_size if vocab_size else sum([v.vocab_size for v in vocab_list])\n",
        "        merged_vocab = Vocabulary(vocab_size)\n",
        "        for voc in vocab_list:\n",
        "            merged_vocab = Vocabulary.merge_vocabulary(merged_vocab, voc, vocab_size)\n",
        "        return merged_vocab\n",
        "\n",
        "    def string2id(self, dataset):\n",
        "        \"\"\"\n",
        "        Converts a dataset of strings into a dataset of ids according to the object dictionary.\n",
        "        :param dataset: any string-based dataset with any nested lists.\n",
        "        :return: a new dataset, with the same shape of dataset, where each string is mapped into its\n",
        "        corresponding id associated in the dictionary (0 for unknown tokens).\n",
        "        \"\"\"\n",
        "\n",
        "        def _recursive_call(items):\n",
        "            new_items = []\n",
        "            for item in items:\n",
        "                if isinstance(item, str) or isinstance(item, int) or isinstance(item, float):\n",
        "                    new_items.append(self.word2id(item))\n",
        "                else:\n",
        "                    new_items.append(_recursive_call(item))\n",
        "            return new_items\n",
        "\n",
        "        return _recursive_call(dataset)\n",
        "\n",
        "    def id2string(self, dataset):\n",
        "        \"\"\"\n",
        "        Converts a dataset of integer ids into a dataset of string according to the reverse dictionary.\n",
        "        :param dataset: any int-based dataset with any nested lists. Allowed types are int, np.int32, np.int64.\n",
        "        :return: a new dataset, with the same shape of dataset, where each token is mapped into its\n",
        "        corresponding string associated in the reverse dictionary.\n",
        "        \"\"\"\n",
        "        def _recursive_call(items):\n",
        "            new_items = []\n",
        "            for item in items:\n",
        "                if isinstance(item, int) or isinstance(item, np.int) or isinstance(item, np.int32) or isinstance(item, np.int64):\n",
        "                    new_items.append(self.id2word(item))\n",
        "                else:\n",
        "                    new_items.append(_recursive_call(item))\n",
        "            return new_items\n",
        "\n",
        "        return _recursive_call(dataset)\n",
        "\n",
        "    def word2id(self, item):\n",
        "        \"\"\"\n",
        "        Maps a string token to its corresponding id.\n",
        "        :param item: a string.\n",
        "        :return: If the token belongs to the vocabulary, it returns an integer id > 0, otherwise\n",
        "        it returns the value associated to the unknown symbol, that is typically 0.\n",
        "        \"\"\"\n",
        "        return self.dictionary[item] if item in self.dictionary else self.dictionary['<UNK>']\n",
        "\n",
        "    def id2word(self, token_id):\n",
        "        \"\"\"\n",
        "        Maps an integer token to its corresponding string.\n",
        "        :param token_id: an integer.\n",
        "        :return: If the id belongs to the vocabulary, it returns the string\n",
        "        associated to it, otherwise it returns the string associated\n",
        "        to the unknown symbol, that is '<UNK>'.\n",
        "        \"\"\"\n",
        "\n",
        "        return self.rev_dictionary[token_id] if token_id in self.rev_dictionary else self.rev_dictionary[self.dictionary['<UNK>']]\n",
        "\n",
        "    def get_unk_count(self):\n",
        "        return self.count[0][1]\n",
        "\n",
        "    def _set_unk_count(self, tokens):\n",
        "        \"\"\"\n",
        "        Sets the number of OOV instances in the tokens provided\n",
        "        :param tokens: a list of tokens\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "        data = list()\n",
        "        unk_count = 0\n",
        "        for word in tokens:\n",
        "            if word in self.dictionary:\n",
        "                index = self.dictionary[word]\n",
        "            else:\n",
        "                index = 0  # dictionary['<UNK>']\n",
        "                unk_count += 1\n",
        "            data.append(index)\n",
        "        self.count[0][1] = unk_count\n",
        "\n",
        "    def add_element(self, name, is_special_token=False):\n",
        "        if name not in self.dictionary:\n",
        "            self.vocab_size += 1\n",
        "            self.dictionary[name] = self.vocab_size\n",
        "            self.rev_dictionary[self.vocab_size] = name\n",
        "\n",
        "            if is_special_token:\n",
        "                self.special_tokens = list(self.special_tokens)\n",
        "                self.special_tokens.append(name)\n",
        "\n",
        "            self.count.append([name, 1])\n",
        "\n",
        "    def set_vocabulary(self, dictionary, rev_dictionary, special_tokens, vocab_size):\n",
        "        self.dictionary = dictionary,\n",
        "        self.rev_dictionary = rev_dictionary\n",
        "        self.special_tokens = special_tokens\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    @staticmethod\n",
        "    def load_vocabulary(filename):\n",
        "        return load_data(filename)\n",
        "\n",
        "    def save_vocabulary(self, filename):\n",
        "        save_data(self, filename)\n",
        "\n",
        "class SyLMDataset(object):\n",
        "    def __init__(self, config, sy_vocab=None):\n",
        "        self.config = config\n",
        "        self.vocabulary = sy_vocab\n",
        "\n",
        "        self.raw_train_x = []\n",
        "        self.raw_val_x = []\n",
        "        self.raw_test_x = []\n",
        "        self.raw_x = []\n",
        "\n",
        "        self.train_x, self.train_y = [], []\n",
        "        self.val_x, self.val_y = [], []\n",
        "        self.test_x, self.test_y = [], []\n",
        "        self.x, self.y = [], []\n",
        "\n",
        "    def initialize(self, sess):\n",
        "        pass\n",
        "\n",
        "    def load(self, sources):\n",
        "        \"\"\"\n",
        "        Extract raw texts form sources and gather them all together.\n",
        "        :param sources: a string or an iterable of strings containing the file(s)\n",
        "        to process in order to build the dataset.\n",
        "        :return: a list of raw strings.\n",
        "        \"\"\"\n",
        "        return NotImplementedError\n",
        "\n",
        "    def build(self, sources, split_size=0.8):\n",
        "        \"\"\"\n",
        "        :param sources: a string or an iterable of strings containing the file(s)\n",
        "        to process in order to build the dataset.\n",
        "        :param split_size: the size to split the dataset, set >=1.0 to not split.\n",
        "        \"\"\"\n",
        "\n",
        "        raw_x = self.load(sources)\n",
        "        # raw_x = self.tokenize([self.preprocess(ex) for ex in raw_x])  # fixme\n",
        "        # splitting data\n",
        "        self.raw_x = raw_x\n",
        "        if split_size < 1.0:\n",
        "            self.raw_train_x, self.raw_test_x = self.split(self.raw_x, train_size=split_size)\n",
        "            self.raw_train_x, self.raw_val_x = self.split(self.raw_train_x, train_size=split_size)\n",
        "        else:\n",
        "            self.raw_train_x = self.raw_x\n",
        "\n",
        "        if self.vocabulary is None:\n",
        "            # creates vocabulary\n",
        "            tokens = [item for sublist in self.raw_train_x for item in sublist]  # get tokens\n",
        "            special_tokens = (\"<GO>\", \"<PAD>\", \"<SEP>\", \"<EOS>\", \"<EOV>\")\n",
        "            self._create_vocab(tokens, special_tokens=special_tokens)\n",
        "\n",
        "        # creates x,y for train\n",
        "        self.train_x = self._build_dataset(self.raw_train_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "        self.train_y = self._build_dataset(self.raw_train_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "\n",
        "        # creates x,y for validation\n",
        "        self.val_x = self._build_dataset(self.raw_val_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "        self.val_y = self._build_dataset(self.raw_val_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "\n",
        "        # creates x,y for validation\n",
        "        self.test_x = self._build_dataset(self.raw_test_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "        self.test_y = self._build_dataset(self.raw_test_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "\n",
        "    def _create_vocab(self, tokens, special_tokens=(\"<PAD>\", \"<GO>\", \"<SEP>\", \"<EOV>\", \"<EOS>\")):\n",
        "        \"\"\"\n",
        "        Create the vocabulary. Special tokens can be added to the tokens obtained from\n",
        "        the corpus.\n",
        "        :param tokens: a list of all the tokens in the corpus. Each token is a string.\n",
        "        :param special_tokens: a list of strings.\n",
        "        \"\"\"\n",
        "        vocab = Vocabulary(vocab_size=self.config.input_vocab_size)\n",
        "        vocab.build_vocabulary_from_tokens(tokens, special_tokens=special_tokens)\n",
        "        self.vocabulary = vocab\n",
        "\n",
        "    @staticmethod\n",
        "    def split(raw_data, train_size=0.8):\n",
        "        size = math.floor(len(raw_data)*train_size)\n",
        "        return raw_data[:size], raw_data[size:]\n",
        "\n",
        "    @staticmethod\n",
        "    def preprocess(txt):\n",
        "        return txt\n",
        "\n",
        "    @staticmethod\n",
        "    def shuffle(x):\n",
        "        return random.sample(x, len(x))\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenize(txt):\n",
        "        return txt\n",
        "\n",
        "    def _build_dataset(self, raw_data, max_len=100, insert_go=True, keep_lasts=False, pad_right=True, shuffle=True):\n",
        "        \"\"\"\n",
        "        Converts all the tokens in e1_raw_data by mapping each token with its corresponding\n",
        "        value in the dictionary. In case of token not in the dictionary, they are assigned to\n",
        "        a specific id. Each sequence is padded up to the seq_max_len setup in the config.\n",
        "\n",
        "        :param raw_data: list of sequences, each sequence is a list of tokens (strings).\n",
        "        :param max_len: max length of a sequence, crop longer and pad smaller ones.\n",
        "        :param insert_go: True to insert <GO>, False otherwise.\n",
        "        :param keep_lasts: True to truncate initial elements of a sequence.\n",
        "        :param pad_right: pad to the right (default value True), otherwise pads to left.\n",
        "        :param shuffle: Optional. If True data are shuffled.\n",
        "        :return: A list of sequences where each token in each sequence is an int id.\n",
        "        \"\"\"\n",
        "        dataset = []\n",
        "        for sentence in raw_data:\n",
        "            sentence_ids = [self.vocabulary.word2id(\"<GO>\")] if insert_go else []\n",
        "            sentence_ids.extend([self.vocabulary.word2id(w) for w in sentence])\n",
        "            sentence_ids.append(self.vocabulary.word2id(\"<EOS>\"))\n",
        "            sentence_ids = pad_list(sentence_ids, self.vocabulary.word2id(\"<PAD>\"), max_len, keep_lasts=keep_lasts, pad_right=pad_right)\n",
        "\n",
        "            dataset.append(sentence_ids)\n",
        "\n",
        "        if shuffle:\n",
        "            return random.sample(dataset, len(dataset))\n",
        "        else:\n",
        "            return dataset\n",
        "\n",
        "    def get_batches(self, batch_size=32, split_sel='train'):\n",
        "        \"\"\"\n",
        "        Iterator over the training set. Useful method to run experiments.\n",
        "        :param batch_size: size of the mini_batch\n",
        "        :return: input and target.\n",
        "        \"\"\"\n",
        "        if split_sel == 'train':\n",
        "            x, y = self.train_x, self.train_y\n",
        "        elif split_sel == 'val':\n",
        "            x, y = self.val_x, self.val_y\n",
        "        else:\n",
        "            x, y = self.test_x, self.test_y\n",
        "        \n",
        "        i = 0#random.randint(0, batch_size)\n",
        "        batches = []\n",
        "        eov = self.vocabulary.word2id(\"<EOV>\")\n",
        "        go = self.vocabulary.word2id(\"<GO>\")\n",
        "        # prepare batches\n",
        "        while i < len(x):\n",
        "            j = 0\n",
        "            batch_x, batch_y = [], []\n",
        "            while j < batch_size and i+j<len(x):\n",
        "                for c in x[i+j]:\n",
        "                  batch_x.append(c)\n",
        "                batch_x.append(eov)\n",
        "                for c in y[i+j]:\n",
        "                  batch_y.append(c)\n",
        "                batch_y.append(eov)\n",
        "                j += 1\n",
        "            i += batch_size\n",
        "            batches.append((batch_x, batch_y))\n",
        "\n",
        "        # supply\n",
        "        i = 0\n",
        "        while i < len(batches):\n",
        "            yield batches[i][0], batches[i][1]\n",
        "            i += 1\n",
        "\n",
        "class DanteSyLMDataset(SyLMDataset):\n",
        "    def __init__(self, config, sy_vocab=None):\n",
        "        \"\"\"\n",
        "        Class to create a dataset from Dante Alighieri's Divine Comedy.\n",
        "        :param config: a Config object\n",
        "        :param sy_vocab: (optional) a Vocabulary object where tokens of the dictionary\n",
        "        are syllables. If None, the vocabulary is create automatically from the source.\n",
        "        \"\"\"\n",
        "        super().__init__(config, sy_vocab)\n",
        "\n",
        "    def load(self, sources):\n",
        "        \"\"\"\n",
        "        Load examples from dataset\n",
        "        :param sources: data filepath.\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        canti, _, raw = get_dc_cantos(filename=sources)  # get raw data from file\n",
        "        canti, tokens = get_dc_hyphenation(canti)  # converts each\n",
        "\n",
        "        tercets = create_tercets(canti)\n",
        "        tercets = get_hyp_lm_tercets(tercets)\n",
        "\n",
        "        x = []\n",
        "        for tercet in tercets:\n",
        "            x.append([])\n",
        "            for verse in tercet:\n",
        "                x[-1].extend(verse)\n",
        "                x[-1].append(\"<EOV>\")\n",
        "\n",
        "        #x = self.shuffle(x)\n",
        "        return x\n",
        "\n",
        "def seq2str(seq):\n",
        "    def output2string(batch, rev_vocabulary, special_tokens, end_of_tokens):\n",
        "        to_print = ''\n",
        "        for token in batch:\n",
        "            if token in special_tokens:\n",
        "                to_print += ' '\n",
        "            elif end_of_tokens and token in end_of_tokens:\n",
        "                to_print += '\\n'\n",
        "            elif token in rev_vocabulary:\n",
        "                to_print += rev_vocabulary[token]\n",
        "            else:\n",
        "                to_print += '<UNK>'\n",
        "        return to_print\n",
        "\n",
        "    return output2string(seq, poetry_sy_lm_dataset.vocabulary.rev_dictionary,\n",
        "      special_tokens=[poetry_sy_lm_dataset.vocabulary.word2id(\"<PAD>\"), 0, poetry_sy_lm_dataset.vocabulary.word2id(\"<SEP>\"),\n",
        "                      poetry_sy_lm_dataset.vocabulary.word2id(\"<GO>\"), poetry_sy_lm_dataset.vocabulary.word2id(\"<EOS>\")],\n",
        "      end_of_tokens=[poetry_sy_lm_dataset.vocabulary.word2id(\"<EOV>\")])\n",
        "\n",
        "class cnfg:\n",
        "  vocab_size = 1884\n",
        "  input_vocab_size = 1884\n",
        "  emb_size = 300\n",
        "  sentence_max_len = 75\n",
        "\n",
        "config = cnfg()\n",
        "poetry_sy_lm_dataset = DanteSyLMDataset(config, sy_vocab=None)\n",
        "url = \"https://gitlab.com/zugo91/nlgpoetry/-/raw/release/data/la_divina_commedia.txt\"\n",
        "response = requests.get(url)\n",
        "response.encoding = 'ISO-8859-1'\n",
        "fi = open(\"divcom.txt\",\"w\")\n",
        "fi.write(response.text)\n",
        "fi.close()\n",
        "data_path = os.path.join(os.getcwd(), \"divcom.txt\")  # dataset location, here just the name of the source file\n",
        "poetry_sy_lm_dataset.build(data_path, split_size=0.9)  # actual creation of  vocabulary (if not provided) and dataset\n",
        "print(\"Train size: \" + str(len(poetry_sy_lm_dataset.train_y)))\n",
        "print(\"Val size: \" + str(len(poetry_sy_lm_dataset.val_y)))\n",
        "print(\"Test size: \" + str(len(poetry_sy_lm_dataset.test_y)))\n",
        "\n",
        "eov = poetry_sy_lm_dataset.vocabulary.word2id(\"<EOV>\")\n",
        "pad = poetry_sy_lm_dataset.vocabulary.word2id(\"<PAD>\")\n",
        "go = poetry_sy_lm_dataset.vocabulary.word2id(\"<GO>\")\n",
        "eos = poetry_sy_lm_dataset.vocabulary.word2id(\"<EOS>\")\n",
        "\n",
        "batches = [b for b in poetry_sy_lm_dataset.get_batches(4)]\n",
        "print(batches[0][0])\n",
        "print(batches[0][1])\n",
        "print(len(batches[0][0]))\n",
        "val_b = [b for b in poetry_sy_lm_dataset.get_batches(4, split_sel='val')]\n",
        "print(val_b[0][0])\n",
        "print(val_b[0][1])\n",
        "print(len(val_b[0][0]))\n",
        "test_b = [b for b in poetry_sy_lm_dataset.get_batches(4, split_sel='test')]\n",
        "print(test_b[0][0])\n",
        "print(test_b[0][1])\n",
        "print(len(test_b[0][0]))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train size: 3815\n",
            "Val size: 424\n",
            "Test size: 472\n",
            "[1792, 84, 1794, 339, 198, 1794, 50, 1794, 284, 188, 1794, 6, 1794, 24, 136, 1794, 37, 14, 1796, 27, 1794, 32, 61, 509, 1794, 18, 1794, 52, 5, 1794, 404, 47, 1794, 40, 276, 30, 1796, 89, 1794, 8, 1794, 6, 683, 14, 1794, 264, 1794, 92, 5, 1794, 655, 32, 14, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 585, 1794, 78, 9, 1794, 5, 1794, 238, 1794, 162, 1794, 92, 5, 1794, 57, 1794, 12, 45, 1794, 116, 30, 1796, 4, 63, 1794, 404, 47, 1794, 404, 748, 148, 1794, 4, 1794, 5, 1203, 1794, 4, 1794, 120, 10, 1796, 7, 1794, 84, 1794, 166, 536, 1794, 32, 24, 47, 1794, 8, 1794, 455, 30, 1794, 153, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 792, 1794, 3, 1794, 57, 1794, 5, 22, 30, 1794, 7, 1794, 49, 12, 1794, 57, 1794, 64, 1794, 144, 10, 1796, 22, 1794, 18, 1794, 416, 202, 1794, 50, 1794, 154, 1794, 46, 1794, 3, 1794, 41, 1794, 37, 1794, 61, 509, 1796, 6, 150, 1794, 193, 1794, 3, 1794, 15, 124, 1794, 12, 11, 1794, 46, 1794, 3, 1794, 53, 1794, 3, 1794, 405, 1794, 3, 1794, 456, 1794, 336, 10, 1796, 1796, 1792, 41, 1794, 31, 1794, 23, 1794, 154, 1794, 32, 238, 1794, 131, 1794, 3, 1794, 41, 1794, 405, 1794, 3, 1794, 257, 716, 1796, 792, 1794, 3, 1794, 92, 5, 1794, 163, 24, 1794, 6, 1794, 138, 24, 1794, 5, 1794, 60, 1794, 293, 9, 1796, 7, 1794, 8, 1794, 25, 30, 43, 1794, 264, 1794, 337, 539, 17, 702, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796]\n",
            "[1792, 84, 1794, 339, 198, 1794, 50, 1794, 284, 188, 1794, 6, 1794, 24, 136, 1794, 37, 14, 1796, 27, 1794, 32, 61, 509, 1794, 18, 1794, 52, 5, 1794, 404, 47, 1794, 40, 276, 30, 1796, 89, 1794, 8, 1794, 6, 683, 14, 1794, 264, 1794, 92, 5, 1794, 655, 32, 14, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 585, 1794, 78, 9, 1794, 5, 1794, 238, 1794, 162, 1794, 92, 5, 1794, 57, 1794, 12, 45, 1794, 116, 30, 1796, 4, 63, 1794, 404, 47, 1794, 404, 748, 148, 1794, 4, 1794, 5, 1203, 1794, 4, 1794, 120, 10, 1796, 7, 1794, 84, 1794, 166, 536, 1794, 32, 24, 47, 1794, 8, 1794, 455, 30, 1794, 153, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 792, 1794, 3, 1794, 57, 1794, 5, 22, 30, 1794, 7, 1794, 49, 12, 1794, 57, 1794, 64, 1794, 144, 10, 1796, 22, 1794, 18, 1794, 416, 202, 1794, 50, 1794, 154, 1794, 46, 1794, 3, 1794, 41, 1794, 37, 1794, 61, 509, 1796, 6, 150, 1794, 193, 1794, 3, 1794, 15, 124, 1794, 12, 11, 1794, 46, 1794, 3, 1794, 53, 1794, 3, 1794, 405, 1794, 3, 1794, 456, 1794, 336, 10, 1796, 1796, 1792, 41, 1794, 31, 1794, 23, 1794, 154, 1794, 32, 238, 1794, 131, 1794, 3, 1794, 41, 1794, 405, 1794, 3, 1794, 257, 716, 1796, 792, 1794, 3, 1794, 92, 5, 1794, 163, 24, 1794, 6, 1794, 138, 24, 1794, 5, 1794, 60, 1794, 293, 9, 1796, 7, 1794, 8, 1794, 25, 30, 43, 1794, 264, 1794, 337, 539, 17, 702, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796]\n",
            "304\n",
            "[1792, 4, 1794, 76, 150, 1794, 70, 1794, 27, 1794, 249, 1794, 4, 1794, 424, 1794, 3, 1794, 41, 1794, 642, 5, 1796, 64, 1794, 1353, 601, 40, 23, 1794, 5, 1794, 10, 1794, 31, 1794, 27, 1794, 17, 134, 6, 1796, 7, 1794, 15, 299, 1794, 15, 61, 1794, 21, 1794, 65, 63, 1794, 486, 161, 1794, 1014, 5, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 59, 1794, 159, 6, 1794, 3, 1794, 16, 1794, 25, 48, 1794, 89, 1794, 53, 1794, 27, 24, 32, 1794, 4, 1794, 3, 1794, 203, 6, 1796, 6, 1794, 65, 63, 1794, 37, 14, 1794, 27, 164, 1794, 84, 34, 1794, 255, 189, 1796, 21, 1794, 7, 1794, 128, 22, 1794, 7, 1794, 166, 13, 1794, 56, 1794, 166, 536, 1794, 624, 6, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 22, 1794, 18, 89, 1794, 3, 1794, 16, 1794, 45, 492, 1794, 5, 42, 29, 1794, 21, 1794, 7, 1794, 41, 1794, 25, 189, 1796, 35, 1794, 18, 76, 246, 1794, 37, 63, 1794, 4, 1794, 7, 1794, 127, 1794, 3, 1794, 217, 11, 14, 1796, 6, 1794, 234, 43, 1794, 6, 573, 106, 1794, 80, 1794, 3, 1794, 5, 562, 209, 1794, 26, 189, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 8, 1794, 66, 43, 1794, 246, 1794, 13, 156, 30, 1794, 577, 36, 1794, 4, 1794, 353, 14, 1796, 119, 55, 1794, 8, 1794, 66, 381, 250, 1794, 119, 55, 1794, 3, 1794, 16, 1794, 6, 349, 1796, 5, 1794, 7, 1794, 8, 1794, 184, 1794, 32, 275, 63, 1794, 57, 1794, 133, 1794, 33, 159, 14, 1794, 153, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796]\n",
            "[1792, 4, 1794, 76, 150, 1794, 70, 1794, 27, 1794, 249, 1794, 4, 1794, 424, 1794, 3, 1794, 41, 1794, 642, 5, 1796, 64, 1794, 1353, 601, 40, 23, 1794, 5, 1794, 10, 1794, 31, 1794, 27, 1794, 17, 134, 6, 1796, 7, 1794, 15, 299, 1794, 15, 61, 1794, 21, 1794, 65, 63, 1794, 486, 161, 1794, 1014, 5, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 59, 1794, 159, 6, 1794, 3, 1794, 16, 1794, 25, 48, 1794, 89, 1794, 53, 1794, 27, 24, 32, 1794, 4, 1794, 3, 1794, 203, 6, 1796, 6, 1794, 65, 63, 1794, 37, 14, 1794, 27, 164, 1794, 84, 34, 1794, 255, 189, 1796, 21, 1794, 7, 1794, 128, 22, 1794, 7, 1794, 166, 13, 1794, 56, 1794, 166, 536, 1794, 624, 6, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 22, 1794, 18, 89, 1794, 3, 1794, 16, 1794, 45, 492, 1794, 5, 42, 29, 1794, 21, 1794, 7, 1794, 41, 1794, 25, 189, 1796, 35, 1794, 18, 76, 246, 1794, 37, 63, 1794, 4, 1794, 7, 1794, 127, 1794, 3, 1794, 217, 11, 14, 1796, 6, 1794, 234, 43, 1794, 6, 573, 106, 1794, 80, 1794, 3, 1794, 5, 562, 209, 1794, 26, 189, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 8, 1794, 66, 43, 1794, 246, 1794, 13, 156, 30, 1794, 577, 36, 1794, 4, 1794, 353, 14, 1796, 119, 55, 1794, 8, 1794, 66, 381, 250, 1794, 119, 55, 1794, 3, 1794, 16, 1794, 6, 349, 1796, 5, 1794, 7, 1794, 8, 1794, 184, 1794, 32, 275, 63, 1794, 57, 1794, 133, 1794, 33, 159, 14, 1794, 153, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796]\n",
            "304\n",
            "[1792, 40, 1794, 98, 14, 1794, 119, 30, 1794, 184, 1794, 7, 1794, 44, 1794, 38, 1794, 121, 501, 1796, 6, 66, 14, 1794, 18, 1794, 34, 1794, 247, 1794, 106, 160, 10, 1794, 301, 432, 9, 1796, 36, 1794, 60, 8, 1794, 245, 8, 1794, 255, 30, 1794, 27, 1794, 6, 1065, 501, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 49, 190, 1794, 199, 22, 9, 1794, 56, 1794, 151, 12, 1794, 81, 38, 253, 9, 1796, 15, 8, 1794, 184, 1794, 265, 39, 1794, 6, 700, 645, 1794, 34, 1794, 230, 48, 1796, 7, 1794, 74, 289, 742, 1794, 12, 44, 1794, 131, 1794, 3, 1794, 53, 1794, 3, 1794, 456, 1794, 253, 9, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 118, 1794, 94, 8, 1794, 40, 1794, 114, 43, 1794, 251, 108, 39, 1794, 50, 1794, 203, 1794, 37, 48, 1796, 5, 1794, 197, 1794, 24, 123, 1794, 13, 559, 1794, 8, 340, 218, 1794, 19, 1794, 179, 37, 1796, 46, 1794, 3, 1794, 330, 1794, 176, 440, 1794, 214, 1794, 6, 1794, 65, 75, 1794, 1353, 211, 1794, 27, 48, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 149, 14, 1794, 12, 484, 1794, 6, 1794, 293, 20, 1794, 353, 37, 1794, 4, 1794, 173, 37, 1796, 12, 26, 1794, 20, 1794, 209, 43, 1794, 21, 101, 24, 1794, 50, 8, 1794, 82, 33, 1796, 18, 1794, 8, 1794, 162, 1794, 59, 1794, 79, 1794, 18, 1794, 34, 1794, 22, 29, 1794, 58, 36, 37, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796]\n",
            "[1792, 40, 1794, 98, 14, 1794, 119, 30, 1794, 184, 1794, 7, 1794, 44, 1794, 38, 1794, 121, 501, 1796, 6, 66, 14, 1794, 18, 1794, 34, 1794, 247, 1794, 106, 160, 10, 1794, 301, 432, 9, 1796, 36, 1794, 60, 8, 1794, 245, 8, 1794, 255, 30, 1794, 27, 1794, 6, 1065, 501, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 49, 190, 1794, 199, 22, 9, 1794, 56, 1794, 151, 12, 1794, 81, 38, 253, 9, 1796, 15, 8, 1794, 184, 1794, 265, 39, 1794, 6, 700, 645, 1794, 34, 1794, 230, 48, 1796, 7, 1794, 74, 289, 742, 1794, 12, 44, 1794, 131, 1794, 3, 1794, 53, 1794, 3, 1794, 456, 1794, 253, 9, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 118, 1794, 94, 8, 1794, 40, 1794, 114, 43, 1794, 251, 108, 39, 1794, 50, 1794, 203, 1794, 37, 48, 1796, 5, 1794, 197, 1794, 24, 123, 1794, 13, 559, 1794, 8, 340, 218, 1794, 19, 1794, 179, 37, 1796, 46, 1794, 3, 1794, 330, 1794, 176, 440, 1794, 214, 1794, 6, 1794, 65, 75, 1794, 1353, 211, 1794, 27, 48, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 149, 14, 1794, 12, 484, 1794, 6, 1794, 293, 20, 1794, 353, 37, 1794, 4, 1794, 173, 37, 1796, 12, 26, 1794, 20, 1794, 209, 43, 1794, 21, 101, 24, 1794, 50, 8, 1794, 82, 33, 1796, 18, 1794, 8, 1794, 162, 1794, 59, 1794, 79, 1794, 18, 1794, 34, 1794, 22, 29, 1794, 58, 36, 37, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796]\n",
            "304\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwdCuj3HsFux",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ae9a68b6-7e85-4946-d3a0-b69a1bf5b5cb"
      },
      "source": [
        " #@title Train loop\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    random.shuffle(batches)\n",
        "    start = time.time()\n",
        "    \n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    \n",
        "    for (batch, (inp, tar)) in enumerate(batches):\n",
        "        if (len(inp) != batch_len or len(tar) != batch_len):\n",
        "            print(\"discarded batch\", batch)\n",
        "            continue\n",
        "        train_step(np.expand_dims(inp, axis=0), np.expand_dims(tar, axis=0))\n",
        "        \n",
        "        if batch % 50 == 0:\n",
        "            print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
        "                epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
        "        \n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        ckpt_save_path = ckpt_manager.save()\n",
        "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n",
        "\n",
        "    print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, train_loss.result(), train_accuracy.result()))\n",
        "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))\n",
        "\n",
        "    wandb.log({\n",
        "        'train_loss': train_loss.result(),\n",
        "        'train_accuracy': train_accuracy.result()\n",
        "    }, step=epoch+1)\n",
        "\n",
        "    # validation\n",
        "    if epoch % 5 == 0:\n",
        "        loss_l, acc_l = [], []\n",
        "        for (batch, (inp, tar)) in enumerate(val_b):\n",
        "            val_loss.reset_states()\n",
        "            val_accuracy.reset_states()\n",
        "            \n",
        "            if (len(inp) != batch_len or len(tar) != batch_len):\n",
        "                print(\"discarded batch\", batch)\n",
        "                continue\n",
        "\n",
        "            val_step(np.expand_dims(inp, axis=0), np.expand_dims(tar, axis=0))\n",
        "\n",
        "            loss_l.append(val_loss.result())\n",
        "            acc_l.append(val_accuracy.result())\n",
        "\n",
        "        loss_mean = sum(loss_l)/len(loss_l)\n",
        "        acc_mean = sum(acc_l)/len(acc_l)\n",
        "        print('Epoch {} VALIDATION: Loss {:.4f} Accuracy {:.4f}\\n'.format(epoch + 1, loss_mean, acc_mean))\n",
        "\n",
        "        wandb.log({\n",
        "            'val_loss': loss_mean,\n",
        "            'val_accuracy': acc_mean\n",
        "        }, step=epoch+1)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 8.0082 Accuracy 0.0000\n",
            "Epoch 1 Batch 50 Loss 7.2223 Accuracy 0.1088\n",
            "Epoch 1 Batch 100 Loss 6.3848 Accuracy 0.1826\n",
            "Epoch 1 Batch 150 Loss 5.9508 Accuracy 0.2124\n",
            "Epoch 1 Batch 200 Loss 5.5902 Accuracy 0.2599\n",
            "Epoch 1 Batch 250 Loss 5.2695 Accuracy 0.2927\n",
            "Epoch 1 Batch 300 Loss 5.0028 Accuracy 0.3178\n",
            "Epoch 1 Batch 350 Loss 4.7782 Accuracy 0.3380\n",
            "discarded batch 400\n",
            "Epoch 1 Batch 450 Loss 4.4313 Accuracy 0.3678\n",
            "Epoch 1 Batch 500 Loss 4.2956 Accuracy 0.3789\n",
            "Epoch 1 Batch 550 Loss 4.1804 Accuracy 0.3886\n",
            "Epoch 1 Batch 600 Loss 4.0796 Accuracy 0.3967\n",
            "Epoch 1 Batch 650 Loss 3.9958 Accuracy 0.4037\n",
            "Epoch 1 Batch 700 Loss 3.9229 Accuracy 0.4098\n",
            "Epoch 1 Batch 750 Loss 3.8589 Accuracy 0.4149\n",
            "Epoch 1 Batch 800 Loss 3.8013 Accuracy 0.4199\n",
            "Epoch 1 Batch 850 Loss 3.7492 Accuracy 0.4242\n",
            "Epoch 1 Batch 900 Loss 3.7023 Accuracy 0.4282\n",
            "Epoch 1 Batch 950 Loss 3.6600 Accuracy 0.4316\n",
            "Epoch 1 Loss 3.6576 Accuracy 0.4318\n",
            "Time taken for 1 epoch: 62.70743131637573 secs\n",
            "\n",
            "Epoch 1 VALIDATION: Loss 2.8003 Accuracy 0.5063\n",
            "\n",
            "Epoch 2 Batch 0 Loss 2.8046 Accuracy 0.5083\n",
            "Epoch 2 Batch 50 Loss 2.8357 Accuracy 0.5026\n",
            "Epoch 2 Batch 100 Loss 2.8501 Accuracy 0.5015\n",
            "Epoch 2 Batch 150 Loss 2.8387 Accuracy 0.5025\n",
            "Epoch 2 Batch 200 Loss 2.8456 Accuracy 0.5014\n",
            "Epoch 2 Batch 250 Loss 2.8402 Accuracy 0.5025\n",
            "Epoch 2 Batch 300 Loss 2.8324 Accuracy 0.5039\n",
            "Epoch 2 Batch 350 Loss 2.8294 Accuracy 0.5042\n",
            "Epoch 2 Batch 400 Loss 2.8187 Accuracy 0.5056\n",
            "Epoch 2 Batch 450 Loss 2.8144 Accuracy 0.5059\n",
            "Epoch 2 Batch 500 Loss 2.8083 Accuracy 0.5064\n",
            "Epoch 2 Batch 550 Loss 2.8019 Accuracy 0.5073\n",
            "discarded batch 592\n",
            "Epoch 2 Batch 600 Loss 2.7970 Accuracy 0.5078\n",
            "Epoch 2 Batch 650 Loss 2.7904 Accuracy 0.5084\n",
            "Epoch 2 Batch 700 Loss 2.7851 Accuracy 0.5089\n",
            "Epoch 2 Batch 750 Loss 2.7793 Accuracy 0.5094\n",
            "Epoch 2 Batch 800 Loss 2.7768 Accuracy 0.5098\n",
            "Epoch 2 Batch 850 Loss 2.7707 Accuracy 0.5104\n",
            "Epoch 2 Batch 900 Loss 2.7666 Accuracy 0.5110\n",
            "Epoch 2 Batch 950 Loss 2.7604 Accuracy 0.5118\n",
            "Epoch 2 Loss 2.7606 Accuracy 0.5119\n",
            "Time taken for 1 epoch: 50.49932408332825 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 2.7730 Accuracy 0.4950\n",
            "Epoch 3 Batch 50 Loss 2.6602 Accuracy 0.5226\n",
            "Epoch 3 Batch 100 Loss 2.6371 Accuracy 0.5265\n",
            "Epoch 3 Batch 150 Loss 2.6219 Accuracy 0.5278\n",
            "Epoch 3 Batch 200 Loss 2.6201 Accuracy 0.5271\n",
            "Epoch 3 Batch 250 Loss 2.6203 Accuracy 0.5267\n",
            "Epoch 3 Batch 300 Loss 2.6129 Accuracy 0.5276\n",
            "Epoch 3 Batch 350 Loss 2.6113 Accuracy 0.5276\n",
            "Epoch 3 Batch 400 Loss 2.6101 Accuracy 0.5276\n",
            "Epoch 3 Batch 450 Loss 2.6070 Accuracy 0.5275\n",
            "Epoch 3 Batch 500 Loss 2.6019 Accuracy 0.5278\n",
            "discarded batch 536\n",
            "Epoch 3 Batch 550 Loss 2.5974 Accuracy 0.5283\n",
            "Epoch 3 Batch 600 Loss 2.5944 Accuracy 0.5284\n",
            "Epoch 3 Batch 650 Loss 2.5931 Accuracy 0.5283\n",
            "Epoch 3 Batch 700 Loss 2.5898 Accuracy 0.5283\n",
            "Epoch 3 Batch 750 Loss 2.5877 Accuracy 0.5281\n",
            "Epoch 3 Batch 800 Loss 2.5859 Accuracy 0.5282\n",
            "Epoch 3 Batch 850 Loss 2.5831 Accuracy 0.5284\n",
            "Epoch 3 Batch 900 Loss 2.5814 Accuracy 0.5285\n",
            "Epoch 3 Batch 950 Loss 2.5793 Accuracy 0.5287\n",
            "Epoch 3 Loss 2.5793 Accuracy 0.5287\n",
            "Time taken for 1 epoch: 50.28963041305542 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 2.4297 Accuracy 0.5512\n",
            "Epoch 4 Batch 50 Loss 2.4686 Accuracy 0.5394\n",
            "Epoch 4 Batch 100 Loss 2.4653 Accuracy 0.5403\n",
            "Epoch 4 Batch 150 Loss 2.4587 Accuracy 0.5418\n",
            "Epoch 4 Batch 200 Loss 2.4604 Accuracy 0.5419\n",
            "Epoch 4 Batch 250 Loss 2.4646 Accuracy 0.5412\n",
            "Epoch 4 Batch 300 Loss 2.4702 Accuracy 0.5406\n",
            "Epoch 4 Batch 350 Loss 2.4694 Accuracy 0.5396\n",
            "Epoch 4 Batch 400 Loss 2.4698 Accuracy 0.5399\n",
            "discarded batch 416\n",
            "Epoch 4 Batch 450 Loss 2.4687 Accuracy 0.5395\n",
            "Epoch 4 Batch 500 Loss 2.4695 Accuracy 0.5396\n",
            "Epoch 4 Batch 550 Loss 2.4678 Accuracy 0.5398\n",
            "Epoch 4 Batch 600 Loss 2.4664 Accuracy 0.5398\n",
            "Epoch 4 Batch 650 Loss 2.4630 Accuracy 0.5400\n",
            "Epoch 4 Batch 700 Loss 2.4613 Accuracy 0.5401\n",
            "Epoch 4 Batch 750 Loss 2.4592 Accuracy 0.5402\n",
            "Epoch 4 Batch 800 Loss 2.4603 Accuracy 0.5403\n",
            "Epoch 4 Batch 850 Loss 2.4603 Accuracy 0.5401\n",
            "Epoch 4 Batch 900 Loss 2.4590 Accuracy 0.5401\n",
            "Epoch 4 Batch 950 Loss 2.4573 Accuracy 0.5404\n",
            "Epoch 4 Loss 2.4572 Accuracy 0.5405\n",
            "Time taken for 1 epoch: 50.580798387527466 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 2.2983 Accuracy 0.5611\n",
            "Epoch 5 Batch 50 Loss 2.3528 Accuracy 0.5483\n",
            "Epoch 5 Batch 100 Loss 2.3576 Accuracy 0.5482\n",
            "Epoch 5 Batch 150 Loss 2.3627 Accuracy 0.5466\n",
            "Epoch 5 Batch 200 Loss 2.3692 Accuracy 0.5475\n",
            "Epoch 5 Batch 250 Loss 2.3765 Accuracy 0.5463\n",
            "discarded batch 274\n",
            "Epoch 5 Batch 300 Loss 2.3742 Accuracy 0.5469\n",
            "Epoch 5 Batch 350 Loss 2.3746 Accuracy 0.5475\n",
            "Epoch 5 Batch 400 Loss 2.3714 Accuracy 0.5480\n",
            "Epoch 5 Batch 450 Loss 2.3696 Accuracy 0.5486\n",
            "Epoch 5 Batch 500 Loss 2.3710 Accuracy 0.5487\n",
            "Epoch 5 Batch 550 Loss 2.3697 Accuracy 0.5492\n",
            "Epoch 5 Batch 600 Loss 2.3666 Accuracy 0.5498\n",
            "Epoch 5 Batch 650 Loss 2.3675 Accuracy 0.5499\n",
            "Epoch 5 Batch 700 Loss 2.3660 Accuracy 0.5502\n",
            "Epoch 5 Batch 750 Loss 2.3640 Accuracy 0.5504\n",
            "Epoch 5 Batch 800 Loss 2.3614 Accuracy 0.5509\n",
            "Epoch 5 Batch 850 Loss 2.3621 Accuracy 0.5510\n",
            "Epoch 5 Batch 900 Loss 2.3619 Accuracy 0.5509\n",
            "Epoch 5 Batch 950 Loss 2.3623 Accuracy 0.5508\n",
            "Saving checkpoint for epoch 5 at ./checkpoints/train/ckpt-1\n",
            "Epoch 5 Loss 2.3621 Accuracy 0.5508\n",
            "Time taken for 1 epoch: 50.74126076698303 secs\n",
            "\n",
            "Epoch 6 Batch 0 Loss 2.0665 Accuracy 0.6007\n",
            "Epoch 6 Batch 50 Loss 2.2360 Accuracy 0.5629\n",
            "Epoch 6 Batch 100 Loss 2.2629 Accuracy 0.5603\n",
            "Epoch 6 Batch 150 Loss 2.2702 Accuracy 0.5607\n",
            "Epoch 6 Batch 200 Loss 2.2744 Accuracy 0.5617\n",
            "Epoch 6 Batch 250 Loss 2.2729 Accuracy 0.5604\n",
            "Epoch 6 Batch 300 Loss 2.2713 Accuracy 0.5604\n",
            "Epoch 6 Batch 350 Loss 2.2692 Accuracy 0.5604\n",
            "Epoch 6 Batch 400 Loss 2.2719 Accuracy 0.5603\n",
            "Epoch 6 Batch 450 Loss 2.2737 Accuracy 0.5606\n",
            "Epoch 6 Batch 500 Loss 2.2748 Accuracy 0.5608\n",
            "Epoch 6 Batch 550 Loss 2.2750 Accuracy 0.5608\n",
            "Epoch 6 Batch 600 Loss 2.2734 Accuracy 0.5613\n",
            "Epoch 6 Batch 650 Loss 2.2730 Accuracy 0.5614\n",
            "Epoch 6 Batch 700 Loss 2.2742 Accuracy 0.5614\n",
            "Epoch 6 Batch 750 Loss 2.2727 Accuracy 0.5615\n",
            "Epoch 6 Batch 800 Loss 2.2704 Accuracy 0.5618\n",
            "Epoch 6 Batch 850 Loss 2.2715 Accuracy 0.5617\n",
            "discarded batch 887\n",
            "Epoch 6 Batch 900 Loss 2.2705 Accuracy 0.5618\n",
            "Epoch 6 Batch 950 Loss 2.2698 Accuracy 0.5620\n",
            "Epoch 6 Loss 2.2694 Accuracy 0.5621\n",
            "Time taken for 1 epoch: 50.37378978729248 secs\n",
            "\n",
            "Epoch 6 VALIDATION: Loss 2.2313 Accuracy 0.5682\n",
            "\n",
            "Epoch 7 Batch 0 Loss 2.1633 Accuracy 0.5875\n",
            "Epoch 7 Batch 50 Loss 2.1706 Accuracy 0.5739\n",
            "Epoch 7 Batch 100 Loss 2.1722 Accuracy 0.5737\n",
            "Epoch 7 Batch 150 Loss 2.1783 Accuracy 0.5735\n",
            "Epoch 7 Batch 200 Loss 2.1861 Accuracy 0.5719\n",
            "Epoch 7 Batch 250 Loss 2.1885 Accuracy 0.5708\n",
            "Epoch 7 Batch 300 Loss 2.1894 Accuracy 0.5707\n",
            "Epoch 7 Batch 350 Loss 2.1890 Accuracy 0.5713\n",
            "Epoch 7 Batch 400 Loss 2.1880 Accuracy 0.5715\n",
            "discarded batch 408\n",
            "Epoch 7 Batch 450 Loss 2.1928 Accuracy 0.5713\n",
            "Epoch 7 Batch 500 Loss 2.1947 Accuracy 0.5712\n",
            "Epoch 7 Batch 550 Loss 2.1984 Accuracy 0.5707\n",
            "Epoch 7 Batch 600 Loss 2.1978 Accuracy 0.5705\n",
            "Epoch 7 Batch 650 Loss 2.1983 Accuracy 0.5708\n",
            "Epoch 7 Batch 700 Loss 2.1974 Accuracy 0.5708\n",
            "Epoch 7 Batch 750 Loss 2.1974 Accuracy 0.5711\n",
            "Epoch 7 Batch 800 Loss 2.1965 Accuracy 0.5708\n",
            "Epoch 7 Batch 850 Loss 2.1975 Accuracy 0.5710\n",
            "Epoch 7 Batch 900 Loss 2.2005 Accuracy 0.5705\n",
            "Epoch 7 Batch 950 Loss 2.2012 Accuracy 0.5705\n",
            "Epoch 7 Loss 2.2010 Accuracy 0.5706\n",
            "Time taken for 1 epoch: 50.24830722808838 secs\n",
            "\n",
            "Epoch 8 Batch 0 Loss 2.0693 Accuracy 0.5743\n",
            "Epoch 8 Batch 50 Loss 2.1380 Accuracy 0.5785\n",
            "Epoch 8 Batch 100 Loss 2.1215 Accuracy 0.5799\n",
            "Epoch 8 Batch 150 Loss 2.1210 Accuracy 0.5795\n",
            "Epoch 8 Batch 200 Loss 2.1172 Accuracy 0.5796\n",
            "Epoch 8 Batch 250 Loss 2.1272 Accuracy 0.5788\n",
            "Epoch 8 Batch 300 Loss 2.1325 Accuracy 0.5779\n",
            "Epoch 8 Batch 350 Loss 2.1329 Accuracy 0.5778\n",
            "Epoch 8 Batch 400 Loss 2.1351 Accuracy 0.5773\n",
            "Epoch 8 Batch 450 Loss 2.1343 Accuracy 0.5771\n",
            "Epoch 8 Batch 500 Loss 2.1375 Accuracy 0.5769\n",
            "discarded batch 546\n",
            "Epoch 8 Batch 550 Loss 2.1402 Accuracy 0.5767\n",
            "Epoch 8 Batch 600 Loss 2.1409 Accuracy 0.5768\n",
            "Epoch 8 Batch 650 Loss 2.1433 Accuracy 0.5766\n",
            "Epoch 8 Batch 700 Loss 2.1430 Accuracy 0.5766\n",
            "Epoch 8 Batch 750 Loss 2.1439 Accuracy 0.5766\n",
            "Epoch 8 Batch 800 Loss 2.1438 Accuracy 0.5768\n",
            "Epoch 8 Batch 850 Loss 2.1431 Accuracy 0.5769\n",
            "Epoch 8 Batch 900 Loss 2.1441 Accuracy 0.5770\n",
            "Epoch 8 Batch 950 Loss 2.1469 Accuracy 0.5768\n",
            "Epoch 8 Loss 2.1471 Accuracy 0.5767\n",
            "Time taken for 1 epoch: 50.40212607383728 secs\n",
            "\n",
            "Epoch 9 Batch 0 Loss 2.0836 Accuracy 0.5743\n",
            "Epoch 9 Batch 50 Loss 2.0742 Accuracy 0.5825\n",
            "Epoch 9 Batch 100 Loss 2.0723 Accuracy 0.5838\n",
            "Epoch 9 Batch 150 Loss 2.0773 Accuracy 0.5840\n",
            "Epoch 9 Batch 200 Loss 2.0819 Accuracy 0.5849\n",
            "Epoch 9 Batch 250 Loss 2.0838 Accuracy 0.5851\n",
            "Epoch 9 Batch 300 Loss 2.0831 Accuracy 0.5850\n",
            "Epoch 9 Batch 350 Loss 2.0872 Accuracy 0.5857\n",
            "Epoch 9 Batch 400 Loss 2.0901 Accuracy 0.5844\n",
            "Epoch 9 Batch 450 Loss 2.0927 Accuracy 0.5837\n",
            "Epoch 9 Batch 500 Loss 2.0935 Accuracy 0.5836\n",
            "Epoch 9 Batch 550 Loss 2.0955 Accuracy 0.5831\n",
            "Epoch 9 Batch 600 Loss 2.0950 Accuracy 0.5831\n",
            "Epoch 9 Batch 650 Loss 2.0980 Accuracy 0.5828\n",
            "Epoch 9 Batch 700 Loss 2.0980 Accuracy 0.5827\n",
            "discarded batch 733\n",
            "Epoch 9 Batch 750 Loss 2.0978 Accuracy 0.5827\n",
            "Epoch 9 Batch 800 Loss 2.0981 Accuracy 0.5828\n",
            "Epoch 9 Batch 850 Loss 2.0985 Accuracy 0.5830\n",
            "Epoch 9 Batch 900 Loss 2.0993 Accuracy 0.5829\n",
            "Epoch 9 Batch 950 Loss 2.0996 Accuracy 0.5829\n",
            "Epoch 9 Loss 2.0998 Accuracy 0.5828\n",
            "Time taken for 1 epoch: 50.47104001045227 secs\n",
            "\n",
            "Epoch 10 Batch 0 Loss 1.9947 Accuracy 0.5842\n",
            "Epoch 10 Batch 50 Loss 2.0329 Accuracy 0.5903\n",
            "Epoch 10 Batch 100 Loss 2.0277 Accuracy 0.5907\n",
            "Epoch 10 Batch 150 Loss 2.0380 Accuracy 0.5896\n",
            "Epoch 10 Batch 200 Loss 2.0374 Accuracy 0.5898\n",
            "Epoch 10 Batch 250 Loss 2.0384 Accuracy 0.5903\n",
            "Epoch 10 Batch 300 Loss 2.0371 Accuracy 0.5907\n",
            "discarded batch 321\n",
            "Epoch 10 Batch 350 Loss 2.0359 Accuracy 0.5904\n",
            "Epoch 10 Batch 400 Loss 2.0400 Accuracy 0.5899\n",
            "Epoch 10 Batch 450 Loss 2.0436 Accuracy 0.5895\n",
            "Epoch 10 Batch 500 Loss 2.0454 Accuracy 0.5893\n",
            "Epoch 10 Batch 550 Loss 2.0491 Accuracy 0.5893\n",
            "Epoch 10 Batch 600 Loss 2.0503 Accuracy 0.5892\n",
            "Epoch 10 Batch 650 Loss 2.0514 Accuracy 0.5891\n",
            "Epoch 10 Batch 700 Loss 2.0540 Accuracy 0.5889\n",
            "Epoch 10 Batch 750 Loss 2.0553 Accuracy 0.5888\n",
            "Epoch 10 Batch 800 Loss 2.0562 Accuracy 0.5888\n",
            "Epoch 10 Batch 850 Loss 2.0572 Accuracy 0.5886\n",
            "Epoch 10 Batch 900 Loss 2.0575 Accuracy 0.5887\n",
            "Epoch 10 Batch 950 Loss 2.0576 Accuracy 0.5886\n",
            "Saving checkpoint for epoch 10 at ./checkpoints/train/ckpt-2\n",
            "Epoch 10 Loss 2.0576 Accuracy 0.5887\n",
            "Time taken for 1 epoch: 50.89773774147034 secs\n",
            "\n",
            "Epoch 11 Batch 0 Loss 1.8669 Accuracy 0.6040\n",
            "Epoch 11 Batch 50 Loss 1.9848 Accuracy 0.6010\n",
            "Epoch 11 Batch 100 Loss 1.9853 Accuracy 0.6011\n",
            "discarded batch 147\n",
            "Epoch 11 Batch 150 Loss 1.9946 Accuracy 0.5979\n",
            "Epoch 11 Batch 200 Loss 1.9947 Accuracy 0.5984\n",
            "Epoch 11 Batch 250 Loss 1.9944 Accuracy 0.5992\n",
            "Epoch 11 Batch 300 Loss 1.9955 Accuracy 0.5986\n",
            "Epoch 11 Batch 350 Loss 1.9997 Accuracy 0.5978\n",
            "Epoch 11 Batch 400 Loss 2.0051 Accuracy 0.5971\n",
            "Epoch 11 Batch 450 Loss 2.0076 Accuracy 0.5964\n",
            "Epoch 11 Batch 500 Loss 2.0077 Accuracy 0.5962\n",
            "Epoch 11 Batch 550 Loss 2.0123 Accuracy 0.5958\n",
            "Epoch 11 Batch 600 Loss 2.0123 Accuracy 0.5957\n",
            "Epoch 11 Batch 650 Loss 2.0125 Accuracy 0.5956\n",
            "Epoch 11 Batch 700 Loss 2.0140 Accuracy 0.5954\n",
            "Epoch 11 Batch 750 Loss 2.0149 Accuracy 0.5954\n",
            "Epoch 11 Batch 800 Loss 2.0172 Accuracy 0.5949\n",
            "Epoch 11 Batch 850 Loss 2.0180 Accuracy 0.5948\n",
            "Epoch 11 Batch 900 Loss 2.0191 Accuracy 0.5946\n",
            "Epoch 11 Batch 950 Loss 2.0203 Accuracy 0.5945\n",
            "Epoch 11 Loss 2.0203 Accuracy 0.5945\n",
            "Time taken for 1 epoch: 50.49139213562012 secs\n",
            "\n",
            "Epoch 11 VALIDATION: Loss 2.0987 Accuracy 0.5887\n",
            "\n",
            "Epoch 12 Batch 0 Loss 1.8070 Accuracy 0.6370\n",
            "discarded batch 47\n",
            "Epoch 12 Batch 50 Loss 1.9480 Accuracy 0.6026\n",
            "Epoch 12 Batch 100 Loss 1.9761 Accuracy 0.5995\n",
            "Epoch 12 Batch 150 Loss 1.9616 Accuracy 0.6017\n",
            "Epoch 12 Batch 200 Loss 1.9501 Accuracy 0.6034\n",
            "Epoch 12 Batch 250 Loss 1.9583 Accuracy 0.6027\n",
            "Epoch 12 Batch 300 Loss 1.9607 Accuracy 0.6021\n",
            "Epoch 12 Batch 350 Loss 1.9612 Accuracy 0.6024\n",
            "Epoch 12 Batch 400 Loss 1.9620 Accuracy 0.6020\n",
            "Epoch 12 Batch 450 Loss 1.9640 Accuracy 0.6018\n",
            "Epoch 12 Batch 500 Loss 1.9664 Accuracy 0.6018\n",
            "Epoch 12 Batch 550 Loss 1.9699 Accuracy 0.6017\n",
            "Epoch 12 Batch 600 Loss 1.9732 Accuracy 0.6013\n",
            "Epoch 12 Batch 650 Loss 1.9728 Accuracy 0.6012\n",
            "Epoch 12 Batch 700 Loss 1.9762 Accuracy 0.6008\n",
            "Epoch 12 Batch 750 Loss 1.9770 Accuracy 0.6008\n",
            "Epoch 12 Batch 800 Loss 1.9781 Accuracy 0.6006\n",
            "Epoch 12 Batch 850 Loss 1.9789 Accuracy 0.6006\n",
            "Epoch 12 Batch 900 Loss 1.9795 Accuracy 0.6006\n",
            "Epoch 12 Batch 950 Loss 1.9795 Accuracy 0.6006\n",
            "Epoch 12 Loss 1.9799 Accuracy 0.6006\n",
            "Time taken for 1 epoch: 50.61095142364502 secs\n",
            "\n",
            "Epoch 13 Batch 0 Loss 2.0222 Accuracy 0.6172\n",
            "discarded batch 8\n",
            "Epoch 13 Batch 50 Loss 1.8820 Accuracy 0.6141\n",
            "Epoch 13 Batch 100 Loss 1.8905 Accuracy 0.6121\n",
            "Epoch 13 Batch 150 Loss 1.9021 Accuracy 0.6096\n",
            "Epoch 13 Batch 200 Loss 1.9020 Accuracy 0.6104\n",
            "Epoch 13 Batch 250 Loss 1.9097 Accuracy 0.6097\n",
            "Epoch 13 Batch 300 Loss 1.9165 Accuracy 0.6092\n",
            "Epoch 13 Batch 350 Loss 1.9219 Accuracy 0.6089\n",
            "Epoch 13 Batch 400 Loss 1.9225 Accuracy 0.6088\n",
            "Epoch 13 Batch 450 Loss 1.9262 Accuracy 0.6081\n",
            "Epoch 13 Batch 500 Loss 1.9267 Accuracy 0.6078\n",
            "Epoch 13 Batch 550 Loss 1.9309 Accuracy 0.6075\n",
            "Epoch 13 Batch 600 Loss 1.9296 Accuracy 0.6076\n",
            "Epoch 13 Batch 650 Loss 1.9303 Accuracy 0.6078\n",
            "Epoch 13 Batch 700 Loss 1.9313 Accuracy 0.6076\n",
            "Epoch 13 Batch 750 Loss 1.9361 Accuracy 0.6069\n",
            "Epoch 13 Batch 800 Loss 1.9369 Accuracy 0.6066\n",
            "Epoch 13 Batch 850 Loss 1.9380 Accuracy 0.6066\n",
            "Epoch 13 Batch 900 Loss 1.9391 Accuracy 0.6066\n",
            "Epoch 13 Batch 950 Loss 1.9396 Accuracy 0.6069\n",
            "Epoch 13 Loss 1.9394 Accuracy 0.6070\n",
            "Time taken for 1 epoch: 50.657721281051636 secs\n",
            "\n",
            "Epoch 14 Batch 0 Loss 2.0332 Accuracy 0.5941\n",
            "Epoch 14 Batch 50 Loss 1.8489 Accuracy 0.6191\n",
            "Epoch 14 Batch 100 Loss 1.8533 Accuracy 0.6193\n",
            "Epoch 14 Batch 150 Loss 1.8621 Accuracy 0.6185\n",
            "Epoch 14 Batch 200 Loss 1.8639 Accuracy 0.6171\n",
            "Epoch 14 Batch 250 Loss 1.8713 Accuracy 0.6166\n",
            "Epoch 14 Batch 300 Loss 1.8747 Accuracy 0.6161\n",
            "discarded batch 311\n",
            "Epoch 14 Batch 350 Loss 1.8748 Accuracy 0.6159\n",
            "Epoch 14 Batch 400 Loss 1.8772 Accuracy 0.6156\n",
            "Epoch 14 Batch 450 Loss 1.8781 Accuracy 0.6158\n",
            "Epoch 14 Batch 500 Loss 1.8791 Accuracy 0.6156\n",
            "Epoch 14 Batch 550 Loss 1.8819 Accuracy 0.6154\n",
            "Epoch 14 Batch 600 Loss 1.8835 Accuracy 0.6155\n",
            "Epoch 14 Batch 650 Loss 1.8862 Accuracy 0.6150\n",
            "Epoch 14 Batch 700 Loss 1.8896 Accuracy 0.6143\n",
            "Epoch 14 Batch 750 Loss 1.8901 Accuracy 0.6144\n",
            "Epoch 14 Batch 800 Loss 1.8903 Accuracy 0.6146\n",
            "Epoch 14 Batch 850 Loss 1.8918 Accuracy 0.6146\n",
            "Epoch 14 Batch 900 Loss 1.8951 Accuracy 0.6144\n",
            "Epoch 14 Batch 950 Loss 1.8958 Accuracy 0.6144\n",
            "Epoch 14 Loss 1.8961 Accuracy 0.6143\n",
            "Time taken for 1 epoch: 50.53088855743408 secs\n",
            "\n",
            "Epoch 15 Batch 0 Loss 1.7377 Accuracy 0.6535\n",
            "Epoch 15 Batch 50 Loss 1.8410 Accuracy 0.6215\n",
            "Epoch 15 Batch 100 Loss 1.8266 Accuracy 0.6256\n",
            "Epoch 15 Batch 150 Loss 1.8303 Accuracy 0.6252\n",
            "Epoch 15 Batch 200 Loss 1.8267 Accuracy 0.6258\n",
            "Epoch 15 Batch 250 Loss 1.8291 Accuracy 0.6259\n",
            "Epoch 15 Batch 300 Loss 1.8351 Accuracy 0.6242\n",
            "Epoch 15 Batch 350 Loss 1.8357 Accuracy 0.6239\n",
            "Epoch 15 Batch 400 Loss 1.8358 Accuracy 0.6239\n",
            "discarded batch 437\n",
            "Epoch 15 Batch 450 Loss 1.8387 Accuracy 0.6234\n",
            "Epoch 15 Batch 500 Loss 1.8389 Accuracy 0.6230\n",
            "Epoch 15 Batch 550 Loss 1.8400 Accuracy 0.6228\n",
            "Epoch 15 Batch 600 Loss 1.8436 Accuracy 0.6223\n",
            "Epoch 15 Batch 650 Loss 1.8435 Accuracy 0.6225\n",
            "Epoch 15 Batch 700 Loss 1.8464 Accuracy 0.6221\n",
            "Epoch 15 Batch 750 Loss 1.8476 Accuracy 0.6221\n",
            "Epoch 15 Batch 800 Loss 1.8523 Accuracy 0.6215\n",
            "Epoch 15 Batch 850 Loss 1.8545 Accuracy 0.6211\n",
            "Epoch 15 Batch 900 Loss 1.8575 Accuracy 0.6207\n",
            "Epoch 15 Batch 950 Loss 1.8586 Accuracy 0.6206\n",
            "Saving checkpoint for epoch 15 at ./checkpoints/train/ckpt-3\n",
            "Epoch 15 Loss 1.8587 Accuracy 0.6206\n",
            "Time taken for 1 epoch: 50.937453508377075 secs\n",
            "\n",
            "Epoch 16 Batch 0 Loss 1.7476 Accuracy 0.6502\n",
            "Epoch 16 Batch 50 Loss 1.7870 Accuracy 0.6300\n",
            "Epoch 16 Batch 100 Loss 1.7830 Accuracy 0.6316\n",
            "discarded batch 102\n",
            "Epoch 16 Batch 150 Loss 1.7889 Accuracy 0.6315\n",
            "Epoch 16 Batch 200 Loss 1.7930 Accuracy 0.6306\n",
            "Epoch 16 Batch 250 Loss 1.7933 Accuracy 0.6301\n",
            "Epoch 16 Batch 300 Loss 1.7965 Accuracy 0.6291\n",
            "Epoch 16 Batch 350 Loss 1.7978 Accuracy 0.6292\n",
            "Epoch 16 Batch 400 Loss 1.8007 Accuracy 0.6286\n",
            "Epoch 16 Batch 450 Loss 1.8008 Accuracy 0.6284\n",
            "Epoch 16 Batch 500 Loss 1.8043 Accuracy 0.6278\n",
            "Epoch 16 Batch 550 Loss 1.8085 Accuracy 0.6273\n",
            "Epoch 16 Batch 600 Loss 1.8110 Accuracy 0.6270\n",
            "Epoch 16 Batch 650 Loss 1.8124 Accuracy 0.6269\n",
            "Epoch 16 Batch 700 Loss 1.8137 Accuracy 0.6266\n",
            "Epoch 16 Batch 750 Loss 1.8150 Accuracy 0.6266\n",
            "Epoch 16 Batch 800 Loss 1.8160 Accuracy 0.6265\n",
            "Epoch 16 Batch 850 Loss 1.8181 Accuracy 0.6263\n",
            "Epoch 16 Batch 900 Loss 1.8199 Accuracy 0.6260\n",
            "Epoch 16 Batch 950 Loss 1.8215 Accuracy 0.6259\n",
            "Epoch 16 Loss 1.8213 Accuracy 0.6259\n",
            "Time taken for 1 epoch: 50.65754294395447 secs\n",
            "\n",
            "Epoch 16 VALIDATION: Loss 2.0842 Accuracy 0.5995\n",
            "\n",
            "Epoch 17 Batch 0 Loss 1.6161 Accuracy 0.6700\n",
            "Epoch 17 Batch 50 Loss 1.7577 Accuracy 0.6372\n",
            "Epoch 17 Batch 100 Loss 1.7474 Accuracy 0.6378\n",
            "Epoch 17 Batch 150 Loss 1.7573 Accuracy 0.6366\n",
            "Epoch 17 Batch 200 Loss 1.7594 Accuracy 0.6367\n",
            "Epoch 17 Batch 250 Loss 1.7636 Accuracy 0.6356\n",
            "Epoch 17 Batch 300 Loss 1.7629 Accuracy 0.6354\n",
            "Epoch 17 Batch 350 Loss 1.7631 Accuracy 0.6357\n",
            "Epoch 17 Batch 400 Loss 1.7651 Accuracy 0.6357\n",
            "Epoch 17 Batch 450 Loss 1.7681 Accuracy 0.6352\n",
            "Epoch 17 Batch 500 Loss 1.7711 Accuracy 0.6346\n",
            "Epoch 17 Batch 550 Loss 1.7728 Accuracy 0.6343\n",
            "Epoch 17 Batch 600 Loss 1.7738 Accuracy 0.6341\n",
            "Epoch 17 Batch 650 Loss 1.7733 Accuracy 0.6342\n",
            "Epoch 17 Batch 700 Loss 1.7760 Accuracy 0.6337\n",
            "Epoch 17 Batch 750 Loss 1.7785 Accuracy 0.6333\n",
            "Epoch 17 Batch 800 Loss 1.7809 Accuracy 0.6331\n",
            "Epoch 17 Batch 850 Loss 1.7833 Accuracy 0.6326\n",
            "discarded batch 857\n",
            "Epoch 17 Batch 900 Loss 1.7837 Accuracy 0.6326\n",
            "Epoch 17 Batch 950 Loss 1.7852 Accuracy 0.6323\n",
            "Epoch 17 Loss 1.7852 Accuracy 0.6322\n",
            "Time taken for 1 epoch: 50.66221475601196 secs\n",
            "\n",
            "Epoch 18 Batch 0 Loss 1.7300 Accuracy 0.6403\n",
            "Epoch 18 Batch 50 Loss 1.7035 Accuracy 0.6452\n",
            "Epoch 18 Batch 100 Loss 1.7071 Accuracy 0.6443\n",
            "discarded batch 150\n",
            "Epoch 18 Batch 200 Loss 1.7117 Accuracy 0.6433\n",
            "Epoch 18 Batch 250 Loss 1.7160 Accuracy 0.6425\n",
            "Epoch 18 Batch 300 Loss 1.7185 Accuracy 0.6416\n",
            "Epoch 18 Batch 350 Loss 1.7169 Accuracy 0.6418\n",
            "Epoch 18 Batch 400 Loss 1.7204 Accuracy 0.6414\n",
            "Epoch 18 Batch 450 Loss 1.7239 Accuracy 0.6406\n",
            "Epoch 18 Batch 500 Loss 1.7285 Accuracy 0.6401\n",
            "Epoch 18 Batch 550 Loss 1.7322 Accuracy 0.6395\n",
            "Epoch 18 Batch 600 Loss 1.7359 Accuracy 0.6391\n",
            "Epoch 18 Batch 650 Loss 1.7368 Accuracy 0.6389\n",
            "Epoch 18 Batch 700 Loss 1.7403 Accuracy 0.6385\n",
            "Epoch 18 Batch 750 Loss 1.7421 Accuracy 0.6383\n",
            "Epoch 18 Batch 800 Loss 1.7435 Accuracy 0.6380\n",
            "Epoch 18 Batch 850 Loss 1.7451 Accuracy 0.6377\n",
            "Epoch 18 Batch 900 Loss 1.7475 Accuracy 0.6375\n",
            "Epoch 18 Batch 950 Loss 1.7491 Accuracy 0.6371\n",
            "Epoch 18 Loss 1.7491 Accuracy 0.6371\n",
            "Time taken for 1 epoch: 50.569130659103394 secs\n",
            "\n",
            "Epoch 19 Batch 0 Loss 1.5884 Accuracy 0.6667\n",
            "Epoch 19 Batch 50 Loss 1.6925 Accuracy 0.6456\n",
            "Epoch 19 Batch 100 Loss 1.6822 Accuracy 0.6497\n",
            "Epoch 19 Batch 150 Loss 1.6833 Accuracy 0.6499\n",
            "discarded batch 189\n",
            "Epoch 19 Batch 200 Loss 1.6771 Accuracy 0.6497\n",
            "Epoch 19 Batch 250 Loss 1.6820 Accuracy 0.6485\n",
            "Epoch 19 Batch 300 Loss 1.6838 Accuracy 0.6477\n",
            "Epoch 19 Batch 350 Loss 1.6849 Accuracy 0.6478\n",
            "Epoch 19 Batch 400 Loss 1.6893 Accuracy 0.6465\n",
            "Epoch 19 Batch 450 Loss 1.6953 Accuracy 0.6458\n",
            "Epoch 19 Batch 500 Loss 1.7003 Accuracy 0.6451\n",
            "Epoch 19 Batch 550 Loss 1.7040 Accuracy 0.6445\n",
            "Epoch 19 Batch 600 Loss 1.7043 Accuracy 0.6442\n",
            "Epoch 19 Batch 650 Loss 1.7052 Accuracy 0.6441\n",
            "Epoch 19 Batch 700 Loss 1.7050 Accuracy 0.6440\n",
            "Epoch 19 Batch 750 Loss 1.7071 Accuracy 0.6437\n",
            "Epoch 19 Batch 800 Loss 1.7104 Accuracy 0.6434\n",
            "Epoch 19 Batch 850 Loss 1.7120 Accuracy 0.6432\n",
            "Epoch 19 Batch 900 Loss 1.7127 Accuracy 0.6432\n",
            "Epoch 19 Batch 950 Loss 1.7143 Accuracy 0.6430\n",
            "Epoch 19 Loss 1.7144 Accuracy 0.6430\n",
            "Time taken for 1 epoch: 50.472519397735596 secs\n",
            "\n",
            "Epoch 20 Batch 0 Loss 1.5402 Accuracy 0.6667\n",
            "Epoch 20 Batch 50 Loss 1.6149 Accuracy 0.6607\n",
            "Epoch 20 Batch 100 Loss 1.6136 Accuracy 0.6594\n",
            "Epoch 20 Batch 150 Loss 1.6182 Accuracy 0.6584\n",
            "Epoch 20 Batch 200 Loss 1.6252 Accuracy 0.6571\n",
            "Epoch 20 Batch 250 Loss 1.6315 Accuracy 0.6562\n",
            "Epoch 20 Batch 300 Loss 1.6384 Accuracy 0.6554\n",
            "discarded batch 314\n",
            "Epoch 20 Batch 350 Loss 1.6420 Accuracy 0.6549\n",
            "Epoch 20 Batch 400 Loss 1.6449 Accuracy 0.6541\n",
            "Epoch 20 Batch 450 Loss 1.6504 Accuracy 0.6535\n",
            "Epoch 20 Batch 500 Loss 1.6554 Accuracy 0.6529\n",
            "Epoch 20 Batch 550 Loss 1.6586 Accuracy 0.6521\n",
            "Epoch 20 Batch 600 Loss 1.6621 Accuracy 0.6517\n",
            "Epoch 20 Batch 650 Loss 1.6653 Accuracy 0.6511\n",
            "Epoch 20 Batch 700 Loss 1.6688 Accuracy 0.6504\n",
            "Epoch 20 Batch 750 Loss 1.6709 Accuracy 0.6499\n",
            "Epoch 20 Batch 800 Loss 1.6733 Accuracy 0.6494\n",
            "Epoch 20 Batch 850 Loss 1.6748 Accuracy 0.6490\n",
            "Epoch 20 Batch 900 Loss 1.6767 Accuracy 0.6487\n",
            "Epoch 20 Batch 950 Loss 1.6789 Accuracy 0.6485\n",
            "Saving checkpoint for epoch 20 at ./checkpoints/train/ckpt-4\n",
            "Epoch 20 Loss 1.6787 Accuracy 0.6486\n",
            "Time taken for 1 epoch: 51.026366233825684 secs\n",
            "\n",
            "Epoch 21 Batch 0 Loss 1.5302 Accuracy 0.6634\n",
            "Epoch 21 Batch 50 Loss 1.5874 Accuracy 0.6605\n",
            "Epoch 21 Batch 100 Loss 1.5981 Accuracy 0.6588\n",
            "Epoch 21 Batch 150 Loss 1.6064 Accuracy 0.6576\n",
            "Epoch 21 Batch 200 Loss 1.6055 Accuracy 0.6586\n",
            "Epoch 21 Batch 250 Loss 1.6099 Accuracy 0.6581\n",
            "Epoch 21 Batch 300 Loss 1.6148 Accuracy 0.6578\n",
            "Epoch 21 Batch 350 Loss 1.6173 Accuracy 0.6575\n",
            "Epoch 21 Batch 400 Loss 1.6193 Accuracy 0.6565\n",
            "Epoch 21 Batch 450 Loss 1.6221 Accuracy 0.6566\n",
            "Epoch 21 Batch 500 Loss 1.6247 Accuracy 0.6556\n",
            "Epoch 21 Batch 550 Loss 1.6277 Accuracy 0.6551\n",
            "Epoch 21 Batch 600 Loss 1.6310 Accuracy 0.6548\n",
            "discarded batch 647\n",
            "Epoch 21 Batch 650 Loss 1.6327 Accuracy 0.6548\n",
            "Epoch 21 Batch 700 Loss 1.6371 Accuracy 0.6540\n",
            "Epoch 21 Batch 750 Loss 1.6385 Accuracy 0.6538\n",
            "Epoch 21 Batch 800 Loss 1.6414 Accuracy 0.6533\n",
            "Epoch 21 Batch 850 Loss 1.6433 Accuracy 0.6531\n",
            "Epoch 21 Batch 900 Loss 1.6447 Accuracy 0.6529\n",
            "Epoch 21 Batch 950 Loss 1.6474 Accuracy 0.6525\n",
            "Epoch 21 Loss 1.6477 Accuracy 0.6525\n",
            "Time taken for 1 epoch: 50.720311403274536 secs\n",
            "\n",
            "Epoch 21 VALIDATION: Loss 2.1487 Accuracy 0.5975\n",
            "\n",
            "Epoch 22 Batch 0 Loss 1.5021 Accuracy 0.6865\n",
            "Epoch 22 Batch 50 Loss 1.5717 Accuracy 0.6662\n",
            "Epoch 22 Batch 100 Loss 1.5817 Accuracy 0.6632\n",
            "Epoch 22 Batch 150 Loss 1.5768 Accuracy 0.6646\n",
            "Epoch 22 Batch 200 Loss 1.5727 Accuracy 0.6647\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-bcebfe331e5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m            \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"discarded batch\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m            \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m        \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjLbfLOU_pRw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#transformer.save_weights(\"./optimus_rhyme_wide100\")\n",
        "#transformer.load_weights(\"./optimus_rhyme_200\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEun8YPR8Sn6",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f4f08129-46c2-4ab1-efd3-5b78e9b5c888"
      },
      "source": [
        "#@title Generation\n",
        "\n",
        "def evaluate(inp_sentence, decoder_input):\n",
        "    inp_sentence = inp_sentence\n",
        "    encoder_input = tf.expand_dims(inp_sentence, 0)\n",
        "    \n",
        "    output = tf.expand_dims(decoder_input, 0)\n",
        "\n",
        "    terces = 0\n",
        "    for i in range(batch_len):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "            encoder_input, output)\n",
        "    \n",
        "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "        predictions, attention_weights = transformer(encoder_input, \n",
        "                                                    output,\n",
        "                                                    False,\n",
        "                                                    enc_padding_mask,\n",
        "                                                    combined_mask,\n",
        "                                                    dec_padding_mask)\n",
        "        \n",
        "        # select the last word from the seq_len dimension\n",
        "        predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "        # return the result if the predicted_id is equal to the end token\n",
        "        if predicted_id == eos:\n",
        "            terces += 1\n",
        "            if terces == 3:\n",
        "                return tf.squeeze(output, axis=0), attention_weights\n",
        "        # concatentate the predicted_id to the output which is given to the decoder\n",
        "        # as its input.\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "    return tf.squeeze(output, axis=0), attention_weights\n",
        "\n",
        "out_list = test_b[1][0]\n",
        "print(seq2str(out_list))\n",
        "txt_gen = seq2str(out_list)+\"\\n--------------------\\n\"\n",
        "offset = 75 # a tercet\n",
        "print(\"---------------------------\")\n",
        "for i in range(10): # 30 terces = cantica\n",
        "    out, att_w = evaluate([pad], out_list[-offset:])\n",
        "    out_list = out.numpy().tolist()\n",
        "    out_str = seq2str(out_list[offset:])\n",
        "    txt_gen += out_str + \"\\n\"\n",
        "    print(out_str) \n",
        "\n",
        "wandb.log({\"generated\": txt_gen})"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " s ' elli ama bene e bene spera e crede\n",
            "non t ' è occulto perché ' l viso hai quivi\n",
            "dov ' ogni cosa dipinta si vede\n",
            "  \n",
            " ma perché questo regno ha fatto civi\n",
            "per la verace fede a glorïarla\n",
            "di lei parlare è ben ch ' a lui arrivi\n",
            "             \n",
            " sí come il baccellier s ' arma e non parla\n",
            "fin che ' l maestro la question propone\n",
            "per approvarla non per terminarla\n",
            "            \n",
            " cosí m ' armava io d ' ogni ragione\n",
            "mentre ch ' ella dicea per esser presto\n",
            "a tal querente ed a tal professione\n",
            "         \n",
            "\n",
            "---------------------------\n",
            " e cominciò tu che se ' l mondo a ' l seme\n",
            "non fu a riso e tu se ' l segno\n",
            "e tu non ti di rise tu vuo ' se ' \n",
            "\n",
            " ma tu se ' saper che tu hai di regno\n",
            "di là dove tu se ' di là dove ' hai ben di retro\n",
            "e tu vuo ' sarai non di di queto\n",
            "gno\n",
            " e se tu ricorditi e tu vuo ' di colui\n",
            "se tu se ' sati di quel che tu hai dimandi\n",
            "e tu vuo ' sarai non hai ti digno\n",
            "     \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                       \n",
            "fia se ne mo del tuo voler di sole\n",
            "non si move e per lo suo disii\n",
            "che si move a di cotanto a ' pristi a\n",
            "             \n",
            " ma perché non si move per disii\n",
            "non si move e per lo suo disiri\n",
            "che si move e non si move a i ' ti ti tii\n",
            "        \n",
            " io non ti move e per li occhi miei\n",
            "per li altri altri e per la riva\n",
            "che tu non mi ritorna e non mi vandi\n",
            "\n",
            "\n",
            " io mi volsi a veder di quella riva\n",
            "ch ' io non mi dissi ri ' mi ricordi\n",
            "ch ' io non mi rivolsi rivolse ' mi riva\n",
            "      \n",
            " ma perché tu non m ' avea di piedi\n",
            "ché tu vuo ' ch ' io dico ' ricono a ' rite e ' l di là dove mi ridi ' ridi\n",
            " e io rispuosi lui ritor che tu hai piedi\n",
            "rispuose ' l maestro e ' l dolo ' arte\n",
            "e ' dico dico di cosa di piedi\n",
            "      \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "e ' tu chi ch ben e e e e ' tu che ' ' non non ' chi chi ' ' non non ' ' non non non ' tu non non '\n",
            " \n",
            "se tu vuo ' come tu vuo ' chi vuo ' sanito ancor non ti vuo ' ti vuo ' sati vuo ' ti vuo ' ti vuo ' testi ' testi non '\n",
            "\n",
            " ti vuo ' saper chi tu vuo ' ti vuo ' chi ti movere e chi ti motito\n",
            "perché non ti move e chi non ti fare\n",
            "           \n",
            " io ti moni e per la mia rispose\n",
            "per ch ' i ' non do tu non ti movesti\n",
            "per ch ' io non non non mi ricono ' io ti remote\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " e se tu se ' se tu chi ricordi e ' l duca mi fece e ' dico ' mi\n",
            "e ' l duca mi rino e ' mi rinova\n",
            "      \n",
            " e io a lui se ' tu chi rispuosi ' l chi ricominciò ' l duca duto\n",
            "e ' mi rito e ' l duca e ' mi rito '\n",
            "   \n",
            " e io a lui chi tu chi ti richi ti richi ' ti richi ti vuo ' ch ' io ti movesti\n",
            "e ' l duca del no a te a te a '\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "e non chi non non non per ' ' tu tu tu che tu non non tu se non chi tu non non tu tu non non non non tu tu ' tu \n",
            "\n",
            "\n",
            " vuo ' io rispuose ' chi vuo ' ch ' io non vuo ' fui ché tu vuo ' sani ' chi vuo ' ti vuo ' sani ' io vuo ' ché tu '\n",
            "\n",
            " tu vuo ' saper chi tu vuo ' ti vuo ' sani ancor se ' ch ' io non vuo ' vuo ' ti vuo ' ti vuo ' chi ti vuo ' odi '    ' ti move tu chi ti vuo ' ti movesti\n",
            "ché tu chi ti vuo ' io ti movere\n",
            "per la mia risposti non mi ricono\n",
            "           \n",
            "\n",
            "\n",
            "\n",
            "sí non ' ' non non ' non non non non ' tu tu non non non non non non non tu chi non ' tu chi non non non non ' tu tu ' tu \n",
            "\n",
            "\n",
            "se tu vuo ' io ti vuo ' tu vuo ' ni anni ancor chi vuo ' ti vuo ' te vuo ' ti vuo ' te non vuo ' te chi tu vuo '  chi '\n",
            " chi tu chi tu chi tu vuo ' ti vuo ' chi ti vuo ' te ti vuo ' ti vuo ' ti vuo ' ti vuo ' chi vuo ' chi ti vuo ' ' \n",
            "\n",
            "    ti fa chi tu chi ti vuo ' ti movesti\n",
            "per la volare e chi tu chi ti movesti\n",
            "e chi tu chi ti fa chi ti resti\n",
            "       \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "sí ne ne fa non non non non non non tu do non tu non non tu vuo vuo vuo ' tu vuo ' tu non ti non non vesti non tu tu tu tu \n",
            "vuo ' chi ricordi chi vuo ' chi vuo ' chi vuo ' chi vuo ' chi vuo ' chi vuo ' sani ancor chi ti vuo ' sani ?\n",
            "se tu vuo '   chi '\n",
            " chi tu chi se ' saper chi vuo ' chi vuo ' chi perché tu vuo ' vuo ' vuo ' chi vuo ' chi vuo ' chi vuo ' chi ti vuo ' '      ti fa chi tu chi vuo ' ti move ancor chi ti vuo ' ti vuo ' chi ti movesti\n",
            "e chi non ti fa chi non ti resti\n",
            "        \n",
            "\n",
            "\n",
            "\n",
            "sí non ti fa non non ti non non non non do non non non non non ' non vuo non ' vuo non ' non ti chi non vesti ' non ti tu tu tu\n",
            " vuo ' chi vesti ché tu vuo ' chi vuo ' ni ancor chi vuo ' chi vuo ' ti vuo ' ti vuo ' ti vuo ' ti vuo ' ché tu vuo ' vuo '\n",
            "\n",
            "  chi è chi tu chi tu vuo ' chi vuo ' chi ti vuo ' chi ti vuo ' chi vuo ' ti vuo ' ti vuo ' chi vuo ' chi ti vuo ' ' \n",
            "\n",
            "    ti fa chi tu vuo ' ti move anni ancor chi ti chi ti vuo ' chi ti move e ' ti move e ' l duro a vo e '\n",
            "  \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "to per ti ti e per chi ti ' ' tu tu chi non ' ' tico e ' tico e ' chi chi non ti ri ' ' tu ' ' '\n",
            "   ti riconi ' cominciò che tu vuo ' niti moni a ' ti vuo ' ti vuo ' ti movere\n",
            "e ' ti mosti e ' mi ricono ' ti ri\n",
            " e ' l duca mio duca che tu vuo ' ti movere\n",
            "per lo duca e ' l duca si movere e ' ti more\n",
            "e ' ti more ' ti rinostro \n",
            " ti mostri e ' l duca e ' ti movere\n",
            "e ' l duca mio se ' ti move ' che tu non ti richi ti vuo ' ti re\n",
            "     \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "e ' e ' ti e ti non ti e ' ' e tu per ri ri non chi chi per ti ri ' ti ti chi ti ri non ' tu ' ti ri '\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}