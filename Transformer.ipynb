{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPHL1NMzFX+0gAgqhk5hfGz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanieleVeri/deep_comedy/blob/master/Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHRo2NsK-Rgd",
        "colab_type": "code",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "a52daa78-069f-49ae-bdac-9a483e34e6d3"
      },
      "source": [
        "#@title Import & seed\n",
        "import time\n",
        "import re\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import requests\n",
        "import collections\n",
        "import pickle\n",
        "import copy, random\n",
        "import nltk as nl\n",
        "from itertools import zip_longest\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Reshape, BatchNormalization, Dense, Dropout, concatenate,\n",
        "    Embedding, LSTM, Dense, GRU, Bidirectional, Add\n",
        ")\n",
        "from tensorflow.keras.activations import elu, relu, softmax, sigmoid\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "print(tf.__version__)\n",
        "nl.download('punkt')\n",
        "np.random.seed(1234)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onTHkWW_YOHp",
        "colab_type": "code",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c8e68625-dea5-40a3-fdbf-ac4c1232ed70"
      },
      "source": [
        "#@title Model\n",
        "\n",
        "vocab_size = 1797\n",
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "input_vocab_size = vocab_size\n",
        "target_vocab_size = vocab_size\n",
        "dropout_rate = 0.1\n",
        "batch_len = 304\n",
        "EPOCHS = 100\n",
        "\n",
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "    return pos * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                            np.arange(d_model)[np.newaxis, :],\n",
        "                            d_model)\n",
        "    # apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    # apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "def create_padding_mask(seq):   \n",
        "    seq = tf.cast(tf.math.equal(seq, pad), tf.float32)\n",
        "    # add extra dimensions to add the padding\n",
        "    # to the attention logits.\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
        "\n",
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask  # (seq_len, seq_len)\n",
        "\n",
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    \"\"\"Calculate the attention weights.\n",
        "    q, k, v must have matching leading dimensions.\n",
        "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "    The mask has different shapes depending on its type(padding or look ahead) \n",
        "    but it must be broadcastable for addition.\n",
        "    \n",
        "    Args:\n",
        "        q: query shape == (..., seq_len_q, depth)\n",
        "        k: key shape == (..., seq_len_k, depth)\n",
        "        v: value shape == (..., seq_len_v, depth_v)\n",
        "        mask: Float tensor with shape broadcastable \n",
        "            to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "        \n",
        "    Returns:\n",
        "        output, attention_weights\n",
        "    \"\"\"\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "    # scale matmul_qk\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "    # add the mask to the scaled tensor.\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)  \n",
        "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "    # add up to 1.\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "    return output, attention_weights\n",
        "\n",
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "    ])\n",
        "\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        assert d_model % self.num_heads == 0\n",
        "        self.depth = d_model // self.num_heads\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "            \n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"Split the last dimension into (num_heads, depth).\n",
        "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "        \n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "        \n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "        \n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "        \n",
        "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "        concat_attention = tf.reshape(scaled_attention, \n",
        "                                    (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "        return output, attention_weights\n",
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "    def __call__(self, x, training, mask):\n",
        "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "        \n",
        "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "        \n",
        "        return out2\n",
        "\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "    \n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "        \n",
        "    def __call__(self, x, enc_output, training, \n",
        "            look_ahead_mask, padding_mask):\n",
        "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "        \n",
        "        attn2, attn_weights_block2 = self.mha2(\n",
        "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "        \n",
        "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
        "        \n",
        "        return out3, attn_weights_block1, attn_weights_block2\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "                maximum_position_encoding, rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)   ########## rm ??????\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
        "                                                self.d_model)\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
        "                        for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "            \n",
        "    def __call__(self, x, training, mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        # adding embedding and position encoding.\n",
        "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "        x = self.dropout(x, training=training)\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "        return x  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "                maximum_position_encoding, rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
        "                        for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "    def __call__(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "        x = self.dropout(x, training=training)\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                                look_ahead_mask, padding_mask)\n",
        "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "        \n",
        "        # x.shape == (batch_size, target_seq_len, d_model)\n",
        "        return x, attention_weights\n",
        "\n",
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
        "                target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
        "                            input_vocab_size, pe_input, rate)\n",
        "\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
        "                            target_vocab_size, pe_target, rate)\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "        \n",
        "    def __call__(self, inp, tar, training, enc_padding_mask, \n",
        "            look_ahead_mask, dec_padding_mask):\n",
        "\n",
        "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
        "        \n",
        "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "        dec_output, attention_weights = self.decoder(\n",
        "            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "        \n",
        "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "        \n",
        "        return final_output, attention_weights\n",
        "\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "        \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "        \n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "learning_rate = CustomSchedule(d_model)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
        "                                     epsilon=1e-9)\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "    name='train_accuracy')\n",
        "\n",
        "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
        "                          input_vocab_size, target_vocab_size, \n",
        "                          pe_input=input_vocab_size, \n",
        "                          pe_target=target_vocab_size,\n",
        "                          rate=dropout_rate)\n",
        "def create_masks(inp, tar):\n",
        "    # Encoder padding mask\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "    \n",
        "    # Used in the 2nd attention block in the decoder.\n",
        "    # This padding mask is used to mask the encoder outputs.\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "    \n",
        "    # Used in the 1st attention block in the decoder.\n",
        "    # It is used to pad and mask future tokens in the input received by \n",
        "    # the decoder.\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "    \n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask\n",
        "\n",
        "checkpoint_path = \"./checkpoints/train\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "# if a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print ('Latest checkpoint restored!!')\n",
        "\n",
        "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
        "# execution. The function specializes to the precise shape of the argument\n",
        "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
        "# batch sizes (the last batch is smaller), use input_signature to specify\n",
        "# more generic shapes.\n",
        "\n",
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, batch_len), dtype=tf.int64),\n",
        "    tf.TensorSpec(shape=(None, batch_len), dtype=tf.int64),\n",
        "]\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _ = transformer(inp, tar_inp, \n",
        "                                    True, \n",
        "                                    enc_padding_mask, \n",
        "                                    combined_mask, \n",
        "                                    dec_padding_mask)\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "    \n",
        "    train_loss(loss)\n",
        "    train_accuracy(tar_real, predictions)\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Latest checkpoint restored!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GI14h2PZX70w",
        "colab_type": "code",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "outputId": "ce854074-e4ae-4d17-d9ab-afa131e55dce"
      },
      "source": [
        "#@title Preprocessing\n",
        "\n",
        "def get_hyp_lm_tercets(tercets):\n",
        "    new_tercets = []\n",
        "    for tercet in tercets:\n",
        "        new_tercets.append([])\n",
        "        for verse in tercet:\n",
        "            new_tercets[-1].append([])\n",
        "            for hyp_w in verse:\n",
        "                new_tercets[-1][-1].extend(hyp_w)\n",
        "                new_tercets[-1][-1].append('<SEP>')\n",
        "            new_tercets[-1][-1] = new_tercets[-1][-1][:-1]\n",
        "\n",
        "    return new_tercets\n",
        "\n",
        "def is_vowel(c):\n",
        "    return c in 'aeiouAEIOUàìíèéùúüòï'\n",
        "\n",
        "def unsplittable_cons():\n",
        "    u_cons = []\n",
        "    for c1 in ('b', 'c', 'd', 'f', 'g', 'p', 't', 'v'):\n",
        "        for c2 in ('l', 'r'):\n",
        "            u_cons.append(c1 + c2)\n",
        "\n",
        "    others = ['gn', 'gh', 'ch']\n",
        "    u_cons.extend(others)\n",
        "    return u_cons\n",
        "\n",
        "\n",
        "def are_cons_to_split(c1, c2):\n",
        "    to_split = ('cq', 'cn', 'lm', 'rc', 'bd', 'mb', 'mn', 'ld', 'ng', 'nd', 'tm', 'nv', 'nc', 'ft', 'nf', 'gm', 'fm', 'rv', 'fp')\n",
        "    return (c1 + c2) in to_split or (not is_vowel(c1) and (c1 == c2)) or ((c1 + c2) not in unsplittable_cons()) and (\n",
        "        (not is_vowel(c1)) and (not is_vowel(c2)) and c1 != 's')\n",
        "\n",
        "\n",
        "def is_diphthong(c1, c2):\n",
        "    return (c1 + c2) in ('ia', 'ie', 'io', 'iu', 'ua', 'ue', 'uo', 'ui', 'ai', 'ei', 'oi', 'ui', 'au', 'eu', 'ïe', 'iú', 'iù')\n",
        "\n",
        "\n",
        "def is_triphthong(c1, c2, c3):\n",
        "    return (c1 + c2 + c3) in ('iai', 'iei', 'uoi', 'uai', 'uei', 'iuo')\n",
        "\n",
        "\n",
        "def is_toned_vowel(c):\n",
        "    return c in 'àìèéùòï'\n",
        "\n",
        "def has_vowels(sy):\n",
        "    for c in sy:\n",
        "        if is_vowel(c):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def hyphenation(word):\n",
        "    \"\"\"\n",
        "    Split word in syllables\n",
        "    :param word: input string\n",
        "    :return: a list containing syllables of the word\n",
        "    \"\"\"\n",
        "    if not word or word == '':\n",
        "        return []\n",
        "    # elif len(word) == 3 and (is_vowel(word[1]) and is_vowel(word[2]) and not is_toned_vowel(word[2]) and (\n",
        "    #     not is_diphthong(word[1], word[2]) or (word[1] == 'i'))):\n",
        "    elif len(word) == 3 and (is_vowel(word[1]) and is_vowel(word[2]) and not is_toned_vowel(word[2]) and (\n",
        "        not is_diphthong(word[1], word[2]))):\n",
        "        return [word[:2]] + [word[2]]\n",
        "    elif len(word) == 3 and is_vowel(word[0]) and not is_vowel(word[1]) and is_vowel(word[2]):\n",
        "        return [word[:2]] + [word[2]]\n",
        "    elif len(word) == 3:\n",
        "        return [word]\n",
        "\n",
        "    syllables = []\n",
        "    is_done = False\n",
        "    count = 0\n",
        "    while not is_done and count <= len(word) - 1:\n",
        "        syllables.append('')\n",
        "        c = word[count]\n",
        "        while not is_vowel(c) and count < len(word) - 1:\n",
        "            syllables[-1] = syllables[-1] + c\n",
        "            count += 1\n",
        "            c = word[count]\n",
        "\n",
        "        syllables[-1] = syllables[-1] + word[count]\n",
        "\n",
        "        if count == len(word) - 1:\n",
        "            is_done = True\n",
        "        else:\n",
        "            count += 1\n",
        "\n",
        "            if count < len(word) and not is_vowel(word[count]):\n",
        "                if count == len(word) - 1:\n",
        "                    syllables[-1] += word[count]\n",
        "                    count += 1\n",
        "                elif count + 1 < len(word) and are_cons_to_split(word[count], word[count + 1]):\n",
        "                    syllables[-1] += word[count]\n",
        "                    count += 1\n",
        "                elif count + 2 < len(word) and not is_vowel(word[count + 1]) and not is_vowel(word[count + 2]) and word[\n",
        "                    count] != 's':\n",
        "                    syllables[-1] += word[count]\n",
        "                    count += 1\n",
        "            elif count < len(word):\n",
        "                if count + 1 < len(word) and is_triphthong(word[count - 1], word[count], word[count + 1]):\n",
        "                    syllables[-1] += word[count] + word[count + 1]\n",
        "                    count += 2\n",
        "                elif is_diphthong(word[count - 1], word[count]):\n",
        "                    syllables[-1] += word[count]\n",
        "                    count += 1\n",
        "\n",
        "                if count + 1 < len(word) and are_cons_to_split(word[count], word[count + 1]):\n",
        "                    syllables[-1] += word[count]\n",
        "                    count += 1\n",
        "\n",
        "            else:\n",
        "                is_done = True\n",
        "\n",
        "    if not has_vowels(syllables[-1]) and len(syllables) > 1:\n",
        "        syllables[-2] = syllables[-2] + syllables[-1]\n",
        "        syllables = syllables[:-1]\n",
        "\n",
        "    return syllables\n",
        "\n",
        "\n",
        "\n",
        "def get_dc_hyphenation(canti):\n",
        "    hyp_canti, hyp_tokens = [], []\n",
        "    for canto in canti:\n",
        "        hyp_canti.append([])\n",
        "        for verso in canto:\n",
        "            syllables = seq_hyphentation(verso)\n",
        "            hyp_canti[-1].append(syllables)\n",
        "            for syllable in syllables:\n",
        "                hyp_tokens.extend(syllable)\n",
        "\n",
        "    return hyp_canti, hyp_tokens\n",
        "\n",
        "\n",
        "def seq_hyphentation(words):\n",
        "    \"\"\"\n",
        "    Converts words in a list of strings into lists of syllables\n",
        "    :param words: a list of words (strings)\n",
        "    :return: a list of lists containing word syllables\n",
        "    \"\"\"\n",
        "    return [hyphenation(w) for w in words]\n",
        "\n",
        "\n",
        "def get_dc_cantos(filename, encoding=None):\n",
        "    # raw_data = read_words(filename=filename)\n",
        "    cantos, words, raw = [], [], []\n",
        "    with open(filename, \"r\", encoding=encoding) as f:\n",
        "        for line in f:\n",
        "            sentence = line.strip()\n",
        "            sentence = str.replace(sentence, \"\\.\", \" \\. \")\n",
        "            sentence = str.replace(sentence, \"[\", '')\n",
        "            sentence = str.replace(sentence, \"]\", '')\n",
        "            sentence = str.replace(sentence, \"-\", '')\n",
        "            sentence = str.replace(sentence, \";\", \" ; \")\n",
        "            sentence = str.replace(sentence, \",\", \" , \")\n",
        "            # sentence = str.replace(sentence, \" \\'\", '')\n",
        "            sentence = str.replace(sentence, \"\\'\", ' \\' ')\n",
        "            if len(sentence) > 1:\n",
        "                # sentence = sentence.translate(string.punctuation)\n",
        "                tokenized_sentence = nl.word_tokenize(sentence)\n",
        "                # tokenized_sentence = sentence.split()\n",
        "                tokenized_sentence = [w.lower() for w in tokenized_sentence if len(w) > 0]\n",
        "                tokenized_sentence = [w for w in tokenized_sentence if \",\" not in w]\n",
        "                tokenized_sentence = [w for w in tokenized_sentence if \".\" not in w]\n",
        "                tokenized_sentence = [w for w in tokenized_sentence if \":\" not in w]\n",
        "                tokenized_sentence = [w for w in tokenized_sentence if \";\" not in w]\n",
        "                tokenized_sentence = [w for w in tokenized_sentence if \"«\" not in w]\n",
        "                tokenized_sentence = [w for w in tokenized_sentence if \"»\" not in w]\n",
        "                # ts = []\n",
        "                ts = tokenized_sentence\n",
        "                # [ts.extend(re.split(\"(\\')\", e)) for e in tokenized_sentence]\n",
        "                tokenized_sentence = [w for w in ts if len(w) > 0]\n",
        "\n",
        "                if len(tokenized_sentence) == 2:\n",
        "                    cantos.append([])\n",
        "                    raw.append([])\n",
        "                elif len(tokenized_sentence) > 2:\n",
        "                    raw[-1].append(sentence)\n",
        "                    cantos[-1].append(tokenized_sentence)\n",
        "                    words.extend(tokenized_sentence)\n",
        "\n",
        "    return cantos, words, raw\n",
        "\n",
        "\n",
        "def create_tercets(cantos):\n",
        "    tercets = []\n",
        "    for i,canto in enumerate(cantos):\n",
        "        for v,verse in enumerate(canto):\n",
        "            if v%3 == 0:\n",
        "                tercets.append([])\n",
        "\n",
        "            tercets[-1].append(verse)\n",
        "        tercets = tercets[:-1]  # removes the last malformed tercets (only 2 verses)\n",
        "\n",
        "    return tercets\n",
        "\n",
        "def pad_list(l, pad_token, max_l_size, keep_lasts=False, pad_right=True):\n",
        "    \"\"\"\n",
        "    Adds a padding token to a list\n",
        "    inputs:\n",
        "    :param l: input list to pad.\n",
        "    :param pad_token: value to add as padding.\n",
        "    :param max_l_size: length of the new padded list to return,\n",
        "    it truncates lists longer that 'max_l_size' without adding\n",
        "    padding values.\n",
        "    :param keep_lasts: If True, preserves the max_l_size last elements\n",
        "    of a sequence (by keeping the same order).  E.g.:\n",
        "    if keep_lasts is True and max_l_size=3 [1,2,3,4] becomes [2,3,4].\n",
        "\n",
        "\n",
        "    :return: the list padded or truncated.\n",
        "    \"\"\"\n",
        "    to_pad = []\n",
        "    max_l = min(max_l_size, len(l))  # maximum len\n",
        "    l_init = len(l) - max_l if len(l) > max_l and keep_lasts else 0  # initial position where to sample from the list\n",
        "    l_end = len(l) if len(l) > max_l and keep_lasts else max_l\n",
        "    for i in range(l_init, l_end):\n",
        "        to_pad.append(l[i])\n",
        "\n",
        "    # for j in range(len(l), max_l_size):\n",
        "    #     to_pad.append(pad_token)\n",
        "    pad_tokens = [pad_token] * (max_l_size-len(l))\n",
        "    padded_l = to_pad + pad_tokens if pad_right else pad_tokens + to_pad\n",
        "\n",
        "    return padded_l\n",
        "\n",
        "\n",
        "def save_data(data, file):\n",
        "    with open(file, 'wb') as output:\n",
        "        pickle.dump(data, output, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def load_data(file):\n",
        "    with open(file, 'rb') as obj:\n",
        "        return pickle.load(obj)\n",
        "\n",
        "def print_and_write(file, s):\n",
        "    print(s)\n",
        "    file.write(s)\n",
        "\n",
        "\n",
        "class Vocabulary(object):\n",
        "    def __init__(self, vocab_size=None):\n",
        "        self.dictionary = dict()\n",
        "        self.rev_dictionary = dict()\n",
        "        self.count = []\n",
        "        self.special_tokens = []\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    def build_vocabulary_from_counts(self, count, special_tokens=[]):\n",
        "        \"\"\"\n",
        "        Sets all the attributes of the Vocabulary object.\n",
        "        :param count: a list of lists as follows: [['token', number_of_occurrences],...]\n",
        "        :param special_tokens: a list of strings. E.g. ['<EOS>', '<PAD>',...]\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "\n",
        "        dictionary = dict()\n",
        "        for word, _ in count:\n",
        "            dictionary[word] = len(dictionary)\n",
        "\n",
        "        # adding eventual special tokens to the dictionary (e.g. <EOS>,<PAD> etc..)\n",
        "        d = len(dictionary)\n",
        "        for i, token in enumerate(special_tokens):\n",
        "            dictionary[token] = d + i\n",
        "\n",
        "        self.count = count\n",
        "        self.dictionary = dictionary\n",
        "        self.rev_dictionary = dict(zip(self.dictionary.values(), self.dictionary.keys()))\n",
        "        self.special_tokens = special_tokens\n",
        "        self.vocab_size = len(dictionary)\n",
        "\n",
        "    def build_vocabulary_from_tokens(self, tokens, vocabulary_size=None, special_tokens=[]):\n",
        "        \"\"\"\n",
        "        Given a list of tokens, it sets the Vocabulary object attributes by constructing\n",
        "        a dictionary mapping each token to a unique id.\n",
        "        :param tokens: a list of strings.\n",
        "         E.g. [\"the\", \"cat\", \"is\", ... \".\", \"the\", \"house\" ,\"is\" ...].\n",
        "         NB: Here you should put all your token instances of the corpus.\n",
        "        :param vocabulary_size: The number of elements of your vocabulary. If there are more\n",
        "        than 'vocabulary_size' elements on tokens, it considers only the 'vocabulary_size'\n",
        "        most frequent ones.\n",
        "        :param special_tokens: Optional. A list of strings. Useful to add special tokens in vocabulary.\n",
        "        If you don't have any, keep it empty.\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "\n",
        "        vocabulary_size = vocabulary_size if vocabulary_size is not None else self.vocab_size\n",
        "        vocabulary_size = vocabulary_size - (len(special_tokens) + 1) if vocabulary_size else None\n",
        "        # counts occurrences of each token\n",
        "        count = [['<UNK>', -1]]\n",
        "        count.extend(collections.Counter(tokens).most_common(vocabulary_size))  # takes only the most frequent ones, if size is None takes them all\n",
        "        self.build_vocabulary_from_counts(count, special_tokens)  # actually build the vocabulary\n",
        "        self._set_unk_count(tokens)  # set the number of OOV instances\n",
        "\n",
        "    @staticmethod\n",
        "    def merge_vocabulary(vocab0, vocab1, vocabulary_size=-1):\n",
        "        \"\"\"\n",
        "        Merge two Vocabulary objects into a new one.\n",
        "        :param vocab0: first Vocabulary object\n",
        "        :param vocab1: second Vocabulary object\n",
        "        :param vocabulary_size: parameter to decide the merged vocabulary size.\n",
        "        With default value -1, all the words of both vocabularies are preserved.\n",
        "        When set to 0, the size of the vocabulary is set to the size of vocab0,\n",
        "        when set to 1 it is kept the size of vocab1.\n",
        "        :return: a new vocabulary\n",
        "        \"\"\"\n",
        "        # get size of the new vocabulary\n",
        "        vocab_size = vocab0.vocab_size + vocab1.vocab_size if vocabulary_size == -1 else vocabulary_size\n",
        "        merged_special_tokens = list(set(vocab0.special_tokens) | set(vocab1.special_tokens))\n",
        "\n",
        "        # merge the counts from the two vocabularies and then selects the most_common tokens\n",
        "        merged_counts = collections.Counter(dict(vocab0.count)) + collections.Counter(dict(vocab1.count))\n",
        "        merged_counts = merged_counts.most_common(vocab_size)\n",
        "        count = [['<UNK>', -1]]\n",
        "        count.extend(merged_counts)\n",
        "\n",
        "        # create the new vocabulary\n",
        "        merged_vocab = Vocabulary(vocab_size)\n",
        "        merged_vocab.build_vocabulary_from_counts(count, merged_special_tokens)\n",
        "        return merged_vocab\n",
        "\n",
        "    @staticmethod\n",
        "    def merge_vocabularies(vocab_list, vocab_size=None):\n",
        "        \"\"\"\n",
        "        Join a list of vocabularies into a new one.\n",
        "        :param vocab_list: a list of Vocabulary objects\n",
        "        :param vocab_size: the maximum size of the merged vocabulary.\n",
        "        :return: a vocabulary merging them all.\n",
        "        \"\"\"\n",
        "        vocab_size = vocab_size if vocab_size else sum([v.vocab_size for v in vocab_list])\n",
        "        merged_vocab = Vocabulary(vocab_size)\n",
        "        for voc in vocab_list:\n",
        "            merged_vocab = Vocabulary.merge_vocabulary(merged_vocab, voc, vocab_size)\n",
        "        return merged_vocab\n",
        "\n",
        "    def string2id(self, dataset):\n",
        "        \"\"\"\n",
        "        Converts a dataset of strings into a dataset of ids according to the object dictionary.\n",
        "        :param dataset: any string-based dataset with any nested lists.\n",
        "        :return: a new dataset, with the same shape of dataset, where each string is mapped into its\n",
        "        corresponding id associated in the dictionary (0 for unknown tokens).\n",
        "        \"\"\"\n",
        "\n",
        "        def _recursive_call(items):\n",
        "            new_items = []\n",
        "            for item in items:\n",
        "                if isinstance(item, str) or isinstance(item, int) or isinstance(item, float):\n",
        "                    new_items.append(self.word2id(item))\n",
        "                else:\n",
        "                    new_items.append(_recursive_call(item))\n",
        "            return new_items\n",
        "\n",
        "        return _recursive_call(dataset)\n",
        "\n",
        "    def id2string(self, dataset):\n",
        "        \"\"\"\n",
        "        Converts a dataset of integer ids into a dataset of string according to the reverse dictionary.\n",
        "        :param dataset: any int-based dataset with any nested lists. Allowed types are int, np.int32, np.int64.\n",
        "        :return: a new dataset, with the same shape of dataset, where each token is mapped into its\n",
        "        corresponding string associated in the reverse dictionary.\n",
        "        \"\"\"\n",
        "        def _recursive_call(items):\n",
        "            new_items = []\n",
        "            for item in items:\n",
        "                if isinstance(item, int) or isinstance(item, np.int) or isinstance(item, np.int32) or isinstance(item, np.int64):\n",
        "                    new_items.append(self.id2word(item))\n",
        "                else:\n",
        "                    new_items.append(_recursive_call(item))\n",
        "            return new_items\n",
        "\n",
        "        return _recursive_call(dataset)\n",
        "\n",
        "    def word2id(self, item):\n",
        "        \"\"\"\n",
        "        Maps a string token to its corresponding id.\n",
        "        :param item: a string.\n",
        "        :return: If the token belongs to the vocabulary, it returns an integer id > 0, otherwise\n",
        "        it returns the value associated to the unknown symbol, that is typically 0.\n",
        "        \"\"\"\n",
        "        return self.dictionary[item] if item in self.dictionary else self.dictionary['<UNK>']\n",
        "\n",
        "    def id2word(self, token_id):\n",
        "        \"\"\"\n",
        "        Maps an integer token to its corresponding string.\n",
        "        :param token_id: an integer.\n",
        "        :return: If the id belongs to the vocabulary, it returns the string\n",
        "        associated to it, otherwise it returns the string associated\n",
        "        to the unknown symbol, that is '<UNK>'.\n",
        "        \"\"\"\n",
        "\n",
        "        return self.rev_dictionary[token_id] if token_id in self.rev_dictionary else self.rev_dictionary[self.dictionary['<UNK>']]\n",
        "\n",
        "    def get_unk_count(self):\n",
        "        return self.count[0][1]\n",
        "\n",
        "    def _set_unk_count(self, tokens):\n",
        "        \"\"\"\n",
        "        Sets the number of OOV instances in the tokens provided\n",
        "        :param tokens: a list of tokens\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "        data = list()\n",
        "        unk_count = 0\n",
        "        for word in tokens:\n",
        "            if word in self.dictionary:\n",
        "                index = self.dictionary[word]\n",
        "            else:\n",
        "                index = 0  # dictionary['<UNK>']\n",
        "                unk_count += 1\n",
        "            data.append(index)\n",
        "        self.count[0][1] = unk_count\n",
        "\n",
        "    def add_element(self, name, is_special_token=False):\n",
        "        if name not in self.dictionary:\n",
        "            self.vocab_size += 1\n",
        "            self.dictionary[name] = self.vocab_size\n",
        "            self.rev_dictionary[self.vocab_size] = name\n",
        "\n",
        "            if is_special_token:\n",
        "                self.special_tokens = list(self.special_tokens)\n",
        "                self.special_tokens.append(name)\n",
        "\n",
        "            self.count.append([name, 1])\n",
        "\n",
        "    def set_vocabulary(self, dictionary, rev_dictionary, special_tokens, vocab_size):\n",
        "        self.dictionary = dictionary,\n",
        "        self.rev_dictionary = rev_dictionary\n",
        "        self.special_tokens = special_tokens\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    @staticmethod\n",
        "    def load_vocabulary(filename):\n",
        "        return load_data(filename)\n",
        "\n",
        "    def save_vocabulary(self, filename):\n",
        "        save_data(self, filename)\n",
        "\n",
        "class SyLMDataset(object):\n",
        "    def __init__(self, config, sy_vocab=None):\n",
        "        self.config = config\n",
        "        self.vocabulary = sy_vocab\n",
        "\n",
        "        self.raw_train_x = []\n",
        "        self.raw_val_x = []\n",
        "        self.raw_test_x = []\n",
        "        self.raw_x = []\n",
        "\n",
        "        self.train_x, self.train_y = [], []\n",
        "        self.val_x, self.val_y = [], []\n",
        "        self.test_x, self.test_y = [], []\n",
        "        self.x, self.y = [], []\n",
        "\n",
        "    def initialize(self, sess):\n",
        "        pass\n",
        "\n",
        "    def load(self, sources):\n",
        "        \"\"\"\n",
        "        Extract raw texts form sources and gather them all together.\n",
        "        :param sources: a string or an iterable of strings containing the file(s)\n",
        "        to process in order to build the dataset.\n",
        "        :return: a list of raw strings.\n",
        "        \"\"\"\n",
        "        return NotImplementedError\n",
        "\n",
        "    def build(self, sources, split_size=0.8):\n",
        "        \"\"\"\n",
        "        :param sources: a string or an iterable of strings containing the file(s)\n",
        "        to process in order to build the dataset.\n",
        "        :param split_size: the size to split the dataset, set >=1.0 to not split.\n",
        "        \"\"\"\n",
        "\n",
        "        raw_x = self.load(sources)\n",
        "        # raw_x = self.tokenize([self.preprocess(ex) for ex in raw_x])  # fixme\n",
        "        # splitting data\n",
        "        self.raw_x = raw_x\n",
        "        if split_size < 1.0:\n",
        "            self.raw_train_x, self.raw_test_x = self.split(self.raw_x, train_size=split_size)\n",
        "            self.raw_train_x, self.raw_val_x = self.split(self.raw_train_x, train_size=split_size)\n",
        "        else:\n",
        "            self.raw_train_x = self.raw_x\n",
        "\n",
        "        if self.vocabulary is None:\n",
        "            # creates vocabulary\n",
        "            tokens = [item for sublist in self.raw_train_x for item in sublist]  # get tokens\n",
        "            special_tokens = (\"<GO>\", \"<PAD>\", \"<SEP>\", \"<EOS>\", \"<EOV>\")\n",
        "            self._create_vocab(tokens, special_tokens=special_tokens)\n",
        "\n",
        "        # creates x,y for train\n",
        "        self.train_x = self._build_dataset(self.raw_train_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "        self.train_y = self._build_dataset(self.raw_train_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "\n",
        "        # creates x,y for validation\n",
        "        self.val_x = self._build_dataset(self.raw_val_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "        self.val_y = self._build_dataset(self.raw_val_x, insert_go=False, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "\n",
        "        # creates x,y for validation\n",
        "        self.test_x = self._build_dataset(self.raw_test_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "        self.test_y = self._build_dataset(self.raw_test_x, insert_go=False, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "\n",
        "    def _create_vocab(self, tokens, special_tokens=(\"<PAD>\", \"<GO>\", \"<SEP>\", \"<EOV>\", \"<EOS>\")):\n",
        "        \"\"\"\n",
        "        Create the vocabulary. Special tokens can be added to the tokens obtained from\n",
        "        the corpus.\n",
        "        :param tokens: a list of all the tokens in the corpus. Each token is a string.\n",
        "        :param special_tokens: a list of strings.\n",
        "        \"\"\"\n",
        "        vocab = Vocabulary(vocab_size=self.config.input_vocab_size)\n",
        "        vocab.build_vocabulary_from_tokens(tokens, special_tokens=special_tokens)\n",
        "        self.vocabulary = vocab\n",
        "\n",
        "    @staticmethod\n",
        "    def split(raw_data, train_size=0.8):\n",
        "        size = math.floor(len(raw_data)*train_size)\n",
        "        return raw_data[:size], raw_data[size:]\n",
        "\n",
        "    @staticmethod\n",
        "    def preprocess(txt):\n",
        "        return txt\n",
        "\n",
        "    @staticmethod\n",
        "    def shuffle(x):\n",
        "        return random.sample(x, len(x))\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenize(txt):\n",
        "        return txt\n",
        "\n",
        "    def _build_dataset(self, raw_data, max_len=100, insert_go=True, keep_lasts=False, pad_right=True, shuffle=True):\n",
        "        \"\"\"\n",
        "        Converts all the tokens in e1_raw_data by mapping each token with its corresponding\n",
        "        value in the dictionary. In case of token not in the dictionary, they are assigned to\n",
        "        a specific id. Each sequence is padded up to the seq_max_len setup in the config.\n",
        "\n",
        "        :param raw_data: list of sequences, each sequence is a list of tokens (strings).\n",
        "        :param max_len: max length of a sequence, crop longer and pad smaller ones.\n",
        "        :param insert_go: True to insert <GO>, False otherwise.\n",
        "        :param keep_lasts: True to truncate initial elements of a sequence.\n",
        "        :param pad_right: pad to the right (default value True), otherwise pads to left.\n",
        "        :param shuffle: Optional. If True data are shuffled.\n",
        "        :return: A list of sequences where each token in each sequence is an int id.\n",
        "        \"\"\"\n",
        "        dataset = []\n",
        "        for sentence in raw_data:\n",
        "            sentence_ids = [self.vocabulary.word2id(\"<GO>\")] if insert_go else []\n",
        "            sentence_ids.extend([self.vocabulary.word2id(w) for w in sentence])\n",
        "            sentence_ids.append(self.vocabulary.word2id(\"<EOS>\"))\n",
        "            sentence_ids = pad_list(sentence_ids, self.vocabulary.word2id(\"<PAD>\"), max_len, keep_lasts=keep_lasts, pad_right=pad_right)\n",
        "\n",
        "            dataset.append(sentence_ids)\n",
        "\n",
        "        if shuffle:\n",
        "            return random.sample(dataset, len(dataset))\n",
        "        else:\n",
        "            return dataset\n",
        "\n",
        "    def get_batches(self, batch_size=32):\n",
        "        \"\"\"\n",
        "        Iterator over the training set. Useful method to run experiments.\n",
        "        :param batch_size: size of the mini_batch\n",
        "        :return: input and target.\n",
        "        \"\"\"\n",
        "        x, y = self.train_x, self.train_y\n",
        "        \n",
        "        i = 0#random.randint(0, batch_size)\n",
        "        batches = []\n",
        "        eov = self.vocabulary.word2id(\"<EOV>\")\n",
        "        go = self.vocabulary.word2id(\"<GO>\")\n",
        "        # prepare batches\n",
        "        while i < len(x):\n",
        "            j = 0\n",
        "            batch_x, batch_y = [], []\n",
        "            while j < batch_size and i+j<len(x):\n",
        "                for c in x[i+j]:\n",
        "                  batch_x.append(c)\n",
        "                batch_x.append(eov)\n",
        "                for c in y[i+j]:\n",
        "                  batch_y.append(c)\n",
        "                batch_y.append(eov)\n",
        "                j += 1\n",
        "            i += batch_size\n",
        "            batches.append((batch_x, batch_y))\n",
        "\n",
        "        # supply\n",
        "        i = 0\n",
        "        while i < len(batches):\n",
        "            yield batches[i][0], batches[i][1]\n",
        "            i += 1\n",
        "\n",
        "class DanteSyLMDataset(SyLMDataset):\n",
        "    def __init__(self, config, sy_vocab=None):\n",
        "        \"\"\"\n",
        "        Class to create a dataset from Dante Alighieri's Divine Comedy.\n",
        "        :param config: a Config object\n",
        "        :param sy_vocab: (optional) a Vocabulary object where tokens of the dictionary\n",
        "        are syllables. If None, the vocabulary is create automatically from the source.\n",
        "        \"\"\"\n",
        "        super().__init__(config, sy_vocab)\n",
        "\n",
        "    def load(self, sources):\n",
        "        \"\"\"\n",
        "        Load examples from dataset\n",
        "        :param sources: data filepath.\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        canti, _, raw = get_dc_cantos(filename=sources)  # get raw data from file\n",
        "        canti, tokens = get_dc_hyphenation(canti)  # converts each\n",
        "\n",
        "        tercets = create_tercets(canti)\n",
        "        tercets = get_hyp_lm_tercets(tercets)\n",
        "\n",
        "        x = []\n",
        "        for tercet in tercets:\n",
        "            x.append([])\n",
        "            for verse in tercet:\n",
        "                x[-1].extend(verse)\n",
        "                x[-1].append(\"<EOV>\")\n",
        "\n",
        "        #x = self.shuffle(x)\n",
        "        return x\n",
        "\n",
        "def seq2str(seq):\n",
        "    def output2string(batch, rev_vocabulary, special_tokens, end_of_tokens):\n",
        "        to_print = ''\n",
        "        for token in batch:\n",
        "            if token in special_tokens:\n",
        "                to_print += ' '\n",
        "            elif end_of_tokens and token in end_of_tokens:\n",
        "                to_print += '\\n'\n",
        "            elif token in rev_vocabulary:\n",
        "                to_print += rev_vocabulary[token]\n",
        "            else:\n",
        "                to_print += '<UNK>'\n",
        "        return to_print\n",
        "\n",
        "    return output2string(seq, poetry_sy_lm_dataset.vocabulary.rev_dictionary,\n",
        "      special_tokens=[poetry_sy_lm_dataset.vocabulary.word2id(\"<PAD>\"), 0, poetry_sy_lm_dataset.vocabulary.word2id(\"<SEP>\"),\n",
        "                      poetry_sy_lm_dataset.vocabulary.word2id(\"<GO>\"), poetry_sy_lm_dataset.vocabulary.word2id(\"<EOS>\")],\n",
        "      end_of_tokens=[poetry_sy_lm_dataset.vocabulary.word2id(\"<EOV>\")])\n",
        "\n",
        "class cnfg:\n",
        "  vocab_size = 1884\n",
        "  input_vocab_size = 1884\n",
        "  emb_size = 300\n",
        "  sentence_max_len = 75\n",
        "\n",
        "config = cnfg()\n",
        "poetry_sy_lm_dataset = DanteSyLMDataset(config, sy_vocab=None)\n",
        "url = \"https://gitlab.com/zugo91/nlgpoetry/-/raw/release/data/la_divina_commedia.txt\"\n",
        "response = requests.get(url)\n",
        "response.encoding = 'ISO-8859-1'\n",
        "fi = open(\"divcom.txt\",\"w\")\n",
        "fi.write(response.text)\n",
        "fi.close()\n",
        "data_path = os.path.join(os.getcwd(), \"divcom.txt\")  # dataset location, here just the name of the source file\n",
        "poetry_sy_lm_dataset.build(data_path, split_size=0.9)  # actual creation of  vocabulary (if not provided) and dataset\n",
        "print(\"Train size: \" + str(len(poetry_sy_lm_dataset.train_y)))\n",
        "print(\"Val size: \" + str(len(poetry_sy_lm_dataset.val_y)))\n",
        "print(\"Test size: \" + str(len(poetry_sy_lm_dataset.test_y)))\n",
        "\n",
        "eov = poetry_sy_lm_dataset.vocabulary.word2id(\"<EOV>\")\n",
        "pad = poetry_sy_lm_dataset.vocabulary.word2id(\"<PAD>\")\n",
        "go = poetry_sy_lm_dataset.vocabulary.word2id(\"<GO>\")\n",
        "eos = poetry_sy_lm_dataset.vocabulary.word2id(\"<EOS>\")\n",
        "\n",
        "batches = [b for b in poetry_sy_lm_dataset.get_batches(4)]\n",
        "print(batches[0][0])\n",
        "print(batches[0][1])\n",
        "print(len(batches[0][0]))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train size: 3815\n",
            "Val size: 424\n",
            "Test size: 472\n",
            "[1792, 84, 1794, 339, 198, 1794, 50, 1794, 284, 188, 1794, 6, 1794, 24, 136, 1794, 37, 14, 1796, 27, 1794, 32, 61, 509, 1794, 18, 1794, 52, 5, 1794, 404, 47, 1794, 40, 276, 30, 1796, 89, 1794, 8, 1794, 6, 683, 14, 1794, 264, 1794, 92, 5, 1794, 655, 32, 14, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 585, 1794, 78, 9, 1794, 5, 1794, 238, 1794, 162, 1794, 92, 5, 1794, 57, 1794, 12, 45, 1794, 116, 30, 1796, 4, 63, 1794, 404, 47, 1794, 404, 748, 148, 1794, 4, 1794, 5, 1203, 1794, 4, 1794, 120, 10, 1796, 7, 1794, 84, 1794, 166, 536, 1794, 32, 24, 47, 1794, 8, 1794, 455, 30, 1794, 153, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 792, 1794, 3, 1794, 57, 1794, 5, 22, 30, 1794, 7, 1794, 49, 12, 1794, 57, 1794, 64, 1794, 144, 10, 1796, 22, 1794, 18, 1794, 416, 202, 1794, 50, 1794, 154, 1794, 46, 1794, 3, 1794, 41, 1794, 37, 1794, 61, 509, 1796, 6, 150, 1794, 193, 1794, 3, 1794, 15, 124, 1794, 12, 11, 1794, 46, 1794, 3, 1794, 53, 1794, 3, 1794, 405, 1794, 3, 1794, 456, 1794, 336, 10, 1796, 1796, 1792, 41, 1794, 31, 1794, 23, 1794, 154, 1794, 32, 238, 1794, 131, 1794, 3, 1794, 41, 1794, 405, 1794, 3, 1794, 257, 716, 1796, 792, 1794, 3, 1794, 92, 5, 1794, 163, 24, 1794, 6, 1794, 138, 24, 1794, 5, 1794, 60, 1794, 293, 9, 1796, 7, 1794, 8, 1794, 25, 30, 43, 1794, 264, 1794, 337, 539, 17, 702, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796]\n",
            "[1792, 84, 1794, 339, 198, 1794, 50, 1794, 284, 188, 1794, 6, 1794, 24, 136, 1794, 37, 14, 1796, 27, 1794, 32, 61, 509, 1794, 18, 1794, 52, 5, 1794, 404, 47, 1794, 40, 276, 30, 1796, 89, 1794, 8, 1794, 6, 683, 14, 1794, 264, 1794, 92, 5, 1794, 655, 32, 14, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 585, 1794, 78, 9, 1794, 5, 1794, 238, 1794, 162, 1794, 92, 5, 1794, 57, 1794, 12, 45, 1794, 116, 30, 1796, 4, 63, 1794, 404, 47, 1794, 404, 748, 148, 1794, 4, 1794, 5, 1203, 1794, 4, 1794, 120, 10, 1796, 7, 1794, 84, 1794, 166, 536, 1794, 32, 24, 47, 1794, 8, 1794, 455, 30, 1794, 153, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 792, 1794, 3, 1794, 57, 1794, 5, 22, 30, 1794, 7, 1794, 49, 12, 1794, 57, 1794, 64, 1794, 144, 10, 1796, 22, 1794, 18, 1794, 416, 202, 1794, 50, 1794, 154, 1794, 46, 1794, 3, 1794, 41, 1794, 37, 1794, 61, 509, 1796, 6, 150, 1794, 193, 1794, 3, 1794, 15, 124, 1794, 12, 11, 1794, 46, 1794, 3, 1794, 53, 1794, 3, 1794, 405, 1794, 3, 1794, 456, 1794, 336, 10, 1796, 1796, 1792, 41, 1794, 31, 1794, 23, 1794, 154, 1794, 32, 238, 1794, 131, 1794, 3, 1794, 41, 1794, 405, 1794, 3, 1794, 257, 716, 1796, 792, 1794, 3, 1794, 92, 5, 1794, 163, 24, 1794, 6, 1794, 138, 24, 1794, 5, 1794, 60, 1794, 293, 9, 1796, 7, 1794, 8, 1794, 25, 30, 43, 1794, 264, 1794, 337, 539, 17, 702, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796]\n",
            "304\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwdCuj3HsFux",
        "colab_type": "code",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "365aa38b-86cb-4de4-ef2e-48022d6bcaea"
      },
      "source": [
        " #@title Train loop\n",
        "\n",
        " for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    \n",
        "    for (batch, (inp, tar)) in enumerate(batches):\n",
        "        if (len(inp) != batch_len or len(tar) != batch_len):\n",
        "            print(\"discarded batch\", batch)\n",
        "            continue\n",
        "        train_step(np.expand_dims(inp, axis=0), np.expand_dims(tar, axis=0))\n",
        "        \n",
        "        if batch % 50 == 0:\n",
        "            print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
        "                epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
        "        \n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        ckpt_save_path = ckpt_manager.save()\n",
        "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
        "                                                            ckpt_save_path))\n",
        "    \n",
        "    print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
        "                                                    train_loss.result(), \n",
        "                                                    train_accuracy.result()))\n",
        "\n",
        "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 1.1884 Accuracy 0.7195\n",
            "Epoch 1 Batch 50 Loss 1.2477 Accuracy 0.7063\n",
            "Epoch 1 Batch 100 Loss 1.2491 Accuracy 0.7086\n",
            "Epoch 1 Batch 150 Loss 1.2577 Accuracy 0.7091\n",
            "Epoch 1 Batch 200 Loss 1.2624 Accuracy 0.7093\n",
            "Epoch 1 Batch 250 Loss 1.2682 Accuracy 0.7091\n",
            "Epoch 1 Batch 300 Loss 1.2725 Accuracy 0.7081\n",
            "Epoch 1 Batch 350 Loss 1.2737 Accuracy 0.7074\n",
            "Epoch 1 Batch 400 Loss 1.2779 Accuracy 0.7065\n",
            "Epoch 1 Batch 450 Loss 1.2760 Accuracy 0.7066\n",
            "Epoch 1 Batch 500 Loss 1.2776 Accuracy 0.7060\n",
            "Epoch 1 Batch 550 Loss 1.2790 Accuracy 0.7054\n",
            "Epoch 1 Batch 600 Loss 1.2771 Accuracy 0.7057\n",
            "Epoch 1 Batch 650 Loss 1.2762 Accuracy 0.7058\n",
            "Epoch 1 Batch 700 Loss 1.2757 Accuracy 0.7058\n",
            "Epoch 1 Batch 750 Loss 1.2744 Accuracy 0.7058\n",
            "Epoch 1 Batch 800 Loss 1.2737 Accuracy 0.7062\n",
            "Epoch 1 Batch 850 Loss 1.2717 Accuracy 0.7063\n",
            "Epoch 1 Batch 900 Loss 1.2692 Accuracy 0.7067\n",
            "Epoch 1 Batch 950 Loss 1.2679 Accuracy 0.7071\n",
            "discarded batch 953\n",
            "Epoch 1 Loss 1.2679 Accuracy 0.7071\n",
            "Time taken for 1 epoch: 67.73261451721191 secs\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.1411 Accuracy 0.7393\n",
            "Epoch 2 Batch 50 Loss 1.2375 Accuracy 0.7107\n",
            "Epoch 2 Batch 100 Loss 1.2466 Accuracy 0.7113\n",
            "Epoch 2 Batch 150 Loss 1.2552 Accuracy 0.7114\n",
            "Epoch 2 Batch 200 Loss 1.2626 Accuracy 0.7105\n",
            "Epoch 2 Batch 250 Loss 1.2668 Accuracy 0.7092\n",
            "Epoch 2 Batch 300 Loss 1.2723 Accuracy 0.7074\n",
            "Epoch 2 Batch 350 Loss 1.2722 Accuracy 0.7080\n",
            "Epoch 2 Batch 400 Loss 1.2755 Accuracy 0.7075\n",
            "Epoch 2 Batch 450 Loss 1.2737 Accuracy 0.7071\n",
            "Epoch 2 Batch 500 Loss 1.2747 Accuracy 0.7068\n",
            "Epoch 2 Batch 550 Loss 1.2751 Accuracy 0.7067\n",
            "Epoch 2 Batch 600 Loss 1.2739 Accuracy 0.7066\n",
            "Epoch 2 Batch 650 Loss 1.2730 Accuracy 0.7067\n",
            "Epoch 2 Batch 700 Loss 1.2717 Accuracy 0.7068\n",
            "Epoch 2 Batch 750 Loss 1.2716 Accuracy 0.7068\n",
            "Epoch 2 Batch 800 Loss 1.2704 Accuracy 0.7072\n",
            "Epoch 2 Batch 850 Loss 1.2687 Accuracy 0.7073\n",
            "Epoch 2 Batch 900 Loss 1.2665 Accuracy 0.7077\n",
            "Epoch 2 Batch 950 Loss 1.2650 Accuracy 0.7081\n",
            "discarded batch 953\n",
            "Epoch 2 Loss 1.2650 Accuracy 0.7081\n",
            "Time taken for 1 epoch: 67.36659240722656 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.0812 Accuracy 0.7525\n",
            "Epoch 3 Batch 50 Loss 1.2186 Accuracy 0.7159\n",
            "Epoch 3 Batch 100 Loss 1.2314 Accuracy 0.7150\n",
            "Epoch 3 Batch 150 Loss 1.2459 Accuracy 0.7132\n",
            "Epoch 3 Batch 200 Loss 1.2552 Accuracy 0.7121\n",
            "Epoch 3 Batch 250 Loss 1.2615 Accuracy 0.7113\n",
            "Epoch 3 Batch 300 Loss 1.2637 Accuracy 0.7103\n",
            "Epoch 3 Batch 350 Loss 1.2649 Accuracy 0.7097\n",
            "Epoch 3 Batch 400 Loss 1.2689 Accuracy 0.7087\n",
            "Epoch 3 Batch 450 Loss 1.2674 Accuracy 0.7086\n",
            "Epoch 3 Batch 500 Loss 1.2698 Accuracy 0.7079\n",
            "Epoch 3 Batch 550 Loss 1.2707 Accuracy 0.7079\n",
            "Epoch 3 Batch 600 Loss 1.2706 Accuracy 0.7080\n",
            "Epoch 3 Batch 650 Loss 1.2702 Accuracy 0.7081\n",
            "Epoch 3 Batch 700 Loss 1.2690 Accuracy 0.7084\n",
            "Epoch 3 Batch 750 Loss 1.2680 Accuracy 0.7084\n",
            "Epoch 3 Batch 800 Loss 1.2665 Accuracy 0.7086\n",
            "Epoch 3 Batch 850 Loss 1.2649 Accuracy 0.7087\n",
            "Epoch 3 Batch 900 Loss 1.2632 Accuracy 0.7088\n",
            "Epoch 3 Batch 950 Loss 1.2616 Accuracy 0.7091\n",
            "discarded batch 953\n",
            "Epoch 3 Loss 1.2614 Accuracy 0.7092\n",
            "Time taken for 1 epoch: 67.0989077091217 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.1448 Accuracy 0.7294\n",
            "Epoch 4 Batch 50 Loss 1.2422 Accuracy 0.7136\n",
            "Epoch 4 Batch 100 Loss 1.2450 Accuracy 0.7121\n",
            "Epoch 4 Batch 150 Loss 1.2509 Accuracy 0.7109\n",
            "Epoch 4 Batch 200 Loss 1.2567 Accuracy 0.7109\n",
            "Epoch 4 Batch 250 Loss 1.2617 Accuracy 0.7102\n",
            "Epoch 4 Batch 300 Loss 1.2681 Accuracy 0.7085\n",
            "Epoch 4 Batch 350 Loss 1.2696 Accuracy 0.7083\n",
            "Epoch 4 Batch 400 Loss 1.2719 Accuracy 0.7079\n",
            "Epoch 4 Batch 450 Loss 1.2704 Accuracy 0.7080\n",
            "Epoch 4 Batch 500 Loss 1.2704 Accuracy 0.7077\n",
            "Epoch 4 Batch 550 Loss 1.2728 Accuracy 0.7074\n",
            "Epoch 4 Batch 600 Loss 1.2722 Accuracy 0.7073\n",
            "Epoch 4 Batch 650 Loss 1.2713 Accuracy 0.7074\n",
            "Epoch 4 Batch 700 Loss 1.2698 Accuracy 0.7077\n",
            "Epoch 4 Batch 750 Loss 1.2688 Accuracy 0.7077\n",
            "Epoch 4 Batch 800 Loss 1.2675 Accuracy 0.7079\n",
            "Epoch 4 Batch 850 Loss 1.2648 Accuracy 0.7083\n",
            "Epoch 4 Batch 900 Loss 1.2625 Accuracy 0.7086\n",
            "Epoch 4 Batch 950 Loss 1.2606 Accuracy 0.7089\n",
            "discarded batch 953\n",
            "Epoch 4 Loss 1.2605 Accuracy 0.7089\n",
            "Time taken for 1 epoch: 66.80855298042297 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.1641 Accuracy 0.7261\n",
            "Epoch 5 Batch 50 Loss 1.2261 Accuracy 0.7125\n",
            "Epoch 5 Batch 100 Loss 1.2319 Accuracy 0.7129\n",
            "Epoch 5 Batch 150 Loss 1.2438 Accuracy 0.7116\n",
            "Epoch 5 Batch 200 Loss 1.2510 Accuracy 0.7113\n",
            "Epoch 5 Batch 250 Loss 1.2577 Accuracy 0.7108\n",
            "Epoch 5 Batch 300 Loss 1.2616 Accuracy 0.7097\n",
            "Epoch 5 Batch 350 Loss 1.2624 Accuracy 0.7097\n",
            "Epoch 5 Batch 400 Loss 1.2633 Accuracy 0.7092\n",
            "Epoch 5 Batch 450 Loss 1.2614 Accuracy 0.7090\n",
            "Epoch 5 Batch 500 Loss 1.2626 Accuracy 0.7084\n",
            "Epoch 5 Batch 550 Loss 1.2632 Accuracy 0.7081\n",
            "Epoch 5 Batch 600 Loss 1.2624 Accuracy 0.7078\n",
            "Epoch 5 Batch 650 Loss 1.2618 Accuracy 0.7083\n",
            "Epoch 5 Batch 700 Loss 1.2614 Accuracy 0.7083\n",
            "Epoch 5 Batch 750 Loss 1.2609 Accuracy 0.7084\n",
            "Epoch 5 Batch 800 Loss 1.2593 Accuracy 0.7087\n",
            "Epoch 5 Batch 850 Loss 1.2567 Accuracy 0.7090\n",
            "Epoch 5 Batch 900 Loss 1.2540 Accuracy 0.7096\n",
            "Epoch 5 Batch 950 Loss 1.2520 Accuracy 0.7100\n",
            "discarded batch 953\n",
            "Saving checkpoint for epoch 5 at ./checkpoints/train/ckpt-23\n",
            "Epoch 5 Loss 1.2519 Accuracy 0.7100\n",
            "Time taken for 1 epoch: 67.55339670181274 secs\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.0766 Accuracy 0.7459\n",
            "Epoch 6 Batch 50 Loss 1.2094 Accuracy 0.7157\n",
            "Epoch 6 Batch 100 Loss 1.2174 Accuracy 0.7161\n",
            "Epoch 6 Batch 150 Loss 1.2301 Accuracy 0.7142\n",
            "Epoch 6 Batch 200 Loss 1.2390 Accuracy 0.7142\n",
            "Epoch 6 Batch 250 Loss 1.2474 Accuracy 0.7119\n",
            "Epoch 6 Batch 300 Loss 1.2518 Accuracy 0.7104\n",
            "Epoch 6 Batch 350 Loss 1.2556 Accuracy 0.7099\n",
            "Epoch 6 Batch 400 Loss 1.2605 Accuracy 0.7092\n",
            "Epoch 6 Batch 450 Loss 1.2577 Accuracy 0.7095\n",
            "Epoch 6 Batch 500 Loss 1.2588 Accuracy 0.7092\n",
            "Epoch 6 Batch 550 Loss 1.2606 Accuracy 0.7089\n",
            "Epoch 6 Batch 600 Loss 1.2592 Accuracy 0.7091\n",
            "Epoch 6 Batch 650 Loss 1.2583 Accuracy 0.7093\n",
            "Epoch 6 Batch 700 Loss 1.2578 Accuracy 0.7091\n",
            "Epoch 6 Batch 750 Loss 1.2577 Accuracy 0.7090\n",
            "Epoch 6 Batch 800 Loss 1.2565 Accuracy 0.7093\n",
            "Epoch 6 Batch 850 Loss 1.2542 Accuracy 0.7096\n",
            "Epoch 6 Batch 900 Loss 1.2523 Accuracy 0.7098\n",
            "Epoch 6 Batch 950 Loss 1.2503 Accuracy 0.7101\n",
            "discarded batch 953\n",
            "Epoch 6 Loss 1.2502 Accuracy 0.7102\n",
            "Time taken for 1 epoch: 68.8142523765564 secs\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.1950 Accuracy 0.7162\n",
            "Epoch 7 Batch 50 Loss 1.2204 Accuracy 0.7147\n",
            "Epoch 7 Batch 100 Loss 1.2210 Accuracy 0.7159\n",
            "Epoch 7 Batch 150 Loss 1.2325 Accuracy 0.7134\n",
            "Epoch 7 Batch 200 Loss 1.2415 Accuracy 0.7124\n",
            "Epoch 7 Batch 250 Loss 1.2501 Accuracy 0.7114\n",
            "Epoch 7 Batch 300 Loss 1.2541 Accuracy 0.7103\n",
            "Epoch 7 Batch 350 Loss 1.2560 Accuracy 0.7104\n",
            "Epoch 7 Batch 400 Loss 1.2601 Accuracy 0.7094\n",
            "Epoch 7 Batch 450 Loss 1.2574 Accuracy 0.7095\n",
            "Epoch 7 Batch 500 Loss 1.2589 Accuracy 0.7092\n",
            "Epoch 7 Batch 550 Loss 1.2615 Accuracy 0.7086\n",
            "Epoch 7 Batch 600 Loss 1.2615 Accuracy 0.7085\n",
            "Epoch 7 Batch 650 Loss 1.2610 Accuracy 0.7085\n",
            "Epoch 7 Batch 700 Loss 1.2591 Accuracy 0.7087\n",
            "Epoch 7 Batch 750 Loss 1.2582 Accuracy 0.7089\n",
            "Epoch 7 Batch 800 Loss 1.2566 Accuracy 0.7093\n",
            "Epoch 7 Batch 850 Loss 1.2538 Accuracy 0.7097\n",
            "Epoch 7 Batch 900 Loss 1.2519 Accuracy 0.7099\n",
            "Epoch 7 Batch 950 Loss 1.2499 Accuracy 0.7101\n",
            "discarded batch 953\n",
            "Epoch 7 Loss 1.2499 Accuracy 0.7101\n",
            "Time taken for 1 epoch: 67.46312642097473 secs\n",
            "\n",
            "Epoch 8 Batch 0 Loss 1.2008 Accuracy 0.7129\n",
            "Epoch 8 Batch 50 Loss 1.2202 Accuracy 0.7122\n",
            "Epoch 8 Batch 100 Loss 1.2235 Accuracy 0.7135\n",
            "Epoch 8 Batch 150 Loss 1.2353 Accuracy 0.7125\n",
            "Epoch 8 Batch 200 Loss 1.2423 Accuracy 0.7125\n",
            "Epoch 8 Batch 250 Loss 1.2506 Accuracy 0.7110\n",
            "Epoch 8 Batch 300 Loss 1.2539 Accuracy 0.7101\n",
            "Epoch 8 Batch 350 Loss 1.2553 Accuracy 0.7098\n",
            "Epoch 8 Batch 400 Loss 1.2584 Accuracy 0.7091\n",
            "Epoch 8 Batch 450 Loss 1.2572 Accuracy 0.7091\n",
            "Epoch 8 Batch 500 Loss 1.2574 Accuracy 0.7089\n",
            "Epoch 8 Batch 550 Loss 1.2592 Accuracy 0.7085\n",
            "Epoch 8 Batch 600 Loss 1.2590 Accuracy 0.7080\n",
            "Epoch 8 Batch 650 Loss 1.2573 Accuracy 0.7083\n",
            "Epoch 8 Batch 700 Loss 1.2567 Accuracy 0.7082\n",
            "Epoch 8 Batch 750 Loss 1.2553 Accuracy 0.7083\n",
            "Epoch 8 Batch 800 Loss 1.2545 Accuracy 0.7085\n",
            "Epoch 8 Batch 850 Loss 1.2525 Accuracy 0.7087\n",
            "Epoch 8 Batch 900 Loss 1.2497 Accuracy 0.7091\n",
            "Epoch 8 Batch 950 Loss 1.2480 Accuracy 0.7094\n",
            "discarded batch 953\n",
            "Epoch 8 Loss 1.2480 Accuracy 0.7094\n",
            "Time taken for 1 epoch: 67.63721060752869 secs\n",
            "\n",
            "Epoch 9 Batch 0 Loss 1.1379 Accuracy 0.7327\n",
            "Epoch 9 Batch 50 Loss 1.2098 Accuracy 0.7147\n",
            "Epoch 9 Batch 100 Loss 1.2148 Accuracy 0.7149\n",
            "Epoch 9 Batch 150 Loss 1.2281 Accuracy 0.7121\n",
            "Epoch 9 Batch 200 Loss 1.2338 Accuracy 0.7126\n",
            "Epoch 9 Batch 250 Loss 1.2385 Accuracy 0.7119\n",
            "Epoch 9 Batch 300 Loss 1.2448 Accuracy 0.7105\n",
            "Epoch 9 Batch 350 Loss 1.2446 Accuracy 0.7107\n",
            "Epoch 9 Batch 400 Loss 1.2482 Accuracy 0.7104\n",
            "Epoch 9 Batch 450 Loss 1.2474 Accuracy 0.7106\n",
            "Epoch 9 Batch 500 Loss 1.2486 Accuracy 0.7100\n",
            "Epoch 9 Batch 550 Loss 1.2498 Accuracy 0.7094\n",
            "Epoch 9 Batch 600 Loss 1.2498 Accuracy 0.7094\n",
            "Epoch 9 Batch 650 Loss 1.2498 Accuracy 0.7095\n",
            "Epoch 9 Batch 700 Loss 1.2485 Accuracy 0.7097\n",
            "Epoch 9 Batch 750 Loss 1.2475 Accuracy 0.7097\n",
            "Epoch 9 Batch 800 Loss 1.2461 Accuracy 0.7101\n",
            "Epoch 9 Batch 850 Loss 1.2447 Accuracy 0.7103\n",
            "Epoch 9 Batch 900 Loss 1.2423 Accuracy 0.7106\n",
            "Epoch 9 Batch 950 Loss 1.2404 Accuracy 0.7110\n",
            "discarded batch 953\n",
            "Epoch 9 Loss 1.2404 Accuracy 0.7110\n",
            "Time taken for 1 epoch: 67.8381679058075 secs\n",
            "\n",
            "Epoch 10 Batch 0 Loss 1.1142 Accuracy 0.7327\n",
            "Epoch 10 Batch 50 Loss 1.1997 Accuracy 0.7158\n",
            "Epoch 10 Batch 100 Loss 1.2076 Accuracy 0.7156\n",
            "Epoch 10 Batch 150 Loss 1.2219 Accuracy 0.7139\n",
            "Epoch 10 Batch 200 Loss 1.2259 Accuracy 0.7143\n",
            "Epoch 10 Batch 250 Loss 1.2312 Accuracy 0.7138\n",
            "Epoch 10 Batch 300 Loss 1.2359 Accuracy 0.7122\n",
            "Epoch 10 Batch 350 Loss 1.2389 Accuracy 0.7119\n",
            "Epoch 10 Batch 400 Loss 1.2434 Accuracy 0.7112\n",
            "Epoch 10 Batch 450 Loss 1.2423 Accuracy 0.7111\n",
            "Epoch 10 Batch 500 Loss 1.2433 Accuracy 0.7107\n",
            "Epoch 10 Batch 550 Loss 1.2455 Accuracy 0.7104\n",
            "Epoch 10 Batch 600 Loss 1.2448 Accuracy 0.7105\n",
            "Epoch 10 Batch 650 Loss 1.2438 Accuracy 0.7107\n",
            "Epoch 10 Batch 700 Loss 1.2424 Accuracy 0.7110\n",
            "Epoch 10 Batch 750 Loss 1.2420 Accuracy 0.7109\n",
            "Epoch 10 Batch 800 Loss 1.2406 Accuracy 0.7112\n",
            "Epoch 10 Batch 850 Loss 1.2383 Accuracy 0.7116\n",
            "Epoch 10 Batch 900 Loss 1.2364 Accuracy 0.7119\n",
            "Epoch 10 Batch 950 Loss 1.2340 Accuracy 0.7124\n",
            "discarded batch 953\n",
            "Saving checkpoint for epoch 10 at ./checkpoints/train/ckpt-24\n",
            "Epoch 10 Loss 1.2339 Accuracy 0.7125\n",
            "Time taken for 1 epoch: 67.75327587127686 secs\n",
            "\n",
            "Epoch 11 Batch 0 Loss 1.0914 Accuracy 0.7360\n",
            "Epoch 11 Batch 50 Loss 1.1948 Accuracy 0.7175\n",
            "Epoch 11 Batch 100 Loss 1.2038 Accuracy 0.7173\n",
            "Epoch 11 Batch 150 Loss 1.2173 Accuracy 0.7158\n",
            "Epoch 11 Batch 200 Loss 1.2265 Accuracy 0.7145\n",
            "Epoch 11 Batch 250 Loss 1.2339 Accuracy 0.7138\n",
            "Epoch 11 Batch 300 Loss 1.2388 Accuracy 0.7123\n",
            "Epoch 11 Batch 350 Loss 1.2413 Accuracy 0.7121\n",
            "Epoch 11 Batch 400 Loss 1.2447 Accuracy 0.7118\n",
            "Epoch 11 Batch 450 Loss 1.2419 Accuracy 0.7122\n",
            "Epoch 11 Batch 500 Loss 1.2435 Accuracy 0.7117\n",
            "Epoch 11 Batch 550 Loss 1.2460 Accuracy 0.7110\n",
            "Epoch 11 Batch 600 Loss 1.2463 Accuracy 0.7109\n",
            "Epoch 11 Batch 650 Loss 1.2459 Accuracy 0.7111\n",
            "Epoch 11 Batch 700 Loss 1.2448 Accuracy 0.7113\n",
            "Epoch 11 Batch 750 Loss 1.2443 Accuracy 0.7112\n",
            "Epoch 11 Batch 800 Loss 1.2427 Accuracy 0.7115\n",
            "Epoch 11 Batch 850 Loss 1.2398 Accuracy 0.7119\n",
            "Epoch 11 Batch 900 Loss 1.2379 Accuracy 0.7121\n",
            "Epoch 11 Batch 950 Loss 1.2352 Accuracy 0.7124\n",
            "discarded batch 953\n",
            "Epoch 11 Loss 1.2351 Accuracy 0.7124\n",
            "Time taken for 1 epoch: 67.92635369300842 secs\n",
            "\n",
            "Epoch 12 Batch 0 Loss 1.0967 Accuracy 0.7459\n",
            "Epoch 12 Batch 50 Loss 1.1995 Accuracy 0.7115\n",
            "Epoch 12 Batch 100 Loss 1.2028 Accuracy 0.7161\n",
            "Epoch 12 Batch 150 Loss 1.2203 Accuracy 0.7140\n",
            "Epoch 12 Batch 200 Loss 1.2267 Accuracy 0.7134\n",
            "Epoch 12 Batch 250 Loss 1.2342 Accuracy 0.7126\n",
            "Epoch 12 Batch 300 Loss 1.2390 Accuracy 0.7117\n",
            "Epoch 12 Batch 350 Loss 1.2394 Accuracy 0.7119\n",
            "Epoch 12 Batch 400 Loss 1.2422 Accuracy 0.7115\n",
            "Epoch 12 Batch 450 Loss 1.2390 Accuracy 0.7118\n",
            "Epoch 12 Batch 500 Loss 1.2394 Accuracy 0.7117\n",
            "Epoch 12 Batch 550 Loss 1.2409 Accuracy 0.7116\n",
            "Epoch 12 Batch 600 Loss 1.2412 Accuracy 0.7114\n",
            "Epoch 12 Batch 650 Loss 1.2404 Accuracy 0.7116\n",
            "Epoch 12 Batch 700 Loss 1.2399 Accuracy 0.7116\n",
            "Epoch 12 Batch 750 Loss 1.2394 Accuracy 0.7114\n",
            "Epoch 12 Batch 800 Loss 1.2383 Accuracy 0.7117\n",
            "Epoch 12 Batch 850 Loss 1.2360 Accuracy 0.7121\n",
            "Epoch 12 Batch 900 Loss 1.2339 Accuracy 0.7123\n",
            "Epoch 12 Batch 950 Loss 1.2316 Accuracy 0.7127\n",
            "discarded batch 953\n",
            "Epoch 12 Loss 1.2316 Accuracy 0.7127\n",
            "Time taken for 1 epoch: 67.51786470413208 secs\n",
            "\n",
            "Epoch 13 Batch 0 Loss 1.1100 Accuracy 0.7558\n",
            "Epoch 13 Batch 50 Loss 1.1944 Accuracy 0.7175\n",
            "Epoch 13 Batch 100 Loss 1.2018 Accuracy 0.7182\n",
            "Epoch 13 Batch 150 Loss 1.2142 Accuracy 0.7174\n",
            "Epoch 13 Batch 200 Loss 1.2216 Accuracy 0.7168\n",
            "Epoch 13 Batch 250 Loss 1.2287 Accuracy 0.7157\n",
            "Epoch 13 Batch 300 Loss 1.2342 Accuracy 0.7143\n",
            "Epoch 13 Batch 350 Loss 1.2366 Accuracy 0.7139\n",
            "Epoch 13 Batch 400 Loss 1.2386 Accuracy 0.7135\n",
            "Epoch 13 Batch 450 Loss 1.2360 Accuracy 0.7138\n",
            "Epoch 13 Batch 500 Loss 1.2363 Accuracy 0.7135\n",
            "Epoch 13 Batch 550 Loss 1.2389 Accuracy 0.7129\n",
            "Epoch 13 Batch 600 Loss 1.2384 Accuracy 0.7128\n",
            "Epoch 13 Batch 650 Loss 1.2376 Accuracy 0.7128\n",
            "Epoch 13 Batch 700 Loss 1.2371 Accuracy 0.7127\n",
            "Epoch 13 Batch 750 Loss 1.2363 Accuracy 0.7129\n",
            "Epoch 13 Batch 800 Loss 1.2352 Accuracy 0.7131\n",
            "Epoch 13 Batch 850 Loss 1.2321 Accuracy 0.7134\n",
            "Epoch 13 Batch 900 Loss 1.2298 Accuracy 0.7135\n",
            "Epoch 13 Batch 950 Loss 1.2281 Accuracy 0.7139\n",
            "discarded batch 953\n",
            "Epoch 13 Loss 1.2281 Accuracy 0.7139\n",
            "Time taken for 1 epoch: 68.30693435668945 secs\n",
            "\n",
            "Epoch 14 Batch 0 Loss 1.0490 Accuracy 0.7492\n",
            "Epoch 14 Batch 50 Loss 1.1951 Accuracy 0.7194\n",
            "Epoch 14 Batch 100 Loss 1.2007 Accuracy 0.7181\n",
            "Epoch 14 Batch 150 Loss 1.2143 Accuracy 0.7164\n",
            "Epoch 14 Batch 200 Loss 1.2219 Accuracy 0.7161\n",
            "Epoch 14 Batch 250 Loss 1.2259 Accuracy 0.7154\n",
            "Epoch 14 Batch 300 Loss 1.2321 Accuracy 0.7139\n",
            "Epoch 14 Batch 350 Loss 1.2338 Accuracy 0.7137\n",
            "Epoch 14 Batch 400 Loss 1.2359 Accuracy 0.7132\n",
            "Epoch 14 Batch 450 Loss 1.2336 Accuracy 0.7133\n",
            "Epoch 14 Batch 500 Loss 1.2353 Accuracy 0.7127\n",
            "Epoch 14 Batch 550 Loss 1.2368 Accuracy 0.7124\n",
            "Epoch 14 Batch 600 Loss 1.2351 Accuracy 0.7126\n",
            "Epoch 14 Batch 650 Loss 1.2347 Accuracy 0.7126\n",
            "Epoch 14 Batch 700 Loss 1.2337 Accuracy 0.7125\n",
            "Epoch 14 Batch 750 Loss 1.2329 Accuracy 0.7127\n",
            "Epoch 14 Batch 800 Loss 1.2320 Accuracy 0.7130\n",
            "Epoch 14 Batch 850 Loss 1.2293 Accuracy 0.7134\n",
            "Epoch 14 Batch 900 Loss 1.2271 Accuracy 0.7137\n",
            "Epoch 14 Batch 950 Loss 1.2257 Accuracy 0.7139\n",
            "discarded batch 953\n",
            "Epoch 14 Loss 1.2256 Accuracy 0.7139\n",
            "Time taken for 1 epoch: 68.59471273422241 secs\n",
            "\n",
            "Epoch 15 Batch 0 Loss 1.1042 Accuracy 0.7162\n",
            "Epoch 15 Batch 50 Loss 1.1895 Accuracy 0.7189\n",
            "Epoch 15 Batch 100 Loss 1.2004 Accuracy 0.7175\n",
            "Epoch 15 Batch 150 Loss 1.2104 Accuracy 0.7162\n",
            "Epoch 15 Batch 200 Loss 1.2148 Accuracy 0.7166\n",
            "Epoch 15 Batch 250 Loss 1.2221 Accuracy 0.7150\n",
            "Epoch 15 Batch 300 Loss 1.2267 Accuracy 0.7140\n",
            "Epoch 15 Batch 350 Loss 1.2296 Accuracy 0.7134\n",
            "Epoch 15 Batch 400 Loss 1.2328 Accuracy 0.7127\n",
            "Epoch 15 Batch 450 Loss 1.2300 Accuracy 0.7131\n",
            "Epoch 15 Batch 500 Loss 1.2312 Accuracy 0.7128\n",
            "Epoch 15 Batch 550 Loss 1.2333 Accuracy 0.7122\n",
            "Epoch 15 Batch 600 Loss 1.2329 Accuracy 0.7124\n",
            "Epoch 15 Batch 650 Loss 1.2319 Accuracy 0.7125\n",
            "Epoch 15 Batch 700 Loss 1.2310 Accuracy 0.7127\n",
            "Epoch 15 Batch 750 Loss 1.2304 Accuracy 0.7127\n",
            "Epoch 15 Batch 800 Loss 1.2298 Accuracy 0.7129\n",
            "Epoch 15 Batch 850 Loss 1.2272 Accuracy 0.7132\n",
            "Epoch 15 Batch 900 Loss 1.2248 Accuracy 0.7135\n",
            "Epoch 15 Batch 950 Loss 1.2226 Accuracy 0.7138\n",
            "discarded batch 953\n",
            "Saving checkpoint for epoch 15 at ./checkpoints/train/ckpt-25\n",
            "Epoch 15 Loss 1.2224 Accuracy 0.7138\n",
            "Time taken for 1 epoch: 67.96287608146667 secs\n",
            "\n",
            "Epoch 16 Batch 0 Loss 1.1848 Accuracy 0.7294\n",
            "Epoch 16 Batch 50 Loss 1.1920 Accuracy 0.7176\n",
            "Epoch 16 Batch 100 Loss 1.1934 Accuracy 0.7185\n",
            "Epoch 16 Batch 150 Loss 1.2068 Accuracy 0.7178\n",
            "Epoch 16 Batch 200 Loss 1.2139 Accuracy 0.7167\n",
            "Epoch 16 Batch 250 Loss 1.2200 Accuracy 0.7157\n",
            "Epoch 16 Batch 300 Loss 1.2251 Accuracy 0.7149\n",
            "Epoch 16 Batch 350 Loss 1.2282 Accuracy 0.7147\n",
            "Epoch 16 Batch 400 Loss 1.2304 Accuracy 0.7144\n",
            "Epoch 16 Batch 450 Loss 1.2280 Accuracy 0.7145\n",
            "Epoch 16 Batch 500 Loss 1.2286 Accuracy 0.7140\n",
            "Epoch 16 Batch 550 Loss 1.2311 Accuracy 0.7135\n",
            "Epoch 16 Batch 600 Loss 1.2307 Accuracy 0.7135\n",
            "Epoch 16 Batch 650 Loss 1.2295 Accuracy 0.7137\n",
            "Epoch 16 Batch 700 Loss 1.2278 Accuracy 0.7140\n",
            "Epoch 16 Batch 750 Loss 1.2269 Accuracy 0.7140\n",
            "Epoch 16 Batch 800 Loss 1.2257 Accuracy 0.7144\n",
            "Epoch 16 Batch 850 Loss 1.2237 Accuracy 0.7146\n",
            "Epoch 16 Batch 900 Loss 1.2221 Accuracy 0.7147\n",
            "Epoch 16 Batch 950 Loss 1.2203 Accuracy 0.7150\n",
            "discarded batch 953\n",
            "Epoch 16 Loss 1.2203 Accuracy 0.7150\n",
            "Time taken for 1 epoch: 67.65638017654419 secs\n",
            "\n",
            "Epoch 17 Batch 0 Loss 1.1147 Accuracy 0.7492\n",
            "Epoch 17 Batch 50 Loss 1.1915 Accuracy 0.7160\n",
            "Epoch 17 Batch 100 Loss 1.1971 Accuracy 0.7161\n",
            "Epoch 17 Batch 150 Loss 1.2052 Accuracy 0.7159\n",
            "Epoch 17 Batch 200 Loss 1.2106 Accuracy 0.7165\n",
            "Epoch 17 Batch 250 Loss 1.2143 Accuracy 0.7156\n",
            "Epoch 17 Batch 300 Loss 1.2195 Accuracy 0.7148\n",
            "Epoch 17 Batch 350 Loss 1.2216 Accuracy 0.7146\n",
            "Epoch 17 Batch 400 Loss 1.2251 Accuracy 0.7144\n",
            "Epoch 17 Batch 450 Loss 1.2231 Accuracy 0.7145\n",
            "Epoch 17 Batch 500 Loss 1.2228 Accuracy 0.7144\n",
            "Epoch 17 Batch 550 Loss 1.2249 Accuracy 0.7140\n",
            "Epoch 17 Batch 600 Loss 1.2248 Accuracy 0.7138\n",
            "Epoch 17 Batch 650 Loss 1.2238 Accuracy 0.7140\n",
            "Epoch 17 Batch 700 Loss 1.2229 Accuracy 0.7140\n",
            "Epoch 17 Batch 750 Loss 1.2225 Accuracy 0.7143\n",
            "Epoch 17 Batch 800 Loss 1.2210 Accuracy 0.7146\n",
            "Epoch 17 Batch 850 Loss 1.2189 Accuracy 0.7149\n",
            "Epoch 17 Batch 900 Loss 1.2167 Accuracy 0.7151\n",
            "Epoch 17 Batch 950 Loss 1.2154 Accuracy 0.7152\n",
            "discarded batch 953\n",
            "Epoch 17 Loss 1.2153 Accuracy 0.7152\n",
            "Time taken for 1 epoch: 67.71061372756958 secs\n",
            "\n",
            "Epoch 18 Batch 0 Loss 1.0844 Accuracy 0.7426\n",
            "Epoch 18 Batch 50 Loss 1.1760 Accuracy 0.7215\n",
            "Epoch 18 Batch 100 Loss 1.1898 Accuracy 0.7188\n",
            "Epoch 18 Batch 150 Loss 1.1997 Accuracy 0.7174\n",
            "Epoch 18 Batch 200 Loss 1.2077 Accuracy 0.7166\n",
            "Epoch 18 Batch 250 Loss 1.2114 Accuracy 0.7160\n",
            "Epoch 18 Batch 300 Loss 1.2158 Accuracy 0.7147\n",
            "Epoch 18 Batch 350 Loss 1.2167 Accuracy 0.7152\n",
            "Epoch 18 Batch 400 Loss 1.2204 Accuracy 0.7147\n",
            "Epoch 18 Batch 450 Loss 1.2189 Accuracy 0.7147\n",
            "Epoch 18 Batch 500 Loss 1.2215 Accuracy 0.7138\n",
            "Epoch 18 Batch 550 Loss 1.2243 Accuracy 0.7131\n",
            "Epoch 18 Batch 600 Loss 1.2243 Accuracy 0.7128\n",
            "Epoch 18 Batch 650 Loss 1.2232 Accuracy 0.7133\n",
            "Epoch 18 Batch 700 Loss 1.2219 Accuracy 0.7136\n",
            "Epoch 18 Batch 750 Loss 1.2212 Accuracy 0.7137\n",
            "Epoch 18 Batch 800 Loss 1.2196 Accuracy 0.7143\n",
            "Epoch 18 Batch 850 Loss 1.2174 Accuracy 0.7147\n",
            "Epoch 18 Batch 900 Loss 1.2155 Accuracy 0.7149\n",
            "Epoch 18 Batch 950 Loss 1.2133 Accuracy 0.7151\n",
            "discarded batch 953\n",
            "Epoch 18 Loss 1.2133 Accuracy 0.7151\n",
            "Time taken for 1 epoch: 67.96366429328918 secs\n",
            "\n",
            "Epoch 19 Batch 0 Loss 1.0731 Accuracy 0.7360\n",
            "Epoch 19 Batch 50 Loss 1.1896 Accuracy 0.7166\n",
            "Epoch 19 Batch 100 Loss 1.1974 Accuracy 0.7174\n",
            "Epoch 19 Batch 150 Loss 1.2081 Accuracy 0.7161\n",
            "Epoch 19 Batch 200 Loss 1.2123 Accuracy 0.7165\n",
            "Epoch 19 Batch 250 Loss 1.2170 Accuracy 0.7163\n",
            "Epoch 19 Batch 300 Loss 1.2210 Accuracy 0.7154\n",
            "Epoch 19 Batch 350 Loss 1.2218 Accuracy 0.7148\n",
            "Epoch 19 Batch 400 Loss 1.2252 Accuracy 0.7142\n",
            "Epoch 19 Batch 450 Loss 1.2233 Accuracy 0.7143\n",
            "Epoch 19 Batch 500 Loss 1.2243 Accuracy 0.7140\n",
            "Epoch 19 Batch 550 Loss 1.2257 Accuracy 0.7138\n",
            "Epoch 19 Batch 600 Loss 1.2251 Accuracy 0.7137\n",
            "Epoch 19 Batch 650 Loss 1.2242 Accuracy 0.7139\n",
            "Epoch 19 Batch 700 Loss 1.2229 Accuracy 0.7140\n",
            "Epoch 19 Batch 750 Loss 1.2218 Accuracy 0.7141\n",
            "Epoch 19 Batch 800 Loss 1.2205 Accuracy 0.7145\n",
            "Epoch 19 Batch 850 Loss 1.2186 Accuracy 0.7147\n",
            "Epoch 19 Batch 900 Loss 1.2168 Accuracy 0.7149\n",
            "Epoch 19 Batch 950 Loss 1.2146 Accuracy 0.7152\n",
            "discarded batch 953\n",
            "Epoch 19 Loss 1.2145 Accuracy 0.7152\n",
            "Time taken for 1 epoch: 68.13417768478394 secs\n",
            "\n",
            "Epoch 20 Batch 0 Loss 1.0557 Accuracy 0.7558\n",
            "Epoch 20 Batch 50 Loss 1.1900 Accuracy 0.7199\n",
            "Epoch 20 Batch 100 Loss 1.1920 Accuracy 0.7215\n",
            "Epoch 20 Batch 150 Loss 1.1980 Accuracy 0.7215\n",
            "Epoch 20 Batch 200 Loss 1.2013 Accuracy 0.7205\n",
            "Epoch 20 Batch 250 Loss 1.2090 Accuracy 0.7190\n",
            "Epoch 20 Batch 300 Loss 1.2129 Accuracy 0.7179\n",
            "Epoch 20 Batch 350 Loss 1.2157 Accuracy 0.7176\n",
            "Epoch 20 Batch 400 Loss 1.2197 Accuracy 0.7166\n",
            "Epoch 20 Batch 450 Loss 1.2185 Accuracy 0.7166\n",
            "Epoch 20 Batch 500 Loss 1.2187 Accuracy 0.7161\n",
            "Epoch 20 Batch 550 Loss 1.2214 Accuracy 0.7156\n",
            "Epoch 20 Batch 600 Loss 1.2201 Accuracy 0.7155\n",
            "Epoch 20 Batch 650 Loss 1.2204 Accuracy 0.7156\n",
            "Epoch 20 Batch 700 Loss 1.2189 Accuracy 0.7156\n",
            "Epoch 20 Batch 750 Loss 1.2183 Accuracy 0.7154\n",
            "Epoch 20 Batch 800 Loss 1.2166 Accuracy 0.7156\n",
            "Epoch 20 Batch 850 Loss 1.2145 Accuracy 0.7160\n",
            "Epoch 20 Batch 900 Loss 1.2122 Accuracy 0.7162\n",
            "Epoch 20 Batch 950 Loss 1.2100 Accuracy 0.7166\n",
            "discarded batch 953\n",
            "Saving checkpoint for epoch 20 at ./checkpoints/train/ckpt-26\n",
            "Epoch 20 Loss 1.2099 Accuracy 0.7166\n",
            "Time taken for 1 epoch: 67.79898262023926 secs\n",
            "\n",
            "Epoch 21 Batch 0 Loss 1.1511 Accuracy 0.7327\n",
            "Epoch 21 Batch 50 Loss 1.1762 Accuracy 0.7200\n",
            "Epoch 21 Batch 100 Loss 1.1771 Accuracy 0.7226\n",
            "Epoch 21 Batch 150 Loss 1.1883 Accuracy 0.7214\n",
            "Epoch 21 Batch 200 Loss 1.1951 Accuracy 0.7214\n",
            "Epoch 21 Batch 250 Loss 1.2027 Accuracy 0.7200\n",
            "Epoch 21 Batch 300 Loss 1.2072 Accuracy 0.7190\n",
            "Epoch 21 Batch 350 Loss 1.2103 Accuracy 0.7180\n",
            "Epoch 21 Batch 400 Loss 1.2139 Accuracy 0.7172\n",
            "Epoch 21 Batch 450 Loss 1.2116 Accuracy 0.7173\n",
            "Epoch 21 Batch 500 Loss 1.2115 Accuracy 0.7168\n",
            "Epoch 21 Batch 550 Loss 1.2134 Accuracy 0.7164\n",
            "Epoch 21 Batch 600 Loss 1.2134 Accuracy 0.7164\n",
            "Epoch 21 Batch 650 Loss 1.2127 Accuracy 0.7164\n",
            "Epoch 21 Batch 700 Loss 1.2118 Accuracy 0.7164\n",
            "Epoch 21 Batch 750 Loss 1.2114 Accuracy 0.7163\n",
            "Epoch 21 Batch 800 Loss 1.2098 Accuracy 0.7167\n",
            "Epoch 21 Batch 850 Loss 1.2074 Accuracy 0.7169\n",
            "Epoch 21 Batch 900 Loss 1.2052 Accuracy 0.7172\n",
            "Epoch 21 Batch 950 Loss 1.2028 Accuracy 0.7176\n",
            "discarded batch 953\n",
            "Epoch 21 Loss 1.2027 Accuracy 0.7176\n",
            "Time taken for 1 epoch: 67.96665716171265 secs\n",
            "\n",
            "Epoch 22 Batch 0 Loss 1.0771 Accuracy 0.7360\n",
            "Epoch 22 Batch 50 Loss 1.1819 Accuracy 0.7208\n",
            "Epoch 22 Batch 100 Loss 1.1822 Accuracy 0.7216\n",
            "Epoch 22 Batch 150 Loss 1.1938 Accuracy 0.7199\n",
            "Epoch 22 Batch 200 Loss 1.1998 Accuracy 0.7190\n",
            "Epoch 22 Batch 250 Loss 1.2058 Accuracy 0.7179\n",
            "Epoch 22 Batch 300 Loss 1.2087 Accuracy 0.7166\n",
            "Epoch 22 Batch 350 Loss 1.2115 Accuracy 0.7162\n",
            "Epoch 22 Batch 400 Loss 1.2165 Accuracy 0.7158\n",
            "Epoch 22 Batch 450 Loss 1.2136 Accuracy 0.7162\n",
            "Epoch 22 Batch 500 Loss 1.2138 Accuracy 0.7161\n",
            "Epoch 22 Batch 550 Loss 1.2151 Accuracy 0.7160\n",
            "Epoch 22 Batch 600 Loss 1.2147 Accuracy 0.7159\n",
            "Epoch 22 Batch 650 Loss 1.2135 Accuracy 0.7159\n",
            "Epoch 22 Batch 700 Loss 1.2126 Accuracy 0.7162\n",
            "Epoch 22 Batch 750 Loss 1.2122 Accuracy 0.7161\n",
            "Epoch 22 Batch 800 Loss 1.2109 Accuracy 0.7165\n",
            "Epoch 22 Batch 850 Loss 1.2086 Accuracy 0.7166\n",
            "Epoch 22 Batch 900 Loss 1.2065 Accuracy 0.7170\n",
            "Epoch 22 Batch 950 Loss 1.2045 Accuracy 0.7174\n",
            "discarded batch 953\n",
            "Epoch 22 Loss 1.2044 Accuracy 0.7174\n",
            "Time taken for 1 epoch: 67.36618089675903 secs\n",
            "\n",
            "Epoch 23 Batch 0 Loss 1.1094 Accuracy 0.7129\n",
            "Epoch 23 Batch 50 Loss 1.1690 Accuracy 0.7240\n",
            "Epoch 23 Batch 100 Loss 1.1771 Accuracy 0.7240\n",
            "Epoch 23 Batch 150 Loss 1.1917 Accuracy 0.7215\n",
            "Epoch 23 Batch 200 Loss 1.1969 Accuracy 0.7205\n",
            "Epoch 23 Batch 250 Loss 1.2016 Accuracy 0.7199\n",
            "Epoch 23 Batch 300 Loss 1.2054 Accuracy 0.7189\n",
            "Epoch 23 Batch 350 Loss 1.2066 Accuracy 0.7182\n",
            "Epoch 23 Batch 400 Loss 1.2099 Accuracy 0.7174\n",
            "Epoch 23 Batch 450 Loss 1.2077 Accuracy 0.7178\n",
            "Epoch 23 Batch 500 Loss 1.2085 Accuracy 0.7172\n",
            "Epoch 23 Batch 550 Loss 1.2097 Accuracy 0.7168\n",
            "Epoch 23 Batch 600 Loss 1.2088 Accuracy 0.7166\n",
            "Epoch 23 Batch 650 Loss 1.2074 Accuracy 0.7169\n",
            "Epoch 23 Batch 700 Loss 1.2068 Accuracy 0.7170\n",
            "Epoch 23 Batch 750 Loss 1.2060 Accuracy 0.7171\n",
            "Epoch 23 Batch 800 Loss 1.2048 Accuracy 0.7175\n",
            "Epoch 23 Batch 850 Loss 1.2021 Accuracy 0.7178\n",
            "Epoch 23 Batch 900 Loss 1.2005 Accuracy 0.7179\n",
            "Epoch 23 Batch 950 Loss 1.1984 Accuracy 0.7182\n",
            "discarded batch 953\n",
            "Epoch 23 Loss 1.1983 Accuracy 0.7182\n",
            "Time taken for 1 epoch: 68.37162184715271 secs\n",
            "\n",
            "Epoch 24 Batch 0 Loss 1.1109 Accuracy 0.7360\n",
            "Epoch 24 Batch 50 Loss 1.1765 Accuracy 0.7220\n",
            "Epoch 24 Batch 100 Loss 1.1857 Accuracy 0.7212\n",
            "Epoch 24 Batch 150 Loss 1.1915 Accuracy 0.7202\n",
            "Epoch 24 Batch 200 Loss 1.1957 Accuracy 0.7206\n",
            "Epoch 24 Batch 250 Loss 1.1981 Accuracy 0.7203\n",
            "Epoch 24 Batch 300 Loss 1.2026 Accuracy 0.7189\n",
            "Epoch 24 Batch 350 Loss 1.2049 Accuracy 0.7184\n",
            "Epoch 24 Batch 400 Loss 1.2085 Accuracy 0.7176\n",
            "Epoch 24 Batch 450 Loss 1.2058 Accuracy 0.7181\n",
            "Epoch 24 Batch 500 Loss 1.2074 Accuracy 0.7175\n",
            "Epoch 24 Batch 550 Loss 1.2084 Accuracy 0.7172\n",
            "Epoch 24 Batch 600 Loss 1.2080 Accuracy 0.7171\n",
            "Epoch 24 Batch 650 Loss 1.2073 Accuracy 0.7171\n",
            "Epoch 24 Batch 700 Loss 1.2065 Accuracy 0.7171\n",
            "Epoch 24 Batch 750 Loss 1.2055 Accuracy 0.7173\n",
            "Epoch 24 Batch 800 Loss 1.2043 Accuracy 0.7178\n",
            "Epoch 24 Batch 850 Loss 1.2026 Accuracy 0.7180\n",
            "Epoch 24 Batch 900 Loss 1.2008 Accuracy 0.7183\n",
            "Epoch 24 Batch 950 Loss 1.1986 Accuracy 0.7185\n",
            "discarded batch 953\n",
            "Epoch 24 Loss 1.1985 Accuracy 0.7185\n",
            "Time taken for 1 epoch: 67.81511855125427 secs\n",
            "\n",
            "Epoch 25 Batch 0 Loss 1.0857 Accuracy 0.7228\n",
            "Epoch 25 Batch 50 Loss 1.1640 Accuracy 0.7235\n",
            "Epoch 25 Batch 100 Loss 1.1669 Accuracy 0.7239\n",
            "Epoch 25 Batch 150 Loss 1.1806 Accuracy 0.7213\n",
            "Epoch 25 Batch 200 Loss 1.1876 Accuracy 0.7211\n",
            "Epoch 25 Batch 250 Loss 1.1944 Accuracy 0.7197\n",
            "Epoch 25 Batch 300 Loss 1.1974 Accuracy 0.7189\n",
            "Epoch 25 Batch 350 Loss 1.1998 Accuracy 0.7181\n",
            "Epoch 25 Batch 400 Loss 1.2038 Accuracy 0.7180\n",
            "Epoch 25 Batch 450 Loss 1.2026 Accuracy 0.7183\n",
            "Epoch 25 Batch 500 Loss 1.2042 Accuracy 0.7179\n",
            "Epoch 25 Batch 550 Loss 1.2060 Accuracy 0.7174\n",
            "Epoch 25 Batch 600 Loss 1.2052 Accuracy 0.7174\n",
            "Epoch 25 Batch 650 Loss 1.2039 Accuracy 0.7173\n",
            "Epoch 25 Batch 700 Loss 1.2036 Accuracy 0.7173\n",
            "Epoch 25 Batch 750 Loss 1.2033 Accuracy 0.7174\n",
            "Epoch 25 Batch 800 Loss 1.2021 Accuracy 0.7175\n",
            "Epoch 25 Batch 850 Loss 1.2004 Accuracy 0.7177\n",
            "Epoch 25 Batch 900 Loss 1.1979 Accuracy 0.7180\n",
            "Epoch 25 Batch 950 Loss 1.1952 Accuracy 0.7186\n",
            "discarded batch 953\n",
            "Saving checkpoint for epoch 25 at ./checkpoints/train/ckpt-27\n",
            "Epoch 25 Loss 1.1951 Accuracy 0.7186\n",
            "Time taken for 1 epoch: 67.69879031181335 secs\n",
            "\n",
            "Epoch 26 Batch 0 Loss 1.1386 Accuracy 0.7261\n",
            "Epoch 26 Batch 50 Loss 1.1667 Accuracy 0.7211\n",
            "Epoch 26 Batch 100 Loss 1.1687 Accuracy 0.7234\n",
            "Epoch 26 Batch 150 Loss 1.1787 Accuracy 0.7227\n",
            "Epoch 26 Batch 200 Loss 1.1852 Accuracy 0.7221\n",
            "Epoch 26 Batch 250 Loss 1.1926 Accuracy 0.7208\n",
            "Epoch 26 Batch 300 Loss 1.1984 Accuracy 0.7194\n",
            "Epoch 26 Batch 350 Loss 1.1992 Accuracy 0.7192\n",
            "Epoch 26 Batch 400 Loss 1.2034 Accuracy 0.7183\n",
            "Epoch 26 Batch 450 Loss 1.2008 Accuracy 0.7189\n",
            "Epoch 26 Batch 500 Loss 1.2028 Accuracy 0.7180\n",
            "Epoch 26 Batch 550 Loss 1.2037 Accuracy 0.7177\n",
            "Epoch 26 Batch 600 Loss 1.2025 Accuracy 0.7179\n",
            "Epoch 26 Batch 650 Loss 1.2014 Accuracy 0.7180\n",
            "Epoch 26 Batch 700 Loss 1.2001 Accuracy 0.7181\n",
            "Epoch 26 Batch 750 Loss 1.1989 Accuracy 0.7183\n",
            "Epoch 26 Batch 800 Loss 1.1979 Accuracy 0.7186\n",
            "Epoch 26 Batch 850 Loss 1.1961 Accuracy 0.7189\n",
            "Epoch 26 Batch 900 Loss 1.1936 Accuracy 0.7193\n",
            "Epoch 26 Batch 950 Loss 1.1919 Accuracy 0.7195\n",
            "discarded batch 953\n",
            "Epoch 26 Loss 1.1919 Accuracy 0.7195\n",
            "Time taken for 1 epoch: 67.26809048652649 secs\n",
            "\n",
            "Epoch 27 Batch 0 Loss 1.0865 Accuracy 0.7327\n",
            "Epoch 27 Batch 50 Loss 1.1653 Accuracy 0.7234\n",
            "Epoch 27 Batch 100 Loss 1.1641 Accuracy 0.7254\n",
            "Epoch 27 Batch 150 Loss 1.1769 Accuracy 0.7232\n",
            "Epoch 27 Batch 200 Loss 1.1811 Accuracy 0.7225\n",
            "Epoch 27 Batch 250 Loss 1.1850 Accuracy 0.7215\n",
            "Epoch 27 Batch 300 Loss 1.1897 Accuracy 0.7203\n",
            "Epoch 27 Batch 350 Loss 1.1940 Accuracy 0.7198\n",
            "Epoch 27 Batch 400 Loss 1.1985 Accuracy 0.7187\n",
            "Epoch 27 Batch 450 Loss 1.1959 Accuracy 0.7187\n",
            "Epoch 27 Batch 500 Loss 1.1968 Accuracy 0.7185\n",
            "Epoch 27 Batch 550 Loss 1.1994 Accuracy 0.7179\n",
            "Epoch 27 Batch 600 Loss 1.1986 Accuracy 0.7179\n",
            "Epoch 27 Batch 650 Loss 1.1983 Accuracy 0.7178\n",
            "Epoch 27 Batch 700 Loss 1.1970 Accuracy 0.7179\n",
            "Epoch 27 Batch 750 Loss 1.1962 Accuracy 0.7180\n",
            "Epoch 27 Batch 800 Loss 1.1952 Accuracy 0.7184\n",
            "Epoch 27 Batch 850 Loss 1.1933 Accuracy 0.7186\n",
            "Epoch 27 Batch 900 Loss 1.1914 Accuracy 0.7188\n",
            "Epoch 27 Batch 950 Loss 1.1893 Accuracy 0.7193\n",
            "discarded batch 953\n",
            "Epoch 27 Loss 1.1893 Accuracy 0.7193\n",
            "Time taken for 1 epoch: 66.49402451515198 secs\n",
            "\n",
            "Epoch 28 Batch 0 Loss 1.1731 Accuracy 0.7096\n",
            "Epoch 28 Batch 50 Loss 1.1562 Accuracy 0.7224\n",
            "Epoch 28 Batch 100 Loss 1.1624 Accuracy 0.7231\n",
            "Epoch 28 Batch 150 Loss 1.1721 Accuracy 0.7223\n",
            "Epoch 28 Batch 200 Loss 1.1806 Accuracy 0.7215\n",
            "Epoch 28 Batch 250 Loss 1.1874 Accuracy 0.7197\n",
            "Epoch 28 Batch 300 Loss 1.1917 Accuracy 0.7192\n",
            "Epoch 28 Batch 350 Loss 1.1929 Accuracy 0.7191\n",
            "Epoch 28 Batch 400 Loss 1.1971 Accuracy 0.7186\n",
            "Epoch 28 Batch 450 Loss 1.1950 Accuracy 0.7188\n",
            "Epoch 28 Batch 500 Loss 1.1961 Accuracy 0.7182\n",
            "Epoch 28 Batch 550 Loss 1.1991 Accuracy 0.7174\n",
            "Epoch 28 Batch 600 Loss 1.1976 Accuracy 0.7176\n",
            "Epoch 28 Batch 650 Loss 1.1977 Accuracy 0.7175\n",
            "Epoch 28 Batch 700 Loss 1.1961 Accuracy 0.7177\n",
            "Epoch 28 Batch 750 Loss 1.1956 Accuracy 0.7177\n",
            "Epoch 28 Batch 800 Loss 1.1936 Accuracy 0.7183\n",
            "Epoch 28 Batch 850 Loss 1.1918 Accuracy 0.7185\n",
            "Epoch 28 Batch 900 Loss 1.1888 Accuracy 0.7188\n",
            "Epoch 28 Batch 950 Loss 1.1871 Accuracy 0.7190\n",
            "discarded batch 953\n",
            "Epoch 28 Loss 1.1870 Accuracy 0.7191\n",
            "Time taken for 1 epoch: 67.08828783035278 secs\n",
            "\n",
            "Epoch 29 Batch 0 Loss 1.1415 Accuracy 0.7030\n",
            "Epoch 29 Batch 50 Loss 1.1609 Accuracy 0.7213\n",
            "Epoch 29 Batch 100 Loss 1.1590 Accuracy 0.7254\n",
            "Epoch 29 Batch 150 Loss 1.1701 Accuracy 0.7248\n",
            "Epoch 29 Batch 200 Loss 1.1743 Accuracy 0.7239\n",
            "Epoch 29 Batch 250 Loss 1.1804 Accuracy 0.7224\n",
            "Epoch 29 Batch 300 Loss 1.1866 Accuracy 0.7206\n",
            "Epoch 29 Batch 350 Loss 1.1891 Accuracy 0.7204\n",
            "Epoch 29 Batch 400 Loss 1.1929 Accuracy 0.7196\n",
            "Epoch 29 Batch 450 Loss 1.1900 Accuracy 0.7199\n",
            "Epoch 29 Batch 500 Loss 1.1897 Accuracy 0.7198\n",
            "Epoch 29 Batch 550 Loss 1.1923 Accuracy 0.7192\n",
            "Epoch 29 Batch 600 Loss 1.1924 Accuracy 0.7190\n",
            "Epoch 29 Batch 650 Loss 1.1917 Accuracy 0.7192\n",
            "Epoch 29 Batch 700 Loss 1.1907 Accuracy 0.7194\n",
            "Epoch 29 Batch 750 Loss 1.1902 Accuracy 0.7195\n",
            "Epoch 29 Batch 800 Loss 1.1893 Accuracy 0.7198\n",
            "Epoch 29 Batch 850 Loss 1.1881 Accuracy 0.7198\n",
            "Epoch 29 Batch 900 Loss 1.1854 Accuracy 0.7203\n",
            "Epoch 29 Batch 950 Loss 1.1833 Accuracy 0.7208\n",
            "discarded batch 953\n",
            "Epoch 29 Loss 1.1833 Accuracy 0.7208\n",
            "Time taken for 1 epoch: 67.31999564170837 secs\n",
            "\n",
            "Epoch 30 Batch 0 Loss 1.0990 Accuracy 0.7525\n",
            "Epoch 30 Batch 50 Loss 1.1544 Accuracy 0.7229\n",
            "Epoch 30 Batch 100 Loss 1.1566 Accuracy 0.7254\n",
            "Epoch 30 Batch 150 Loss 1.1670 Accuracy 0.7244\n",
            "Epoch 30 Batch 200 Loss 1.1763 Accuracy 0.7244\n",
            "Epoch 30 Batch 250 Loss 1.1819 Accuracy 0.7233\n",
            "Epoch 30 Batch 300 Loss 1.1868 Accuracy 0.7217\n",
            "Epoch 30 Batch 350 Loss 1.1870 Accuracy 0.7211\n",
            "Epoch 30 Batch 400 Loss 1.1903 Accuracy 0.7205\n",
            "Epoch 30 Batch 450 Loss 1.1874 Accuracy 0.7202\n",
            "Epoch 30 Batch 500 Loss 1.1893 Accuracy 0.7197\n",
            "Epoch 30 Batch 550 Loss 1.1920 Accuracy 0.7192\n",
            "Epoch 30 Batch 600 Loss 1.1915 Accuracy 0.7192\n",
            "Epoch 30 Batch 650 Loss 1.1910 Accuracy 0.7191\n",
            "Epoch 30 Batch 700 Loss 1.1897 Accuracy 0.7193\n",
            "Epoch 30 Batch 750 Loss 1.1889 Accuracy 0.7194\n",
            "Epoch 30 Batch 800 Loss 1.1886 Accuracy 0.7195\n",
            "Epoch 30 Batch 850 Loss 1.1858 Accuracy 0.7201\n",
            "Epoch 30 Batch 900 Loss 1.1836 Accuracy 0.7204\n",
            "Epoch 30 Batch 950 Loss 1.1812 Accuracy 0.7207\n",
            "discarded batch 953\n",
            "Saving checkpoint for epoch 30 at ./checkpoints/train/ckpt-28\n",
            "Epoch 30 Loss 1.1811 Accuracy 0.7207\n",
            "Time taken for 1 epoch: 68.48426604270935 secs\n",
            "\n",
            "Epoch 31 Batch 0 Loss 1.0849 Accuracy 0.7228\n",
            "Epoch 31 Batch 50 Loss 1.1572 Accuracy 0.7233\n",
            "Epoch 31 Batch 100 Loss 1.1579 Accuracy 0.7241\n",
            "Epoch 31 Batch 150 Loss 1.1708 Accuracy 0.7227\n",
            "Epoch 31 Batch 200 Loss 1.1737 Accuracy 0.7232\n",
            "Epoch 31 Batch 250 Loss 1.1808 Accuracy 0.7213\n",
            "Epoch 31 Batch 300 Loss 1.1861 Accuracy 0.7201\n",
            "Epoch 31 Batch 350 Loss 1.1879 Accuracy 0.7199\n",
            "Epoch 31 Batch 400 Loss 1.1917 Accuracy 0.7193\n",
            "Epoch 31 Batch 450 Loss 1.1890 Accuracy 0.7193\n",
            "Epoch 31 Batch 500 Loss 1.1892 Accuracy 0.7191\n",
            "Epoch 31 Batch 550 Loss 1.1916 Accuracy 0.7188\n",
            "Epoch 31 Batch 600 Loss 1.1903 Accuracy 0.7190\n",
            "Epoch 31 Batch 650 Loss 1.1897 Accuracy 0.7192\n",
            "Epoch 31 Batch 700 Loss 1.1886 Accuracy 0.7195\n",
            "Epoch 31 Batch 750 Loss 1.1882 Accuracy 0.7195\n",
            "Epoch 31 Batch 800 Loss 1.1866 Accuracy 0.7200\n",
            "Epoch 31 Batch 850 Loss 1.1850 Accuracy 0.7201\n",
            "Epoch 31 Batch 900 Loss 1.1825 Accuracy 0.7205\n",
            "Epoch 31 Batch 950 Loss 1.1808 Accuracy 0.7207\n",
            "discarded batch 953\n",
            "Epoch 31 Loss 1.1807 Accuracy 0.7207\n",
            "Time taken for 1 epoch: 67.53760099411011 secs\n",
            "\n",
            "Epoch 32 Batch 0 Loss 1.0647 Accuracy 0.7393\n",
            "Epoch 32 Batch 50 Loss 1.1449 Accuracy 0.7244\n",
            "Epoch 32 Batch 100 Loss 1.1483 Accuracy 0.7247\n",
            "Epoch 32 Batch 150 Loss 1.1600 Accuracy 0.7241\n",
            "Epoch 32 Batch 200 Loss 1.1707 Accuracy 0.7238\n",
            "Epoch 32 Batch 250 Loss 1.1778 Accuracy 0.7227\n",
            "Epoch 32 Batch 300 Loss 1.1826 Accuracy 0.7223\n",
            "Epoch 32 Batch 350 Loss 1.1851 Accuracy 0.7214\n",
            "Epoch 32 Batch 400 Loss 1.1879 Accuracy 0.7207\n",
            "Epoch 32 Batch 450 Loss 1.1852 Accuracy 0.7210\n",
            "Epoch 32 Batch 500 Loss 1.1855 Accuracy 0.7206\n",
            "Epoch 32 Batch 550 Loss 1.1876 Accuracy 0.7201\n",
            "Epoch 32 Batch 600 Loss 1.1879 Accuracy 0.7198\n",
            "Epoch 32 Batch 650 Loss 1.1862 Accuracy 0.7200\n",
            "Epoch 32 Batch 700 Loss 1.1848 Accuracy 0.7201\n",
            "Epoch 32 Batch 750 Loss 1.1847 Accuracy 0.7200\n",
            "Epoch 32 Batch 800 Loss 1.1830 Accuracy 0.7206\n",
            "Epoch 32 Batch 850 Loss 1.1806 Accuracy 0.7210\n",
            "Epoch 32 Batch 900 Loss 1.1781 Accuracy 0.7214\n",
            "Epoch 32 Batch 950 Loss 1.1767 Accuracy 0.7217\n",
            "discarded batch 953\n",
            "Epoch 32 Loss 1.1765 Accuracy 0.7217\n",
            "Time taken for 1 epoch: 67.98367500305176 secs\n",
            "\n",
            "Epoch 33 Batch 0 Loss 1.0947 Accuracy 0.7525\n",
            "Epoch 33 Batch 50 Loss 1.1471 Accuracy 0.7272\n",
            "Epoch 33 Batch 100 Loss 1.1523 Accuracy 0.7274\n",
            "Epoch 33 Batch 150 Loss 1.1632 Accuracy 0.7250\n",
            "Epoch 33 Batch 200 Loss 1.1691 Accuracy 0.7243\n",
            "Epoch 33 Batch 250 Loss 1.1745 Accuracy 0.7232\n",
            "Epoch 33 Batch 300 Loss 1.1773 Accuracy 0.7225\n",
            "Epoch 33 Batch 350 Loss 1.1771 Accuracy 0.7225\n",
            "Epoch 33 Batch 400 Loss 1.1815 Accuracy 0.7217\n",
            "Epoch 33 Batch 450 Loss 1.1794 Accuracy 0.7217\n",
            "Epoch 33 Batch 500 Loss 1.1802 Accuracy 0.7212\n",
            "Epoch 33 Batch 550 Loss 1.1822 Accuracy 0.7207\n",
            "Epoch 33 Batch 600 Loss 1.1821 Accuracy 0.7209\n",
            "Epoch 33 Batch 650 Loss 1.1816 Accuracy 0.7210\n",
            "Epoch 33 Batch 700 Loss 1.1814 Accuracy 0.7209\n",
            "Epoch 33 Batch 750 Loss 1.1802 Accuracy 0.7211\n",
            "Epoch 33 Batch 800 Loss 1.1794 Accuracy 0.7211\n",
            "Epoch 33 Batch 850 Loss 1.1770 Accuracy 0.7214\n",
            "Epoch 33 Batch 900 Loss 1.1745 Accuracy 0.7219\n",
            "Epoch 33 Batch 950 Loss 1.1722 Accuracy 0.7223\n",
            "discarded batch 953\n",
            "Epoch 33 Loss 1.1721 Accuracy 0.7223\n",
            "Time taken for 1 epoch: 67.43649482727051 secs\n",
            "\n",
            "Epoch 34 Batch 0 Loss 1.1150 Accuracy 0.7393\n",
            "Epoch 34 Batch 50 Loss 1.1275 Accuracy 0.7298\n",
            "Epoch 34 Batch 100 Loss 1.1408 Accuracy 0.7282\n",
            "Epoch 34 Batch 150 Loss 1.1515 Accuracy 0.7271\n",
            "Epoch 34 Batch 200 Loss 1.1589 Accuracy 0.7268\n",
            "Epoch 34 Batch 250 Loss 1.1671 Accuracy 0.7250\n",
            "Epoch 34 Batch 300 Loss 1.1728 Accuracy 0.7238\n",
            "Epoch 34 Batch 350 Loss 1.1740 Accuracy 0.7235\n",
            "Epoch 34 Batch 400 Loss 1.1781 Accuracy 0.7222\n",
            "Epoch 34 Batch 450 Loss 1.1764 Accuracy 0.7223\n",
            "Epoch 34 Batch 500 Loss 1.1775 Accuracy 0.7220\n",
            "Epoch 34 Batch 550 Loss 1.1789 Accuracy 0.7215\n",
            "Epoch 34 Batch 600 Loss 1.1796 Accuracy 0.7213\n",
            "Epoch 34 Batch 650 Loss 1.1789 Accuracy 0.7214\n",
            "Epoch 34 Batch 700 Loss 1.1777 Accuracy 0.7216\n",
            "Epoch 34 Batch 750 Loss 1.1771 Accuracy 0.7216\n",
            "Epoch 34 Batch 800 Loss 1.1755 Accuracy 0.7221\n",
            "Epoch 34 Batch 850 Loss 1.1730 Accuracy 0.7224\n",
            "Epoch 34 Batch 900 Loss 1.1702 Accuracy 0.7228\n",
            "Epoch 34 Batch 950 Loss 1.1685 Accuracy 0.7230\n",
            "discarded batch 953\n",
            "Epoch 34 Loss 1.1684 Accuracy 0.7231\n",
            "Time taken for 1 epoch: 67.18417835235596 secs\n",
            "\n",
            "Epoch 35 Batch 0 Loss 1.0941 Accuracy 0.7426\n",
            "Epoch 35 Batch 50 Loss 1.1473 Accuracy 0.7260\n",
            "Epoch 35 Batch 100 Loss 1.1460 Accuracy 0.7270\n",
            "Epoch 35 Batch 150 Loss 1.1560 Accuracy 0.7255\n",
            "Epoch 35 Batch 200 Loss 1.1638 Accuracy 0.7254\n",
            "Epoch 35 Batch 250 Loss 1.1684 Accuracy 0.7242\n",
            "Epoch 35 Batch 300 Loss 1.1742 Accuracy 0.7227\n",
            "Epoch 35 Batch 350 Loss 1.1754 Accuracy 0.7222\n",
            "Epoch 35 Batch 400 Loss 1.1788 Accuracy 0.7219\n",
            "Epoch 35 Batch 450 Loss 1.1765 Accuracy 0.7218\n",
            "Epoch 35 Batch 500 Loss 1.1786 Accuracy 0.7212\n",
            "Epoch 35 Batch 550 Loss 1.1803 Accuracy 0.7205\n",
            "Epoch 35 Batch 600 Loss 1.1795 Accuracy 0.7207\n",
            "Epoch 35 Batch 650 Loss 1.1780 Accuracy 0.7211\n",
            "Epoch 35 Batch 700 Loss 1.1773 Accuracy 0.7212\n",
            "Epoch 35 Batch 750 Loss 1.1761 Accuracy 0.7213\n",
            "Epoch 35 Batch 800 Loss 1.1752 Accuracy 0.7215\n",
            "Epoch 35 Batch 850 Loss 1.1732 Accuracy 0.7216\n",
            "Epoch 35 Batch 900 Loss 1.1703 Accuracy 0.7221\n",
            "Epoch 35 Batch 950 Loss 1.1681 Accuracy 0.7225\n",
            "discarded batch 953\n",
            "Saving checkpoint for epoch 35 at ./checkpoints/train/ckpt-29\n",
            "Epoch 35 Loss 1.1681 Accuracy 0.7225\n",
            "Time taken for 1 epoch: 67.73495125770569 secs\n",
            "\n",
            "Epoch 36 Batch 0 Loss 1.0344 Accuracy 0.7459\n",
            "Epoch 36 Batch 50 Loss 1.1309 Accuracy 0.7284\n",
            "Epoch 36 Batch 100 Loss 1.1459 Accuracy 0.7281\n",
            "Epoch 36 Batch 150 Loss 1.1546 Accuracy 0.7266\n",
            "Epoch 36 Batch 200 Loss 1.1593 Accuracy 0.7258\n",
            "Epoch 36 Batch 250 Loss 1.1641 Accuracy 0.7252\n",
            "Epoch 36 Batch 300 Loss 1.1692 Accuracy 0.7241\n",
            "Epoch 36 Batch 350 Loss 1.1710 Accuracy 0.7241\n",
            "Epoch 36 Batch 400 Loss 1.1760 Accuracy 0.7229\n",
            "Epoch 36 Batch 450 Loss 1.1739 Accuracy 0.7232\n",
            "Epoch 36 Batch 500 Loss 1.1752 Accuracy 0.7228\n",
            "Epoch 36 Batch 550 Loss 1.1770 Accuracy 0.7221\n",
            "Epoch 36 Batch 600 Loss 1.1763 Accuracy 0.7218\n",
            "Epoch 36 Batch 650 Loss 1.1753 Accuracy 0.7219\n",
            "Epoch 36 Batch 700 Loss 1.1745 Accuracy 0.7220\n",
            "Epoch 36 Batch 750 Loss 1.1739 Accuracy 0.7221\n",
            "Epoch 36 Batch 800 Loss 1.1725 Accuracy 0.7225\n",
            "Epoch 36 Batch 850 Loss 1.1707 Accuracy 0.7226\n",
            "Epoch 36 Batch 900 Loss 1.1682 Accuracy 0.7230\n",
            "Epoch 36 Batch 950 Loss 1.1663 Accuracy 0.7234\n",
            "discarded batch 953\n",
            "Epoch 36 Loss 1.1661 Accuracy 0.7234\n",
            "Time taken for 1 epoch: 67.42689633369446 secs\n",
            "\n",
            "Epoch 37 Batch 0 Loss 0.9680 Accuracy 0.7558\n",
            "Epoch 37 Batch 50 Loss 1.1403 Accuracy 0.7257\n",
            "Epoch 37 Batch 100 Loss 1.1410 Accuracy 0.7271\n",
            "Epoch 37 Batch 150 Loss 1.1556 Accuracy 0.7235\n",
            "Epoch 37 Batch 200 Loss 1.1611 Accuracy 0.7243\n",
            "Epoch 37 Batch 250 Loss 1.1654 Accuracy 0.7238\n",
            "Epoch 37 Batch 300 Loss 1.1702 Accuracy 0.7228\n",
            "Epoch 37 Batch 350 Loss 1.1711 Accuracy 0.7228\n",
            "Epoch 37 Batch 400 Loss 1.1758 Accuracy 0.7222\n",
            "Epoch 37 Batch 450 Loss 1.1723 Accuracy 0.7228\n",
            "Epoch 37 Batch 500 Loss 1.1738 Accuracy 0.7225\n",
            "Epoch 37 Batch 550 Loss 1.1756 Accuracy 0.7220\n",
            "Epoch 37 Batch 600 Loss 1.1746 Accuracy 0.7221\n",
            "Epoch 37 Batch 650 Loss 1.1741 Accuracy 0.7223\n",
            "Epoch 37 Batch 700 Loss 1.1730 Accuracy 0.7224\n",
            "Epoch 37 Batch 750 Loss 1.1725 Accuracy 0.7225\n",
            "Epoch 37 Batch 800 Loss 1.1705 Accuracy 0.7228\n",
            "Epoch 37 Batch 850 Loss 1.1683 Accuracy 0.7231\n",
            "Epoch 37 Batch 900 Loss 1.1656 Accuracy 0.7238\n",
            "Epoch 37 Batch 950 Loss 1.1637 Accuracy 0.7239\n",
            "discarded batch 953\n",
            "Epoch 37 Loss 1.1637 Accuracy 0.7239\n",
            "Time taken for 1 epoch: 67.9647843837738 secs\n",
            "\n",
            "Epoch 38 Batch 0 Loss 1.0965 Accuracy 0.7558\n",
            "Epoch 38 Batch 50 Loss 1.1309 Accuracy 0.7294\n",
            "Epoch 38 Batch 100 Loss 1.1356 Accuracy 0.7291\n",
            "Epoch 38 Batch 150 Loss 1.1463 Accuracy 0.7270\n",
            "Epoch 38 Batch 200 Loss 1.1536 Accuracy 0.7264\n",
            "Epoch 38 Batch 250 Loss 1.1601 Accuracy 0.7255\n",
            "Epoch 38 Batch 300 Loss 1.1639 Accuracy 0.7241\n",
            "Epoch 38 Batch 350 Loss 1.1655 Accuracy 0.7238\n",
            "Epoch 38 Batch 400 Loss 1.1700 Accuracy 0.7234\n",
            "Epoch 38 Batch 450 Loss 1.1680 Accuracy 0.7235\n",
            "Epoch 38 Batch 500 Loss 1.1685 Accuracy 0.7230\n",
            "Epoch 38 Batch 550 Loss 1.1709 Accuracy 0.7225\n",
            "Epoch 38 Batch 600 Loss 1.1705 Accuracy 0.7225\n",
            "Epoch 38 Batch 650 Loss 1.1696 Accuracy 0.7225\n",
            "Epoch 38 Batch 700 Loss 1.1685 Accuracy 0.7228\n",
            "Epoch 38 Batch 750 Loss 1.1679 Accuracy 0.7228\n",
            "Epoch 38 Batch 800 Loss 1.1662 Accuracy 0.7234\n",
            "Epoch 38 Batch 850 Loss 1.1643 Accuracy 0.7235\n",
            "Epoch 38 Batch 900 Loss 1.1621 Accuracy 0.7239\n",
            "Epoch 38 Batch 950 Loss 1.1601 Accuracy 0.7242\n",
            "discarded batch 953\n",
            "Epoch 38 Loss 1.1600 Accuracy 0.7242\n",
            "Time taken for 1 epoch: 67.44378113746643 secs\n",
            "\n",
            "Epoch 39 Batch 0 Loss 0.9979 Accuracy 0.7492\n",
            "Epoch 39 Batch 50 Loss 1.1270 Accuracy 0.7298\n",
            "Epoch 39 Batch 100 Loss 1.1339 Accuracy 0.7300\n",
            "Epoch 39 Batch 150 Loss 1.1500 Accuracy 0.7273\n",
            "Epoch 39 Batch 200 Loss 1.1572 Accuracy 0.7265\n",
            "Epoch 39 Batch 250 Loss 1.1620 Accuracy 0.7256\n",
            "Epoch 39 Batch 300 Loss 1.1657 Accuracy 0.7246\n",
            "Epoch 39 Batch 350 Loss 1.1671 Accuracy 0.7243\n",
            "Epoch 39 Batch 400 Loss 1.1709 Accuracy 0.7236\n",
            "Epoch 39 Batch 450 Loss 1.1692 Accuracy 0.7234\n",
            "Epoch 39 Batch 500 Loss 1.1683 Accuracy 0.7232\n",
            "Epoch 39 Batch 550 Loss 1.1707 Accuracy 0.7227\n",
            "Epoch 39 Batch 600 Loss 1.1710 Accuracy 0.7223\n",
            "Epoch 39 Batch 650 Loss 1.1698 Accuracy 0.7226\n",
            "Epoch 39 Batch 700 Loss 1.1681 Accuracy 0.7230\n",
            "Epoch 39 Batch 750 Loss 1.1674 Accuracy 0.7231\n",
            "Epoch 39 Batch 800 Loss 1.1659 Accuracy 0.7234\n",
            "Epoch 39 Batch 850 Loss 1.1642 Accuracy 0.7236\n",
            "Epoch 39 Batch 900 Loss 1.1616 Accuracy 0.7242\n",
            "Epoch 39 Batch 950 Loss 1.1596 Accuracy 0.7245\n",
            "discarded batch 953\n",
            "Epoch 39 Loss 1.1596 Accuracy 0.7245\n",
            "Time taken for 1 epoch: 67.37428975105286 secs\n",
            "\n",
            "Epoch 40 Batch 0 Loss 1.0748 Accuracy 0.7591\n",
            "Epoch 40 Batch 50 Loss 1.1253 Accuracy 0.7306\n",
            "Epoch 40 Batch 100 Loss 1.1307 Accuracy 0.7301\n",
            "Epoch 40 Batch 150 Loss 1.1388 Accuracy 0.7286\n",
            "Epoch 40 Batch 200 Loss 1.1436 Accuracy 0.7285\n",
            "Epoch 40 Batch 250 Loss 1.1529 Accuracy 0.7266\n",
            "Epoch 40 Batch 300 Loss 1.1585 Accuracy 0.7247\n",
            "Epoch 40 Batch 350 Loss 1.1617 Accuracy 0.7240\n",
            "Epoch 40 Batch 400 Loss 1.1655 Accuracy 0.7234\n",
            "Epoch 40 Batch 450 Loss 1.1629 Accuracy 0.7239\n",
            "Epoch 40 Batch 500 Loss 1.1637 Accuracy 0.7237\n",
            "Epoch 40 Batch 550 Loss 1.1664 Accuracy 0.7231\n",
            "Epoch 40 Batch 600 Loss 1.1669 Accuracy 0.7229\n",
            "Epoch 40 Batch 650 Loss 1.1668 Accuracy 0.7231\n",
            "Epoch 40 Batch 700 Loss 1.1661 Accuracy 0.7230\n",
            "Epoch 40 Batch 750 Loss 1.1658 Accuracy 0.7231\n",
            "Epoch 40 Batch 800 Loss 1.1647 Accuracy 0.7234\n",
            "Epoch 40 Batch 850 Loss 1.1624 Accuracy 0.7237\n",
            "Epoch 40 Batch 900 Loss 1.1598 Accuracy 0.7242\n",
            "Epoch 40 Batch 950 Loss 1.1584 Accuracy 0.7243\n",
            "discarded batch 953\n",
            "Saving checkpoint for epoch 40 at ./checkpoints/train/ckpt-30\n",
            "Epoch 40 Loss 1.1584 Accuracy 0.7243\n",
            "Time taken for 1 epoch: 67.38155102729797 secs\n",
            "\n",
            "Epoch 41 Batch 0 Loss 1.0924 Accuracy 0.7360\n",
            "Epoch 41 Batch 50 Loss 1.1286 Accuracy 0.7340\n",
            "Epoch 41 Batch 100 Loss 1.1339 Accuracy 0.7330\n",
            "Epoch 41 Batch 150 Loss 1.1442 Accuracy 0.7294\n",
            "Epoch 41 Batch 200 Loss 1.1499 Accuracy 0.7281\n",
            "Epoch 41 Batch 250 Loss 1.1566 Accuracy 0.7269\n",
            "Epoch 41 Batch 300 Loss 1.1601 Accuracy 0.7259\n",
            "Epoch 41 Batch 350 Loss 1.1610 Accuracy 0.7255\n",
            "Epoch 41 Batch 400 Loss 1.1646 Accuracy 0.7247\n",
            "Epoch 41 Batch 450 Loss 1.1633 Accuracy 0.7250\n",
            "Epoch 41 Batch 500 Loss 1.1640 Accuracy 0.7245\n",
            "Epoch 41 Batch 550 Loss 1.1660 Accuracy 0.7240\n",
            "Epoch 41 Batch 600 Loss 1.1661 Accuracy 0.7237\n",
            "Epoch 41 Batch 650 Loss 1.1646 Accuracy 0.7240\n",
            "Epoch 41 Batch 700 Loss 1.1624 Accuracy 0.7245\n",
            "Epoch 41 Batch 750 Loss 1.1621 Accuracy 0.7245\n",
            "Epoch 41 Batch 800 Loss 1.1608 Accuracy 0.7247\n",
            "Epoch 41 Batch 850 Loss 1.1589 Accuracy 0.7249\n",
            "Epoch 41 Batch 900 Loss 1.1569 Accuracy 0.7251\n",
            "Epoch 41 Batch 950 Loss 1.1549 Accuracy 0.7254\n",
            "discarded batch 953\n",
            "Epoch 41 Loss 1.1548 Accuracy 0.7254\n",
            "Time taken for 1 epoch: 67.83394503593445 secs\n",
            "\n",
            "Epoch 42 Batch 0 Loss 1.0394 Accuracy 0.7591\n",
            "Epoch 42 Batch 50 Loss 1.1289 Accuracy 0.7276\n",
            "Epoch 42 Batch 100 Loss 1.1304 Accuracy 0.7299\n",
            "Epoch 42 Batch 150 Loss 1.1397 Accuracy 0.7290\n",
            "Epoch 42 Batch 200 Loss 1.1455 Accuracy 0.7287\n",
            "Epoch 42 Batch 250 Loss 1.1519 Accuracy 0.7269\n",
            "Epoch 42 Batch 300 Loss 1.1550 Accuracy 0.7260\n",
            "Epoch 42 Batch 350 Loss 1.1558 Accuracy 0.7259\n",
            "Epoch 42 Batch 400 Loss 1.1601 Accuracy 0.7254\n",
            "Epoch 42 Batch 450 Loss 1.1580 Accuracy 0.7258\n",
            "Epoch 42 Batch 500 Loss 1.1595 Accuracy 0.7250\n",
            "Epoch 42 Batch 550 Loss 1.1617 Accuracy 0.7242\n",
            "Epoch 42 Batch 600 Loss 1.1618 Accuracy 0.7243\n",
            "Epoch 42 Batch 650 Loss 1.1614 Accuracy 0.7241\n",
            "Epoch 42 Batch 700 Loss 1.1612 Accuracy 0.7242\n",
            "Epoch 42 Batch 750 Loss 1.1608 Accuracy 0.7241\n",
            "Epoch 42 Batch 800 Loss 1.1601 Accuracy 0.7245\n",
            "Epoch 42 Batch 850 Loss 1.1577 Accuracy 0.7249\n",
            "Epoch 42 Batch 900 Loss 1.1556 Accuracy 0.7251\n",
            "Epoch 42 Batch 950 Loss 1.1532 Accuracy 0.7255\n",
            "discarded batch 953\n",
            "Epoch 42 Loss 1.1532 Accuracy 0.7255\n",
            "Time taken for 1 epoch: 67.27096223831177 secs\n",
            "\n",
            "Epoch 43 Batch 0 Loss 0.9972 Accuracy 0.7327\n",
            "Epoch 43 Batch 50 Loss 1.1362 Accuracy 0.7259\n",
            "Epoch 43 Batch 100 Loss 1.1316 Accuracy 0.7276\n",
            "Epoch 43 Batch 150 Loss 1.1391 Accuracy 0.7258\n",
            "Epoch 43 Batch 200 Loss 1.1463 Accuracy 0.7262\n",
            "Epoch 43 Batch 250 Loss 1.1521 Accuracy 0.7252\n",
            "Epoch 43 Batch 300 Loss 1.1556 Accuracy 0.7246\n",
            "Epoch 43 Batch 350 Loss 1.1568 Accuracy 0.7240\n",
            "Epoch 43 Batch 400 Loss 1.1611 Accuracy 0.7236\n",
            "Epoch 43 Batch 450 Loss 1.1585 Accuracy 0.7242\n",
            "Epoch 43 Batch 500 Loss 1.1591 Accuracy 0.7239\n",
            "Epoch 43 Batch 550 Loss 1.1607 Accuracy 0.7237\n",
            "Epoch 43 Batch 600 Loss 1.1599 Accuracy 0.7237\n",
            "Epoch 43 Batch 650 Loss 1.1587 Accuracy 0.7239\n",
            "Epoch 43 Batch 700 Loss 1.1586 Accuracy 0.7239\n",
            "Epoch 43 Batch 750 Loss 1.1578 Accuracy 0.7239\n",
            "Epoch 43 Batch 800 Loss 1.1571 Accuracy 0.7240\n",
            "Epoch 43 Batch 850 Loss 1.1551 Accuracy 0.7243\n",
            "Epoch 43 Batch 900 Loss 1.1524 Accuracy 0.7248\n",
            "Epoch 43 Batch 950 Loss 1.1506 Accuracy 0.7250\n",
            "discarded batch 953\n",
            "Epoch 43 Loss 1.1504 Accuracy 0.7250\n",
            "Time taken for 1 epoch: 66.85318517684937 secs\n",
            "\n",
            "Epoch 44 Batch 0 Loss 0.9626 Accuracy 0.7624\n",
            "Epoch 44 Batch 50 Loss 1.1283 Accuracy 0.7259\n",
            "Epoch 44 Batch 100 Loss 1.1332 Accuracy 0.7276\n",
            "Epoch 44 Batch 150 Loss 1.1406 Accuracy 0.7268\n",
            "Epoch 44 Batch 200 Loss 1.1448 Accuracy 0.7282\n",
            "Epoch 44 Batch 250 Loss 1.1509 Accuracy 0.7269\n",
            "Epoch 44 Batch 300 Loss 1.1553 Accuracy 0.7259\n",
            "Epoch 44 Batch 350 Loss 1.1558 Accuracy 0.7256\n",
            "Epoch 44 Batch 400 Loss 1.1589 Accuracy 0.7247\n",
            "Epoch 44 Batch 450 Loss 1.1581 Accuracy 0.7245\n",
            "Epoch 44 Batch 500 Loss 1.1581 Accuracy 0.7243\n",
            "Epoch 44 Batch 550 Loss 1.1602 Accuracy 0.7237\n",
            "Epoch 44 Batch 600 Loss 1.1593 Accuracy 0.7235\n",
            "Epoch 44 Batch 650 Loss 1.1587 Accuracy 0.7236\n",
            "Epoch 44 Batch 700 Loss 1.1579 Accuracy 0.7239\n",
            "Epoch 44 Batch 750 Loss 1.1565 Accuracy 0.7243\n",
            "Epoch 44 Batch 800 Loss 1.1555 Accuracy 0.7246\n",
            "Epoch 44 Batch 850 Loss 1.1536 Accuracy 0.7248\n",
            "Epoch 44 Batch 900 Loss 1.1507 Accuracy 0.7254\n",
            "Epoch 44 Batch 950 Loss 1.1490 Accuracy 0.7256\n",
            "discarded batch 953\n",
            "Epoch 44 Loss 1.1490 Accuracy 0.7256\n",
            "Time taken for 1 epoch: 67.35283517837524 secs\n",
            "\n",
            "Epoch 45 Batch 0 Loss 1.1152 Accuracy 0.7294\n",
            "Epoch 45 Batch 50 Loss 1.1250 Accuracy 0.7287\n",
            "Epoch 45 Batch 100 Loss 1.1276 Accuracy 0.7306\n",
            "Epoch 45 Batch 150 Loss 1.1376 Accuracy 0.7295\n",
            "Epoch 45 Batch 200 Loss 1.1421 Accuracy 0.7299\n",
            "Epoch 45 Batch 250 Loss 1.1484 Accuracy 0.7287\n",
            "Epoch 45 Batch 300 Loss 1.1536 Accuracy 0.7275\n",
            "Epoch 45 Batch 350 Loss 1.1544 Accuracy 0.7269\n",
            "Epoch 45 Batch 400 Loss 1.1573 Accuracy 0.7266\n",
            "Epoch 45 Batch 450 Loss 1.1567 Accuracy 0.7265\n",
            "Epoch 45 Batch 500 Loss 1.1581 Accuracy 0.7258\n",
            "Epoch 45 Batch 550 Loss 1.1582 Accuracy 0.7257\n",
            "Epoch 45 Batch 600 Loss 1.1577 Accuracy 0.7256\n",
            "Epoch 45 Batch 650 Loss 1.1556 Accuracy 0.7260\n",
            "Epoch 45 Batch 700 Loss 1.1553 Accuracy 0.7259\n",
            "Epoch 45 Batch 750 Loss 1.1548 Accuracy 0.7257\n",
            "Epoch 45 Batch 800 Loss 1.1543 Accuracy 0.7262\n",
            "Epoch 45 Batch 850 Loss 1.1522 Accuracy 0.7265\n",
            "Epoch 45 Batch 900 Loss 1.1497 Accuracy 0.7269\n",
            "Epoch 45 Batch 950 Loss 1.1479 Accuracy 0.7271\n",
            "discarded batch 953\n",
            "Saving checkpoint for epoch 45 at ./checkpoints/train/ckpt-31\n",
            "Epoch 45 Loss 1.1478 Accuracy 0.7271\n",
            "Time taken for 1 epoch: 68.15862250328064 secs\n",
            "\n",
            "Epoch 46 Batch 0 Loss 0.9981 Accuracy 0.7459\n",
            "Epoch 46 Batch 50 Loss 1.1168 Accuracy 0.7322\n",
            "Epoch 46 Batch 100 Loss 1.1185 Accuracy 0.7316\n",
            "Epoch 46 Batch 150 Loss 1.1339 Accuracy 0.7306\n",
            "Epoch 46 Batch 200 Loss 1.1412 Accuracy 0.7300\n",
            "Epoch 46 Batch 250 Loss 1.1473 Accuracy 0.7287\n",
            "Epoch 46 Batch 300 Loss 1.1500 Accuracy 0.7274\n",
            "Epoch 46 Batch 350 Loss 1.1512 Accuracy 0.7269\n",
            "Epoch 46 Batch 400 Loss 1.1550 Accuracy 0.7265\n",
            "Epoch 46 Batch 450 Loss 1.1521 Accuracy 0.7267\n",
            "Epoch 46 Batch 500 Loss 1.1528 Accuracy 0.7264\n",
            "Epoch 46 Batch 550 Loss 1.1544 Accuracy 0.7256\n",
            "Epoch 46 Batch 600 Loss 1.1539 Accuracy 0.7258\n",
            "Epoch 46 Batch 650 Loss 1.1535 Accuracy 0.7259\n",
            "Epoch 46 Batch 700 Loss 1.1526 Accuracy 0.7262\n",
            "Epoch 46 Batch 750 Loss 1.1516 Accuracy 0.7261\n",
            "Epoch 46 Batch 800 Loss 1.1504 Accuracy 0.7263\n",
            "Epoch 46 Batch 850 Loss 1.1479 Accuracy 0.7267\n",
            "Epoch 46 Batch 900 Loss 1.1460 Accuracy 0.7270\n",
            "Epoch 46 Batch 950 Loss 1.1448 Accuracy 0.7271\n",
            "discarded batch 953\n",
            "Epoch 46 Loss 1.1446 Accuracy 0.7271\n",
            "Time taken for 1 epoch: 68.25566029548645 secs\n",
            "\n",
            "Epoch 47 Batch 0 Loss 1.0142 Accuracy 0.7261\n",
            "Epoch 47 Batch 50 Loss 1.1203 Accuracy 0.7267\n",
            "Epoch 47 Batch 100 Loss 1.1160 Accuracy 0.7298\n",
            "Epoch 47 Batch 150 Loss 1.1266 Accuracy 0.7290\n",
            "Epoch 47 Batch 200 Loss 1.1329 Accuracy 0.7291\n",
            "Epoch 47 Batch 250 Loss 1.1390 Accuracy 0.7279\n",
            "Epoch 47 Batch 300 Loss 1.1440 Accuracy 0.7270\n",
            "Epoch 47 Batch 350 Loss 1.1452 Accuracy 0.7270\n",
            "Epoch 47 Batch 400 Loss 1.1496 Accuracy 0.7262\n",
            "Epoch 47 Batch 450 Loss 1.1488 Accuracy 0.7260\n",
            "Epoch 47 Batch 500 Loss 1.1495 Accuracy 0.7256\n",
            "Epoch 47 Batch 550 Loss 1.1525 Accuracy 0.7247\n",
            "Epoch 47 Batch 600 Loss 1.1528 Accuracy 0.7246\n",
            "Epoch 47 Batch 650 Loss 1.1523 Accuracy 0.7248\n",
            "Epoch 47 Batch 700 Loss 1.1513 Accuracy 0.7248\n",
            "Epoch 47 Batch 750 Loss 1.1503 Accuracy 0.7250\n",
            "Epoch 47 Batch 800 Loss 1.1502 Accuracy 0.7253\n",
            "Epoch 47 Batch 850 Loss 1.1480 Accuracy 0.7256\n",
            "Epoch 47 Batch 900 Loss 1.1454 Accuracy 0.7261\n",
            "Epoch 47 Batch 950 Loss 1.1440 Accuracy 0.7262\n",
            "discarded batch 953\n",
            "Epoch 47 Loss 1.1439 Accuracy 0.7262\n",
            "Time taken for 1 epoch: 66.95121598243713 secs\n",
            "\n",
            "Epoch 48 Batch 0 Loss 0.9981 Accuracy 0.7657\n",
            "Epoch 48 Batch 50 Loss 1.1177 Accuracy 0.7276\n",
            "Epoch 48 Batch 100 Loss 1.1198 Accuracy 0.7305\n",
            "Epoch 48 Batch 150 Loss 1.1302 Accuracy 0.7300\n",
            "Epoch 48 Batch 200 Loss 1.1393 Accuracy 0.7287\n",
            "Epoch 48 Batch 250 Loss 1.1455 Accuracy 0.7275\n",
            "Epoch 48 Batch 300 Loss 1.1476 Accuracy 0.7267\n",
            "Epoch 48 Batch 350 Loss 1.1466 Accuracy 0.7264\n",
            "Epoch 48 Batch 400 Loss 1.1496 Accuracy 0.7262\n",
            "Epoch 48 Batch 450 Loss 1.1472 Accuracy 0.7265\n",
            "Epoch 48 Batch 500 Loss 1.1459 Accuracy 0.7267\n",
            "Epoch 48 Batch 550 Loss 1.1481 Accuracy 0.7263\n",
            "Epoch 48 Batch 600 Loss 1.1489 Accuracy 0.7261\n",
            "Epoch 48 Batch 650 Loss 1.1484 Accuracy 0.7264\n",
            "Epoch 48 Batch 700 Loss 1.1480 Accuracy 0.7264\n",
            "Epoch 48 Batch 750 Loss 1.1486 Accuracy 0.7261\n",
            "Epoch 48 Batch 800 Loss 1.1479 Accuracy 0.7264\n",
            "Epoch 48 Batch 850 Loss 1.1453 Accuracy 0.7267\n",
            "Epoch 48 Batch 900 Loss 1.1435 Accuracy 0.7271\n",
            "Epoch 48 Batch 950 Loss 1.1421 Accuracy 0.7271\n",
            "discarded batch 953\n",
            "Epoch 48 Loss 1.1420 Accuracy 0.7272\n",
            "Time taken for 1 epoch: 67.6028151512146 secs\n",
            "\n",
            "Epoch 49 Batch 0 Loss 0.9452 Accuracy 0.7624\n",
            "Epoch 49 Batch 50 Loss 1.1062 Accuracy 0.7321\n",
            "Epoch 49 Batch 100 Loss 1.1136 Accuracy 0.7327\n",
            "Epoch 49 Batch 150 Loss 1.1263 Accuracy 0.7309\n",
            "Epoch 49 Batch 200 Loss 1.1324 Accuracy 0.7308\n",
            "Epoch 49 Batch 250 Loss 1.1394 Accuracy 0.7297\n",
            "Epoch 49 Batch 300 Loss 1.1431 Accuracy 0.7285\n",
            "Epoch 49 Batch 350 Loss 1.1436 Accuracy 0.7285\n",
            "Epoch 49 Batch 400 Loss 1.1474 Accuracy 0.7278\n",
            "Epoch 49 Batch 450 Loss 1.1446 Accuracy 0.7281\n",
            "Epoch 49 Batch 500 Loss 1.1454 Accuracy 0.7275\n",
            "Epoch 49 Batch 550 Loss 1.1477 Accuracy 0.7266\n",
            "Epoch 49 Batch 600 Loss 1.1482 Accuracy 0.7264\n",
            "Epoch 49 Batch 650 Loss 1.1475 Accuracy 0.7265\n",
            "Epoch 49 Batch 700 Loss 1.1464 Accuracy 0.7265\n",
            "Epoch 49 Batch 750 Loss 1.1455 Accuracy 0.7269\n",
            "Epoch 49 Batch 800 Loss 1.1440 Accuracy 0.7274\n",
            "Epoch 49 Batch 850 Loss 1.1419 Accuracy 0.7275\n",
            "Epoch 49 Batch 900 Loss 1.1391 Accuracy 0.7281\n",
            "Epoch 49 Batch 950 Loss 1.1368 Accuracy 0.7283\n",
            "discarded batch 953\n",
            "Epoch 49 Loss 1.1367 Accuracy 0.7284\n",
            "Time taken for 1 epoch: 66.70556926727295 secs\n",
            "\n",
            "Epoch 50 Batch 0 Loss 1.0522 Accuracy 0.7492\n",
            "Epoch 50 Batch 50 Loss 1.1029 Accuracy 0.7334\n",
            "Epoch 50 Batch 100 Loss 1.1112 Accuracy 0.7340\n",
            "Epoch 50 Batch 150 Loss 1.1197 Accuracy 0.7328\n",
            "Epoch 50 Batch 200 Loss 1.1276 Accuracy 0.7320\n",
            "Epoch 50 Batch 250 Loss 1.1320 Accuracy 0.7308\n",
            "Epoch 50 Batch 300 Loss 1.1359 Accuracy 0.7298\n",
            "Epoch 50 Batch 350 Loss 1.1364 Accuracy 0.7296\n",
            "Epoch 50 Batch 400 Loss 1.1400 Accuracy 0.7288\n",
            "Epoch 50 Batch 450 Loss 1.1400 Accuracy 0.7286\n",
            "Epoch 50 Batch 500 Loss 1.1411 Accuracy 0.7281\n",
            "Epoch 50 Batch 550 Loss 1.1435 Accuracy 0.7274\n",
            "Epoch 50 Batch 600 Loss 1.1426 Accuracy 0.7275\n",
            "Epoch 50 Batch 650 Loss 1.1424 Accuracy 0.7275\n",
            "Epoch 50 Batch 700 Loss 1.1419 Accuracy 0.7275\n",
            "Epoch 50 Batch 750 Loss 1.1411 Accuracy 0.7275\n",
            "Epoch 50 Batch 800 Loss 1.1403 Accuracy 0.7278\n",
            "Epoch 50 Batch 850 Loss 1.1384 Accuracy 0.7281\n",
            "Epoch 50 Batch 900 Loss 1.1362 Accuracy 0.7285\n",
            "Epoch 50 Batch 950 Loss 1.1350 Accuracy 0.7286\n",
            "discarded batch 953\n",
            "Saving checkpoint for epoch 50 at ./checkpoints/train/ckpt-32\n",
            "Epoch 50 Loss 1.1349 Accuracy 0.7286\n",
            "Time taken for 1 epoch: 67.33302569389343 secs\n",
            "\n",
            "Epoch 51 Batch 0 Loss 1.0634 Accuracy 0.7360\n",
            "Epoch 51 Batch 50 Loss 1.1051 Accuracy 0.7314\n",
            "Epoch 51 Batch 100 Loss 1.1100 Accuracy 0.7333\n",
            "Epoch 51 Batch 150 Loss 1.1230 Accuracy 0.7316\n",
            "Epoch 51 Batch 200 Loss 1.1298 Accuracy 0.7308\n",
            "Epoch 51 Batch 250 Loss 1.1350 Accuracy 0.7293\n",
            "Epoch 51 Batch 300 Loss 1.1395 Accuracy 0.7289\n",
            "Epoch 51 Batch 350 Loss 1.1402 Accuracy 0.7288\n",
            "Epoch 51 Batch 400 Loss 1.1429 Accuracy 0.7284\n",
            "Epoch 51 Batch 450 Loss 1.1415 Accuracy 0.7283\n",
            "Epoch 51 Batch 500 Loss 1.1419 Accuracy 0.7277\n",
            "Epoch 51 Batch 550 Loss 1.1445 Accuracy 0.7269\n",
            "Epoch 51 Batch 600 Loss 1.1445 Accuracy 0.7268\n",
            "Epoch 51 Batch 650 Loss 1.1438 Accuracy 0.7269\n",
            "Epoch 51 Batch 700 Loss 1.1434 Accuracy 0.7270\n",
            "Epoch 51 Batch 750 Loss 1.1420 Accuracy 0.7270\n",
            "Epoch 51 Batch 800 Loss 1.1415 Accuracy 0.7272\n",
            "Epoch 51 Batch 850 Loss 1.1400 Accuracy 0.7274\n",
            "Epoch 51 Batch 900 Loss 1.1381 Accuracy 0.7276\n",
            "Epoch 51 Batch 950 Loss 1.1363 Accuracy 0.7279\n",
            "discarded batch 953\n",
            "Epoch 51 Loss 1.1364 Accuracy 0.7278\n",
            "Time taken for 1 epoch: 67.62603068351746 secs\n",
            "\n",
            "Epoch 52 Batch 0 Loss 1.0550 Accuracy 0.7360\n",
            "Epoch 52 Batch 50 Loss 1.1082 Accuracy 0.7288\n",
            "Epoch 52 Batch 100 Loss 1.1129 Accuracy 0.7303\n",
            "Epoch 52 Batch 150 Loss 1.1220 Accuracy 0.7287\n",
            "Epoch 52 Batch 200 Loss 1.1281 Accuracy 0.7286\n",
            "Epoch 52 Batch 250 Loss 1.1297 Accuracy 0.7296\n",
            "Epoch 52 Batch 300 Loss 1.1338 Accuracy 0.7283\n",
            "Epoch 52 Batch 350 Loss 1.1337 Accuracy 0.7281\n",
            "Epoch 52 Batch 400 Loss 1.1385 Accuracy 0.7271\n",
            "Epoch 52 Batch 450 Loss 1.1367 Accuracy 0.7270\n",
            "Epoch 52 Batch 500 Loss 1.1369 Accuracy 0.7274\n",
            "Epoch 52 Batch 550 Loss 1.1391 Accuracy 0.7269\n",
            "Epoch 52 Batch 600 Loss 1.1401 Accuracy 0.7265\n",
            "Epoch 52 Batch 650 Loss 1.1397 Accuracy 0.7269\n",
            "Epoch 52 Batch 700 Loss 1.1389 Accuracy 0.7271\n",
            "Epoch 52 Batch 750 Loss 1.1381 Accuracy 0.7272\n",
            "Epoch 52 Batch 800 Loss 1.1372 Accuracy 0.7277\n",
            "Epoch 52 Batch 850 Loss 1.1352 Accuracy 0.7278\n",
            "Epoch 52 Batch 900 Loss 1.1323 Accuracy 0.7283\n",
            "Epoch 52 Batch 950 Loss 1.1307 Accuracy 0.7286\n",
            "discarded batch 953\n",
            "Epoch 52 Loss 1.1306 Accuracy 0.7286\n",
            "Time taken for 1 epoch: 67.41276454925537 secs\n",
            "\n",
            "Epoch 53 Batch 0 Loss 1.0249 Accuracy 0.7492\n",
            "Epoch 53 Batch 50 Loss 1.0918 Accuracy 0.7339\n",
            "Epoch 53 Batch 100 Loss 1.1055 Accuracy 0.7356\n",
            "Epoch 53 Batch 150 Loss 1.1195 Accuracy 0.7321\n",
            "Epoch 53 Batch 200 Loss 1.1263 Accuracy 0.7315\n",
            "Epoch 53 Batch 250 Loss 1.1329 Accuracy 0.7298\n",
            "Epoch 53 Batch 300 Loss 1.1386 Accuracy 0.7283\n",
            "Epoch 53 Batch 350 Loss 1.1380 Accuracy 0.7282\n",
            "Epoch 53 Batch 400 Loss 1.1393 Accuracy 0.7278\n",
            "Epoch 53 Batch 450 Loss 1.1379 Accuracy 0.7278\n",
            "Epoch 53 Batch 500 Loss 1.1381 Accuracy 0.7275\n",
            "Epoch 53 Batch 550 Loss 1.1396 Accuracy 0.7270\n",
            "Epoch 53 Batch 600 Loss 1.1402 Accuracy 0.7270\n",
            "Epoch 53 Batch 650 Loss 1.1391 Accuracy 0.7272\n",
            "Epoch 53 Batch 700 Loss 1.1390 Accuracy 0.7273\n",
            "Epoch 53 Batch 750 Loss 1.1382 Accuracy 0.7274\n",
            "Epoch 53 Batch 800 Loss 1.1379 Accuracy 0.7276\n",
            "Epoch 53 Batch 850 Loss 1.1358 Accuracy 0.7277\n",
            "Epoch 53 Batch 900 Loss 1.1336 Accuracy 0.7280\n",
            "Epoch 53 Batch 950 Loss 1.1318 Accuracy 0.7285\n",
            "discarded batch 953\n",
            "Epoch 53 Loss 1.1318 Accuracy 0.7285\n",
            "Time taken for 1 epoch: 67.84126353263855 secs\n",
            "\n",
            "Epoch 54 Batch 0 Loss 1.0496 Accuracy 0.7360\n",
            "Epoch 54 Batch 50 Loss 1.0957 Accuracy 0.7332\n",
            "Epoch 54 Batch 100 Loss 1.0979 Accuracy 0.7357\n",
            "Epoch 54 Batch 150 Loss 1.1132 Accuracy 0.7330\n",
            "Epoch 54 Batch 200 Loss 1.1178 Accuracy 0.7333\n",
            "Epoch 54 Batch 250 Loss 1.1244 Accuracy 0.7313\n",
            "Epoch 54 Batch 300 Loss 1.1291 Accuracy 0.7300\n",
            "Epoch 54 Batch 350 Loss 1.1303 Accuracy 0.7298\n",
            "Epoch 54 Batch 400 Loss 1.1325 Accuracy 0.7293\n",
            "Epoch 54 Batch 450 Loss 1.1308 Accuracy 0.7296\n",
            "Epoch 54 Batch 500 Loss 1.1319 Accuracy 0.7291\n",
            "Epoch 54 Batch 550 Loss 1.1350 Accuracy 0.7282\n",
            "Epoch 54 Batch 600 Loss 1.1342 Accuracy 0.7284\n",
            "Epoch 54 Batch 650 Loss 1.1338 Accuracy 0.7283\n",
            "Epoch 54 Batch 700 Loss 1.1346 Accuracy 0.7282\n",
            "Epoch 54 Batch 750 Loss 1.1339 Accuracy 0.7283\n",
            "Epoch 54 Batch 800 Loss 1.1328 Accuracy 0.7286\n",
            "Epoch 54 Batch 850 Loss 1.1303 Accuracy 0.7290\n",
            "Epoch 54 Batch 900 Loss 1.1280 Accuracy 0.7294\n",
            "Epoch 54 Batch 950 Loss 1.1260 Accuracy 0.7298\n",
            "discarded batch 953\n",
            "Epoch 54 Loss 1.1259 Accuracy 0.7298\n",
            "Time taken for 1 epoch: 67.47560596466064 secs\n",
            "\n",
            "Epoch 55 Batch 0 Loss 1.0897 Accuracy 0.7525\n",
            "Epoch 55 Batch 50 Loss 1.0986 Accuracy 0.7329\n",
            "Epoch 55 Batch 100 Loss 1.1028 Accuracy 0.7347\n",
            "Epoch 55 Batch 150 Loss 1.1105 Accuracy 0.7335\n",
            "Epoch 55 Batch 200 Loss 1.1140 Accuracy 0.7326\n",
            "Epoch 55 Batch 250 Loss 1.1169 Accuracy 0.7325\n",
            "Epoch 55 Batch 300 Loss 1.1218 Accuracy 0.7314\n",
            "Epoch 55 Batch 350 Loss 1.1243 Accuracy 0.7306\n",
            "Epoch 55 Batch 400 Loss 1.1284 Accuracy 0.7294\n",
            "Epoch 55 Batch 450 Loss 1.1279 Accuracy 0.7294\n",
            "Epoch 55 Batch 500 Loss 1.1294 Accuracy 0.7289\n",
            "Epoch 55 Batch 550 Loss 1.1317 Accuracy 0.7283\n",
            "Epoch 55 Batch 600 Loss 1.1325 Accuracy 0.7282\n",
            "Epoch 55 Batch 650 Loss 1.1323 Accuracy 0.7282\n",
            "Epoch 55 Batch 700 Loss 1.1321 Accuracy 0.7283\n",
            "Epoch 55 Batch 750 Loss 1.1317 Accuracy 0.7284\n",
            "Epoch 55 Batch 800 Loss 1.1306 Accuracy 0.7285\n",
            "Epoch 55 Batch 850 Loss 1.1287 Accuracy 0.7286\n",
            "Epoch 55 Batch 900 Loss 1.1265 Accuracy 0.7290\n",
            "Epoch 55 Batch 950 Loss 1.1249 Accuracy 0.7293\n",
            "discarded batch 953\n",
            "Saving checkpoint for epoch 55 at ./checkpoints/train/ckpt-33\n",
            "Epoch 55 Loss 1.1248 Accuracy 0.7293\n",
            "Time taken for 1 epoch: 68.23143100738525 secs\n",
            "\n",
            "Epoch 56 Batch 0 Loss 1.0496 Accuracy 0.7393\n",
            "Epoch 56 Batch 50 Loss 1.0899 Accuracy 0.7389\n",
            "Epoch 56 Batch 100 Loss 1.0975 Accuracy 0.7356\n",
            "Epoch 56 Batch 150 Loss 1.1064 Accuracy 0.7345\n",
            "Epoch 56 Batch 200 Loss 1.1098 Accuracy 0.7342\n",
            "Epoch 56 Batch 250 Loss 1.1148 Accuracy 0.7334\n",
            "Epoch 56 Batch 300 Loss 1.1210 Accuracy 0.7321\n",
            "Epoch 56 Batch 350 Loss 1.1224 Accuracy 0.7315\n",
            "Epoch 56 Batch 400 Loss 1.1259 Accuracy 0.7312\n",
            "Epoch 56 Batch 450 Loss 1.1255 Accuracy 0.7312\n",
            "Epoch 56 Batch 500 Loss 1.1257 Accuracy 0.7311\n",
            "Epoch 56 Batch 550 Loss 1.1289 Accuracy 0.7305\n",
            "Epoch 56 Batch 600 Loss 1.1296 Accuracy 0.7301\n",
            "Epoch 56 Batch 650 Loss 1.1287 Accuracy 0.7303\n",
            "Epoch 56 Batch 700 Loss 1.1276 Accuracy 0.7303\n",
            "Epoch 56 Batch 750 Loss 1.1268 Accuracy 0.7304\n",
            "Epoch 56 Batch 800 Loss 1.1259 Accuracy 0.7306\n",
            "Epoch 56 Batch 850 Loss 1.1246 Accuracy 0.7308\n",
            "Epoch 56 Batch 900 Loss 1.1229 Accuracy 0.7310\n",
            "Epoch 56 Batch 950 Loss 1.1210 Accuracy 0.7313\n",
            "discarded batch 953\n",
            "Epoch 56 Loss 1.1208 Accuracy 0.7314\n",
            "Time taken for 1 epoch: 68.78260660171509 secs\n",
            "\n",
            "Epoch 57 Batch 0 Loss 1.0257 Accuracy 0.7492\n",
            "Epoch 57 Batch 50 Loss 1.0939 Accuracy 0.7320\n",
            "Epoch 57 Batch 100 Loss 1.0949 Accuracy 0.7345\n",
            "Epoch 57 Batch 150 Loss 1.1073 Accuracy 0.7325\n",
            "Epoch 57 Batch 200 Loss 1.1158 Accuracy 0.7314\n",
            "Epoch 57 Batch 250 Loss 1.1191 Accuracy 0.7308\n",
            "Epoch 57 Batch 300 Loss 1.1254 Accuracy 0.7300\n",
            "Epoch 57 Batch 350 Loss 1.1245 Accuracy 0.7305\n",
            "Epoch 57 Batch 400 Loss 1.1276 Accuracy 0.7300\n",
            "Epoch 57 Batch 450 Loss 1.1248 Accuracy 0.7303\n",
            "Epoch 57 Batch 500 Loss 1.1261 Accuracy 0.7298\n",
            "Epoch 57 Batch 550 Loss 1.1277 Accuracy 0.7295\n",
            "Epoch 57 Batch 600 Loss 1.1285 Accuracy 0.7290\n",
            "Epoch 57 Batch 650 Loss 1.1280 Accuracy 0.7292\n",
            "Epoch 57 Batch 700 Loss 1.1279 Accuracy 0.7291\n",
            "Epoch 57 Batch 750 Loss 1.1276 Accuracy 0.7290\n",
            "Epoch 57 Batch 800 Loss 1.1266 Accuracy 0.7293\n",
            "Epoch 57 Batch 850 Loss 1.1243 Accuracy 0.7297\n",
            "Epoch 57 Batch 900 Loss 1.1226 Accuracy 0.7300\n",
            "Epoch 57 Batch 950 Loss 1.1209 Accuracy 0.7302\n",
            "discarded batch 953\n",
            "Epoch 57 Loss 1.1208 Accuracy 0.7303\n",
            "Time taken for 1 epoch: 66.11418914794922 secs\n",
            "\n",
            "Epoch 58 Batch 0 Loss 1.0575 Accuracy 0.7624\n",
            "Epoch 58 Batch 50 Loss 1.0915 Accuracy 0.7335\n",
            "Epoch 58 Batch 100 Loss 1.0939 Accuracy 0.7356\n",
            "Epoch 58 Batch 150 Loss 1.1080 Accuracy 0.7344\n",
            "Epoch 58 Batch 200 Loss 1.1131 Accuracy 0.7343\n",
            "Epoch 58 Batch 250 Loss 1.1189 Accuracy 0.7330\n",
            "Epoch 58 Batch 300 Loss 1.1229 Accuracy 0.7313\n",
            "Epoch 58 Batch 350 Loss 1.1240 Accuracy 0.7310\n",
            "Epoch 58 Batch 400 Loss 1.1270 Accuracy 0.7301\n",
            "Epoch 58 Batch 450 Loss 1.1248 Accuracy 0.7307\n",
            "Epoch 58 Batch 500 Loss 1.1255 Accuracy 0.7305\n",
            "Epoch 58 Batch 550 Loss 1.1277 Accuracy 0.7299\n",
            "Epoch 58 Batch 600 Loss 1.1270 Accuracy 0.7298\n",
            "Epoch 58 Batch 650 Loss 1.1262 Accuracy 0.7297\n",
            "Epoch 58 Batch 700 Loss 1.1255 Accuracy 0.7297\n",
            "Epoch 58 Batch 750 Loss 1.1247 Accuracy 0.7297\n",
            "Epoch 58 Batch 800 Loss 1.1238 Accuracy 0.7299\n",
            "Epoch 58 Batch 850 Loss 1.1218 Accuracy 0.7303\n",
            "Epoch 58 Batch 900 Loss 1.1200 Accuracy 0.7305\n",
            "Epoch 58 Batch 950 Loss 1.1185 Accuracy 0.7307\n",
            "discarded batch 953\n",
            "Epoch 58 Loss 1.1184 Accuracy 0.7308\n",
            "Time taken for 1 epoch: 66.42658400535583 secs\n",
            "\n",
            "Epoch 59 Batch 0 Loss 0.9305 Accuracy 0.7756\n",
            "Epoch 59 Batch 50 Loss 1.0848 Accuracy 0.7349\n",
            "Epoch 59 Batch 100 Loss 1.0936 Accuracy 0.7339\n",
            "Epoch 59 Batch 150 Loss 1.1044 Accuracy 0.7327\n",
            "Epoch 59 Batch 200 Loss 1.1098 Accuracy 0.7323\n",
            "Epoch 59 Batch 250 Loss 1.1146 Accuracy 0.7315\n",
            "Epoch 59 Batch 300 Loss 1.1183 Accuracy 0.7308\n",
            "Epoch 59 Batch 350 Loss 1.1196 Accuracy 0.7307\n",
            "Epoch 59 Batch 400 Loss 1.1220 Accuracy 0.7307\n",
            "Epoch 59 Batch 450 Loss 1.1197 Accuracy 0.7305\n",
            "Epoch 59 Batch 500 Loss 1.1212 Accuracy 0.7299\n",
            "Epoch 59 Batch 550 Loss 1.1240 Accuracy 0.7294\n",
            "Epoch 59 Batch 600 Loss 1.1250 Accuracy 0.7291\n",
            "Epoch 59 Batch 650 Loss 1.1246 Accuracy 0.7292\n",
            "Epoch 59 Batch 700 Loss 1.1240 Accuracy 0.7293\n",
            "Epoch 59 Batch 750 Loss 1.1236 Accuracy 0.7290\n",
            "Epoch 59 Batch 800 Loss 1.1228 Accuracy 0.7293\n",
            "Epoch 59 Batch 850 Loss 1.1211 Accuracy 0.7295\n",
            "Epoch 59 Batch 900 Loss 1.1187 Accuracy 0.7298\n",
            "Epoch 59 Batch 950 Loss 1.1172 Accuracy 0.7301\n",
            "discarded batch 953\n",
            "Epoch 59 Loss 1.1171 Accuracy 0.7301\n",
            "Time taken for 1 epoch: 67.8541111946106 secs\n",
            "\n",
            "Epoch 60 Batch 0 Loss 1.0313 Accuracy 0.7558\n",
            "Epoch 60 Batch 50 Loss 1.0791 Accuracy 0.7390\n",
            "Epoch 60 Batch 100 Loss 1.0891 Accuracy 0.7392\n",
            "Epoch 60 Batch 150 Loss 1.0991 Accuracy 0.7358\n",
            "Epoch 60 Batch 200 Loss 1.1064 Accuracy 0.7353\n",
            "Epoch 60 Batch 250 Loss 1.1110 Accuracy 0.7348\n",
            "Epoch 60 Batch 300 Loss 1.1150 Accuracy 0.7335\n",
            "Epoch 60 Batch 350 Loss 1.1158 Accuracy 0.7334\n",
            "Epoch 60 Batch 400 Loss 1.1198 Accuracy 0.7325\n",
            "Epoch 60 Batch 450 Loss 1.1186 Accuracy 0.7325\n",
            "Epoch 60 Batch 500 Loss 1.1178 Accuracy 0.7323\n",
            "Epoch 60 Batch 550 Loss 1.1186 Accuracy 0.7321\n",
            "Epoch 60 Batch 600 Loss 1.1199 Accuracy 0.7317\n",
            "Epoch 60 Batch 650 Loss 1.1197 Accuracy 0.7317\n",
            "Epoch 60 Batch 700 Loss 1.1183 Accuracy 0.7315\n",
            "Epoch 60 Batch 750 Loss 1.1174 Accuracy 0.7315\n",
            "Epoch 60 Batch 800 Loss 1.1172 Accuracy 0.7317\n",
            "Epoch 60 Batch 850 Loss 1.1156 Accuracy 0.7320\n",
            "Epoch 60 Batch 900 Loss 1.1132 Accuracy 0.7325\n",
            "Epoch 60 Batch 950 Loss 1.1117 Accuracy 0.7325\n",
            "discarded batch 953\n",
            "Saving checkpoint for epoch 60 at ./checkpoints/train/ckpt-34\n",
            "Epoch 60 Loss 1.1116 Accuracy 0.7325\n",
            "Time taken for 1 epoch: 66.28704237937927 secs\n",
            "\n",
            "Epoch 61 Batch 0 Loss 1.0319 Accuracy 0.7492\n",
            "Epoch 61 Batch 50 Loss 1.0935 Accuracy 0.7351\n",
            "Epoch 61 Batch 100 Loss 1.1002 Accuracy 0.7345\n",
            "Epoch 61 Batch 150 Loss 1.1071 Accuracy 0.7339\n",
            "Epoch 61 Batch 200 Loss 1.1109 Accuracy 0.7336\n",
            "Epoch 61 Batch 250 Loss 1.1149 Accuracy 0.7331\n",
            "Epoch 61 Batch 300 Loss 1.1184 Accuracy 0.7322\n",
            "Epoch 61 Batch 350 Loss 1.1200 Accuracy 0.7319\n",
            "Epoch 61 Batch 400 Loss 1.1236 Accuracy 0.7312\n",
            "Epoch 61 Batch 450 Loss 1.1215 Accuracy 0.7314\n",
            "Epoch 61 Batch 500 Loss 1.1217 Accuracy 0.7313\n",
            "Epoch 61 Batch 550 Loss 1.1238 Accuracy 0.7307\n",
            "Epoch 61 Batch 600 Loss 1.1226 Accuracy 0.7307\n",
            "Epoch 61 Batch 650 Loss 1.1232 Accuracy 0.7306\n",
            "Epoch 61 Batch 700 Loss 1.1226 Accuracy 0.7305\n",
            "Epoch 61 Batch 750 Loss 1.1219 Accuracy 0.7307\n",
            "Epoch 61 Batch 800 Loss 1.1212 Accuracy 0.7309\n",
            "Epoch 61 Batch 850 Loss 1.1195 Accuracy 0.7311\n",
            "Epoch 61 Batch 900 Loss 1.1174 Accuracy 0.7315\n",
            "Epoch 61 Batch 950 Loss 1.1152 Accuracy 0.7317\n",
            "discarded batch 953\n",
            "Epoch 61 Loss 1.1151 Accuracy 0.7317\n",
            "Time taken for 1 epoch: 65.50242614746094 secs\n",
            "\n",
            "Epoch 62 Batch 0 Loss 1.0271 Accuracy 0.7492\n",
            "Epoch 62 Batch 50 Loss 1.0763 Accuracy 0.7390\n",
            "Epoch 62 Batch 100 Loss 1.0855 Accuracy 0.7369\n",
            "Epoch 62 Batch 150 Loss 1.0978 Accuracy 0.7349\n",
            "Epoch 62 Batch 200 Loss 1.1046 Accuracy 0.7346\n",
            "Epoch 62 Batch 250 Loss 1.1110 Accuracy 0.7336\n",
            "Epoch 62 Batch 300 Loss 1.1149 Accuracy 0.7328\n",
            "Epoch 62 Batch 350 Loss 1.1154 Accuracy 0.7327\n",
            "Epoch 62 Batch 400 Loss 1.1193 Accuracy 0.7320\n",
            "Epoch 62 Batch 450 Loss 1.1175 Accuracy 0.7320\n",
            "Epoch 62 Batch 500 Loss 1.1176 Accuracy 0.7319\n",
            "Epoch 62 Batch 550 Loss 1.1199 Accuracy 0.7311\n",
            "Epoch 62 Batch 600 Loss 1.1196 Accuracy 0.7309\n",
            "Epoch 62 Batch 650 Loss 1.1193 Accuracy 0.7311\n",
            "Epoch 62 Batch 700 Loss 1.1186 Accuracy 0.7311\n",
            "Epoch 62 Batch 750 Loss 1.1178 Accuracy 0.7312\n",
            "Epoch 62 Batch 800 Loss 1.1164 Accuracy 0.7315\n",
            "Epoch 62 Batch 850 Loss 1.1153 Accuracy 0.7314\n",
            "Epoch 62 Batch 900 Loss 1.1125 Accuracy 0.7319\n",
            "Epoch 62 Batch 950 Loss 1.1108 Accuracy 0.7321\n",
            "discarded batch 953\n",
            "Epoch 62 Loss 1.1108 Accuracy 0.7321\n",
            "Time taken for 1 epoch: 65.36645245552063 secs\n",
            "\n",
            "Epoch 63 Batch 0 Loss 1.0097 Accuracy 0.7525\n",
            "Epoch 63 Batch 50 Loss 1.0830 Accuracy 0.7391\n",
            "Epoch 63 Batch 100 Loss 1.0895 Accuracy 0.7377\n",
            "Epoch 63 Batch 150 Loss 1.0991 Accuracy 0.7352\n",
            "Epoch 63 Batch 200 Loss 1.1048 Accuracy 0.7348\n",
            "Epoch 63 Batch 250 Loss 1.1101 Accuracy 0.7330\n",
            "Epoch 63 Batch 300 Loss 1.1143 Accuracy 0.7319\n",
            "Epoch 63 Batch 350 Loss 1.1136 Accuracy 0.7320\n",
            "Epoch 63 Batch 400 Loss 1.1168 Accuracy 0.7316\n",
            "Epoch 63 Batch 450 Loss 1.1163 Accuracy 0.7313\n",
            "Epoch 63 Batch 500 Loss 1.1184 Accuracy 0.7306\n",
            "Epoch 63 Batch 550 Loss 1.1195 Accuracy 0.7304\n",
            "Epoch 63 Batch 600 Loss 1.1192 Accuracy 0.7304\n",
            "Epoch 63 Batch 650 Loss 1.1184 Accuracy 0.7307\n",
            "Epoch 63 Batch 700 Loss 1.1175 Accuracy 0.7307\n",
            "Epoch 63 Batch 750 Loss 1.1171 Accuracy 0.7308\n",
            "Epoch 63 Batch 800 Loss 1.1161 Accuracy 0.7310\n",
            "Epoch 63 Batch 850 Loss 1.1143 Accuracy 0.7312\n",
            "Epoch 63 Batch 900 Loss 1.1114 Accuracy 0.7316\n",
            "Epoch 63 Batch 950 Loss 1.1097 Accuracy 0.7321\n",
            "discarded batch 953\n",
            "Epoch 63 Loss 1.1096 Accuracy 0.7321\n",
            "Time taken for 1 epoch: 65.17174339294434 secs\n",
            "\n",
            "Epoch 64 Batch 0 Loss 0.9114 Accuracy 0.7822\n",
            "Epoch 64 Batch 50 Loss 1.0778 Accuracy 0.7361\n",
            "Epoch 64 Batch 100 Loss 1.0838 Accuracy 0.7390\n",
            "Epoch 64 Batch 150 Loss 1.0961 Accuracy 0.7369\n",
            "Epoch 64 Batch 200 Loss 1.1021 Accuracy 0.7360\n",
            "Epoch 64 Batch 250 Loss 1.1078 Accuracy 0.7347\n",
            "Epoch 64 Batch 300 Loss 1.1099 Accuracy 0.7340\n",
            "Epoch 64 Batch 350 Loss 1.1096 Accuracy 0.7340\n",
            "Epoch 64 Batch 400 Loss 1.1137 Accuracy 0.7326\n",
            "Epoch 64 Batch 450 Loss 1.1114 Accuracy 0.7330\n",
            "Epoch 64 Batch 500 Loss 1.1119 Accuracy 0.7326\n",
            "Epoch 64 Batch 550 Loss 1.1145 Accuracy 0.7317\n",
            "Epoch 64 Batch 600 Loss 1.1143 Accuracy 0.7317\n",
            "Epoch 64 Batch 650 Loss 1.1131 Accuracy 0.7320\n",
            "Epoch 64 Batch 700 Loss 1.1132 Accuracy 0.7320\n",
            "Epoch 64 Batch 750 Loss 1.1129 Accuracy 0.7319\n",
            "Epoch 64 Batch 800 Loss 1.1121 Accuracy 0.7323\n",
            "Epoch 64 Batch 850 Loss 1.1106 Accuracy 0.7325\n",
            "Epoch 64 Batch 900 Loss 1.1082 Accuracy 0.7330\n",
            "Epoch 64 Batch 950 Loss 1.1059 Accuracy 0.7333\n",
            "discarded batch 953\n",
            "Epoch 64 Loss 1.1060 Accuracy 0.7333\n",
            "Time taken for 1 epoch: 66.60519242286682 secs\n",
            "\n",
            "Epoch 65 Batch 0 Loss 1.0235 Accuracy 0.7492\n",
            "Epoch 65 Batch 50 Loss 1.0834 Accuracy 0.7371\n",
            "Epoch 65 Batch 100 Loss 1.0878 Accuracy 0.7382\n",
            "Epoch 65 Batch 150 Loss 1.0953 Accuracy 0.7365\n",
            "Epoch 65 Batch 200 Loss 1.0986 Accuracy 0.7367\n",
            "Epoch 65 Batch 250 Loss 1.1037 Accuracy 0.7351\n",
            "Epoch 65 Batch 300 Loss 1.1084 Accuracy 0.7336\n",
            "Epoch 65 Batch 350 Loss 1.1101 Accuracy 0.7333\n",
            "Epoch 65 Batch 400 Loss 1.1133 Accuracy 0.7330\n",
            "Epoch 65 Batch 450 Loss 1.1107 Accuracy 0.7328\n",
            "Epoch 65 Batch 500 Loss 1.1117 Accuracy 0.7325\n",
            "Epoch 65 Batch 550 Loss 1.1134 Accuracy 0.7318\n",
            "Epoch 65 Batch 600 Loss 1.1137 Accuracy 0.7316\n",
            "Epoch 65 Batch 650 Loss 1.1121 Accuracy 0.7318\n",
            "Epoch 65 Batch 700 Loss 1.1119 Accuracy 0.7321\n",
            "Epoch 65 Batch 750 Loss 1.1117 Accuracy 0.7320\n",
            "Epoch 65 Batch 800 Loss 1.1101 Accuracy 0.7324\n",
            "Epoch 65 Batch 850 Loss 1.1078 Accuracy 0.7326\n",
            "Epoch 65 Batch 900 Loss 1.1057 Accuracy 0.7330\n",
            "Epoch 65 Batch 950 Loss 1.1038 Accuracy 0.7335\n",
            "discarded batch 953\n",
            "Saving checkpoint for epoch 65 at ./checkpoints/train/ckpt-35\n",
            "Epoch 65 Loss 1.1038 Accuracy 0.7335\n",
            "Time taken for 1 epoch: 67.17946815490723 secs\n",
            "\n",
            "Epoch 66 Batch 0 Loss 1.0766 Accuracy 0.7195\n",
            "Epoch 66 Batch 50 Loss 1.0905 Accuracy 0.7309\n",
            "Epoch 66 Batch 100 Loss 1.0917 Accuracy 0.7346\n",
            "Epoch 66 Batch 150 Loss 1.1027 Accuracy 0.7334\n",
            "Epoch 66 Batch 200 Loss 1.1073 Accuracy 0.7336\n",
            "Epoch 66 Batch 250 Loss 1.1072 Accuracy 0.7336\n",
            "Epoch 66 Batch 300 Loss 1.1126 Accuracy 0.7326\n",
            "Epoch 66 Batch 350 Loss 1.1107 Accuracy 0.7335\n",
            "Epoch 66 Batch 400 Loss 1.1138 Accuracy 0.7326\n",
            "Epoch 66 Batch 450 Loss 1.1107 Accuracy 0.7329\n",
            "Epoch 66 Batch 500 Loss 1.1120 Accuracy 0.7324\n",
            "Epoch 66 Batch 550 Loss 1.1142 Accuracy 0.7318\n",
            "Epoch 66 Batch 600 Loss 1.1149 Accuracy 0.7316\n",
            "Epoch 66 Batch 650 Loss 1.1133 Accuracy 0.7317\n",
            "Epoch 66 Batch 700 Loss 1.1137 Accuracy 0.7316\n",
            "Epoch 66 Batch 750 Loss 1.1131 Accuracy 0.7318\n",
            "Epoch 66 Batch 800 Loss 1.1111 Accuracy 0.7322\n",
            "Epoch 66 Batch 850 Loss 1.1097 Accuracy 0.7323\n",
            "Epoch 66 Batch 900 Loss 1.1068 Accuracy 0.7329\n",
            "Epoch 66 Batch 950 Loss 1.1048 Accuracy 0.7334\n",
            "discarded batch 953\n",
            "Epoch 66 Loss 1.1048 Accuracy 0.7333\n",
            "Time taken for 1 epoch: 66.50821495056152 secs\n",
            "\n",
            "Epoch 67 Batch 0 Loss 0.9393 Accuracy 0.7789\n",
            "Epoch 67 Batch 50 Loss 1.0723 Accuracy 0.7377\n",
            "Epoch 67 Batch 100 Loss 1.0802 Accuracy 0.7377\n",
            "Epoch 67 Batch 150 Loss 1.0939 Accuracy 0.7353\n",
            "Epoch 67 Batch 200 Loss 1.0977 Accuracy 0.7352\n",
            "Epoch 67 Batch 250 Loss 1.1006 Accuracy 0.7353\n",
            "Epoch 67 Batch 300 Loss 1.1047 Accuracy 0.7343\n",
            "Epoch 67 Batch 350 Loss 1.1053 Accuracy 0.7340\n",
            "Epoch 67 Batch 400 Loss 1.1085 Accuracy 0.7330\n",
            "Epoch 67 Batch 450 Loss 1.1075 Accuracy 0.7331\n",
            "Epoch 67 Batch 500 Loss 1.1090 Accuracy 0.7326\n",
            "Epoch 67 Batch 550 Loss 1.1107 Accuracy 0.7326\n",
            "Epoch 67 Batch 600 Loss 1.1119 Accuracy 0.7323\n",
            "Epoch 67 Batch 650 Loss 1.1110 Accuracy 0.7323\n",
            "Epoch 67 Batch 700 Loss 1.1102 Accuracy 0.7324\n",
            "Epoch 67 Batch 750 Loss 1.1091 Accuracy 0.7325\n",
            "Epoch 67 Batch 800 Loss 1.1071 Accuracy 0.7330\n",
            "Epoch 67 Batch 850 Loss 1.1049 Accuracy 0.7335\n",
            "Epoch 67 Batch 900 Loss 1.1033 Accuracy 0.7338\n",
            "Epoch 67 Batch 950 Loss 1.1013 Accuracy 0.7342\n",
            "discarded batch 953\n",
            "Epoch 67 Loss 1.1012 Accuracy 0.7342\n",
            "Time taken for 1 epoch: 66.22920608520508 secs\n",
            "\n",
            "Epoch 68 Batch 0 Loss 0.9451 Accuracy 0.7525\n",
            "Epoch 68 Batch 50 Loss 1.0655 Accuracy 0.7431\n",
            "Epoch 68 Batch 100 Loss 1.0772 Accuracy 0.7397\n",
            "Epoch 68 Batch 150 Loss 1.0881 Accuracy 0.7382\n",
            "Epoch 68 Batch 200 Loss 1.0956 Accuracy 0.7365\n",
            "Epoch 68 Batch 250 Loss 1.1023 Accuracy 0.7349\n",
            "Epoch 68 Batch 300 Loss 1.1049 Accuracy 0.7340\n",
            "Epoch 68 Batch 350 Loss 1.1061 Accuracy 0.7337\n",
            "Epoch 68 Batch 400 Loss 1.1104 Accuracy 0.7329\n",
            "Epoch 68 Batch 450 Loss 1.1091 Accuracy 0.7329\n",
            "Epoch 68 Batch 500 Loss 1.1092 Accuracy 0.7325\n",
            "Epoch 68 Batch 550 Loss 1.1106 Accuracy 0.7320\n",
            "Epoch 68 Batch 600 Loss 1.1109 Accuracy 0.7319\n",
            "Epoch 68 Batch 650 Loss 1.1094 Accuracy 0.7322\n",
            "Epoch 68 Batch 700 Loss 1.1082 Accuracy 0.7325\n",
            "Epoch 68 Batch 750 Loss 1.1072 Accuracy 0.7327\n",
            "Epoch 68 Batch 800 Loss 1.1056 Accuracy 0.7331\n",
            "Epoch 68 Batch 850 Loss 1.1042 Accuracy 0.7333\n",
            "Epoch 68 Batch 900 Loss 1.1014 Accuracy 0.7337\n",
            "Epoch 68 Batch 950 Loss 1.1000 Accuracy 0.7339\n",
            "discarded batch 953\n",
            "Epoch 68 Loss 1.1000 Accuracy 0.7339\n",
            "Time taken for 1 epoch: 65.95380353927612 secs\n",
            "\n",
            "Epoch 69 Batch 0 Loss 1.0217 Accuracy 0.7591\n",
            "Epoch 69 Batch 50 Loss 1.0785 Accuracy 0.7371\n",
            "Epoch 69 Batch 100 Loss 1.0849 Accuracy 0.7373\n",
            "Epoch 69 Batch 150 Loss 1.0935 Accuracy 0.7362\n",
            "Epoch 69 Batch 200 Loss 1.0978 Accuracy 0.7357\n",
            "Epoch 69 Batch 250 Loss 1.1018 Accuracy 0.7351\n",
            "Epoch 69 Batch 300 Loss 1.1062 Accuracy 0.7340\n",
            "Epoch 69 Batch 350 Loss 1.1065 Accuracy 0.7338\n",
            "Epoch 69 Batch 400 Loss 1.1098 Accuracy 0.7335\n",
            "Epoch 69 Batch 450 Loss 1.1097 Accuracy 0.7331\n",
            "Epoch 69 Batch 500 Loss 1.1108 Accuracy 0.7328\n",
            "Epoch 69 Batch 550 Loss 1.1136 Accuracy 0.7320\n",
            "Epoch 69 Batch 600 Loss 1.1136 Accuracy 0.7317\n",
            "Epoch 69 Batch 650 Loss 1.1121 Accuracy 0.7320\n",
            "Epoch 69 Batch 700 Loss 1.1114 Accuracy 0.7322\n",
            "Epoch 69 Batch 750 Loss 1.1110 Accuracy 0.7324\n",
            "Epoch 69 Batch 800 Loss 1.1098 Accuracy 0.7325\n",
            "Epoch 69 Batch 850 Loss 1.1075 Accuracy 0.7329\n",
            "Epoch 69 Batch 900 Loss 1.1054 Accuracy 0.7334\n",
            "Epoch 69 Batch 950 Loss 1.1032 Accuracy 0.7337\n",
            "discarded batch 953\n",
            "Epoch 69 Loss 1.1032 Accuracy 0.7337\n",
            "Time taken for 1 epoch: 66.63678884506226 secs\n",
            "\n",
            "Epoch 70 Batch 0 Loss 0.9455 Accuracy 0.7690\n",
            "Epoch 70 Batch 50 Loss 1.0633 Accuracy 0.7392\n",
            "Epoch 70 Batch 100 Loss 1.0692 Accuracy 0.7400\n",
            "Epoch 70 Batch 150 Loss 1.0832 Accuracy 0.7369\n",
            "Epoch 70 Batch 200 Loss 1.0878 Accuracy 0.7371\n",
            "Epoch 70 Batch 250 Loss 1.0923 Accuracy 0.7365\n",
            "Epoch 70 Batch 300 Loss 1.0974 Accuracy 0.7351\n",
            "Epoch 70 Batch 350 Loss 1.0986 Accuracy 0.7351\n",
            "Epoch 70 Batch 400 Loss 1.1032 Accuracy 0.7342\n",
            "Epoch 70 Batch 450 Loss 1.1026 Accuracy 0.7340\n",
            "Epoch 70 Batch 500 Loss 1.1040 Accuracy 0.7334\n",
            "Epoch 70 Batch 550 Loss 1.1059 Accuracy 0.7329\n",
            "Epoch 70 Batch 600 Loss 1.1061 Accuracy 0.7329\n",
            "Epoch 70 Batch 650 Loss 1.1063 Accuracy 0.7328\n",
            "Epoch 70 Batch 700 Loss 1.1054 Accuracy 0.7327\n",
            "Epoch 70 Batch 750 Loss 1.1044 Accuracy 0.7328\n",
            "Epoch 70 Batch 800 Loss 1.1031 Accuracy 0.7332\n",
            "Epoch 70 Batch 850 Loss 1.1014 Accuracy 0.7336\n",
            "Epoch 70 Batch 900 Loss 1.0989 Accuracy 0.7340\n",
            "Epoch 70 Batch 950 Loss 1.0972 Accuracy 0.7342\n",
            "discarded batch 953\n",
            "Saving checkpoint for epoch 70 at ./checkpoints/train/ckpt-36\n",
            "Epoch 70 Loss 1.0973 Accuracy 0.7342\n",
            "Time taken for 1 epoch: 65.73063659667969 secs\n",
            "\n",
            "Epoch 71 Batch 0 Loss 1.0153 Accuracy 0.6997\n",
            "Epoch 71 Batch 50 Loss 1.0676 Accuracy 0.7367\n",
            "Epoch 71 Batch 100 Loss 1.0760 Accuracy 0.7372\n",
            "Epoch 71 Batch 150 Loss 1.0841 Accuracy 0.7359\n",
            "Epoch 71 Batch 200 Loss 1.0884 Accuracy 0.7369\n",
            "Epoch 71 Batch 250 Loss 1.0923 Accuracy 0.7367\n",
            "Epoch 71 Batch 300 Loss 1.0955 Accuracy 0.7360\n",
            "Epoch 71 Batch 350 Loss 1.0967 Accuracy 0.7361\n",
            "Epoch 71 Batch 400 Loss 1.0999 Accuracy 0.7353\n",
            "Epoch 71 Batch 450 Loss 1.0993 Accuracy 0.7354\n",
            "Epoch 71 Batch 500 Loss 1.1005 Accuracy 0.7345\n",
            "Epoch 71 Batch 550 Loss 1.1027 Accuracy 0.7337\n",
            "Epoch 71 Batch 600 Loss 1.1032 Accuracy 0.7335\n",
            "Epoch 71 Batch 650 Loss 1.1021 Accuracy 0.7337\n",
            "Epoch 71 Batch 700 Loss 1.1018 Accuracy 0.7334\n",
            "Epoch 71 Batch 750 Loss 1.1014 Accuracy 0.7333\n",
            "Epoch 71 Batch 800 Loss 1.1002 Accuracy 0.7336\n",
            "Epoch 71 Batch 850 Loss 1.0982 Accuracy 0.7340\n",
            "Epoch 71 Batch 900 Loss 1.0955 Accuracy 0.7343\n",
            "Epoch 71 Batch 950 Loss 1.0941 Accuracy 0.7345\n",
            "discarded batch 953\n",
            "Epoch 71 Loss 1.0941 Accuracy 0.7345\n",
            "Time taken for 1 epoch: 65.84165954589844 secs\n",
            "\n",
            "Epoch 72 Batch 0 Loss 1.0485 Accuracy 0.7327\n",
            "Epoch 72 Batch 50 Loss 1.0583 Accuracy 0.7379\n",
            "Epoch 72 Batch 100 Loss 1.0696 Accuracy 0.7377\n",
            "Epoch 72 Batch 150 Loss 1.0836 Accuracy 0.7365\n",
            "Epoch 72 Batch 200 Loss 1.0877 Accuracy 0.7360\n",
            "Epoch 72 Batch 250 Loss 1.0932 Accuracy 0.7352\n",
            "Epoch 72 Batch 300 Loss 1.0959 Accuracy 0.7346\n",
            "Epoch 72 Batch 350 Loss 1.0968 Accuracy 0.7343\n",
            "Epoch 72 Batch 400 Loss 1.0997 Accuracy 0.7338\n",
            "Epoch 72 Batch 450 Loss 1.0991 Accuracy 0.7340\n",
            "Epoch 72 Batch 500 Loss 1.1008 Accuracy 0.7335\n",
            "Epoch 72 Batch 550 Loss 1.1020 Accuracy 0.7332\n",
            "Epoch 72 Batch 600 Loss 1.1026 Accuracy 0.7334\n",
            "Epoch 72 Batch 650 Loss 1.1018 Accuracy 0.7338\n",
            "Epoch 72 Batch 700 Loss 1.1007 Accuracy 0.7342\n",
            "Epoch 72 Batch 750 Loss 1.1007 Accuracy 0.7342\n",
            "Epoch 72 Batch 800 Loss 1.0991 Accuracy 0.7346\n",
            "Epoch 72 Batch 850 Loss 1.0978 Accuracy 0.7348\n",
            "Epoch 72 Batch 900 Loss 1.0955 Accuracy 0.7353\n",
            "Epoch 72 Batch 950 Loss 1.0937 Accuracy 0.7355\n",
            "discarded batch 953\n",
            "Epoch 72 Loss 1.0936 Accuracy 0.7355\n",
            "Time taken for 1 epoch: 65.59667158126831 secs\n",
            "\n",
            "Epoch 73 Batch 0 Loss 0.9997 Accuracy 0.7492\n",
            "Epoch 73 Batch 50 Loss 1.0745 Accuracy 0.7366\n",
            "Epoch 73 Batch 100 Loss 1.0774 Accuracy 0.7383\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-9b356e941f3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m            \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"discarded batch\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m            \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m        \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjLbfLOU_pRw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transformer.save_weights(\"./optimus_rhyme\")"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEun8YPR8Sn6",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3f92a495-d61e-477c-9a96-dd8c750c718d"
      },
      "source": [
        "#@title Generation\n",
        "\n",
        "def evaluate(inp_sentence, decoder_input):\n",
        "    inp_sentence = inp_sentence\n",
        "    encoder_input = tf.expand_dims(inp_sentence, 0)\n",
        "    \n",
        "    output = tf.expand_dims(decoder_input, 0)\n",
        "\n",
        "    terces = 0\n",
        "    for i in range(batch_len):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "            encoder_input, output)\n",
        "    \n",
        "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "        predictions, attention_weights = transformer(encoder_input, \n",
        "                                                    output,\n",
        "                                                    False,\n",
        "                                                    enc_padding_mask,\n",
        "                                                    combined_mask,\n",
        "                                                    dec_padding_mask)\n",
        "        \n",
        "        # select the last word from the seq_len dimension\n",
        "        predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "        \n",
        "        # return the result if the predicted_id is equal to the end token\n",
        "        if predicted_id == eos:\n",
        "            terces += 1\n",
        "            if terces == 4:\n",
        "                return tf.squeeze(output, axis=0), attention_weights\n",
        "        # concatentate the predicted_id to the output which is given to the decoder\n",
        "        # as its input.\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "    return tf.squeeze(output, axis=0), attention_weights\n",
        "\n",
        "out_list = batches[0][0]\n",
        "for i in range(10):\n",
        "    out, att_w = evaluate([pad], out_list[-75:])\n",
        "    out_list = out.numpy().tolist()\n",
        "    out_str = seq2str(out_list)\n",
        "    print(\"========= batch\", i) \n",
        "    print(out_str) "
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "========= batch 0\n",
            "io non so ben ridir com ' io v ' entrai\n",
            "tant ' era pieno di sonno a quel punto\n",
            "che la verace via abbandonai\n",
            "             \n",
            " ma non ti appresso e non vi ti disio\n",
            "non vide dette non vi disi riga\n",
            "che la testa in te misera intesato\n",
            "                \n",
            " ma dimmi se ' mar non mi riga\n",
            "li occhi suoi gravi mi riguarda in arti\n",
            "per te del viso che tu mi scega\n",
            "                 \n",
            " ma percostui ma di che tu ti doga\n",
            "se non ti può dentro a minòs te\n",
            "ché tu non ti far ne risce intenga\n",
            "                  \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "========= batch 1\n",
            "costui ma di che tu ti doga\n",
            "se non ti può dentro a minòs te\n",
            "ché tu non ti far ne risce intenga\n",
            "                  \n",
            "\n",
            "\n",
            "\n",
            " ed el mi   se ' che tu mi ti dimovi\n",
            "ché se ' l tuo vo m ' in questo passo\n",
            "sarà nel vostro in vesti mi fa pa pie\n",
            "      \n",
            " cosí disse ' l tuo vostro di sospeso\n",
            "fosse qui s ' avea per moto rimorti\n",
            "non ti move di veder lo in giro\n",
            "            \n",
            " e io a lui da questo in disio porti\n",
            "che tu mi dimanda che tu mi razio\n",
            "che tu mi fa diver che tu ti dolenti\n",
            "             \n",
            "\n",
            "\n",
            "corpi t porti porta mi t t ' ti pa ' ti ti ti ti son ti son ti ti son ricordiamovi t ' ' t ' so mi ti ti t t \n",
            "========= batch 2\n",
            "\n",
            "\n",
            "corpi t porti porta mi t t ' ti pa ' ti ti ti ti son ti son ti ti son ricordiamovi t ' ' t ' so mi ti ti t t hai \n",
            " va la mia ti ti sca mi piedi\n",
            "se non sarei mi di cosí che si rammenra\n",
            "ciò che tu mi fa da lei mi ciò mi ciò mi ?\n",
            "       \n",
            " poi che voi nel tuo tristi non si mira\n",
            "va sí come di sé sarà sorrita\n",
            "la se voi lo mio dura mi piera\n",
            "                  \n",
            " poi che se la mia nata che la morta\n",
            "per te che tu se voi tu non sarà lieta\n",
            "non sarà per te di ciò che non sarebta\n",
            "           \n",
            "\n",
            "\n",
            "\n",
            "========= batch 3\n",
            "che se la mia nata che la morta\n",
            "per te che tu se voi tu non sarà lieta\n",
            "non sarà per te di ciò che non sarebta\n",
            "           \n",
            "\n",
            "\n",
            " tu se ' l duca mio vostro in ciò non si diseta\n",
            "se ' l mio duca ' l mio duca a te rifiume divita\n",
            "               \n",
            " un a me di quel che tu non temena\n",
            "par ' l mio vole te te teco m ' inganni\n",
            "sen gine ' teminci e ' l mio segno a\n",
            "    \n",
            " poi si va l ' andare e ' l tuo rivini\n",
            "ma se ' tu non fosti se ' di voi mostra\n",
            "ma nel primo mi riguardando a lo ?\n",
            "    \n",
            "\n",
            "\n",
            "\n",
            "riguardi di di di di mi mi nell nell ' mi nel qual di qual te te te te te te te riguardando mi vi me mi sia si qual qual qual ti te\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-a27de580cd6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mout_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matt_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m75\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mout_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mout_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq2str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-a27de580cd6c>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(inp_sentence, decoder_input)\u001b[0m\n\u001b[1;32m     18\u001b[0m                                                     \u001b[0menc_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                                                     \u001b[0mcombined_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                                                     dec_padding_mask)\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# select the last word from the seq_len dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-7e3e1f31e170>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask)\u001b[0m\n\u001b[1;32m    242\u001b[0m             look_ahead_mask, dec_padding_mask):\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0menc_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size, inp_seq_len, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;31m# dec_output.shape == (batch_size, tar_seq_len, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-7e3e1f31e170>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, training, mask)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m  \u001b[0;31m# (batch_size, input_seq_len, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-7e3e1f31e170>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, training, mask)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size, input_seq_len, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mout1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayernorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size, input_seq_len, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mffn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size, input_seq_len, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1126\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m         \u001b[0;31m# Even if dispatching the op failed, the RHS may be a tensor aware\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36m_add_dispatch\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1446\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1447\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36madd_v2\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    481\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m    482\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"AddV2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m         tld.op_callbacks, x, y)\n\u001b[0m\u001b[1;32m    484\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}