{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPBfBxwGDQiozrTKujW3D9F"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHRo2NsK-Rgd",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "abe68f98-d0ec-4782-8311-7923e6989ad1"
      },
      "source": [
        "#@title Import & seed\n",
        "import time\n",
        "import re\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import requests\n",
        "import collections\n",
        "import pickle\n",
        "import copy, random\n",
        "import nltk as nl\n",
        "nl.download('punkt')\n",
        "from itertools import zip_longest\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Reshape, BatchNormalization, Dense, Dropout, concatenate,\n",
        "    Embedding, LSTM, Dense, GRU, Bidirectional, Add\n",
        ")\n",
        "from tensorflow.keras.activations import elu, relu, softmax, sigmoid\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "np.random.seed(1234)\n",
        "!nvidia-smi"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "2.3.0\n",
            "Fri Aug 14 21:52:07 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.57       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLZ0LreHzhhr",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "75a0c01c-d0bc-47b6-c09f-92d854e0db18"
      },
      "source": [
        "#@title Setup wandb\n",
        "!pip install wandb\n",
        "!wandb login f57cb185d23a8b60d349a4ea02278a6eee82550a\n",
        "import wandb\n",
        "wandb.init(project=\"deep_comedy\", name=\"daniele-6\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "  Using cached https://files.pythonhosted.org/packages/94/19/f8db9eff4b0173adf6dd2e8b0c3d8de0bfe10ec9ed63d247665980d82258/wandb-0.9.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.8.1)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Using cached https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.352.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.13)\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/1e/a45320cab182bf1c8656107b3d4c042e659742822fc6bff150d769a984dd/GitPython-3.1.7-py3-none-any.whl (158kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 4.5MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Using cached https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Collecting configparser>=3.8.1\n",
            "  Using cached https://files.pythonhosted.org/packages/4b/6b/01baa293090240cf0562cc5eccb69c6f5006282127f2b846fad011305c79/configparser-5.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.15.0)\n",
            "Collecting watchdog>=0.8.3\n",
            "  Using cached https://files.pythonhosted.org/packages/0e/06/121302598a4fc01aca942d937f4a2c33430b7181137b35758913a8db10ad/watchdog-0.10.3.tar.gz\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "  Using cached https://files.pythonhosted.org/packages/f4/4c/49f899856e3a83e02bc88f2c4945aa0bda4f56b804baa9f71e6664a574a2/sentry_sdk-0.16.5-py2.py3-none-any.whl\n",
            "Collecting subprocess32>=3.5.3\n",
            "  Using cached https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz\n",
            "Collecting gql==0.2.0\n",
            "  Using cached https://files.pythonhosted.org/packages/c4/6f/cf9a3056045518f06184e804bae89390eb706168349daa9dff8ac609962a/gql-0.2.0.tar.gz\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (2020.6.20)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.8MB/s \n",
            "\u001b[?25hCollecting pathtools>=0.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Collecting graphql-core<2,>=0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/89/00ad5e07524d8c523b14d70c685e0299a8b0de6d0727e368c41b89b7ed0b/graphql-core-1.1.tar.gz (70kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.6/dist-packages (from gql==0.2.0->wandb) (2.3)\n",
            "Collecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/b0/9a/4d409a6234eb940e6a78dfdfc66156e7522262f5f2fecca07dc55915952d/smmap-3.0.4-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: watchdog, subprocess32, gql, pathtools, graphql-core\n",
            "  Building wheel for watchdog (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for watchdog: filename=watchdog-0.10.3-cp36-none-any.whl size=73870 sha256=0f938be5eda4353fb055eff2f2f0a7a1e7048675ca080d4a8fadb0d62b043b49\n",
            "  Stored in directory: /root/.cache/pip/wheels/a8/1d/38/2c19bb311f67cc7b4d07a2ec5ea36ab1a0a0ea50db994a5bc7\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp36-none-any.whl size=6489 sha256=6f934eb0709585b0acc5ebda3502437675586654b2291f7822f4ca5c4dd54e4e\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for gql (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gql: filename=gql-0.2.0-cp36-none-any.whl size=7630 sha256=bf0c50a20581144ed30f1eae377a8b8163a18489b2ced79ca878a3451f607804\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/0e/7b/58a8a5268655b3ad74feef5aa97946f0addafb3cbb6bd2da23\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp36-none-any.whl size=8784 sha256=a464689a0f82ef9a2514207807e3e537940411dfb7b785b78073629e022116b8\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "  Building wheel for graphql-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for graphql-core: filename=graphql_core-1.1-cp36-none-any.whl size=104650 sha256=1b6a07aa186dca5e47b8180fcfe18819af2a9c6fce019fdd4f517dc30dae99d5\n",
            "  Stored in directory: /root/.cache/pip/wheels/45/99/d7/c424029bb0fe910c63b68dbf2aa20d3283d023042521bcd7d5\n",
            "Successfully built watchdog subprocess32 gql pathtools graphql-core\n",
            "Installing collected packages: docker-pycreds, smmap, gitdb, GitPython, shortuuid, configparser, pathtools, watchdog, sentry-sdk, subprocess32, graphql-core, gql, wandb\n",
            "Successfully installed GitPython-3.1.7 configparser-5.0.0 docker-pycreds-0.4.0 gitdb-4.0.5 gql-0.2.0 graphql-core-1.1 pathtools-0.1.2 sentry-sdk-0.16.5 shortuuid-1.0.1 smmap-3.0.4 subprocess32-3.5.4 wandb-0.9.4 watchdog-0.10.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[32mSuccessfully logged in to Weights & Biases!\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/veri/deep_comedy\" target=\"_blank\">https://app.wandb.ai/veri/deep_comedy</a><br/>\n",
              "                Run page: <a href=\"https://app.wandb.ai/veri/deep_comedy/runs/2a6xrqfg\" target=\"_blank\">https://app.wandb.ai/veri/deep_comedy/runs/2a6xrqfg</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "W&B Run: https://app.wandb.ai/veri/deep_comedy/runs/2a6xrqfg"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onTHkWW_YOHp",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Model\n",
        "\n",
        "vocab_size = 1797\n",
        "wandb.config.num_layers = 5\n",
        "wandb.config.d_model = 256\n",
        "wandb.config.dff = 1024\n",
        "wandb.config.num_heads = 4\n",
        "input_vocab_size = vocab_size\n",
        "target_vocab_size = vocab_size\n",
        "wandb.config.dropout = 0.3\n",
        "batch_len = 304\n",
        "EPOCHS = 200\n",
        "\n",
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "    return pos * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                            np.arange(d_model)[np.newaxis, :],\n",
        "                            d_model)\n",
        "    # apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    # apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "def create_padding_mask(seq):   \n",
        "    seq = tf.cast(tf.math.equal(seq, pad), tf.float32)\n",
        "    # add extra dimensions to add the padding\n",
        "    # to the attention logits.\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
        "\n",
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask  # (seq_len, seq_len)\n",
        "\n",
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    \"\"\"Calculate the attention weights.\n",
        "    q, k, v must have matching leading dimensions.\n",
        "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "    The mask has different shapes depending on its type(padding or look ahead) \n",
        "    but it must be broadcastable for addition.\n",
        "    \n",
        "    Args:\n",
        "        q: query shape == (..., seq_len_q, depth)\n",
        "        k: key shape == (..., seq_len_k, depth)\n",
        "        v: value shape == (..., seq_len_v, depth_v)\n",
        "        mask: Float tensor with shape broadcastable \n",
        "            to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "        \n",
        "    Returns:\n",
        "        output, attention_weights\n",
        "    \"\"\"\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "    # scale matmul_qk\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "    # add the mask to the scaled tensor.\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)  \n",
        "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "    # add up to 1.\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "    return output, attention_weights\n",
        "\n",
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "    ])\n",
        "\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        assert d_model % self.num_heads == 0\n",
        "        self.depth = d_model // self.num_heads\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "            \n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"Split the last dimension into (num_heads, depth).\n",
        "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "        \n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "        \n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "        \n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "        \n",
        "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "        concat_attention = tf.reshape(scaled_attention, \n",
        "                                    (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "        return output, attention_weights\n",
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "    def __call__(self, x, training, mask):\n",
        "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "        \n",
        "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "        \n",
        "        return out2\n",
        "\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "    \n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "        \n",
        "    def __call__(self, x, enc_output, training, \n",
        "            look_ahead_mask, padding_mask):\n",
        "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "        \n",
        "        attn2, attn_weights_block2 = self.mha2(\n",
        "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "        \n",
        "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
        "        \n",
        "        return out3, attn_weights_block1, attn_weights_block2\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "                maximum_position_encoding, rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)   ########## rm ??????\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
        "                                                self.d_model)\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
        "                        for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "            \n",
        "    def __call__(self, x, training, mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        # adding embedding and position encoding.\n",
        "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "        x = self.dropout(x, training=training)\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "        return x  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "                maximum_position_encoding, rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
        "                        for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "    def __call__(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "        x = self.dropout(x, training=training)\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                                look_ahead_mask, padding_mask)\n",
        "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "        \n",
        "        # x.shape == (batch_size, target_seq_len, d_model)\n",
        "        return x, attention_weights\n",
        "\n",
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
        "                target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
        "                            input_vocab_size, pe_input, rate)\n",
        "\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
        "                            target_vocab_size, pe_target, rate)\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "        \n",
        "    def __call__(self, inp, tar, training, enc_padding_mask, \n",
        "            look_ahead_mask, dec_padding_mask):\n",
        "\n",
        "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
        "        \n",
        "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "        dec_output, attention_weights = self.decoder(\n",
        "            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "        \n",
        "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "        \n",
        "        return final_output, attention_weights\n",
        "\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "        \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "        \n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "learning_rate = CustomSchedule(wandb.config.d_model)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
        "                                     epsilon=1e-9)\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "    name='train_accuracy')\n",
        "\n",
        "val_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "    name='train_accuracy')\n",
        "\n",
        "transformer = Transformer(wandb.config.num_layers, wandb.config.d_model, \n",
        "                          wandb.config.num_heads, wandb.config.dff,\n",
        "                          input_vocab_size, target_vocab_size, \n",
        "                          pe_input=input_vocab_size, \n",
        "                          pe_target=target_vocab_size,\n",
        "                          rate=wandb.config.dropout)\n",
        "def create_masks(inp, tar):\n",
        "    # Encoder padding mask\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "    \n",
        "    # Used in the 2nd attention block in the decoder.\n",
        "    # This padding mask is used to mask the encoder outputs.\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "    \n",
        "    # Used in the 1st attention block in the decoder.\n",
        "    # It is used to pad and mask future tokens in the input received by \n",
        "    # the decoder.\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "    \n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask\n",
        "\n",
        "checkpoint_path = \"./checkpoints/train\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "# if a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print ('Latest checkpoint restored!!')\n",
        "\n",
        "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
        "# execution. The function specializes to the precise shape of the argument\n",
        "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
        "# batch sizes (the last batch is smaller), use input_signature to specify\n",
        "# more generic shapes.\n",
        "\n",
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, batch_len), dtype=tf.int64),\n",
        "    tf.TensorSpec(shape=(None, batch_len), dtype=tf.int64),\n",
        "]\n",
        "\n",
        "@tf.function()#(input_signature=train_step_signature)\n",
        "def train_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _ = transformer(inp, tar_inp, \n",
        "                                    True, \n",
        "                                    enc_padding_mask, \n",
        "                                    combined_mask, \n",
        "                                    dec_padding_mask)\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "        gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "    \n",
        "    train_loss(loss)\n",
        "    train_accuracy(tar_real, predictions)\n",
        "\n",
        "@tf.function()#(input_signature=train_step_signature)\n",
        "def val_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "    \n",
        "    predictions, _ = transformer(inp, tar_inp, \n",
        "                                False, \n",
        "                                enc_padding_mask, \n",
        "                                combined_mask, \n",
        "                                dec_padding_mask)\n",
        "    loss = loss_function(tar_real, predictions)\n",
        "    \n",
        "    val_loss(loss)\n",
        "    val_accuracy(tar_real, predictions)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GI14h2PZX70w",
        "colab_type": "code",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "outputId": "aecbe82f-90a3-4d21-ba70-bf62a7c2f82e"
      },
      "source": [
        "#@title Preprocessing\n",
        "\n",
        "def get_hyp_lm_tercets(tercets):\n",
        "    new_tercets = []\n",
        "    for tercet in tercets:\n",
        "        new_tercets.append([])\n",
        "        for verse in tercet:\n",
        "            new_tercets[-1].append([])\n",
        "            for hyp_w in verse:\n",
        "                new_tercets[-1][-1].extend(hyp_w)\n",
        "                new_tercets[-1][-1].append('<SEP>')\n",
        "            new_tercets[-1][-1] = new_tercets[-1][-1][:-1]\n",
        "\n",
        "    return new_tercets\n",
        "\n",
        "def is_vowel(c):\n",
        "    return c in 'aeiouAEIOUàìíèéùúüòï'\n",
        "\n",
        "def unsplittable_cons():\n",
        "    u_cons = []\n",
        "    for c1 in ('b', 'c', 'd', 'f', 'g', 'p', 't', 'v'):\n",
        "        for c2 in ('l', 'r'):\n",
        "            u_cons.append(c1 + c2)\n",
        "\n",
        "    others = ['gn', 'gh', 'ch']\n",
        "    u_cons.extend(others)\n",
        "    return u_cons\n",
        "\n",
        "\n",
        "def are_cons_to_split(c1, c2):\n",
        "    to_split = ('cq', 'cn', 'lm', 'rc', 'bd', 'mb', 'mn', 'ld', 'ng', 'nd', 'tm', 'nv', 'nc', 'ft', 'nf', 'gm', 'fm', 'rv', 'fp')\n",
        "    return (c1 + c2) in to_split or (not is_vowel(c1) and (c1 == c2)) or ((c1 + c2) not in unsplittable_cons()) and (\n",
        "        (not is_vowel(c1)) and (not is_vowel(c2)) and c1 != 's')\n",
        "\n",
        "\n",
        "def is_diphthong(c1, c2):\n",
        "    return (c1 + c2) in ('ia', 'ie', 'io', 'iu', 'ua', 'ue', 'uo', 'ui', 'ai', 'ei', 'oi', 'ui', 'au', 'eu', 'ïe', 'iú', 'iù')\n",
        "\n",
        "\n",
        "def is_triphthong(c1, c2, c3):\n",
        "    return (c1 + c2 + c3) in ('iai', 'iei', 'uoi', 'uai', 'uei', 'iuo')\n",
        "\n",
        "\n",
        "def is_toned_vowel(c):\n",
        "    return c in 'àìèéùòï'\n",
        "\n",
        "def has_vowels(sy):\n",
        "    for c in sy:\n",
        "        if is_vowel(c):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def hyphenation(word):\n",
        "    \"\"\"\n",
        "    Split word in syllables\n",
        "    :param word: input string\n",
        "    :return: a list containing syllables of the word\n",
        "    \"\"\"\n",
        "    if not word or word == '':\n",
        "        return []\n",
        "    # elif len(word) == 3 and (is_vowel(word[1]) and is_vowel(word[2]) and not is_toned_vowel(word[2]) and (\n",
        "    #     not is_diphthong(word[1], word[2]) or (word[1] == 'i'))):\n",
        "    elif len(word) == 3 and (is_vowel(word[1]) and is_vowel(word[2]) and not is_toned_vowel(word[2]) and (\n",
        "        not is_diphthong(word[1], word[2]))):\n",
        "        return [word[:2]] + [word[2]]\n",
        "    elif len(word) == 3 and is_vowel(word[0]) and not is_vowel(word[1]) and is_vowel(word[2]):\n",
        "        return [word[:2]] + [word[2]]\n",
        "    elif len(word) == 3:\n",
        "        return [word]\n",
        "\n",
        "    syllables = []\n",
        "    is_done = False\n",
        "    count = 0\n",
        "    while not is_done and count <= len(word) - 1:\n",
        "        syllables.append('')\n",
        "        c = word[count]\n",
        "        while not is_vowel(c) and count < len(word) - 1:\n",
        "            syllables[-1] = syllables[-1] + c\n",
        "            count += 1\n",
        "            c = word[count]\n",
        "\n",
        "        syllables[-1] = syllables[-1] + word[count]\n",
        "\n",
        "        if count == len(word) - 1:\n",
        "            is_done = True\n",
        "        else:\n",
        "            count += 1\n",
        "\n",
        "            if count < len(word) and not is_vowel(word[count]):\n",
        "                if count == len(word) - 1:\n",
        "                    syllables[-1] += word[count]\n",
        "                    count += 1\n",
        "                elif count + 1 < len(word) and are_cons_to_split(word[count], word[count + 1]):\n",
        "                    syllables[-1] += word[count]\n",
        "                    count += 1\n",
        "                elif count + 2 < len(word) and not is_vowel(word[count + 1]) and not is_vowel(word[count + 2]) and word[\n",
        "                    count] != 's':\n",
        "                    syllables[-1] += word[count]\n",
        "                    count += 1\n",
        "            elif count < len(word):\n",
        "                if count + 1 < len(word) and is_triphthong(word[count - 1], word[count], word[count + 1]):\n",
        "                    syllables[-1] += word[count] + word[count + 1]\n",
        "                    count += 2\n",
        "                elif is_diphthong(word[count - 1], word[count]):\n",
        "                    syllables[-1] += word[count]\n",
        "                    count += 1\n",
        "\n",
        "                if count + 1 < len(word) and are_cons_to_split(word[count], word[count + 1]):\n",
        "                    syllables[-1] += word[count]\n",
        "                    count += 1\n",
        "\n",
        "            else:\n",
        "                is_done = True\n",
        "\n",
        "    if not has_vowels(syllables[-1]) and len(syllables) > 1:\n",
        "        syllables[-2] = syllables[-2] + syllables[-1]\n",
        "        syllables = syllables[:-1]\n",
        "\n",
        "    return syllables\n",
        "\n",
        "\n",
        "\n",
        "def get_dc_hyphenation(canti):\n",
        "    hyp_canti, hyp_tokens = [], []\n",
        "    for canto in canti:\n",
        "        hyp_canti.append([])\n",
        "        for verso in canto:\n",
        "            syllables = seq_hyphentation(verso)\n",
        "            hyp_canti[-1].append(syllables)\n",
        "            for syllable in syllables:\n",
        "                hyp_tokens.extend(syllable)\n",
        "\n",
        "    return hyp_canti, hyp_tokens\n",
        "\n",
        "\n",
        "def seq_hyphentation(words):\n",
        "    \"\"\"\n",
        "    Converts words in a list of strings into lists of syllables\n",
        "    :param words: a list of words (strings)\n",
        "    :return: a list of lists containing word syllables\n",
        "    \"\"\"\n",
        "    return [hyphenation(w) for w in words]\n",
        "\n",
        "\n",
        "def get_dc_cantos(filename, encoding=None):\n",
        "    # raw_data = read_words(filename=filename)\n",
        "    cantos, words, raw = [], [], []\n",
        "    with open(filename, \"r\", encoding=encoding) as f:\n",
        "        for line in f:\n",
        "            sentence = line.strip()\n",
        "            sentence = str.replace(sentence, \"\\.\", \" \\. \")\n",
        "            sentence = str.replace(sentence, \"[\", '')\n",
        "            sentence = str.replace(sentence, \"]\", '')\n",
        "            sentence = str.replace(sentence, \"-\", '')\n",
        "            sentence = str.replace(sentence, \";\", \" ; \")\n",
        "            sentence = str.replace(sentence, \",\", \" , \")\n",
        "            # sentence = str.replace(sentence, \" \\'\", '')\n",
        "            sentence = str.replace(sentence, \"\\'\", ' \\' ')\n",
        "            if len(sentence) > 1:\n",
        "                # sentence = sentence.translate(string.punctuation)\n",
        "                tokenized_sentence = nl.word_tokenize(sentence)\n",
        "                # tokenized_sentence = sentence.split()\n",
        "                tokenized_sentence = [w.lower() for w in tokenized_sentence if len(w) > 0]\n",
        "                tokenized_sentence = [w for w in tokenized_sentence if \",\" not in w]\n",
        "                tokenized_sentence = [w for w in tokenized_sentence if \".\" not in w]\n",
        "                tokenized_sentence = [w for w in tokenized_sentence if \":\" not in w]\n",
        "                tokenized_sentence = [w for w in tokenized_sentence if \";\" not in w]\n",
        "                tokenized_sentence = [w for w in tokenized_sentence if \"«\" not in w]\n",
        "                tokenized_sentence = [w for w in tokenized_sentence if \"»\" not in w]\n",
        "                # ts = []\n",
        "                ts = tokenized_sentence\n",
        "                # [ts.extend(re.split(\"(\\')\", e)) for e in tokenized_sentence]\n",
        "                tokenized_sentence = [w for w in ts if len(w) > 0]\n",
        "\n",
        "                if len(tokenized_sentence) == 2:\n",
        "                    cantos.append([])\n",
        "                    raw.append([])\n",
        "                elif len(tokenized_sentence) > 2:\n",
        "                    raw[-1].append(sentence)\n",
        "                    cantos[-1].append(tokenized_sentence)\n",
        "                    words.extend(tokenized_sentence)\n",
        "\n",
        "    return cantos, words, raw\n",
        "\n",
        "\n",
        "def create_tercets(cantos):\n",
        "    tercets = []\n",
        "    for i,canto in enumerate(cantos):\n",
        "        for v,verse in enumerate(canto):\n",
        "            if v%3 == 0:\n",
        "                tercets.append([])\n",
        "\n",
        "            tercets[-1].append(verse)\n",
        "        tercets = tercets[:-1]  # removes the last malformed tercets (only 2 verses)\n",
        "\n",
        "    return tercets\n",
        "\n",
        "def pad_list(l, pad_token, max_l_size, keep_lasts=False, pad_right=True):\n",
        "    \"\"\"\n",
        "    Adds a padding token to a list\n",
        "    inputs:\n",
        "    :param l: input list to pad.\n",
        "    :param pad_token: value to add as padding.\n",
        "    :param max_l_size: length of the new padded list to return,\n",
        "    it truncates lists longer that 'max_l_size' without adding\n",
        "    padding values.\n",
        "    :param keep_lasts: If True, preserves the max_l_size last elements\n",
        "    of a sequence (by keeping the same order).  E.g.:\n",
        "    if keep_lasts is True and max_l_size=3 [1,2,3,4] becomes [2,3,4].\n",
        "\n",
        "\n",
        "    :return: the list padded or truncated.\n",
        "    \"\"\"\n",
        "    to_pad = []\n",
        "    max_l = min(max_l_size, len(l))  # maximum len\n",
        "    l_init = len(l) - max_l if len(l) > max_l and keep_lasts else 0  # initial position where to sample from the list\n",
        "    l_end = len(l) if len(l) > max_l and keep_lasts else max_l\n",
        "    for i in range(l_init, l_end):\n",
        "        to_pad.append(l[i])\n",
        "\n",
        "    # for j in range(len(l), max_l_size):\n",
        "    #     to_pad.append(pad_token)\n",
        "    pad_tokens = [pad_token] * (max_l_size-len(l))\n",
        "    padded_l = to_pad + pad_tokens if pad_right else pad_tokens + to_pad\n",
        "\n",
        "    return padded_l\n",
        "\n",
        "\n",
        "def save_data(data, file):\n",
        "    with open(file, 'wb') as output:\n",
        "        pickle.dump(data, output, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def load_data(file):\n",
        "    with open(file, 'rb') as obj:\n",
        "        return pickle.load(obj)\n",
        "\n",
        "def print_and_write(file, s):\n",
        "    print(s)\n",
        "    file.write(s)\n",
        "\n",
        "\n",
        "class Vocabulary(object):\n",
        "    def __init__(self, vocab_size=None):\n",
        "        self.dictionary = dict()\n",
        "        self.rev_dictionary = dict()\n",
        "        self.count = []\n",
        "        self.special_tokens = []\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    def build_vocabulary_from_counts(self, count, special_tokens=[]):\n",
        "        \"\"\"\n",
        "        Sets all the attributes of the Vocabulary object.\n",
        "        :param count: a list of lists as follows: [['token', number_of_occurrences],...]\n",
        "        :param special_tokens: a list of strings. E.g. ['<EOS>', '<PAD>',...]\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "\n",
        "        dictionary = dict()\n",
        "        for word, _ in count:\n",
        "            dictionary[word] = len(dictionary)\n",
        "\n",
        "        # adding eventual special tokens to the dictionary (e.g. <EOS>,<PAD> etc..)\n",
        "        d = len(dictionary)\n",
        "        for i, token in enumerate(special_tokens):\n",
        "            dictionary[token] = d + i\n",
        "\n",
        "        self.count = count\n",
        "        self.dictionary = dictionary\n",
        "        self.rev_dictionary = dict(zip(self.dictionary.values(), self.dictionary.keys()))\n",
        "        self.special_tokens = special_tokens\n",
        "        self.vocab_size = len(dictionary)\n",
        "\n",
        "    def build_vocabulary_from_tokens(self, tokens, vocabulary_size=None, special_tokens=[]):\n",
        "        \"\"\"\n",
        "        Given a list of tokens, it sets the Vocabulary object attributes by constructing\n",
        "        a dictionary mapping each token to a unique id.\n",
        "        :param tokens: a list of strings.\n",
        "         E.g. [\"the\", \"cat\", \"is\", ... \".\", \"the\", \"house\" ,\"is\" ...].\n",
        "         NB: Here you should put all your token instances of the corpus.\n",
        "        :param vocabulary_size: The number of elements of your vocabulary. If there are more\n",
        "        than 'vocabulary_size' elements on tokens, it considers only the 'vocabulary_size'\n",
        "        most frequent ones.\n",
        "        :param special_tokens: Optional. A list of strings. Useful to add special tokens in vocabulary.\n",
        "        If you don't have any, keep it empty.\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "\n",
        "        vocabulary_size = vocabulary_size if vocabulary_size is not None else self.vocab_size\n",
        "        vocabulary_size = vocabulary_size - (len(special_tokens) + 1) if vocabulary_size else None\n",
        "        # counts occurrences of each token\n",
        "        count = [['<UNK>', -1]]\n",
        "        count.extend(collections.Counter(tokens).most_common(vocabulary_size))  # takes only the most frequent ones, if size is None takes them all\n",
        "        self.build_vocabulary_from_counts(count, special_tokens)  # actually build the vocabulary\n",
        "        self._set_unk_count(tokens)  # set the number of OOV instances\n",
        "\n",
        "    @staticmethod\n",
        "    def merge_vocabulary(vocab0, vocab1, vocabulary_size=-1):\n",
        "        \"\"\"\n",
        "        Merge two Vocabulary objects into a new one.\n",
        "        :param vocab0: first Vocabulary object\n",
        "        :param vocab1: second Vocabulary object\n",
        "        :param vocabulary_size: parameter to decide the merged vocabulary size.\n",
        "        With default value -1, all the words of both vocabularies are preserved.\n",
        "        When set to 0, the size of the vocabulary is set to the size of vocab0,\n",
        "        when set to 1 it is kept the size of vocab1.\n",
        "        :return: a new vocabulary\n",
        "        \"\"\"\n",
        "        # get size of the new vocabulary\n",
        "        vocab_size = vocab0.vocab_size + vocab1.vocab_size if vocabulary_size == -1 else vocabulary_size\n",
        "        merged_special_tokens = list(set(vocab0.special_tokens) | set(vocab1.special_tokens))\n",
        "\n",
        "        # merge the counts from the two vocabularies and then selects the most_common tokens\n",
        "        merged_counts = collections.Counter(dict(vocab0.count)) + collections.Counter(dict(vocab1.count))\n",
        "        merged_counts = merged_counts.most_common(vocab_size)\n",
        "        count = [['<UNK>', -1]]\n",
        "        count.extend(merged_counts)\n",
        "\n",
        "        # create the new vocabulary\n",
        "        merged_vocab = Vocabulary(vocab_size)\n",
        "        merged_vocab.build_vocabulary_from_counts(count, merged_special_tokens)\n",
        "        return merged_vocab\n",
        "\n",
        "    @staticmethod\n",
        "    def merge_vocabularies(vocab_list, vocab_size=None):\n",
        "        \"\"\"\n",
        "        Join a list of vocabularies into a new one.\n",
        "        :param vocab_list: a list of Vocabulary objects\n",
        "        :param vocab_size: the maximum size of the merged vocabulary.\n",
        "        :return: a vocabulary merging them all.\n",
        "        \"\"\"\n",
        "        vocab_size = vocab_size if vocab_size else sum([v.vocab_size for v in vocab_list])\n",
        "        merged_vocab = Vocabulary(vocab_size)\n",
        "        for voc in vocab_list:\n",
        "            merged_vocab = Vocabulary.merge_vocabulary(merged_vocab, voc, vocab_size)\n",
        "        return merged_vocab\n",
        "\n",
        "    def string2id(self, dataset):\n",
        "        \"\"\"\n",
        "        Converts a dataset of strings into a dataset of ids according to the object dictionary.\n",
        "        :param dataset: any string-based dataset with any nested lists.\n",
        "        :return: a new dataset, with the same shape of dataset, where each string is mapped into its\n",
        "        corresponding id associated in the dictionary (0 for unknown tokens).\n",
        "        \"\"\"\n",
        "\n",
        "        def _recursive_call(items):\n",
        "            new_items = []\n",
        "            for item in items:\n",
        "                if isinstance(item, str) or isinstance(item, int) or isinstance(item, float):\n",
        "                    new_items.append(self.word2id(item))\n",
        "                else:\n",
        "                    new_items.append(_recursive_call(item))\n",
        "            return new_items\n",
        "\n",
        "        return _recursive_call(dataset)\n",
        "\n",
        "    def id2string(self, dataset):\n",
        "        \"\"\"\n",
        "        Converts a dataset of integer ids into a dataset of string according to the reverse dictionary.\n",
        "        :param dataset: any int-based dataset with any nested lists. Allowed types are int, np.int32, np.int64.\n",
        "        :return: a new dataset, with the same shape of dataset, where each token is mapped into its\n",
        "        corresponding string associated in the reverse dictionary.\n",
        "        \"\"\"\n",
        "        def _recursive_call(items):\n",
        "            new_items = []\n",
        "            for item in items:\n",
        "                if isinstance(item, int) or isinstance(item, np.int) or isinstance(item, np.int32) or isinstance(item, np.int64):\n",
        "                    new_items.append(self.id2word(item))\n",
        "                else:\n",
        "                    new_items.append(_recursive_call(item))\n",
        "            return new_items\n",
        "\n",
        "        return _recursive_call(dataset)\n",
        "\n",
        "    def word2id(self, item):\n",
        "        \"\"\"\n",
        "        Maps a string token to its corresponding id.\n",
        "        :param item: a string.\n",
        "        :return: If the token belongs to the vocabulary, it returns an integer id > 0, otherwise\n",
        "        it returns the value associated to the unknown symbol, that is typically 0.\n",
        "        \"\"\"\n",
        "        return self.dictionary[item] if item in self.dictionary else self.dictionary['<UNK>']\n",
        "\n",
        "    def id2word(self, token_id):\n",
        "        \"\"\"\n",
        "        Maps an integer token to its corresponding string.\n",
        "        :param token_id: an integer.\n",
        "        :return: If the id belongs to the vocabulary, it returns the string\n",
        "        associated to it, otherwise it returns the string associated\n",
        "        to the unknown symbol, that is '<UNK>'.\n",
        "        \"\"\"\n",
        "\n",
        "        return self.rev_dictionary[token_id] if token_id in self.rev_dictionary else self.rev_dictionary[self.dictionary['<UNK>']]\n",
        "\n",
        "    def get_unk_count(self):\n",
        "        return self.count[0][1]\n",
        "\n",
        "    def _set_unk_count(self, tokens):\n",
        "        \"\"\"\n",
        "        Sets the number of OOV instances in the tokens provided\n",
        "        :param tokens: a list of tokens\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "        data = list()\n",
        "        unk_count = 0\n",
        "        for word in tokens:\n",
        "            if word in self.dictionary:\n",
        "                index = self.dictionary[word]\n",
        "            else:\n",
        "                index = 0  # dictionary['<UNK>']\n",
        "                unk_count += 1\n",
        "            data.append(index)\n",
        "        self.count[0][1] = unk_count\n",
        "\n",
        "    def add_element(self, name, is_special_token=False):\n",
        "        if name not in self.dictionary:\n",
        "            self.vocab_size += 1\n",
        "            self.dictionary[name] = self.vocab_size\n",
        "            self.rev_dictionary[self.vocab_size] = name\n",
        "\n",
        "            if is_special_token:\n",
        "                self.special_tokens = list(self.special_tokens)\n",
        "                self.special_tokens.append(name)\n",
        "\n",
        "            self.count.append([name, 1])\n",
        "\n",
        "    def set_vocabulary(self, dictionary, rev_dictionary, special_tokens, vocab_size):\n",
        "        self.dictionary = dictionary,\n",
        "        self.rev_dictionary = rev_dictionary\n",
        "        self.special_tokens = special_tokens\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    @staticmethod\n",
        "    def load_vocabulary(filename):\n",
        "        return load_data(filename)\n",
        "\n",
        "    def save_vocabulary(self, filename):\n",
        "        save_data(self, filename)\n",
        "\n",
        "class SyLMDataset(object):\n",
        "    def __init__(self, config, sy_vocab=None):\n",
        "        self.config = config\n",
        "        self.vocabulary = sy_vocab\n",
        "\n",
        "        self.raw_train_x = []\n",
        "        self.raw_val_x = []\n",
        "        self.raw_test_x = []\n",
        "        self.raw_x = []\n",
        "\n",
        "        self.train_x, self.train_y = [], []\n",
        "        self.val_x, self.val_y = [], []\n",
        "        self.test_x, self.test_y = [], []\n",
        "        self.x, self.y = [], []\n",
        "\n",
        "    def initialize(self, sess):\n",
        "        pass\n",
        "\n",
        "    def load(self, sources):\n",
        "        \"\"\"\n",
        "        Extract raw texts form sources and gather them all together.\n",
        "        :param sources: a string or an iterable of strings containing the file(s)\n",
        "        to process in order to build the dataset.\n",
        "        :return: a list of raw strings.\n",
        "        \"\"\"\n",
        "        return NotImplementedError\n",
        "\n",
        "    def build(self, sources, split_size=0.8):\n",
        "        \"\"\"\n",
        "        :param sources: a string or an iterable of strings containing the file(s)\n",
        "        to process in order to build the dataset.\n",
        "        :param split_size: the size to split the dataset, set >=1.0 to not split.\n",
        "        \"\"\"\n",
        "\n",
        "        raw_x = self.load(sources)\n",
        "        # raw_x = self.tokenize([self.preprocess(ex) for ex in raw_x])  # fixme\n",
        "        # splitting data\n",
        "        self.raw_x = raw_x\n",
        "        if split_size < 1.0:\n",
        "            self.raw_train_x, self.raw_test_x = self.split(self.raw_x, train_size=split_size)\n",
        "            self.raw_train_x, self.raw_val_x = self.split(self.raw_train_x, train_size=split_size)\n",
        "        else:\n",
        "            self.raw_train_x = self.raw_x\n",
        "\n",
        "        if self.vocabulary is None:\n",
        "            # creates vocabulary\n",
        "            tokens = [item for sublist in self.raw_train_x for item in sublist]  # get tokens\n",
        "            special_tokens = (\"<GO>\", \"<PAD>\", \"<SEP>\", \"<EOS>\", \"<EOV>\")\n",
        "            self._create_vocab(tokens, special_tokens=special_tokens)\n",
        "\n",
        "        # creates x,y for train\n",
        "        self.train_x = self._build_dataset(self.raw_train_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "        self.train_y = self._build_dataset(self.raw_train_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "\n",
        "        # creates x,y for validation\n",
        "        self.val_x = self._build_dataset(self.raw_val_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "        self.val_y = self._build_dataset(self.raw_val_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "\n",
        "        # creates x,y for validation\n",
        "        self.test_x = self._build_dataset(self.raw_test_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "        self.test_y = self._build_dataset(self.raw_test_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "\n",
        "    def _create_vocab(self, tokens, special_tokens=(\"<PAD>\", \"<GO>\", \"<SEP>\", \"<EOV>\", \"<EOS>\")):\n",
        "        \"\"\"\n",
        "        Create the vocabulary. Special tokens can be added to the tokens obtained from\n",
        "        the corpus.\n",
        "        :param tokens: a list of all the tokens in the corpus. Each token is a string.\n",
        "        :param special_tokens: a list of strings.\n",
        "        \"\"\"\n",
        "        vocab = Vocabulary(vocab_size=self.config.input_vocab_size)\n",
        "        vocab.build_vocabulary_from_tokens(tokens, special_tokens=special_tokens)\n",
        "        self.vocabulary = vocab\n",
        "\n",
        "    @staticmethod\n",
        "    def split(raw_data, train_size=0.8):\n",
        "        size = math.floor(len(raw_data)*train_size)\n",
        "        return raw_data[:size], raw_data[size:]\n",
        "\n",
        "    @staticmethod\n",
        "    def preprocess(txt):\n",
        "        return txt\n",
        "\n",
        "    @staticmethod\n",
        "    def shuffle(x):\n",
        "        return random.sample(x, len(x))\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenize(txt):\n",
        "        return txt\n",
        "\n",
        "    def _build_dataset(self, raw_data, max_len=100, insert_go=True, keep_lasts=False, pad_right=True, shuffle=True):\n",
        "        \"\"\"\n",
        "        Converts all the tokens in e1_raw_data by mapping each token with its corresponding\n",
        "        value in the dictionary. In case of token not in the dictionary, they are assigned to\n",
        "        a specific id. Each sequence is padded up to the seq_max_len setup in the config.\n",
        "\n",
        "        :param raw_data: list of sequences, each sequence is a list of tokens (strings).\n",
        "        :param max_len: max length of a sequence, crop longer and pad smaller ones.\n",
        "        :param insert_go: True to insert <GO>, False otherwise.\n",
        "        :param keep_lasts: True to truncate initial elements of a sequence.\n",
        "        :param pad_right: pad to the right (default value True), otherwise pads to left.\n",
        "        :param shuffle: Optional. If True data are shuffled.\n",
        "        :return: A list of sequences where each token in each sequence is an int id.\n",
        "        \"\"\"\n",
        "        dataset = []\n",
        "        for sentence in raw_data:\n",
        "            sentence_ids = [self.vocabulary.word2id(\"<GO>\")] if insert_go else []\n",
        "            sentence_ids.extend([self.vocabulary.word2id(w) for w in sentence])\n",
        "            sentence_ids.append(self.vocabulary.word2id(\"<EOS>\"))\n",
        "            sentence_ids = pad_list(sentence_ids, self.vocabulary.word2id(\"<PAD>\"), max_len, keep_lasts=keep_lasts, pad_right=pad_right)\n",
        "\n",
        "            dataset.append(sentence_ids)\n",
        "\n",
        "        if shuffle:\n",
        "            return random.sample(dataset, len(dataset))\n",
        "        else:\n",
        "            return dataset\n",
        "\n",
        "    def get_batches(self, batch_size=32, split_sel='train'):\n",
        "        \"\"\"\n",
        "        Iterator over the training set. Useful method to run experiments.\n",
        "        :param batch_size: size of the mini_batch\n",
        "        :return: input and target.\n",
        "        \"\"\"\n",
        "        if split_sel == 'train':\n",
        "            x, y = self.train_x, self.train_y\n",
        "        elif split_sel == 'val':\n",
        "            x, y = self.val_x, self.val_y\n",
        "        else:\n",
        "            x, y = self.test_x, self.test_y\n",
        "        \n",
        "        i = 0#random.randint(0, batch_size)\n",
        "        batches = []\n",
        "        eov = self.vocabulary.word2id(\"<EOV>\")\n",
        "        go = self.vocabulary.word2id(\"<GO>\")\n",
        "        # prepare batches\n",
        "        while i < len(x):\n",
        "            j = 0\n",
        "            batch_x, batch_y = [], []\n",
        "            while j < batch_size and i+j<len(x):\n",
        "                for c in x[i+j]:\n",
        "                  batch_x.append(c)\n",
        "                batch_x.append(eov)\n",
        "                for c in y[i+j]:\n",
        "                  batch_y.append(c)\n",
        "                batch_y.append(eov)\n",
        "                j += 1\n",
        "            i += batch_size\n",
        "            batches.append((batch_x, batch_y))\n",
        "\n",
        "        # supply\n",
        "        i = 0\n",
        "        while i < len(batches):\n",
        "            yield batches[i][0], batches[i][1]\n",
        "            i += 1\n",
        "\n",
        "class DanteSyLMDataset(SyLMDataset):\n",
        "    def __init__(self, config, sy_vocab=None):\n",
        "        \"\"\"\n",
        "        Class to create a dataset from Dante Alighieri's Divine Comedy.\n",
        "        :param config: a Config object\n",
        "        :param sy_vocab: (optional) a Vocabulary object where tokens of the dictionary\n",
        "        are syllables. If None, the vocabulary is create automatically from the source.\n",
        "        \"\"\"\n",
        "        super().__init__(config, sy_vocab)\n",
        "\n",
        "    def load(self, sources):\n",
        "        \"\"\"\n",
        "        Load examples from dataset\n",
        "        :param sources: data filepath.\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        canti, _, raw = get_dc_cantos(filename=sources)  # get raw data from file\n",
        "        canti, tokens = get_dc_hyphenation(canti)  # converts each\n",
        "\n",
        "        tercets = create_tercets(canti)\n",
        "        tercets = get_hyp_lm_tercets(tercets)\n",
        "\n",
        "        x = []\n",
        "        for tercet in tercets:\n",
        "            x.append([])\n",
        "            for verse in tercet:\n",
        "                x[-1].extend(verse)\n",
        "                x[-1].append(\"<EOV>\")\n",
        "\n",
        "        #x = self.shuffle(x)\n",
        "        return x\n",
        "\n",
        "def seq2str(seq):\n",
        "    def output2string(batch, rev_vocabulary, special_tokens, end_of_tokens):\n",
        "        to_print = ''\n",
        "        for token in batch:\n",
        "            if token in special_tokens:\n",
        "                to_print += ' '\n",
        "            elif end_of_tokens and token in end_of_tokens:\n",
        "                to_print += '\\n'\n",
        "            elif token in rev_vocabulary:\n",
        "                to_print += rev_vocabulary[token]\n",
        "            else:\n",
        "                to_print += '<UNK>'\n",
        "        return to_print\n",
        "\n",
        "    return output2string(seq, poetry_sy_lm_dataset.vocabulary.rev_dictionary,\n",
        "      special_tokens=[poetry_sy_lm_dataset.vocabulary.word2id(\"<PAD>\"), 0, poetry_sy_lm_dataset.vocabulary.word2id(\"<SEP>\"),\n",
        "                      poetry_sy_lm_dataset.vocabulary.word2id(\"<GO>\"), poetry_sy_lm_dataset.vocabulary.word2id(\"<EOS>\")],\n",
        "      end_of_tokens=[poetry_sy_lm_dataset.vocabulary.word2id(\"<EOV>\")])\n",
        "\n",
        "class cnfg:\n",
        "  vocab_size = 1884\n",
        "  input_vocab_size = 1884\n",
        "  emb_size = 300\n",
        "  sentence_max_len = 75\n",
        "\n",
        "config = cnfg()\n",
        "poetry_sy_lm_dataset = DanteSyLMDataset(config, sy_vocab=None)\n",
        "url = \"https://gitlab.com/zugo91/nlgpoetry/-/raw/release/data/la_divina_commedia.txt\"\n",
        "response = requests.get(url)\n",
        "response.encoding = 'ISO-8859-1'\n",
        "fi = open(\"divcom.txt\",\"w\")\n",
        "fi.write(response.text)\n",
        "fi.close()\n",
        "data_path = os.path.join(os.getcwd(), \"divcom.txt\")  # dataset location, here just the name of the source file\n",
        "poetry_sy_lm_dataset.build(data_path, split_size=0.9)  # actual creation of  vocabulary (if not provided) and dataset\n",
        "print(\"Train size: \" + str(len(poetry_sy_lm_dataset.train_y)))\n",
        "print(\"Val size: \" + str(len(poetry_sy_lm_dataset.val_y)))\n",
        "print(\"Test size: \" + str(len(poetry_sy_lm_dataset.test_y)))\n",
        "\n",
        "eov = poetry_sy_lm_dataset.vocabulary.word2id(\"<EOV>\")\n",
        "pad = poetry_sy_lm_dataset.vocabulary.word2id(\"<PAD>\")\n",
        "go = poetry_sy_lm_dataset.vocabulary.word2id(\"<GO>\")\n",
        "eos = poetry_sy_lm_dataset.vocabulary.word2id(\"<EOS>\")\n",
        "\n",
        "batches = [b for b in poetry_sy_lm_dataset.get_batches(4)]\n",
        "print(batches[0][0])\n",
        "print(batches[0][1])\n",
        "print(len(batches[0][0]))\n",
        "val_b = [b for b in poetry_sy_lm_dataset.get_batches(4, split_sel='val')]\n",
        "print(val_b[0][0])\n",
        "print(val_b[0][1])\n",
        "print(len(val_b[0][0]))\n",
        "test_b = [b for b in poetry_sy_lm_dataset.get_batches(4, split_sel='test')]\n",
        "print(test_b[0][0])\n",
        "print(test_b[0][1])\n",
        "print(len(test_b[0][0]))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train size: 3815\n",
            "Val size: 424\n",
            "Test size: 472\n",
            "[1792, 84, 1794, 339, 198, 1794, 50, 1794, 284, 188, 1794, 6, 1794, 24, 136, 1794, 37, 14, 1796, 27, 1794, 32, 61, 509, 1794, 18, 1794, 52, 5, 1794, 404, 47, 1794, 40, 276, 30, 1796, 89, 1794, 8, 1794, 6, 683, 14, 1794, 264, 1794, 92, 5, 1794, 655, 32, 14, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 585, 1794, 78, 9, 1794, 5, 1794, 238, 1794, 162, 1794, 92, 5, 1794, 57, 1794, 12, 45, 1794, 116, 30, 1796, 4, 63, 1794, 404, 47, 1794, 404, 748, 148, 1794, 4, 1794, 5, 1203, 1794, 4, 1794, 120, 10, 1796, 7, 1794, 84, 1794, 166, 536, 1794, 32, 24, 47, 1794, 8, 1794, 455, 30, 1794, 153, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 792, 1794, 3, 1794, 57, 1794, 5, 22, 30, 1794, 7, 1794, 49, 12, 1794, 57, 1794, 64, 1794, 144, 10, 1796, 22, 1794, 18, 1794, 416, 202, 1794, 50, 1794, 154, 1794, 46, 1794, 3, 1794, 41, 1794, 37, 1794, 61, 509, 1796, 6, 150, 1794, 193, 1794, 3, 1794, 15, 124, 1794, 12, 11, 1794, 46, 1794, 3, 1794, 53, 1794, 3, 1794, 405, 1794, 3, 1794, 456, 1794, 336, 10, 1796, 1796, 1792, 41, 1794, 31, 1794, 23, 1794, 154, 1794, 32, 238, 1794, 131, 1794, 3, 1794, 41, 1794, 405, 1794, 3, 1794, 257, 716, 1796, 792, 1794, 3, 1794, 92, 5, 1794, 163, 24, 1794, 6, 1794, 138, 24, 1794, 5, 1794, 60, 1794, 293, 9, 1796, 7, 1794, 8, 1794, 25, 30, 43, 1794, 264, 1794, 337, 539, 17, 702, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796]\n",
            "[1792, 84, 1794, 339, 198, 1794, 50, 1794, 284, 188, 1794, 6, 1794, 24, 136, 1794, 37, 14, 1796, 27, 1794, 32, 61, 509, 1794, 18, 1794, 52, 5, 1794, 404, 47, 1794, 40, 276, 30, 1796, 89, 1794, 8, 1794, 6, 683, 14, 1794, 264, 1794, 92, 5, 1794, 655, 32, 14, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 585, 1794, 78, 9, 1794, 5, 1794, 238, 1794, 162, 1794, 92, 5, 1794, 57, 1794, 12, 45, 1794, 116, 30, 1796, 4, 63, 1794, 404, 47, 1794, 404, 748, 148, 1794, 4, 1794, 5, 1203, 1794, 4, 1794, 120, 10, 1796, 7, 1794, 84, 1794, 166, 536, 1794, 32, 24, 47, 1794, 8, 1794, 455, 30, 1794, 153, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 792, 1794, 3, 1794, 57, 1794, 5, 22, 30, 1794, 7, 1794, 49, 12, 1794, 57, 1794, 64, 1794, 144, 10, 1796, 22, 1794, 18, 1794, 416, 202, 1794, 50, 1794, 154, 1794, 46, 1794, 3, 1794, 41, 1794, 37, 1794, 61, 509, 1796, 6, 150, 1794, 193, 1794, 3, 1794, 15, 124, 1794, 12, 11, 1794, 46, 1794, 3, 1794, 53, 1794, 3, 1794, 405, 1794, 3, 1794, 456, 1794, 336, 10, 1796, 1796, 1792, 41, 1794, 31, 1794, 23, 1794, 154, 1794, 32, 238, 1794, 131, 1794, 3, 1794, 41, 1794, 405, 1794, 3, 1794, 257, 716, 1796, 792, 1794, 3, 1794, 92, 5, 1794, 163, 24, 1794, 6, 1794, 138, 24, 1794, 5, 1794, 60, 1794, 293, 9, 1796, 7, 1794, 8, 1794, 25, 30, 43, 1794, 264, 1794, 337, 539, 17, 702, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796]\n",
            "304\n",
            "[1792, 4, 1794, 76, 150, 1794, 70, 1794, 27, 1794, 249, 1794, 4, 1794, 424, 1794, 3, 1794, 41, 1794, 642, 5, 1796, 64, 1794, 1353, 601, 40, 23, 1794, 5, 1794, 10, 1794, 31, 1794, 27, 1794, 17, 134, 6, 1796, 7, 1794, 15, 299, 1794, 15, 61, 1794, 21, 1794, 65, 63, 1794, 486, 161, 1794, 1014, 5, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 59, 1794, 159, 6, 1794, 3, 1794, 16, 1794, 25, 48, 1794, 89, 1794, 53, 1794, 27, 24, 32, 1794, 4, 1794, 3, 1794, 203, 6, 1796, 6, 1794, 65, 63, 1794, 37, 14, 1794, 27, 164, 1794, 84, 34, 1794, 255, 189, 1796, 21, 1794, 7, 1794, 128, 22, 1794, 7, 1794, 166, 13, 1794, 56, 1794, 166, 536, 1794, 624, 6, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 22, 1794, 18, 89, 1794, 3, 1794, 16, 1794, 45, 492, 1794, 5, 42, 29, 1794, 21, 1794, 7, 1794, 41, 1794, 25, 189, 1796, 35, 1794, 18, 76, 246, 1794, 37, 63, 1794, 4, 1794, 7, 1794, 127, 1794, 3, 1794, 217, 11, 14, 1796, 6, 1794, 234, 43, 1794, 6, 573, 106, 1794, 80, 1794, 3, 1794, 5, 562, 209, 1794, 26, 189, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 8, 1794, 66, 43, 1794, 246, 1794, 13, 156, 30, 1794, 577, 36, 1794, 4, 1794, 353, 14, 1796, 119, 55, 1794, 8, 1794, 66, 381, 250, 1794, 119, 55, 1794, 3, 1794, 16, 1794, 6, 349, 1796, 5, 1794, 7, 1794, 8, 1794, 184, 1794, 32, 275, 63, 1794, 57, 1794, 133, 1794, 33, 159, 14, 1794, 153, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796]\n",
            "[1792, 4, 1794, 76, 150, 1794, 70, 1794, 27, 1794, 249, 1794, 4, 1794, 424, 1794, 3, 1794, 41, 1794, 642, 5, 1796, 64, 1794, 1353, 601, 40, 23, 1794, 5, 1794, 10, 1794, 31, 1794, 27, 1794, 17, 134, 6, 1796, 7, 1794, 15, 299, 1794, 15, 61, 1794, 21, 1794, 65, 63, 1794, 486, 161, 1794, 1014, 5, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 59, 1794, 159, 6, 1794, 3, 1794, 16, 1794, 25, 48, 1794, 89, 1794, 53, 1794, 27, 24, 32, 1794, 4, 1794, 3, 1794, 203, 6, 1796, 6, 1794, 65, 63, 1794, 37, 14, 1794, 27, 164, 1794, 84, 34, 1794, 255, 189, 1796, 21, 1794, 7, 1794, 128, 22, 1794, 7, 1794, 166, 13, 1794, 56, 1794, 166, 536, 1794, 624, 6, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 22, 1794, 18, 89, 1794, 3, 1794, 16, 1794, 45, 492, 1794, 5, 42, 29, 1794, 21, 1794, 7, 1794, 41, 1794, 25, 189, 1796, 35, 1794, 18, 76, 246, 1794, 37, 63, 1794, 4, 1794, 7, 1794, 127, 1794, 3, 1794, 217, 11, 14, 1796, 6, 1794, 234, 43, 1794, 6, 573, 106, 1794, 80, 1794, 3, 1794, 5, 562, 209, 1794, 26, 189, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 8, 1794, 66, 43, 1794, 246, 1794, 13, 156, 30, 1794, 577, 36, 1794, 4, 1794, 353, 14, 1796, 119, 55, 1794, 8, 1794, 66, 381, 250, 1794, 119, 55, 1794, 3, 1794, 16, 1794, 6, 349, 1796, 5, 1794, 7, 1794, 8, 1794, 184, 1794, 32, 275, 63, 1794, 57, 1794, 133, 1794, 33, 159, 14, 1794, 153, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796]\n",
            "304\n",
            "[1792, 40, 1794, 98, 14, 1794, 119, 30, 1794, 184, 1794, 7, 1794, 44, 1794, 38, 1794, 121, 501, 1796, 6, 66, 14, 1794, 18, 1794, 34, 1794, 247, 1794, 106, 160, 10, 1794, 301, 432, 9, 1796, 36, 1794, 60, 8, 1794, 245, 8, 1794, 255, 30, 1794, 27, 1794, 6, 1065, 501, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 49, 190, 1794, 199, 22, 9, 1794, 56, 1794, 151, 12, 1794, 81, 38, 253, 9, 1796, 15, 8, 1794, 184, 1794, 265, 39, 1794, 6, 700, 645, 1794, 34, 1794, 230, 48, 1796, 7, 1794, 74, 289, 742, 1794, 12, 44, 1794, 131, 1794, 3, 1794, 53, 1794, 3, 1794, 456, 1794, 253, 9, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 118, 1794, 94, 8, 1794, 40, 1794, 114, 43, 1794, 251, 108, 39, 1794, 50, 1794, 203, 1794, 37, 48, 1796, 5, 1794, 197, 1794, 24, 123, 1794, 13, 559, 1794, 8, 340, 218, 1794, 19, 1794, 179, 37, 1796, 46, 1794, 3, 1794, 330, 1794, 176, 440, 1794, 214, 1794, 6, 1794, 65, 75, 1794, 1353, 211, 1794, 27, 48, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 149, 14, 1794, 12, 484, 1794, 6, 1794, 293, 20, 1794, 353, 37, 1794, 4, 1794, 173, 37, 1796, 12, 26, 1794, 20, 1794, 209, 43, 1794, 21, 101, 24, 1794, 50, 8, 1794, 82, 33, 1796, 18, 1794, 8, 1794, 162, 1794, 59, 1794, 79, 1794, 18, 1794, 34, 1794, 22, 29, 1794, 58, 36, 37, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796]\n",
            "[1792, 40, 1794, 98, 14, 1794, 119, 30, 1794, 184, 1794, 7, 1794, 44, 1794, 38, 1794, 121, 501, 1796, 6, 66, 14, 1794, 18, 1794, 34, 1794, 247, 1794, 106, 160, 10, 1794, 301, 432, 9, 1796, 36, 1794, 60, 8, 1794, 245, 8, 1794, 255, 30, 1794, 27, 1794, 6, 1065, 501, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 49, 190, 1794, 199, 22, 9, 1794, 56, 1794, 151, 12, 1794, 81, 38, 253, 9, 1796, 15, 8, 1794, 184, 1794, 265, 39, 1794, 6, 700, 645, 1794, 34, 1794, 230, 48, 1796, 7, 1794, 74, 289, 742, 1794, 12, 44, 1794, 131, 1794, 3, 1794, 53, 1794, 3, 1794, 456, 1794, 253, 9, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 118, 1794, 94, 8, 1794, 40, 1794, 114, 43, 1794, 251, 108, 39, 1794, 50, 1794, 203, 1794, 37, 48, 1796, 5, 1794, 197, 1794, 24, 123, 1794, 13, 559, 1794, 8, 340, 218, 1794, 19, 1794, 179, 37, 1796, 46, 1794, 3, 1794, 330, 1794, 176, 440, 1794, 214, 1794, 6, 1794, 65, 75, 1794, 1353, 211, 1794, 27, 48, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 149, 14, 1794, 12, 484, 1794, 6, 1794, 293, 20, 1794, 353, 37, 1794, 4, 1794, 173, 37, 1796, 12, 26, 1794, 20, 1794, 209, 43, 1794, 21, 101, 24, 1794, 50, 8, 1794, 82, 33, 1796, 18, 1794, 8, 1794, 162, 1794, 59, 1794, 79, 1794, 18, 1794, 34, 1794, 22, 29, 1794, 58, 36, 37, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796]\n",
            "304\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwdCuj3HsFux",
        "colab_type": "code",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "48fbc478-6a70-4b60-ae46-5499290c9c65"
      },
      "source": [
        " #@title Train loop\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    random.shuffle(batches)\n",
        "    start = time.time()\n",
        "    \n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    \n",
        "    for (batch, (inp, tar)) in enumerate(batches):\n",
        "        if (len(inp) != batch_len or len(tar) != batch_len):\n",
        "            print(\"discarded batch\", batch)\n",
        "            continue\n",
        "        train_step(np.expand_dims(inp, axis=0), np.expand_dims(tar, axis=0))\n",
        "        \n",
        "        if batch % 50 == 0:\n",
        "            print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
        "                epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
        "        \n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        ckpt_save_path = ckpt_manager.save()\n",
        "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n",
        "\n",
        "    print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, train_loss.result(), train_accuracy.result()))\n",
        "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))\n",
        "\n",
        "    wandb.log({\n",
        "        'train_loss': train_loss.result(),\n",
        "        'train_accuracy': train_accuracy.result()\n",
        "    }, step=epoch+1)\n",
        "\n",
        "    # validation\n",
        "    if epoch % 5 == 0:\n",
        "        loss_l, acc_l = [], []\n",
        "        for (batch, (inp, tar)) in enumerate(val_b):\n",
        "            val_loss.reset_states()\n",
        "            val_accuracy.reset_states()\n",
        "            \n",
        "            if (len(inp) != batch_len or len(tar) != batch_len):\n",
        "                print(\"discarded batch\", batch)\n",
        "                continue\n",
        "\n",
        "            val_step(np.expand_dims(inp, axis=0), np.expand_dims(tar, axis=0))\n",
        "\n",
        "            loss_l.append(val_loss.result())\n",
        "            acc_l.append(val_accuracy.result())\n",
        "\n",
        "        loss_mean = sum(loss_l)/len(loss_l)\n",
        "        acc_mean = sum(acc_l)/len(acc_l)\n",
        "        print('Epoch {} VALIDATION: Loss {:.4f} Accuracy {:.4f}\\n'.format(epoch + 1, loss_mean, acc_mean))\n",
        "\n",
        "        wandb.log({\n",
        "            'val_loss': loss_mean,\n",
        "            'val_accuracy': acc_mean\n",
        "        }, step=epoch+1)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 7.6211 Accuracy 0.0000\n",
            "Epoch 1 Batch 50 Loss 7.0601 Accuracy 0.0993\n",
            "Epoch 1 Batch 100 Loss 6.4613 Accuracy 0.1817\n",
            "Epoch 1 Batch 150 Loss 6.0808 Accuracy 0.2105\n",
            "Epoch 1 Batch 200 Loss 5.7832 Accuracy 0.2245\n",
            "discarded batch 206\n",
            "Epoch 1 Batch 250 Loss 5.5087 Accuracy 0.2542\n",
            "Epoch 1 Batch 300 Loss 5.2316 Accuracy 0.2824\n",
            "Epoch 1 Batch 350 Loss 4.9994 Accuracy 0.3040\n",
            "Epoch 1 Batch 400 Loss 4.8057 Accuracy 0.3220\n",
            "Epoch 1 Batch 450 Loss 4.6414 Accuracy 0.3374\n",
            "Epoch 1 Batch 500 Loss 4.5013 Accuracy 0.3495\n",
            "Epoch 1 Batch 550 Loss 4.3829 Accuracy 0.3597\n",
            "Epoch 1 Batch 600 Loss 4.2767 Accuracy 0.3687\n",
            "Epoch 1 Batch 650 Loss 4.1850 Accuracy 0.3766\n",
            "Epoch 1 Batch 700 Loss 4.1065 Accuracy 0.3834\n",
            "Epoch 1 Batch 750 Loss 4.0354 Accuracy 0.3893\n",
            "Epoch 1 Batch 800 Loss 3.9724 Accuracy 0.3947\n",
            "Epoch 1 Batch 850 Loss 3.9164 Accuracy 0.3996\n",
            "Epoch 1 Batch 900 Loss 3.8660 Accuracy 0.4041\n",
            "Epoch 1 Batch 950 Loss 3.8182 Accuracy 0.4082\n",
            "Epoch 1 Loss 3.8156 Accuracy 0.4084\n",
            "Time taken for 1 epoch: 36.281002044677734 secs\n",
            "\n",
            "Epoch 1 VALIDATION: Loss 2.9083 Accuracy 0.4970\n",
            "\n",
            "Epoch 2 Batch 0 Loss 3.0073 Accuracy 0.4554\n",
            "Epoch 2 Batch 50 Loss 2.9713 Accuracy 0.4827\n",
            "Epoch 2 Batch 100 Loss 2.9647 Accuracy 0.4834\n",
            "Epoch 2 Batch 150 Loss 2.9575 Accuracy 0.4844\n",
            "Epoch 2 Batch 200 Loss 2.9556 Accuracy 0.4842\n",
            "Epoch 2 Batch 250 Loss 2.9537 Accuracy 0.4844\n",
            "Epoch 2 Batch 300 Loss 2.9566 Accuracy 0.4846\n",
            "Epoch 2 Batch 350 Loss 2.9574 Accuracy 0.4848\n",
            "Epoch 2 Batch 400 Loss 2.9519 Accuracy 0.4854\n",
            "discarded batch 433\n",
            "Epoch 2 Batch 450 Loss 2.9488 Accuracy 0.4857\n",
            "Epoch 2 Batch 500 Loss 2.9467 Accuracy 0.4858\n",
            "Epoch 2 Batch 550 Loss 2.9470 Accuracy 0.4858\n",
            "Epoch 2 Batch 600 Loss 2.9455 Accuracy 0.4860\n",
            "Epoch 2 Batch 650 Loss 2.9432 Accuracy 0.4863\n",
            "Epoch 2 Batch 700 Loss 2.9396 Accuracy 0.4869\n",
            "Epoch 2 Batch 750 Loss 2.9375 Accuracy 0.4873\n",
            "Epoch 2 Batch 800 Loss 2.9349 Accuracy 0.4878\n",
            "Epoch 2 Batch 850 Loss 2.9327 Accuracy 0.4881\n",
            "Epoch 2 Batch 900 Loss 2.9307 Accuracy 0.4880\n",
            "Epoch 2 Batch 950 Loss 2.9277 Accuracy 0.4884\n",
            "Epoch 2 Loss 2.9279 Accuracy 0.4884\n",
            "Time taken for 1 epoch: 24.90796709060669 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 2.9108 Accuracy 0.4818\n",
            "Epoch 3 Batch 50 Loss 2.8623 Accuracy 0.4946\n",
            "Epoch 3 Batch 100 Loss 2.8695 Accuracy 0.4954\n",
            "Epoch 3 Batch 150 Loss 2.8593 Accuracy 0.4967\n",
            "discarded batch 197\n",
            "Epoch 3 Batch 200 Loss 2.8565 Accuracy 0.4973\n",
            "Epoch 3 Batch 250 Loss 2.8492 Accuracy 0.4981\n",
            "Epoch 3 Batch 300 Loss 2.8496 Accuracy 0.4980\n",
            "Epoch 3 Batch 350 Loss 2.8465 Accuracy 0.4982\n",
            "Epoch 3 Batch 400 Loss 2.8495 Accuracy 0.4980\n",
            "Epoch 3 Batch 450 Loss 2.8489 Accuracy 0.4981\n",
            "Epoch 3 Batch 500 Loss 2.8465 Accuracy 0.4986\n",
            "Epoch 3 Batch 550 Loss 2.8439 Accuracy 0.4989\n",
            "Epoch 3 Batch 600 Loss 2.8444 Accuracy 0.4989\n",
            "Epoch 3 Batch 650 Loss 2.8399 Accuracy 0.4997\n",
            "Epoch 3 Batch 700 Loss 2.8408 Accuracy 0.4997\n",
            "Epoch 3 Batch 750 Loss 2.8393 Accuracy 0.5000\n",
            "Epoch 3 Batch 800 Loss 2.8392 Accuracy 0.5001\n",
            "Epoch 3 Batch 850 Loss 2.8395 Accuracy 0.5000\n",
            "Epoch 3 Batch 900 Loss 2.8375 Accuracy 0.5001\n",
            "Epoch 3 Batch 950 Loss 2.8369 Accuracy 0.5002\n",
            "Epoch 3 Loss 2.8368 Accuracy 0.5002\n",
            "Time taken for 1 epoch: 25.481547355651855 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 2.7620 Accuracy 0.5017\n",
            "Epoch 4 Batch 50 Loss 2.7929 Accuracy 0.4997\n",
            "Epoch 4 Batch 100 Loss 2.7909 Accuracy 0.5019\n",
            "Epoch 4 Batch 150 Loss 2.7907 Accuracy 0.5037\n",
            "Epoch 4 Batch 200 Loss 2.7863 Accuracy 0.5043\n",
            "Epoch 4 Batch 250 Loss 2.7932 Accuracy 0.5040\n",
            "Epoch 4 Batch 300 Loss 2.7935 Accuracy 0.5043\n",
            "Epoch 4 Batch 350 Loss 2.7919 Accuracy 0.5045\n",
            "Epoch 4 Batch 400 Loss 2.7866 Accuracy 0.5053\n",
            "Epoch 4 Batch 450 Loss 2.7824 Accuracy 0.5055\n",
            "Epoch 4 Batch 500 Loss 2.7800 Accuracy 0.5057\n",
            "Epoch 4 Batch 550 Loss 2.7822 Accuracy 0.5055\n",
            "discarded batch 583\n",
            "Epoch 4 Batch 600 Loss 2.7808 Accuracy 0.5055\n",
            "Epoch 4 Batch 650 Loss 2.7804 Accuracy 0.5056\n",
            "Epoch 4 Batch 700 Loss 2.7814 Accuracy 0.5055\n",
            "Epoch 4 Batch 750 Loss 2.7813 Accuracy 0.5052\n",
            "Epoch 4 Batch 800 Loss 2.7831 Accuracy 0.5050\n",
            "Epoch 4 Batch 850 Loss 2.7842 Accuracy 0.5051\n",
            "Epoch 4 Batch 900 Loss 2.7847 Accuracy 0.5051\n",
            "Epoch 4 Batch 950 Loss 2.7851 Accuracy 0.5051\n",
            "Epoch 4 Loss 2.7849 Accuracy 0.5052\n",
            "Time taken for 1 epoch: 25.634037494659424 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 2.6433 Accuracy 0.5314\n",
            "Epoch 5 Batch 50 Loss 2.7455 Accuracy 0.5075\n",
            "Epoch 5 Batch 100 Loss 2.7438 Accuracy 0.5078\n",
            "Epoch 5 Batch 150 Loss 2.7480 Accuracy 0.5067\n",
            "Epoch 5 Batch 200 Loss 2.7508 Accuracy 0.5063\n",
            "Epoch 5 Batch 250 Loss 2.7483 Accuracy 0.5068\n",
            "Epoch 5 Batch 300 Loss 2.7558 Accuracy 0.5060\n",
            "Epoch 5 Batch 350 Loss 2.7577 Accuracy 0.5058\n",
            "Epoch 5 Batch 400 Loss 2.7556 Accuracy 0.5061\n",
            "Epoch 5 Batch 450 Loss 2.7538 Accuracy 0.5067\n",
            "Epoch 5 Batch 500 Loss 2.7518 Accuracy 0.5073\n",
            "discarded batch 537\n",
            "Epoch 5 Batch 550 Loss 2.7511 Accuracy 0.5076\n",
            "Epoch 5 Batch 600 Loss 2.7489 Accuracy 0.5082\n",
            "Epoch 5 Batch 650 Loss 2.7475 Accuracy 0.5086\n",
            "Epoch 5 Batch 700 Loss 2.7471 Accuracy 0.5085\n",
            "Epoch 5 Batch 750 Loss 2.7460 Accuracy 0.5085\n",
            "Epoch 5 Batch 800 Loss 2.7441 Accuracy 0.5087\n",
            "Epoch 5 Batch 850 Loss 2.7441 Accuracy 0.5086\n",
            "Epoch 5 Batch 900 Loss 2.7434 Accuracy 0.5087\n",
            "Epoch 5 Batch 950 Loss 2.7434 Accuracy 0.5087\n",
            "Saving checkpoint for epoch 5 at ./checkpoints/train/ckpt-1\n",
            "Epoch 5 Loss 2.7435 Accuracy 0.5087\n",
            "Time taken for 1 epoch: 26.200345993041992 secs\n",
            "\n",
            "Epoch 6 Batch 0 Loss 2.9754 Accuracy 0.4752\n",
            "Epoch 6 Batch 50 Loss 2.7047 Accuracy 0.5101\n",
            "Epoch 6 Batch 100 Loss 2.6980 Accuracy 0.5122\n",
            "Epoch 6 Batch 150 Loss 2.7151 Accuracy 0.5110\n",
            "Epoch 6 Batch 200 Loss 2.7098 Accuracy 0.5116\n",
            "Epoch 6 Batch 250 Loss 2.7049 Accuracy 0.5124\n",
            "Epoch 6 Batch 300 Loss 2.7038 Accuracy 0.5123\n",
            "Epoch 6 Batch 350 Loss 2.7058 Accuracy 0.5123\n",
            "Epoch 6 Batch 400 Loss 2.7031 Accuracy 0.5131\n",
            "discarded batch 408\n",
            "Epoch 6 Batch 450 Loss 2.7036 Accuracy 0.5129\n",
            "Epoch 6 Batch 500 Loss 2.7037 Accuracy 0.5131\n",
            "Epoch 6 Batch 550 Loss 2.7007 Accuracy 0.5130\n",
            "Epoch 6 Batch 600 Loss 2.7020 Accuracy 0.5128\n",
            "Epoch 6 Batch 650 Loss 2.7017 Accuracy 0.5128\n",
            "Epoch 6 Batch 700 Loss 2.6998 Accuracy 0.5133\n",
            "Epoch 6 Batch 750 Loss 2.6998 Accuracy 0.5134\n",
            "Epoch 6 Batch 800 Loss 2.6990 Accuracy 0.5136\n",
            "Epoch 6 Batch 850 Loss 2.6987 Accuracy 0.5138\n",
            "Epoch 6 Batch 900 Loss 2.6990 Accuracy 0.5142\n",
            "Epoch 6 Batch 950 Loss 2.6986 Accuracy 0.5143\n",
            "Epoch 6 Loss 2.6987 Accuracy 0.5142\n",
            "Time taken for 1 epoch: 25.366793632507324 secs\n",
            "\n",
            "Epoch 6 VALIDATION: Loss 2.6447 Accuracy 0.5233\n",
            "\n",
            "Epoch 7 Batch 0 Loss 2.6354 Accuracy 0.5149\n",
            "Epoch 7 Batch 50 Loss 2.6504 Accuracy 0.5195\n",
            "Epoch 7 Batch 100 Loss 2.6607 Accuracy 0.5188\n",
            "Epoch 7 Batch 150 Loss 2.6590 Accuracy 0.5186\n",
            "Epoch 7 Batch 200 Loss 2.6557 Accuracy 0.5186\n",
            "Epoch 7 Batch 250 Loss 2.6588 Accuracy 0.5180\n",
            "Epoch 7 Batch 300 Loss 2.6638 Accuracy 0.5180\n",
            "Epoch 7 Batch 350 Loss 2.6631 Accuracy 0.5184\n",
            "discarded batch 369\n",
            "Epoch 7 Batch 400 Loss 2.6607 Accuracy 0.5187\n",
            "Epoch 7 Batch 450 Loss 2.6596 Accuracy 0.5188\n",
            "Epoch 7 Batch 500 Loss 2.6611 Accuracy 0.5181\n",
            "Epoch 7 Batch 550 Loss 2.6628 Accuracy 0.5182\n",
            "Epoch 7 Batch 600 Loss 2.6638 Accuracy 0.5178\n",
            "Epoch 7 Batch 650 Loss 2.6636 Accuracy 0.5178\n",
            "Epoch 7 Batch 700 Loss 2.6655 Accuracy 0.5177\n",
            "Epoch 7 Batch 750 Loss 2.6657 Accuracy 0.5177\n",
            "Epoch 7 Batch 800 Loss 2.6643 Accuracy 0.5182\n",
            "Epoch 7 Batch 850 Loss 2.6623 Accuracy 0.5182\n",
            "Epoch 7 Batch 900 Loss 2.6620 Accuracy 0.5182\n",
            "Epoch 7 Batch 950 Loss 2.6622 Accuracy 0.5181\n",
            "Epoch 7 Loss 2.6622 Accuracy 0.5181\n",
            "Time taken for 1 epoch: 25.540205001831055 secs\n",
            "\n",
            "Epoch 8 Batch 0 Loss 2.2959 Accuracy 0.5578\n",
            "Epoch 8 Batch 50 Loss 2.6145 Accuracy 0.5268\n",
            "Epoch 8 Batch 100 Loss 2.6316 Accuracy 0.5209\n",
            "Epoch 8 Batch 150 Loss 2.6306 Accuracy 0.5225\n",
            "Epoch 8 Batch 200 Loss 2.6325 Accuracy 0.5225\n",
            "Epoch 8 Batch 250 Loss 2.6347 Accuracy 0.5228\n",
            "Epoch 8 Batch 300 Loss 2.6379 Accuracy 0.5221\n",
            "Epoch 8 Batch 350 Loss 2.6352 Accuracy 0.5221\n",
            "Epoch 8 Batch 400 Loss 2.6324 Accuracy 0.5226\n",
            "Epoch 8 Batch 450 Loss 2.6311 Accuracy 0.5224\n",
            "Epoch 8 Batch 500 Loss 2.6321 Accuracy 0.5226\n",
            "Epoch 8 Batch 550 Loss 2.6304 Accuracy 0.5226\n",
            "Epoch 8 Batch 600 Loss 2.6316 Accuracy 0.5224\n",
            "Epoch 8 Batch 650 Loss 2.6322 Accuracy 0.5223\n",
            "Epoch 8 Batch 700 Loss 2.6351 Accuracy 0.5221\n",
            "Epoch 8 Batch 750 Loss 2.6371 Accuracy 0.5216\n",
            "Epoch 8 Batch 800 Loss 2.6367 Accuracy 0.5220\n",
            "Epoch 8 Batch 850 Loss 2.6355 Accuracy 0.5222\n",
            "Epoch 8 Batch 900 Loss 2.6330 Accuracy 0.5225\n",
            "discarded batch 908\n",
            "Epoch 8 Batch 950 Loss 2.6312 Accuracy 0.5227\n",
            "Epoch 8 Loss 2.6310 Accuracy 0.5227\n",
            "Time taken for 1 epoch: 25.560179471969604 secs\n",
            "\n",
            "Epoch 9 Batch 0 Loss 2.4581 Accuracy 0.5446\n",
            "Epoch 9 Batch 50 Loss 2.5878 Accuracy 0.5253\n",
            "Epoch 9 Batch 100 Loss 2.5890 Accuracy 0.5265\n",
            "Epoch 9 Batch 150 Loss 2.5942 Accuracy 0.5257\n",
            "Epoch 9 Batch 200 Loss 2.5964 Accuracy 0.5256\n",
            "discarded batch 234\n",
            "Epoch 9 Batch 250 Loss 2.5970 Accuracy 0.5251\n",
            "Epoch 9 Batch 300 Loss 2.5992 Accuracy 0.5253\n",
            "Epoch 9 Batch 350 Loss 2.6023 Accuracy 0.5246\n",
            "Epoch 9 Batch 400 Loss 2.6046 Accuracy 0.5244\n",
            "Epoch 9 Batch 450 Loss 2.6039 Accuracy 0.5240\n",
            "Epoch 9 Batch 500 Loss 2.6025 Accuracy 0.5238\n",
            "Epoch 9 Batch 550 Loss 2.6013 Accuracy 0.5240\n",
            "Epoch 9 Batch 600 Loss 2.6019 Accuracy 0.5241\n",
            "Epoch 9 Batch 650 Loss 2.6015 Accuracy 0.5245\n",
            "Epoch 9 Batch 700 Loss 2.6018 Accuracy 0.5245\n",
            "Epoch 9 Batch 750 Loss 2.6028 Accuracy 0.5245\n",
            "Epoch 9 Batch 800 Loss 2.6030 Accuracy 0.5249\n",
            "Epoch 9 Batch 850 Loss 2.6017 Accuracy 0.5248\n",
            "Epoch 9 Batch 900 Loss 2.6029 Accuracy 0.5247\n",
            "Epoch 9 Batch 950 Loss 2.6037 Accuracy 0.5248\n",
            "Epoch 9 Loss 2.6041 Accuracy 0.5248\n",
            "Time taken for 1 epoch: 25.671457052230835 secs\n",
            "\n",
            "Epoch 10 Batch 0 Loss 2.5528 Accuracy 0.5083\n",
            "Epoch 10 Batch 50 Loss 2.5861 Accuracy 0.5259\n",
            "Epoch 10 Batch 100 Loss 2.5768 Accuracy 0.5288\n",
            "Epoch 10 Batch 150 Loss 2.5766 Accuracy 0.5279\n",
            "Epoch 10 Batch 200 Loss 2.5839 Accuracy 0.5270\n",
            "Epoch 10 Batch 250 Loss 2.5817 Accuracy 0.5269\n",
            "Epoch 10 Batch 300 Loss 2.5798 Accuracy 0.5267\n",
            "Epoch 10 Batch 350 Loss 2.5807 Accuracy 0.5260\n",
            "Epoch 10 Batch 400 Loss 2.5805 Accuracy 0.5267\n",
            "Epoch 10 Batch 450 Loss 2.5789 Accuracy 0.5273\n",
            "Epoch 10 Batch 500 Loss 2.5794 Accuracy 0.5273\n",
            "discarded batch 508\n",
            "Epoch 10 Batch 550 Loss 2.5803 Accuracy 0.5271\n",
            "Epoch 10 Batch 600 Loss 2.5797 Accuracy 0.5271\n",
            "Epoch 10 Batch 650 Loss 2.5793 Accuracy 0.5272\n",
            "Epoch 10 Batch 700 Loss 2.5794 Accuracy 0.5269\n",
            "Epoch 10 Batch 750 Loss 2.5811 Accuracy 0.5269\n",
            "Epoch 10 Batch 800 Loss 2.5823 Accuracy 0.5267\n",
            "Epoch 10 Batch 850 Loss 2.5835 Accuracy 0.5266\n",
            "Epoch 10 Batch 900 Loss 2.5846 Accuracy 0.5265\n",
            "Epoch 10 Batch 950 Loss 2.5821 Accuracy 0.5268\n",
            "Saving checkpoint for epoch 10 at ./checkpoints/train/ckpt-2\n",
            "Epoch 10 Loss 2.5820 Accuracy 0.5269\n",
            "Time taken for 1 epoch: 26.038271188735962 secs\n",
            "\n",
            "Epoch 11 Batch 0 Loss 2.4652 Accuracy 0.5644\n",
            "Epoch 11 Batch 50 Loss 2.5713 Accuracy 0.5319\n",
            "Epoch 11 Batch 100 Loss 2.5644 Accuracy 0.5308\n",
            "Epoch 11 Batch 150 Loss 2.5559 Accuracy 0.5311\n",
            "Epoch 11 Batch 200 Loss 2.5620 Accuracy 0.5302\n",
            "Epoch 11 Batch 250 Loss 2.5662 Accuracy 0.5292\n",
            "Epoch 11 Batch 300 Loss 2.5640 Accuracy 0.5294\n",
            "Epoch 11 Batch 350 Loss 2.5646 Accuracy 0.5294\n",
            "Epoch 11 Batch 400 Loss 2.5592 Accuracy 0.5301\n",
            "Epoch 11 Batch 450 Loss 2.5576 Accuracy 0.5301\n",
            "Epoch 11 Batch 500 Loss 2.5581 Accuracy 0.5300\n",
            "Epoch 11 Batch 550 Loss 2.5575 Accuracy 0.5301\n",
            "Epoch 11 Batch 600 Loss 2.5554 Accuracy 0.5305\n",
            "Epoch 11 Batch 650 Loss 2.5554 Accuracy 0.5306\n",
            "Epoch 11 Batch 700 Loss 2.5576 Accuracy 0.5301\n",
            "discarded batch 704\n",
            "Epoch 11 Batch 750 Loss 2.5600 Accuracy 0.5300\n",
            "Epoch 11 Batch 800 Loss 2.5609 Accuracy 0.5299\n",
            "Epoch 11 Batch 850 Loss 2.5618 Accuracy 0.5296\n",
            "Epoch 11 Batch 900 Loss 2.5619 Accuracy 0.5296\n",
            "Epoch 11 Batch 950 Loss 2.5614 Accuracy 0.5298\n",
            "Epoch 11 Loss 2.5615 Accuracy 0.5298\n",
            "Time taken for 1 epoch: 24.803723573684692 secs\n",
            "\n",
            "Epoch 11 VALIDATION: Loss 2.5566 Accuracy 0.5362\n",
            "\n",
            "Epoch 12 Batch 0 Loss 2.4620 Accuracy 0.5545\n",
            "Epoch 12 Batch 50 Loss 2.5230 Accuracy 0.5341\n",
            "Epoch 12 Batch 100 Loss 2.5472 Accuracy 0.5319\n",
            "Epoch 12 Batch 150 Loss 2.5430 Accuracy 0.5316\n",
            "discarded batch 176\n",
            "Epoch 12 Batch 200 Loss 2.5480 Accuracy 0.5308\n",
            "Epoch 12 Batch 250 Loss 2.5479 Accuracy 0.5303\n",
            "Epoch 12 Batch 300 Loss 2.5416 Accuracy 0.5319\n",
            "Epoch 12 Batch 350 Loss 2.5418 Accuracy 0.5315\n",
            "Epoch 12 Batch 400 Loss 2.5412 Accuracy 0.5323\n",
            "Epoch 12 Batch 450 Loss 2.5409 Accuracy 0.5326\n",
            "Epoch 12 Batch 500 Loss 2.5386 Accuracy 0.5328\n",
            "Epoch 12 Batch 550 Loss 2.5401 Accuracy 0.5327\n",
            "Epoch 12 Batch 600 Loss 2.5422 Accuracy 0.5325\n",
            "Epoch 12 Batch 650 Loss 2.5412 Accuracy 0.5325\n",
            "Epoch 12 Batch 700 Loss 2.5409 Accuracy 0.5326\n",
            "Epoch 12 Batch 750 Loss 2.5405 Accuracy 0.5325\n",
            "Epoch 12 Batch 800 Loss 2.5423 Accuracy 0.5319\n",
            "Epoch 12 Batch 850 Loss 2.5424 Accuracy 0.5316\n",
            "Epoch 12 Batch 900 Loss 2.5439 Accuracy 0.5314\n",
            "Epoch 12 Batch 950 Loss 2.5451 Accuracy 0.5314\n",
            "Epoch 12 Loss 2.5453 Accuracy 0.5313\n",
            "Time taken for 1 epoch: 24.607744932174683 secs\n",
            "\n",
            "Epoch 13 Batch 0 Loss 2.4250 Accuracy 0.5644\n",
            "Epoch 13 Batch 50 Loss 2.5425 Accuracy 0.5326\n",
            "Epoch 13 Batch 100 Loss 2.5437 Accuracy 0.5308\n",
            "Epoch 13 Batch 150 Loss 2.5300 Accuracy 0.5310\n",
            "Epoch 13 Batch 200 Loss 2.5246 Accuracy 0.5315\n",
            "Epoch 13 Batch 250 Loss 2.5294 Accuracy 0.5317\n",
            "Epoch 13 Batch 300 Loss 2.5268 Accuracy 0.5322\n",
            "Epoch 13 Batch 350 Loss 2.5255 Accuracy 0.5324\n",
            "Epoch 13 Batch 400 Loss 2.5267 Accuracy 0.5324\n",
            "Epoch 13 Batch 450 Loss 2.5293 Accuracy 0.5322\n",
            "Epoch 13 Batch 500 Loss 2.5295 Accuracy 0.5323\n",
            "discarded batch 514\n",
            "Epoch 13 Batch 550 Loss 2.5303 Accuracy 0.5320\n",
            "Epoch 13 Batch 600 Loss 2.5302 Accuracy 0.5320\n",
            "Epoch 13 Batch 650 Loss 2.5303 Accuracy 0.5321\n",
            "Epoch 13 Batch 700 Loss 2.5306 Accuracy 0.5321\n",
            "Epoch 13 Batch 750 Loss 2.5289 Accuracy 0.5323\n",
            "Epoch 13 Batch 800 Loss 2.5301 Accuracy 0.5321\n",
            "Epoch 13 Batch 850 Loss 2.5298 Accuracy 0.5322\n",
            "Epoch 13 Batch 900 Loss 2.5282 Accuracy 0.5323\n",
            "Epoch 13 Batch 950 Loss 2.5302 Accuracy 0.5321\n",
            "Epoch 13 Loss 2.5305 Accuracy 0.5321\n",
            "Time taken for 1 epoch: 24.69539713859558 secs\n",
            "\n",
            "Epoch 14 Batch 0 Loss 2.3593 Accuracy 0.5215\n",
            "Epoch 14 Batch 50 Loss 2.5029 Accuracy 0.5360\n",
            "Epoch 14 Batch 100 Loss 2.5065 Accuracy 0.5353\n",
            "Epoch 14 Batch 150 Loss 2.5130 Accuracy 0.5339\n",
            "Epoch 14 Batch 200 Loss 2.5102 Accuracy 0.5339\n",
            "Epoch 14 Batch 250 Loss 2.5100 Accuracy 0.5339\n",
            "Epoch 14 Batch 300 Loss 2.5118 Accuracy 0.5339\n",
            "Epoch 14 Batch 350 Loss 2.5141 Accuracy 0.5335\n",
            "Epoch 14 Batch 400 Loss 2.5131 Accuracy 0.5339\n",
            "discarded batch 429\n",
            "Epoch 14 Batch 450 Loss 2.5122 Accuracy 0.5336\n",
            "Epoch 14 Batch 500 Loss 2.5119 Accuracy 0.5334\n",
            "Epoch 14 Batch 550 Loss 2.5148 Accuracy 0.5333\n",
            "Epoch 14 Batch 600 Loss 2.5162 Accuracy 0.5332\n",
            "Epoch 14 Batch 650 Loss 2.5165 Accuracy 0.5332\n",
            "Epoch 14 Batch 700 Loss 2.5159 Accuracy 0.5331\n",
            "Epoch 14 Batch 750 Loss 2.5180 Accuracy 0.5331\n",
            "Epoch 14 Batch 800 Loss 2.5172 Accuracy 0.5333\n",
            "Epoch 14 Batch 850 Loss 2.5186 Accuracy 0.5332\n",
            "Epoch 14 Batch 900 Loss 2.5185 Accuracy 0.5335\n",
            "Epoch 14 Batch 950 Loss 2.5181 Accuracy 0.5334\n",
            "Epoch 14 Loss 2.5179 Accuracy 0.5334\n",
            "Time taken for 1 epoch: 24.40257215499878 secs\n",
            "\n",
            "Epoch 15 Batch 0 Loss 2.4601 Accuracy 0.5281\n",
            "Epoch 15 Batch 50 Loss 2.4898 Accuracy 0.5314\n",
            "Epoch 15 Batch 100 Loss 2.4927 Accuracy 0.5337\n",
            "Epoch 15 Batch 150 Loss 2.4994 Accuracy 0.5335\n",
            "Epoch 15 Batch 200 Loss 2.4971 Accuracy 0.5342\n",
            "Epoch 15 Batch 250 Loss 2.4954 Accuracy 0.5341\n",
            "Epoch 15 Batch 300 Loss 2.4982 Accuracy 0.5338\n",
            "Epoch 15 Batch 350 Loss 2.4983 Accuracy 0.5344\n",
            "Epoch 15 Batch 400 Loss 2.4974 Accuracy 0.5346\n",
            "discarded batch 410\n",
            "Epoch 15 Batch 450 Loss 2.4993 Accuracy 0.5347\n",
            "Epoch 15 Batch 500 Loss 2.5008 Accuracy 0.5343\n",
            "Epoch 15 Batch 550 Loss 2.5026 Accuracy 0.5343\n",
            "Epoch 15 Batch 600 Loss 2.5063 Accuracy 0.5338\n",
            "Epoch 15 Batch 650 Loss 2.5055 Accuracy 0.5340\n",
            "Epoch 15 Batch 700 Loss 2.5040 Accuracy 0.5343\n",
            "Epoch 15 Batch 750 Loss 2.5053 Accuracy 0.5341\n",
            "Epoch 15 Batch 800 Loss 2.5049 Accuracy 0.5343\n",
            "Epoch 15 Batch 850 Loss 2.5055 Accuracy 0.5342\n",
            "Epoch 15 Batch 900 Loss 2.5061 Accuracy 0.5341\n",
            "Epoch 15 Batch 950 Loss 2.5057 Accuracy 0.5344\n",
            "Saving checkpoint for epoch 15 at ./checkpoints/train/ckpt-3\n",
            "Epoch 15 Loss 2.5056 Accuracy 0.5344\n",
            "Time taken for 1 epoch: 24.761680841445923 secs\n",
            "\n",
            "Epoch 16 Batch 0 Loss 2.2866 Accuracy 0.5578\n",
            "discarded batch 9\n",
            "Epoch 16 Batch 50 Loss 2.4711 Accuracy 0.5383\n",
            "Epoch 16 Batch 100 Loss 2.4794 Accuracy 0.5375\n",
            "Epoch 16 Batch 150 Loss 2.4744 Accuracy 0.5372\n",
            "Epoch 16 Batch 200 Loss 2.4825 Accuracy 0.5366\n",
            "Epoch 16 Batch 250 Loss 2.4846 Accuracy 0.5362\n",
            "Epoch 16 Batch 300 Loss 2.4925 Accuracy 0.5355\n",
            "Epoch 16 Batch 350 Loss 2.4913 Accuracy 0.5360\n",
            "Epoch 16 Batch 400 Loss 2.4925 Accuracy 0.5357\n",
            "Epoch 16 Batch 450 Loss 2.4914 Accuracy 0.5358\n",
            "Epoch 16 Batch 500 Loss 2.4923 Accuracy 0.5357\n",
            "Epoch 16 Batch 550 Loss 2.4934 Accuracy 0.5356\n",
            "Epoch 16 Batch 600 Loss 2.4927 Accuracy 0.5357\n",
            "Epoch 16 Batch 650 Loss 2.4935 Accuracy 0.5355\n",
            "Epoch 16 Batch 700 Loss 2.4928 Accuracy 0.5357\n",
            "Epoch 16 Batch 750 Loss 2.4929 Accuracy 0.5355\n",
            "Epoch 16 Batch 800 Loss 2.4931 Accuracy 0.5353\n",
            "Epoch 16 Batch 850 Loss 2.4925 Accuracy 0.5355\n",
            "Epoch 16 Batch 900 Loss 2.4910 Accuracy 0.5356\n",
            "Epoch 16 Batch 950 Loss 2.4921 Accuracy 0.5355\n",
            "Epoch 16 Loss 2.4916 Accuracy 0.5356\n",
            "Time taken for 1 epoch: 24.426297664642334 secs\n",
            "\n",
            "Epoch 16 VALIDATION: Loss 2.4675 Accuracy 0.5400\n",
            "\n",
            "Epoch 17 Batch 0 Loss 2.2592 Accuracy 0.5611\n",
            "Epoch 17 Batch 50 Loss 2.4666 Accuracy 0.5394\n",
            "Epoch 17 Batch 100 Loss 2.4530 Accuracy 0.5390\n",
            "discarded batch 117\n",
            "Epoch 17 Batch 150 Loss 2.4578 Accuracy 0.5374\n",
            "Epoch 17 Batch 200 Loss 2.4657 Accuracy 0.5369\n",
            "Epoch 17 Batch 250 Loss 2.4656 Accuracy 0.5373\n",
            "Epoch 17 Batch 300 Loss 2.4712 Accuracy 0.5376\n",
            "Epoch 17 Batch 350 Loss 2.4709 Accuracy 0.5378\n",
            "Epoch 17 Batch 400 Loss 2.4722 Accuracy 0.5375\n",
            "Epoch 17 Batch 450 Loss 2.4744 Accuracy 0.5371\n",
            "Epoch 17 Batch 500 Loss 2.4736 Accuracy 0.5373\n",
            "Epoch 17 Batch 550 Loss 2.4741 Accuracy 0.5368\n",
            "Epoch 17 Batch 600 Loss 2.4753 Accuracy 0.5365\n",
            "Epoch 17 Batch 650 Loss 2.4782 Accuracy 0.5363\n",
            "Epoch 17 Batch 700 Loss 2.4802 Accuracy 0.5360\n",
            "Epoch 17 Batch 750 Loss 2.4800 Accuracy 0.5361\n",
            "Epoch 17 Batch 800 Loss 2.4812 Accuracy 0.5361\n",
            "Epoch 17 Batch 850 Loss 2.4818 Accuracy 0.5359\n",
            "Epoch 17 Batch 900 Loss 2.4790 Accuracy 0.5363\n",
            "Epoch 17 Batch 950 Loss 2.4779 Accuracy 0.5366\n",
            "Epoch 17 Loss 2.4780 Accuracy 0.5366\n",
            "Time taken for 1 epoch: 24.341983318328857 secs\n",
            "\n",
            "Epoch 18 Batch 0 Loss 2.3309 Accuracy 0.5347\n",
            "Epoch 18 Batch 50 Loss 2.4638 Accuracy 0.5392\n",
            "Epoch 18 Batch 100 Loss 2.4651 Accuracy 0.5363\n",
            "Epoch 18 Batch 150 Loss 2.4559 Accuracy 0.5372\n",
            "discarded batch 183\n",
            "Epoch 18 Batch 200 Loss 2.4529 Accuracy 0.5373\n",
            "Epoch 18 Batch 250 Loss 2.4514 Accuracy 0.5387\n",
            "Epoch 18 Batch 300 Loss 2.4489 Accuracy 0.5384\n",
            "Epoch 18 Batch 350 Loss 2.4512 Accuracy 0.5381\n",
            "Epoch 18 Batch 400 Loss 2.4503 Accuracy 0.5381\n",
            "Epoch 18 Batch 450 Loss 2.4499 Accuracy 0.5387\n",
            "Epoch 18 Batch 500 Loss 2.4519 Accuracy 0.5388\n",
            "Epoch 18 Batch 550 Loss 2.4542 Accuracy 0.5389\n",
            "Epoch 18 Batch 600 Loss 2.4537 Accuracy 0.5388\n",
            "Epoch 18 Batch 650 Loss 2.4549 Accuracy 0.5385\n",
            "Epoch 18 Batch 700 Loss 2.4552 Accuracy 0.5385\n",
            "Epoch 18 Batch 750 Loss 2.4559 Accuracy 0.5387\n",
            "Epoch 18 Batch 800 Loss 2.4583 Accuracy 0.5382\n",
            "Epoch 18 Batch 850 Loss 2.4594 Accuracy 0.5380\n",
            "Epoch 18 Batch 900 Loss 2.4617 Accuracy 0.5380\n",
            "Epoch 18 Batch 950 Loss 2.4625 Accuracy 0.5380\n",
            "Epoch 18 Loss 2.4630 Accuracy 0.5380\n",
            "Time taken for 1 epoch: 24.422404766082764 secs\n",
            "\n",
            "Epoch 19 Batch 0 Loss 2.3510 Accuracy 0.5314\n",
            "Epoch 19 Batch 50 Loss 2.3999 Accuracy 0.5424\n",
            "Epoch 19 Batch 100 Loss 2.4218 Accuracy 0.5406\n",
            "Epoch 19 Batch 150 Loss 2.4237 Accuracy 0.5420\n",
            "Epoch 19 Batch 200 Loss 2.4304 Accuracy 0.5413\n",
            "discarded batch 216\n",
            "Epoch 19 Batch 250 Loss 2.4293 Accuracy 0.5418\n",
            "Epoch 19 Batch 300 Loss 2.4346 Accuracy 0.5408\n",
            "Epoch 19 Batch 350 Loss 2.4380 Accuracy 0.5404\n",
            "Epoch 19 Batch 400 Loss 2.4385 Accuracy 0.5405\n",
            "Epoch 19 Batch 450 Loss 2.4422 Accuracy 0.5400\n",
            "Epoch 19 Batch 500 Loss 2.4403 Accuracy 0.5403\n",
            "Epoch 19 Batch 550 Loss 2.4387 Accuracy 0.5403\n",
            "Epoch 19 Batch 600 Loss 2.4407 Accuracy 0.5404\n",
            "Epoch 19 Batch 650 Loss 2.4436 Accuracy 0.5400\n",
            "Epoch 19 Batch 700 Loss 2.4464 Accuracy 0.5398\n",
            "Epoch 19 Batch 750 Loss 2.4461 Accuracy 0.5398\n",
            "Epoch 19 Batch 800 Loss 2.4470 Accuracy 0.5398\n",
            "Epoch 19 Batch 850 Loss 2.4488 Accuracy 0.5394\n",
            "Epoch 19 Batch 900 Loss 2.4507 Accuracy 0.5391\n",
            "Epoch 19 Batch 950 Loss 2.4497 Accuracy 0.5393\n",
            "Epoch 19 Loss 2.4498 Accuracy 0.5393\n",
            "Time taken for 1 epoch: 24.19725251197815 secs\n",
            "\n",
            "Epoch 20 Batch 0 Loss 2.4782 Accuracy 0.5446\n",
            "Epoch 20 Batch 50 Loss 2.4554 Accuracy 0.5354\n",
            "Epoch 20 Batch 100 Loss 2.4416 Accuracy 0.5373\n",
            "Epoch 20 Batch 150 Loss 2.4364 Accuracy 0.5389\n",
            "Epoch 20 Batch 200 Loss 2.4311 Accuracy 0.5396\n",
            "Epoch 20 Batch 250 Loss 2.4359 Accuracy 0.5394\n",
            "discarded batch 264\n",
            "Epoch 20 Batch 300 Loss 2.4406 Accuracy 0.5397\n",
            "Epoch 20 Batch 350 Loss 2.4437 Accuracy 0.5393\n",
            "Epoch 20 Batch 400 Loss 2.4368 Accuracy 0.5402\n",
            "Epoch 20 Batch 450 Loss 2.4354 Accuracy 0.5405\n",
            "Epoch 20 Batch 500 Loss 2.4357 Accuracy 0.5407\n",
            "Epoch 20 Batch 550 Loss 2.4365 Accuracy 0.5406\n",
            "Epoch 20 Batch 600 Loss 2.4340 Accuracy 0.5412\n",
            "Epoch 20 Batch 650 Loss 2.4364 Accuracy 0.5410\n",
            "Epoch 20 Batch 700 Loss 2.4382 Accuracy 0.5405\n",
            "Epoch 20 Batch 750 Loss 2.4369 Accuracy 0.5406\n",
            "Epoch 20 Batch 800 Loss 2.4368 Accuracy 0.5407\n",
            "Epoch 20 Batch 850 Loss 2.4377 Accuracy 0.5405\n",
            "Epoch 20 Batch 900 Loss 2.4399 Accuracy 0.5403\n",
            "Epoch 20 Batch 950 Loss 2.4396 Accuracy 0.5405\n",
            "Saving checkpoint for epoch 20 at ./checkpoints/train/ckpt-4\n",
            "Epoch 20 Loss 2.4393 Accuracy 0.5405\n",
            "Time taken for 1 epoch: 24.607117176055908 secs\n",
            "\n",
            "Epoch 21 Batch 0 Loss 2.3003 Accuracy 0.5743\n",
            "Epoch 21 Batch 50 Loss 2.4238 Accuracy 0.5449\n",
            "Epoch 21 Batch 100 Loss 2.4247 Accuracy 0.5420\n",
            "discarded batch 138\n",
            "Epoch 21 Batch 150 Loss 2.4264 Accuracy 0.5417\n",
            "Epoch 21 Batch 200 Loss 2.4254 Accuracy 0.5420\n",
            "Epoch 21 Batch 250 Loss 2.4274 Accuracy 0.5421\n",
            "Epoch 21 Batch 300 Loss 2.4181 Accuracy 0.5432\n",
            "Epoch 21 Batch 350 Loss 2.4211 Accuracy 0.5423\n",
            "Epoch 21 Batch 400 Loss 2.4219 Accuracy 0.5420\n",
            "Epoch 21 Batch 450 Loss 2.4232 Accuracy 0.5420\n",
            "Epoch 21 Batch 500 Loss 2.4220 Accuracy 0.5421\n",
            "Epoch 21 Batch 550 Loss 2.4216 Accuracy 0.5422\n",
            "Epoch 21 Batch 600 Loss 2.4230 Accuracy 0.5419\n",
            "Epoch 21 Batch 650 Loss 2.4264 Accuracy 0.5413\n",
            "Epoch 21 Batch 700 Loss 2.4274 Accuracy 0.5412\n",
            "Epoch 21 Batch 750 Loss 2.4273 Accuracy 0.5412\n",
            "Epoch 21 Batch 800 Loss 2.4282 Accuracy 0.5412\n",
            "Epoch 21 Batch 850 Loss 2.4274 Accuracy 0.5414\n",
            "Epoch 21 Batch 900 Loss 2.4275 Accuracy 0.5416\n",
            "Epoch 21 Batch 950 Loss 2.4278 Accuracy 0.5417\n",
            "Epoch 21 Loss 2.4279 Accuracy 0.5417\n",
            "Time taken for 1 epoch: 24.150582790374756 secs\n",
            "\n",
            "Epoch 21 VALIDATION: Loss 2.4085 Accuracy 0.5482\n",
            "\n",
            "Epoch 22 Batch 0 Loss 2.3745 Accuracy 0.5512\n",
            "Epoch 22 Batch 50 Loss 2.4183 Accuracy 0.5363\n",
            "Epoch 22 Batch 100 Loss 2.4122 Accuracy 0.5399\n",
            "Epoch 22 Batch 150 Loss 2.4170 Accuracy 0.5400\n",
            "Epoch 22 Batch 200 Loss 2.4164 Accuracy 0.5395\n",
            "Epoch 22 Batch 250 Loss 2.4204 Accuracy 0.5398\n",
            "Epoch 22 Batch 300 Loss 2.4179 Accuracy 0.5400\n",
            "Epoch 22 Batch 350 Loss 2.4139 Accuracy 0.5410\n",
            "Epoch 22 Batch 400 Loss 2.4149 Accuracy 0.5417\n",
            "Epoch 22 Batch 450 Loss 2.4144 Accuracy 0.5414\n",
            "Epoch 22 Batch 500 Loss 2.4125 Accuracy 0.5417\n",
            "Epoch 22 Batch 550 Loss 2.4140 Accuracy 0.5414\n",
            "Epoch 22 Batch 600 Loss 2.4148 Accuracy 0.5414\n",
            "Epoch 22 Batch 650 Loss 2.4153 Accuracy 0.5411\n",
            "Epoch 22 Batch 700 Loss 2.4158 Accuracy 0.5412\n",
            "Epoch 22 Batch 750 Loss 2.4152 Accuracy 0.5414\n",
            "Epoch 22 Batch 800 Loss 2.4156 Accuracy 0.5416\n",
            "Epoch 22 Batch 850 Loss 2.4149 Accuracy 0.5417\n",
            "discarded batch 861\n",
            "Epoch 22 Batch 900 Loss 2.4142 Accuracy 0.5420\n",
            "Epoch 22 Batch 950 Loss 2.4141 Accuracy 0.5423\n",
            "Epoch 22 Loss 2.4143 Accuracy 0.5423\n",
            "Time taken for 1 epoch: 23.996095895767212 secs\n",
            "\n",
            "Epoch 23 Batch 0 Loss 2.3738 Accuracy 0.5380\n",
            "Epoch 23 Batch 50 Loss 2.3823 Accuracy 0.5463\n",
            "Epoch 23 Batch 100 Loss 2.3917 Accuracy 0.5446\n",
            "Epoch 23 Batch 150 Loss 2.3892 Accuracy 0.5451\n",
            "discarded batch 160\n",
            "Epoch 23 Batch 200 Loss 2.3930 Accuracy 0.5454\n",
            "Epoch 23 Batch 250 Loss 2.3944 Accuracy 0.5454\n",
            "Epoch 23 Batch 300 Loss 2.4006 Accuracy 0.5439\n",
            "Epoch 23 Batch 350 Loss 2.4005 Accuracy 0.5443\n",
            "Epoch 23 Batch 400 Loss 2.4009 Accuracy 0.5439\n",
            "Epoch 23 Batch 450 Loss 2.3993 Accuracy 0.5442\n",
            "Epoch 23 Batch 500 Loss 2.3976 Accuracy 0.5447\n",
            "Epoch 23 Batch 550 Loss 2.4009 Accuracy 0.5443\n",
            "Epoch 23 Batch 600 Loss 2.4034 Accuracy 0.5440\n",
            "Epoch 23 Batch 650 Loss 2.4024 Accuracy 0.5440\n",
            "Epoch 23 Batch 700 Loss 2.4012 Accuracy 0.5440\n",
            "Epoch 23 Batch 750 Loss 2.4032 Accuracy 0.5438\n",
            "Epoch 23 Batch 800 Loss 2.4047 Accuracy 0.5437\n",
            "Epoch 23 Batch 850 Loss 2.4033 Accuracy 0.5436\n",
            "Epoch 23 Batch 900 Loss 2.4034 Accuracy 0.5438\n",
            "Epoch 23 Batch 950 Loss 2.4036 Accuracy 0.5438\n",
            "Epoch 23 Loss 2.4037 Accuracy 0.5438\n",
            "Time taken for 1 epoch: 23.991630792617798 secs\n",
            "\n",
            "Epoch 24 Batch 0 Loss 2.2286 Accuracy 0.5875\n",
            "Epoch 24 Batch 50 Loss 2.3767 Accuracy 0.5464\n",
            "Epoch 24 Batch 100 Loss 2.3722 Accuracy 0.5477\n",
            "Epoch 24 Batch 150 Loss 2.3769 Accuracy 0.5468\n",
            "Epoch 24 Batch 200 Loss 2.3749 Accuracy 0.5465\n",
            "Epoch 24 Batch 250 Loss 2.3829 Accuracy 0.5465\n",
            "Epoch 24 Batch 300 Loss 2.3851 Accuracy 0.5462\n",
            "discarded batch 345\n",
            "Epoch 24 Batch 350 Loss 2.3891 Accuracy 0.5458\n",
            "Epoch 24 Batch 400 Loss 2.3858 Accuracy 0.5464\n",
            "Epoch 24 Batch 450 Loss 2.3846 Accuracy 0.5462\n",
            "Epoch 24 Batch 500 Loss 2.3860 Accuracy 0.5455\n",
            "Epoch 24 Batch 550 Loss 2.3862 Accuracy 0.5455\n",
            "Epoch 24 Batch 600 Loss 2.3874 Accuracy 0.5454\n",
            "Epoch 24 Batch 650 Loss 2.3887 Accuracy 0.5456\n",
            "Epoch 24 Batch 700 Loss 2.3887 Accuracy 0.5454\n",
            "Epoch 24 Batch 750 Loss 2.3887 Accuracy 0.5455\n",
            "Epoch 24 Batch 800 Loss 2.3892 Accuracy 0.5456\n",
            "Epoch 24 Batch 850 Loss 2.3892 Accuracy 0.5454\n",
            "Epoch 24 Batch 900 Loss 2.3902 Accuracy 0.5452\n",
            "Epoch 24 Batch 950 Loss 2.3902 Accuracy 0.5452\n",
            "Epoch 24 Loss 2.3907 Accuracy 0.5452\n",
            "Time taken for 1 epoch: 23.882744312286377 secs\n",
            "\n",
            "Epoch 25 Batch 0 Loss 2.3574 Accuracy 0.5710\n",
            "Epoch 25 Batch 50 Loss 2.3571 Accuracy 0.5473\n",
            "Epoch 25 Batch 100 Loss 2.3723 Accuracy 0.5445\n",
            "Epoch 25 Batch 150 Loss 2.3733 Accuracy 0.5448\n",
            "Epoch 25 Batch 200 Loss 2.3722 Accuracy 0.5447\n",
            "Epoch 25 Batch 250 Loss 2.3680 Accuracy 0.5463\n",
            "Epoch 25 Batch 300 Loss 2.3680 Accuracy 0.5465\n",
            "Epoch 25 Batch 350 Loss 2.3676 Accuracy 0.5464\n",
            "discarded batch 358\n",
            "Epoch 25 Batch 400 Loss 2.3690 Accuracy 0.5466\n",
            "Epoch 25 Batch 450 Loss 2.3690 Accuracy 0.5466\n",
            "Epoch 25 Batch 500 Loss 2.3700 Accuracy 0.5467\n",
            "Epoch 25 Batch 550 Loss 2.3699 Accuracy 0.5463\n",
            "Epoch 25 Batch 600 Loss 2.3711 Accuracy 0.5463\n",
            "Epoch 25 Batch 650 Loss 2.3717 Accuracy 0.5464\n",
            "Epoch 25 Batch 700 Loss 2.3737 Accuracy 0.5463\n",
            "Epoch 25 Batch 750 Loss 2.3755 Accuracy 0.5464\n",
            "Epoch 25 Batch 800 Loss 2.3759 Accuracy 0.5465\n",
            "Epoch 25 Batch 850 Loss 2.3772 Accuracy 0.5465\n",
            "Epoch 25 Batch 900 Loss 2.3777 Accuracy 0.5464\n",
            "Epoch 25 Batch 950 Loss 2.3791 Accuracy 0.5465\n",
            "Saving checkpoint for epoch 25 at ./checkpoints/train/ckpt-5\n",
            "Epoch 25 Loss 2.3792 Accuracy 0.5465\n",
            "Time taken for 1 epoch: 24.21572256088257 secs\n",
            "\n",
            "Epoch 26 Batch 0 Loss 2.4573 Accuracy 0.5314\n",
            "Epoch 26 Batch 50 Loss 2.3582 Accuracy 0.5479\n",
            "Epoch 26 Batch 100 Loss 2.3601 Accuracy 0.5478\n",
            "Epoch 26 Batch 150 Loss 2.3658 Accuracy 0.5478\n",
            "Epoch 26 Batch 200 Loss 2.3661 Accuracy 0.5478\n",
            "Epoch 26 Batch 250 Loss 2.3673 Accuracy 0.5480\n",
            "Epoch 26 Batch 300 Loss 2.3646 Accuracy 0.5481\n",
            "Epoch 26 Batch 350 Loss 2.3664 Accuracy 0.5483\n",
            "Epoch 26 Batch 400 Loss 2.3670 Accuracy 0.5486\n",
            "Epoch 26 Batch 450 Loss 2.3645 Accuracy 0.5485\n",
            "Epoch 26 Batch 500 Loss 2.3647 Accuracy 0.5485\n",
            "Epoch 26 Batch 550 Loss 2.3674 Accuracy 0.5484\n",
            "Epoch 26 Batch 600 Loss 2.3658 Accuracy 0.5486\n",
            "Epoch 26 Batch 650 Loss 2.3664 Accuracy 0.5482\n",
            "Epoch 26 Batch 700 Loss 2.3659 Accuracy 0.5482\n",
            "Epoch 26 Batch 750 Loss 2.3657 Accuracy 0.5484\n",
            "Epoch 26 Batch 800 Loss 2.3653 Accuracy 0.5483\n",
            "discarded batch 837\n",
            "Epoch 26 Batch 850 Loss 2.3662 Accuracy 0.5483\n",
            "Epoch 26 Batch 900 Loss 2.3688 Accuracy 0.5478\n",
            "Epoch 26 Batch 950 Loss 2.3686 Accuracy 0.5479\n",
            "Epoch 26 Loss 2.3684 Accuracy 0.5479\n",
            "Time taken for 1 epoch: 24.14201831817627 secs\n",
            "\n",
            "Epoch 26 VALIDATION: Loss 2.3603 Accuracy 0.5547\n",
            "\n",
            "Epoch 27 Batch 0 Loss 2.3039 Accuracy 0.5446\n",
            "Epoch 27 Batch 50 Loss 2.3355 Accuracy 0.5480\n",
            "Epoch 27 Batch 100 Loss 2.3529 Accuracy 0.5472\n",
            "Epoch 27 Batch 150 Loss 2.3598 Accuracy 0.5480\n",
            "Epoch 27 Batch 200 Loss 2.3561 Accuracy 0.5483\n",
            "Epoch 27 Batch 250 Loss 2.3540 Accuracy 0.5484\n",
            "Epoch 27 Batch 300 Loss 2.3521 Accuracy 0.5480\n",
            "Epoch 27 Batch 350 Loss 2.3522 Accuracy 0.5485\n",
            "Epoch 27 Batch 400 Loss 2.3527 Accuracy 0.5486\n",
            "Epoch 27 Batch 450 Loss 2.3516 Accuracy 0.5488\n",
            "discarded batch 477\n",
            "Epoch 27 Batch 500 Loss 2.3548 Accuracy 0.5485\n",
            "Epoch 27 Batch 550 Loss 2.3583 Accuracy 0.5481\n",
            "Epoch 27 Batch 600 Loss 2.3585 Accuracy 0.5483\n",
            "Epoch 27 Batch 650 Loss 2.3582 Accuracy 0.5481\n",
            "Epoch 27 Batch 700 Loss 2.3575 Accuracy 0.5486\n",
            "Epoch 27 Batch 750 Loss 2.3575 Accuracy 0.5487\n",
            "Epoch 27 Batch 800 Loss 2.3592 Accuracy 0.5486\n",
            "Epoch 27 Batch 850 Loss 2.3590 Accuracy 0.5487\n",
            "Epoch 27 Batch 900 Loss 2.3593 Accuracy 0.5485\n",
            "Epoch 27 Batch 950 Loss 2.3583 Accuracy 0.5485\n",
            "Epoch 27 Loss 2.3582 Accuracy 0.5485\n",
            "Time taken for 1 epoch: 24.0204496383667 secs\n",
            "\n",
            "Epoch 28 Batch 0 Loss 2.3132 Accuracy 0.5380\n",
            "Epoch 28 Batch 50 Loss 2.3153 Accuracy 0.5505\n",
            "Epoch 28 Batch 100 Loss 2.3320 Accuracy 0.5496\n",
            "Epoch 28 Batch 150 Loss 2.3340 Accuracy 0.5508\n",
            "Epoch 28 Batch 200 Loss 2.3401 Accuracy 0.5499\n",
            "Epoch 28 Batch 250 Loss 2.3395 Accuracy 0.5501\n",
            "Epoch 28 Batch 300 Loss 2.3420 Accuracy 0.5500\n",
            "Epoch 28 Batch 350 Loss 2.3430 Accuracy 0.5495\n",
            "Epoch 28 Batch 400 Loss 2.3444 Accuracy 0.5496\n",
            "Epoch 28 Batch 450 Loss 2.3430 Accuracy 0.5503\n",
            "Epoch 28 Batch 500 Loss 2.3417 Accuracy 0.5504\n",
            "Epoch 28 Batch 550 Loss 2.3424 Accuracy 0.5506\n",
            "Epoch 28 Batch 600 Loss 2.3423 Accuracy 0.5507\n",
            "Epoch 28 Batch 650 Loss 2.3413 Accuracy 0.5512\n",
            "Epoch 28 Batch 700 Loss 2.3440 Accuracy 0.5511\n",
            "Epoch 28 Batch 750 Loss 2.3446 Accuracy 0.5510\n",
            "Epoch 28 Batch 800 Loss 2.3447 Accuracy 0.5509\n",
            "Epoch 28 Batch 850 Loss 2.3442 Accuracy 0.5509\n",
            "discarded batch 854\n",
            "Epoch 28 Batch 900 Loss 2.3448 Accuracy 0.5509\n",
            "Epoch 28 Batch 950 Loss 2.3467 Accuracy 0.5508\n",
            "Epoch 28 Loss 2.3467 Accuracy 0.5508\n",
            "Time taken for 1 epoch: 24.12346053123474 secs\n",
            "\n",
            "Epoch 29 Batch 0 Loss 2.3652 Accuracy 0.5677\n",
            "Epoch 29 Batch 50 Loss 2.2941 Accuracy 0.5539\n",
            "Epoch 29 Batch 100 Loss 2.3069 Accuracy 0.5537\n",
            "Epoch 29 Batch 150 Loss 2.3067 Accuracy 0.5541\n",
            "Epoch 29 Batch 200 Loss 2.3142 Accuracy 0.5538\n",
            "Epoch 29 Batch 250 Loss 2.3222 Accuracy 0.5533\n",
            "Epoch 29 Batch 300 Loss 2.3250 Accuracy 0.5531\n",
            "Epoch 29 Batch 350 Loss 2.3262 Accuracy 0.5526\n",
            "Epoch 29 Batch 400 Loss 2.3291 Accuracy 0.5529\n",
            "Epoch 29 Batch 450 Loss 2.3283 Accuracy 0.5531\n",
            "Epoch 29 Batch 500 Loss 2.3283 Accuracy 0.5529\n",
            "Epoch 29 Batch 550 Loss 2.3291 Accuracy 0.5525\n",
            "Epoch 29 Batch 600 Loss 2.3280 Accuracy 0.5530\n",
            "Epoch 29 Batch 650 Loss 2.3298 Accuracy 0.5528\n",
            "Epoch 29 Batch 700 Loss 2.3309 Accuracy 0.5530\n",
            "Epoch 29 Batch 750 Loss 2.3308 Accuracy 0.5529\n",
            "Epoch 29 Batch 800 Loss 2.3321 Accuracy 0.5528\n",
            "Epoch 29 Batch 850 Loss 2.3330 Accuracy 0.5525\n",
            "discarded batch 862\n",
            "Epoch 29 Batch 900 Loss 2.3334 Accuracy 0.5526\n",
            "Epoch 29 Batch 950 Loss 2.3332 Accuracy 0.5526\n",
            "Epoch 29 Loss 2.3332 Accuracy 0.5526\n",
            "Time taken for 1 epoch: 23.988481760025024 secs\n",
            "\n",
            "Epoch 30 Batch 0 Loss 2.2396 Accuracy 0.5611\n",
            "Epoch 30 Batch 50 Loss 2.3290 Accuracy 0.5519\n",
            "Epoch 30 Batch 100 Loss 2.3183 Accuracy 0.5549\n",
            "Epoch 30 Batch 150 Loss 2.3187 Accuracy 0.5547\n",
            "Epoch 30 Batch 200 Loss 2.3210 Accuracy 0.5534\n",
            "Epoch 30 Batch 250 Loss 2.3196 Accuracy 0.5535\n",
            "Epoch 30 Batch 300 Loss 2.3212 Accuracy 0.5527\n",
            "Epoch 30 Batch 350 Loss 2.3179 Accuracy 0.5530\n",
            "Epoch 30 Batch 400 Loss 2.3240 Accuracy 0.5522\n",
            "Epoch 30 Batch 450 Loss 2.3189 Accuracy 0.5530\n",
            "Epoch 30 Batch 500 Loss 2.3187 Accuracy 0.5530\n",
            "discarded batch 507\n",
            "Epoch 30 Batch 550 Loss 2.3205 Accuracy 0.5530\n",
            "Epoch 30 Batch 600 Loss 2.3245 Accuracy 0.5531\n",
            "Epoch 30 Batch 650 Loss 2.3251 Accuracy 0.5533\n",
            "Epoch 30 Batch 700 Loss 2.3234 Accuracy 0.5533\n",
            "Epoch 30 Batch 750 Loss 2.3224 Accuracy 0.5536\n",
            "Epoch 30 Batch 800 Loss 2.3228 Accuracy 0.5534\n",
            "Epoch 30 Batch 850 Loss 2.3239 Accuracy 0.5534\n",
            "Epoch 30 Batch 900 Loss 2.3229 Accuracy 0.5538\n",
            "Epoch 30 Batch 950 Loss 2.3242 Accuracy 0.5538\n",
            "Saving checkpoint for epoch 30 at ./checkpoints/train/ckpt-6\n",
            "Epoch 30 Loss 2.3241 Accuracy 0.5538\n",
            "Time taken for 1 epoch: 24.89116597175598 secs\n",
            "\n",
            "Epoch 31 Batch 0 Loss 2.2892 Accuracy 0.5512\n",
            "Epoch 31 Batch 50 Loss 2.2989 Accuracy 0.5577\n",
            "Epoch 31 Batch 100 Loss 2.2933 Accuracy 0.5563\n",
            "Epoch 31 Batch 150 Loss 2.2892 Accuracy 0.5564\n",
            "Epoch 31 Batch 200 Loss 2.3018 Accuracy 0.5557\n",
            "Epoch 31 Batch 250 Loss 2.3030 Accuracy 0.5557\n",
            "Epoch 31 Batch 300 Loss 2.3026 Accuracy 0.5562\n",
            "Epoch 31 Batch 350 Loss 2.3056 Accuracy 0.5559\n",
            "discarded batch 361\n",
            "Epoch 31 Batch 400 Loss 2.3064 Accuracy 0.5564\n",
            "Epoch 31 Batch 450 Loss 2.3092 Accuracy 0.5560\n",
            "Epoch 31 Batch 500 Loss 2.3095 Accuracy 0.5561\n",
            "Epoch 31 Batch 550 Loss 2.3102 Accuracy 0.5561\n",
            "Epoch 31 Batch 600 Loss 2.3091 Accuracy 0.5562\n",
            "Epoch 31 Batch 650 Loss 2.3109 Accuracy 0.5561\n",
            "Epoch 31 Batch 700 Loss 2.3097 Accuracy 0.5561\n",
            "Epoch 31 Batch 750 Loss 2.3112 Accuracy 0.5558\n",
            "Epoch 31 Batch 800 Loss 2.3112 Accuracy 0.5558\n",
            "Epoch 31 Batch 850 Loss 2.3103 Accuracy 0.5558\n",
            "Epoch 31 Batch 900 Loss 2.3109 Accuracy 0.5558\n",
            "Epoch 31 Batch 950 Loss 2.3127 Accuracy 0.5557\n",
            "Epoch 31 Loss 2.3127 Accuracy 0.5557\n",
            "Time taken for 1 epoch: 24.325727462768555 secs\n",
            "\n",
            "Epoch 31 VALIDATION: Loss 2.3171 Accuracy 0.5593\n",
            "\n",
            "Epoch 32 Batch 0 Loss 2.4421 Accuracy 0.5512\n",
            "discarded batch 40\n",
            "Epoch 32 Batch 50 Loss 2.2861 Accuracy 0.5579\n",
            "Epoch 32 Batch 100 Loss 2.2810 Accuracy 0.5574\n",
            "Epoch 32 Batch 150 Loss 2.2921 Accuracy 0.5563\n",
            "Epoch 32 Batch 200 Loss 2.2928 Accuracy 0.5562\n",
            "Epoch 32 Batch 250 Loss 2.2948 Accuracy 0.5562\n",
            "Epoch 32 Batch 300 Loss 2.2933 Accuracy 0.5570\n",
            "Epoch 32 Batch 350 Loss 2.2948 Accuracy 0.5566\n",
            "Epoch 32 Batch 400 Loss 2.2956 Accuracy 0.5567\n",
            "Epoch 32 Batch 450 Loss 2.2963 Accuracy 0.5565\n",
            "Epoch 32 Batch 500 Loss 2.2989 Accuracy 0.5565\n",
            "Epoch 32 Batch 550 Loss 2.2999 Accuracy 0.5562\n",
            "Epoch 32 Batch 600 Loss 2.2997 Accuracy 0.5563\n",
            "Epoch 32 Batch 650 Loss 2.2993 Accuracy 0.5565\n",
            "Epoch 32 Batch 700 Loss 2.3001 Accuracy 0.5565\n",
            "Epoch 32 Batch 750 Loss 2.3000 Accuracy 0.5563\n",
            "Epoch 32 Batch 800 Loss 2.3002 Accuracy 0.5565\n",
            "Epoch 32 Batch 850 Loss 2.3019 Accuracy 0.5565\n",
            "Epoch 32 Batch 900 Loss 2.3030 Accuracy 0.5564\n",
            "Epoch 32 Batch 950 Loss 2.3038 Accuracy 0.5563\n",
            "Epoch 32 Loss 2.3039 Accuracy 0.5563\n",
            "Time taken for 1 epoch: 24.0812885761261 secs\n",
            "\n",
            "Epoch 33 Batch 0 Loss 2.2309 Accuracy 0.5611\n",
            "Epoch 33 Batch 50 Loss 2.2722 Accuracy 0.5576\n",
            "Epoch 33 Batch 100 Loss 2.2817 Accuracy 0.5580\n",
            "Epoch 33 Batch 150 Loss 2.2785 Accuracy 0.5583\n",
            "Epoch 33 Batch 200 Loss 2.2782 Accuracy 0.5576\n",
            "Epoch 33 Batch 250 Loss 2.2767 Accuracy 0.5581\n",
            "Epoch 33 Batch 300 Loss 2.2814 Accuracy 0.5571\n",
            "Epoch 33 Batch 350 Loss 2.2814 Accuracy 0.5566\n",
            "Epoch 33 Batch 400 Loss 2.2836 Accuracy 0.5567\n",
            "Epoch 33 Batch 450 Loss 2.2841 Accuracy 0.5569\n",
            "Epoch 33 Batch 500 Loss 2.2870 Accuracy 0.5568\n",
            "Epoch 33 Batch 550 Loss 2.2892 Accuracy 0.5567\n",
            "Epoch 33 Batch 600 Loss 2.2910 Accuracy 0.5568\n",
            "Epoch 33 Batch 650 Loss 2.2926 Accuracy 0.5568\n",
            "Epoch 33 Batch 700 Loss 2.2939 Accuracy 0.5567\n",
            "Epoch 33 Batch 750 Loss 2.2934 Accuracy 0.5568\n",
            "Epoch 33 Batch 800 Loss 2.2918 Accuracy 0.5569\n",
            "Epoch 33 Batch 850 Loss 2.2931 Accuracy 0.5568\n",
            "discarded batch 889\n",
            "Epoch 33 Batch 900 Loss 2.2935 Accuracy 0.5568\n",
            "Epoch 33 Batch 950 Loss 2.2939 Accuracy 0.5568\n",
            "Epoch 33 Loss 2.2940 Accuracy 0.5569\n",
            "Time taken for 1 epoch: 24.128921270370483 secs\n",
            "\n",
            "Epoch 34 Batch 0 Loss 2.2038 Accuracy 0.5776\n",
            "Epoch 34 Batch 50 Loss 2.2636 Accuracy 0.5629\n",
            "Epoch 34 Batch 100 Loss 2.2702 Accuracy 0.5616\n",
            "Epoch 34 Batch 150 Loss 2.2744 Accuracy 0.5609\n",
            "Epoch 34 Batch 200 Loss 2.2750 Accuracy 0.5599\n",
            "Epoch 34 Batch 250 Loss 2.2780 Accuracy 0.5589\n",
            "Epoch 34 Batch 300 Loss 2.2786 Accuracy 0.5587\n",
            "Epoch 34 Batch 350 Loss 2.2830 Accuracy 0.5578\n",
            "Epoch 34 Batch 400 Loss 2.2830 Accuracy 0.5575\n",
            "Epoch 34 Batch 450 Loss 2.2846 Accuracy 0.5576\n",
            "Epoch 34 Batch 500 Loss 2.2870 Accuracy 0.5577\n",
            "Epoch 34 Batch 550 Loss 2.2866 Accuracy 0.5575\n",
            "Epoch 34 Batch 600 Loss 2.2873 Accuracy 0.5575\n",
            "Epoch 34 Batch 650 Loss 2.2862 Accuracy 0.5577\n",
            "Epoch 34 Batch 700 Loss 2.2855 Accuracy 0.5578\n",
            "Epoch 34 Batch 750 Loss 2.2841 Accuracy 0.5579\n",
            "discarded batch 799\n",
            "Epoch 34 Batch 800 Loss 2.2835 Accuracy 0.5581\n",
            "Epoch 34 Batch 850 Loss 2.2841 Accuracy 0.5581\n",
            "Epoch 34 Batch 900 Loss 2.2857 Accuracy 0.5581\n",
            "Epoch 34 Batch 950 Loss 2.2837 Accuracy 0.5583\n",
            "Epoch 34 Loss 2.2837 Accuracy 0.5583\n",
            "Time taken for 1 epoch: 23.95704746246338 secs\n",
            "\n",
            "Epoch 35 Batch 0 Loss 2.1508 Accuracy 0.5611\n",
            "Epoch 35 Batch 50 Loss 2.2633 Accuracy 0.5606\n",
            "Epoch 35 Batch 100 Loss 2.2580 Accuracy 0.5611\n",
            "Epoch 35 Batch 150 Loss 2.2426 Accuracy 0.5632\n",
            "Epoch 35 Batch 200 Loss 2.2522 Accuracy 0.5619\n",
            "Epoch 35 Batch 250 Loss 2.2594 Accuracy 0.5614\n",
            "Epoch 35 Batch 300 Loss 2.2594 Accuracy 0.5611\n",
            "Epoch 35 Batch 350 Loss 2.2615 Accuracy 0.5606\n",
            "Epoch 35 Batch 400 Loss 2.2602 Accuracy 0.5609\n",
            "Epoch 35 Batch 450 Loss 2.2632 Accuracy 0.5608\n",
            "Epoch 35 Batch 500 Loss 2.2686 Accuracy 0.5604\n",
            "Epoch 35 Batch 550 Loss 2.2695 Accuracy 0.5602\n",
            "discarded batch 598\n",
            "Epoch 35 Batch 600 Loss 2.2709 Accuracy 0.5601\n",
            "Epoch 35 Batch 650 Loss 2.2717 Accuracy 0.5602\n",
            "Epoch 35 Batch 700 Loss 2.2725 Accuracy 0.5602\n",
            "Epoch 35 Batch 750 Loss 2.2739 Accuracy 0.5600\n",
            "Epoch 35 Batch 800 Loss 2.2723 Accuracy 0.5602\n",
            "Epoch 35 Batch 850 Loss 2.2721 Accuracy 0.5600\n",
            "Epoch 35 Batch 900 Loss 2.2735 Accuracy 0.5600\n",
            "Epoch 35 Batch 950 Loss 2.2741 Accuracy 0.5599\n",
            "Saving checkpoint for epoch 35 at ./checkpoints/train/ckpt-7\n",
            "Epoch 35 Loss 2.2745 Accuracy 0.5598\n",
            "Time taken for 1 epoch: 24.729914903640747 secs\n",
            "\n",
            "Epoch 36 Batch 0 Loss 2.1581 Accuracy 0.5941\n",
            "Epoch 36 Batch 50 Loss 2.2529 Accuracy 0.5606\n",
            "Epoch 36 Batch 100 Loss 2.2456 Accuracy 0.5612\n",
            "Epoch 36 Batch 150 Loss 2.2481 Accuracy 0.5606\n",
            "Epoch 36 Batch 200 Loss 2.2487 Accuracy 0.5611\n",
            "Epoch 36 Batch 250 Loss 2.2525 Accuracy 0.5614\n",
            "Epoch 36 Batch 300 Loss 2.2517 Accuracy 0.5619\n",
            "Epoch 36 Batch 350 Loss 2.2497 Accuracy 0.5623\n",
            "Epoch 36 Batch 400 Loss 2.2540 Accuracy 0.5621\n",
            "Epoch 36 Batch 450 Loss 2.2564 Accuracy 0.5617\n",
            "Epoch 36 Batch 500 Loss 2.2597 Accuracy 0.5613\n",
            "Epoch 36 Batch 550 Loss 2.2607 Accuracy 0.5613\n",
            "Epoch 36 Batch 600 Loss 2.2639 Accuracy 0.5610\n",
            "Epoch 36 Batch 650 Loss 2.2644 Accuracy 0.5613\n",
            "Epoch 36 Batch 700 Loss 2.2658 Accuracy 0.5611\n",
            "Epoch 36 Batch 750 Loss 2.2664 Accuracy 0.5612\n",
            "discarded batch 775\n",
            "Epoch 36 Batch 800 Loss 2.2680 Accuracy 0.5607\n",
            "Epoch 36 Batch 850 Loss 2.2668 Accuracy 0.5608\n",
            "Epoch 36 Batch 900 Loss 2.2688 Accuracy 0.5604\n",
            "Epoch 36 Batch 950 Loss 2.2682 Accuracy 0.5604\n",
            "Epoch 36 Loss 2.2684 Accuracy 0.5604\n",
            "Time taken for 1 epoch: 24.235649585723877 secs\n",
            "\n",
            "Epoch 36 VALIDATION: Loss 2.2801 Accuracy 0.5655\n",
            "\n",
            "Epoch 37 Batch 0 Loss 2.1137 Accuracy 0.5875\n",
            "Epoch 37 Batch 50 Loss 2.2445 Accuracy 0.5644\n",
            "Epoch 37 Batch 100 Loss 2.2490 Accuracy 0.5626\n",
            "Epoch 37 Batch 150 Loss 2.2519 Accuracy 0.5610\n",
            "Epoch 37 Batch 200 Loss 2.2445 Accuracy 0.5623\n",
            "Epoch 37 Batch 250 Loss 2.2453 Accuracy 0.5620\n",
            "Epoch 37 Batch 300 Loss 2.2520 Accuracy 0.5610\n",
            "Epoch 37 Batch 350 Loss 2.2540 Accuracy 0.5615\n",
            "Epoch 37 Batch 400 Loss 2.2546 Accuracy 0.5614\n",
            "Epoch 37 Batch 450 Loss 2.2535 Accuracy 0.5613\n",
            "Epoch 37 Batch 500 Loss 2.2527 Accuracy 0.5613\n",
            "Epoch 37 Batch 550 Loss 2.2546 Accuracy 0.5613\n",
            "Epoch 37 Batch 600 Loss 2.2570 Accuracy 0.5614\n",
            "Epoch 37 Batch 650 Loss 2.2561 Accuracy 0.5614\n",
            "Epoch 37 Batch 700 Loss 2.2574 Accuracy 0.5613\n",
            "Epoch 37 Batch 750 Loss 2.2592 Accuracy 0.5612\n",
            "Epoch 37 Batch 800 Loss 2.2585 Accuracy 0.5613\n",
            "discarded batch 848\n",
            "Epoch 37 Batch 850 Loss 2.2596 Accuracy 0.5613\n",
            "Epoch 37 Batch 900 Loss 2.2589 Accuracy 0.5613\n",
            "Epoch 37 Batch 950 Loss 2.2596 Accuracy 0.5612\n",
            "Epoch 37 Loss 2.2597 Accuracy 0.5612\n",
            "Time taken for 1 epoch: 24.18920135498047 secs\n",
            "\n",
            "Epoch 38 Batch 0 Loss 2.3183 Accuracy 0.5446\n",
            "Epoch 38 Batch 50 Loss 2.2553 Accuracy 0.5588\n",
            "Epoch 38 Batch 100 Loss 2.2312 Accuracy 0.5640\n",
            "Epoch 38 Batch 150 Loss 2.2292 Accuracy 0.5642\n",
            "Epoch 38 Batch 200 Loss 2.2320 Accuracy 0.5640\n",
            "Epoch 38 Batch 250 Loss 2.2294 Accuracy 0.5648\n",
            "Epoch 38 Batch 300 Loss 2.2354 Accuracy 0.5639\n",
            "Epoch 38 Batch 350 Loss 2.2334 Accuracy 0.5642\n",
            "Epoch 38 Batch 400 Loss 2.2364 Accuracy 0.5634\n",
            "Epoch 38 Batch 450 Loss 2.2384 Accuracy 0.5630\n",
            "Epoch 38 Batch 500 Loss 2.2373 Accuracy 0.5631\n",
            "Epoch 38 Batch 550 Loss 2.2401 Accuracy 0.5628\n",
            "Epoch 38 Batch 600 Loss 2.2423 Accuracy 0.5626\n",
            "Epoch 38 Batch 650 Loss 2.2433 Accuracy 0.5626\n",
            "Epoch 38 Batch 700 Loss 2.2435 Accuracy 0.5626\n",
            "Epoch 38 Batch 750 Loss 2.2453 Accuracy 0.5628\n",
            "Epoch 38 Batch 800 Loss 2.2467 Accuracy 0.5625\n",
            "Epoch 38 Batch 850 Loss 2.2482 Accuracy 0.5625\n",
            "Epoch 38 Batch 900 Loss 2.2493 Accuracy 0.5624\n",
            "Epoch 38 Batch 950 Loss 2.2502 Accuracy 0.5626\n",
            "discarded batch 951\n",
            "Epoch 38 Loss 2.2500 Accuracy 0.5626\n",
            "Time taken for 1 epoch: 24.005635023117065 secs\n",
            "\n",
            "Epoch 39 Batch 0 Loss 2.2604 Accuracy 0.5710\n",
            "Epoch 39 Batch 50 Loss 2.2233 Accuracy 0.5649\n",
            "Epoch 39 Batch 100 Loss 2.2237 Accuracy 0.5646\n",
            "Epoch 39 Batch 150 Loss 2.2256 Accuracy 0.5649\n",
            "Epoch 39 Batch 200 Loss 2.2277 Accuracy 0.5651\n",
            "Epoch 39 Batch 250 Loss 2.2341 Accuracy 0.5650\n",
            "Epoch 39 Batch 300 Loss 2.2313 Accuracy 0.5646\n",
            "Epoch 39 Batch 350 Loss 2.2330 Accuracy 0.5642\n",
            "discarded batch 392\n",
            "Epoch 39 Batch 400 Loss 2.2383 Accuracy 0.5641\n",
            "Epoch 39 Batch 450 Loss 2.2377 Accuracy 0.5642\n",
            "Epoch 39 Batch 500 Loss 2.2345 Accuracy 0.5647\n",
            "Epoch 39 Batch 550 Loss 2.2365 Accuracy 0.5645\n",
            "Epoch 39 Batch 600 Loss 2.2365 Accuracy 0.5646\n",
            "Epoch 39 Batch 650 Loss 2.2365 Accuracy 0.5644\n",
            "Epoch 39 Batch 700 Loss 2.2369 Accuracy 0.5642\n",
            "Epoch 39 Batch 750 Loss 2.2393 Accuracy 0.5638\n",
            "Epoch 39 Batch 800 Loss 2.2401 Accuracy 0.5637\n",
            "Epoch 39 Batch 850 Loss 2.2417 Accuracy 0.5635\n",
            "Epoch 39 Batch 900 Loss 2.2429 Accuracy 0.5633\n",
            "Epoch 39 Batch 950 Loss 2.2429 Accuracy 0.5633\n",
            "Epoch 39 Loss 2.2430 Accuracy 0.5633\n",
            "Time taken for 1 epoch: 23.824400424957275 secs\n",
            "\n",
            "Epoch 40 Batch 0 Loss 2.2796 Accuracy 0.5479\n",
            "Epoch 40 Batch 50 Loss 2.2438 Accuracy 0.5601\n",
            "Epoch 40 Batch 100 Loss 2.2327 Accuracy 0.5620\n",
            "Epoch 40 Batch 150 Loss 2.2312 Accuracy 0.5632\n",
            "Epoch 40 Batch 200 Loss 2.2252 Accuracy 0.5634\n",
            "Epoch 40 Batch 250 Loss 2.2260 Accuracy 0.5632\n",
            "Epoch 40 Batch 300 Loss 2.2237 Accuracy 0.5639\n",
            "Epoch 40 Batch 350 Loss 2.2250 Accuracy 0.5640\n",
            "Epoch 40 Batch 400 Loss 2.2274 Accuracy 0.5640\n",
            "Epoch 40 Batch 450 Loss 2.2265 Accuracy 0.5644\n",
            "Epoch 40 Batch 500 Loss 2.2242 Accuracy 0.5651\n",
            "discarded batch 530\n",
            "Epoch 40 Batch 550 Loss 2.2268 Accuracy 0.5649\n",
            "Epoch 40 Batch 600 Loss 2.2291 Accuracy 0.5647\n",
            "Epoch 40 Batch 650 Loss 2.2293 Accuracy 0.5645\n",
            "Epoch 40 Batch 700 Loss 2.2313 Accuracy 0.5640\n",
            "Epoch 40 Batch 750 Loss 2.2308 Accuracy 0.5639\n",
            "Epoch 40 Batch 800 Loss 2.2317 Accuracy 0.5639\n",
            "Epoch 40 Batch 850 Loss 2.2326 Accuracy 0.5638\n",
            "Epoch 40 Batch 900 Loss 2.2356 Accuracy 0.5635\n",
            "Epoch 40 Batch 950 Loss 2.2372 Accuracy 0.5634\n",
            "Saving checkpoint for epoch 40 at ./checkpoints/train/ckpt-8\n",
            "Epoch 40 Loss 2.2374 Accuracy 0.5635\n",
            "Time taken for 1 epoch: 24.315860986709595 secs\n",
            "\n",
            "Epoch 41 Batch 0 Loss 2.1004 Accuracy 0.5710\n",
            "Epoch 41 Batch 50 Loss 2.2116 Accuracy 0.5633\n",
            "Epoch 41 Batch 100 Loss 2.2087 Accuracy 0.5655\n",
            "Epoch 41 Batch 150 Loss 2.2103 Accuracy 0.5653\n",
            "Epoch 41 Batch 200 Loss 2.2118 Accuracy 0.5646\n",
            "Epoch 41 Batch 250 Loss 2.2137 Accuracy 0.5654\n",
            "Epoch 41 Batch 300 Loss 2.2136 Accuracy 0.5658\n",
            "Epoch 41 Batch 350 Loss 2.2146 Accuracy 0.5657\n",
            "Epoch 41 Batch 400 Loss 2.2176 Accuracy 0.5658\n",
            "Epoch 41 Batch 450 Loss 2.2175 Accuracy 0.5657\n",
            "Epoch 41 Batch 500 Loss 2.2191 Accuracy 0.5659\n",
            "discarded batch 523\n",
            "Epoch 41 Batch 550 Loss 2.2198 Accuracy 0.5662\n",
            "Epoch 41 Batch 600 Loss 2.2194 Accuracy 0.5663\n",
            "Epoch 41 Batch 650 Loss 2.2232 Accuracy 0.5657\n",
            "Epoch 41 Batch 700 Loss 2.2261 Accuracy 0.5654\n",
            "Epoch 41 Batch 750 Loss 2.2265 Accuracy 0.5655\n",
            "Epoch 41 Batch 800 Loss 2.2267 Accuracy 0.5656\n",
            "Epoch 41 Batch 850 Loss 2.2246 Accuracy 0.5660\n",
            "Epoch 41 Batch 900 Loss 2.2254 Accuracy 0.5659\n",
            "Epoch 41 Batch 950 Loss 2.2268 Accuracy 0.5656\n",
            "Epoch 41 Loss 2.2277 Accuracy 0.5656\n",
            "Time taken for 1 epoch: 23.82088041305542 secs\n",
            "\n",
            "Epoch 41 VALIDATION: Loss 2.2822 Accuracy 0.5663\n",
            "\n",
            "Epoch 42 Batch 0 Loss 2.2884 Accuracy 0.5644\n",
            "Epoch 42 Batch 50 Loss 2.1769 Accuracy 0.5696\n",
            "Epoch 42 Batch 100 Loss 2.2091 Accuracy 0.5666\n",
            "Epoch 42 Batch 150 Loss 2.2120 Accuracy 0.5670\n",
            "Epoch 42 Batch 200 Loss 2.2140 Accuracy 0.5664\n",
            "Epoch 42 Batch 250 Loss 2.2214 Accuracy 0.5657\n",
            "Epoch 42 Batch 300 Loss 2.2185 Accuracy 0.5672\n",
            "Epoch 42 Batch 350 Loss 2.2210 Accuracy 0.5665\n",
            "Epoch 42 Batch 400 Loss 2.2210 Accuracy 0.5662\n",
            "Epoch 42 Batch 450 Loss 2.2222 Accuracy 0.5660\n",
            "Epoch 42 Batch 500 Loss 2.2226 Accuracy 0.5657\n",
            "Epoch 42 Batch 550 Loss 2.2223 Accuracy 0.5658\n",
            "Epoch 42 Batch 600 Loss 2.2217 Accuracy 0.5659\n",
            "Epoch 42 Batch 650 Loss 2.2211 Accuracy 0.5661\n",
            "Epoch 42 Batch 700 Loss 2.2200 Accuracy 0.5664\n",
            "discarded batch 738\n",
            "Epoch 42 Batch 750 Loss 2.2228 Accuracy 0.5659\n",
            "Epoch 42 Batch 800 Loss 2.2215 Accuracy 0.5660\n",
            "Epoch 42 Batch 850 Loss 2.2221 Accuracy 0.5659\n",
            "Epoch 42 Batch 900 Loss 2.2214 Accuracy 0.5660\n",
            "Epoch 42 Batch 950 Loss 2.2218 Accuracy 0.5659\n",
            "Epoch 42 Loss 2.2217 Accuracy 0.5659\n",
            "Time taken for 1 epoch: 23.89850950241089 secs\n",
            "\n",
            "Epoch 43 Batch 0 Loss 2.2202 Accuracy 0.5644\n",
            "Epoch 43 Batch 50 Loss 2.1917 Accuracy 0.5664\n",
            "Epoch 43 Batch 100 Loss 2.1948 Accuracy 0.5675\n",
            "Epoch 43 Batch 150 Loss 2.2041 Accuracy 0.5671\n",
            "Epoch 43 Batch 200 Loss 2.2004 Accuracy 0.5667\n",
            "Epoch 43 Batch 250 Loss 2.1986 Accuracy 0.5670\n",
            "Epoch 43 Batch 300 Loss 2.1997 Accuracy 0.5675\n",
            "Epoch 43 Batch 350 Loss 2.1997 Accuracy 0.5683\n",
            "discarded batch 400\n",
            "Epoch 43 Batch 450 Loss 2.2004 Accuracy 0.5685\n",
            "Epoch 43 Batch 500 Loss 2.2031 Accuracy 0.5683\n",
            "Epoch 43 Batch 550 Loss 2.2061 Accuracy 0.5681\n",
            "Epoch 43 Batch 600 Loss 2.2062 Accuracy 0.5680\n",
            "Epoch 43 Batch 650 Loss 2.2077 Accuracy 0.5680\n",
            "Epoch 43 Batch 700 Loss 2.2077 Accuracy 0.5680\n",
            "Epoch 43 Batch 750 Loss 2.2092 Accuracy 0.5676\n",
            "Epoch 43 Batch 800 Loss 2.2128 Accuracy 0.5672\n",
            "Epoch 43 Batch 850 Loss 2.2129 Accuracy 0.5672\n",
            "Epoch 43 Batch 900 Loss 2.2137 Accuracy 0.5671\n",
            "Epoch 43 Batch 950 Loss 2.2143 Accuracy 0.5671\n",
            "Epoch 43 Loss 2.2142 Accuracy 0.5671\n",
            "Time taken for 1 epoch: 23.936317920684814 secs\n",
            "\n",
            "Epoch 44 Batch 0 Loss 2.3285 Accuracy 0.5578\n",
            "Epoch 44 Batch 50 Loss 2.1954 Accuracy 0.5663\n",
            "Epoch 44 Batch 100 Loss 2.2023 Accuracy 0.5651\n",
            "Epoch 44 Batch 150 Loss 2.2084 Accuracy 0.5659\n",
            "Epoch 44 Batch 200 Loss 2.2000 Accuracy 0.5672\n",
            "Epoch 44 Batch 250 Loss 2.2015 Accuracy 0.5675\n",
            "Epoch 44 Batch 300 Loss 2.2070 Accuracy 0.5672\n",
            "Epoch 44 Batch 350 Loss 2.2066 Accuracy 0.5674\n",
            "Epoch 44 Batch 400 Loss 2.2048 Accuracy 0.5679\n",
            "Epoch 44 Batch 450 Loss 2.2021 Accuracy 0.5682\n",
            "Epoch 44 Batch 500 Loss 2.2018 Accuracy 0.5685\n",
            "Epoch 44 Batch 550 Loss 2.2037 Accuracy 0.5682\n",
            "Epoch 44 Batch 600 Loss 2.2028 Accuracy 0.5686\n",
            "Epoch 44 Batch 650 Loss 2.2028 Accuracy 0.5685\n",
            "Epoch 44 Batch 700 Loss 2.2055 Accuracy 0.5683\n",
            "Epoch 44 Batch 750 Loss 2.2069 Accuracy 0.5682\n",
            "Epoch 44 Batch 800 Loss 2.2075 Accuracy 0.5678\n",
            "Epoch 44 Batch 850 Loss 2.2080 Accuracy 0.5679\n",
            "Epoch 44 Batch 900 Loss 2.2076 Accuracy 0.5678\n",
            "discarded batch 910\n",
            "Epoch 44 Batch 950 Loss 2.2088 Accuracy 0.5677\n",
            "Epoch 44 Loss 2.2086 Accuracy 0.5677\n",
            "Time taken for 1 epoch: 24.110575437545776 secs\n",
            "\n",
            "Epoch 45 Batch 0 Loss 2.0841 Accuracy 0.5875\n",
            "Epoch 45 Batch 50 Loss 2.1721 Accuracy 0.5686\n",
            "Epoch 45 Batch 100 Loss 2.1781 Accuracy 0.5692\n",
            "Epoch 45 Batch 150 Loss 2.1734 Accuracy 0.5701\n",
            "Epoch 45 Batch 200 Loss 2.1805 Accuracy 0.5693\n",
            "Epoch 45 Batch 250 Loss 2.1862 Accuracy 0.5695\n",
            "Epoch 45 Batch 300 Loss 2.1870 Accuracy 0.5694\n",
            "Epoch 45 Batch 350 Loss 2.1884 Accuracy 0.5689\n",
            "Epoch 45 Batch 400 Loss 2.1898 Accuracy 0.5693\n",
            "Epoch 45 Batch 450 Loss 2.1923 Accuracy 0.5692\n",
            "Epoch 45 Batch 500 Loss 2.1936 Accuracy 0.5690\n",
            "Epoch 45 Batch 550 Loss 2.1938 Accuracy 0.5689\n",
            "Epoch 45 Batch 600 Loss 2.1942 Accuracy 0.5691\n",
            "Epoch 45 Batch 650 Loss 2.1946 Accuracy 0.5689\n",
            "Epoch 45 Batch 700 Loss 2.1968 Accuracy 0.5687\n",
            "Epoch 45 Batch 750 Loss 2.1987 Accuracy 0.5686\n",
            "Epoch 45 Batch 800 Loss 2.1993 Accuracy 0.5687\n",
            "discarded batch 836\n",
            "Epoch 45 Batch 850 Loss 2.2001 Accuracy 0.5685\n",
            "Epoch 45 Batch 900 Loss 2.2001 Accuracy 0.5685\n",
            "Epoch 45 Batch 950 Loss 2.2008 Accuracy 0.5687\n",
            "Saving checkpoint for epoch 45 at ./checkpoints/train/ckpt-9\n",
            "Epoch 45 Loss 2.2010 Accuracy 0.5687\n",
            "Time taken for 1 epoch: 24.286882162094116 secs\n",
            "\n",
            "Epoch 46 Batch 0 Loss 2.2048 Accuracy 0.5644\n",
            "Epoch 46 Batch 50 Loss 2.1650 Accuracy 0.5693\n",
            "Epoch 46 Batch 100 Loss 2.1812 Accuracy 0.5690\n",
            "Epoch 46 Batch 150 Loss 2.1814 Accuracy 0.5687\n",
            "discarded batch 153\n",
            "Epoch 46 Batch 200 Loss 2.1886 Accuracy 0.5691\n",
            "Epoch 46 Batch 250 Loss 2.1849 Accuracy 0.5696\n",
            "Epoch 46 Batch 300 Loss 2.1896 Accuracy 0.5696\n",
            "Epoch 46 Batch 350 Loss 2.1895 Accuracy 0.5700\n",
            "Epoch 46 Batch 400 Loss 2.1865 Accuracy 0.5703\n",
            "Epoch 46 Batch 450 Loss 2.1876 Accuracy 0.5703\n",
            "Epoch 46 Batch 500 Loss 2.1887 Accuracy 0.5701\n",
            "Epoch 46 Batch 550 Loss 2.1892 Accuracy 0.5701\n",
            "Epoch 46 Batch 600 Loss 2.1889 Accuracy 0.5701\n",
            "Epoch 46 Batch 650 Loss 2.1897 Accuracy 0.5699\n",
            "Epoch 46 Batch 700 Loss 2.1887 Accuracy 0.5699\n",
            "Epoch 46 Batch 750 Loss 2.1901 Accuracy 0.5696\n",
            "Epoch 46 Batch 800 Loss 2.1907 Accuracy 0.5693\n",
            "Epoch 46 Batch 850 Loss 2.1913 Accuracy 0.5693\n",
            "Epoch 46 Batch 900 Loss 2.1922 Accuracy 0.5693\n",
            "Epoch 46 Batch 950 Loss 2.1932 Accuracy 0.5692\n",
            "Epoch 46 Loss 2.1931 Accuracy 0.5692\n",
            "Time taken for 1 epoch: 23.906972646713257 secs\n",
            "\n",
            "Epoch 46 VALIDATION: Loss 2.2502 Accuracy 0.5728\n",
            "\n",
            "Epoch 47 Batch 0 Loss 2.2961 Accuracy 0.5809\n",
            "Epoch 47 Batch 50 Loss 2.1468 Accuracy 0.5730\n",
            "Epoch 47 Batch 100 Loss 2.1550 Accuracy 0.5716\n",
            "Epoch 47 Batch 150 Loss 2.1674 Accuracy 0.5714\n",
            "Epoch 47 Batch 200 Loss 2.1828 Accuracy 0.5701\n",
            "Epoch 47 Batch 250 Loss 2.1812 Accuracy 0.5699\n",
            "Epoch 47 Batch 300 Loss 2.1804 Accuracy 0.5705\n",
            "Epoch 47 Batch 350 Loss 2.1764 Accuracy 0.5707\n",
            "Epoch 47 Batch 400 Loss 2.1772 Accuracy 0.5702\n",
            "Epoch 47 Batch 450 Loss 2.1790 Accuracy 0.5701\n",
            "Epoch 47 Batch 500 Loss 2.1816 Accuracy 0.5705\n",
            "Epoch 47 Batch 550 Loss 2.1806 Accuracy 0.5708\n",
            "Epoch 47 Batch 600 Loss 2.1821 Accuracy 0.5708\n",
            "Epoch 47 Batch 650 Loss 2.1820 Accuracy 0.5707\n",
            "Epoch 47 Batch 700 Loss 2.1825 Accuracy 0.5706\n",
            "discarded batch 730\n",
            "Epoch 47 Batch 750 Loss 2.1834 Accuracy 0.5704\n",
            "Epoch 47 Batch 800 Loss 2.1859 Accuracy 0.5701\n",
            "Epoch 47 Batch 850 Loss 2.1879 Accuracy 0.5700\n",
            "Epoch 47 Batch 900 Loss 2.1877 Accuracy 0.5701\n",
            "Epoch 47 Batch 950 Loss 2.1902 Accuracy 0.5696\n",
            "Epoch 47 Loss 2.1902 Accuracy 0.5696\n",
            "Time taken for 1 epoch: 23.888710260391235 secs\n",
            "\n",
            "Epoch 48 Batch 0 Loss 2.2623 Accuracy 0.5875\n",
            "Epoch 48 Batch 50 Loss 2.1475 Accuracy 0.5722\n",
            "Epoch 48 Batch 100 Loss 2.1356 Accuracy 0.5749\n",
            "Epoch 48 Batch 150 Loss 2.1434 Accuracy 0.5753\n",
            "Epoch 48 Batch 200 Loss 2.1581 Accuracy 0.5723\n",
            "Epoch 48 Batch 250 Loss 2.1632 Accuracy 0.5719\n",
            "Epoch 48 Batch 300 Loss 2.1677 Accuracy 0.5717\n",
            "Epoch 48 Batch 350 Loss 2.1695 Accuracy 0.5718\n",
            "Epoch 48 Batch 400 Loss 2.1755 Accuracy 0.5716\n",
            "Epoch 48 Batch 450 Loss 2.1747 Accuracy 0.5715\n",
            "Epoch 48 Batch 500 Loss 2.1767 Accuracy 0.5708\n",
            "discarded batch 512\n",
            "Epoch 48 Batch 550 Loss 2.1777 Accuracy 0.5707\n",
            "Epoch 48 Batch 600 Loss 2.1753 Accuracy 0.5711\n",
            "Epoch 48 Batch 650 Loss 2.1788 Accuracy 0.5708\n",
            "Epoch 48 Batch 700 Loss 2.1789 Accuracy 0.5709\n",
            "Epoch 48 Batch 750 Loss 2.1783 Accuracy 0.5708\n",
            "Epoch 48 Batch 800 Loss 2.1802 Accuracy 0.5707\n",
            "Epoch 48 Batch 850 Loss 2.1801 Accuracy 0.5709\n",
            "Epoch 48 Batch 900 Loss 2.1816 Accuracy 0.5709\n",
            "Epoch 48 Batch 950 Loss 2.1816 Accuracy 0.5711\n",
            "Epoch 48 Loss 2.1819 Accuracy 0.5711\n",
            "Time taken for 1 epoch: 23.829081535339355 secs\n",
            "\n",
            "Epoch 49 Batch 0 Loss 2.2055 Accuracy 0.5677\n",
            "Epoch 49 Batch 50 Loss 2.1614 Accuracy 0.5728\n",
            "Epoch 49 Batch 100 Loss 2.1546 Accuracy 0.5746\n",
            "Epoch 49 Batch 150 Loss 2.1547 Accuracy 0.5733\n",
            "Epoch 49 Batch 200 Loss 2.1540 Accuracy 0.5736\n",
            "Epoch 49 Batch 250 Loss 2.1581 Accuracy 0.5738\n",
            "Epoch 49 Batch 300 Loss 2.1589 Accuracy 0.5742\n",
            "Epoch 49 Batch 350 Loss 2.1627 Accuracy 0.5739\n",
            "Epoch 49 Batch 400 Loss 2.1666 Accuracy 0.5734\n",
            "Epoch 49 Batch 450 Loss 2.1660 Accuracy 0.5736\n",
            "Epoch 49 Batch 500 Loss 2.1640 Accuracy 0.5737\n",
            "Epoch 49 Batch 550 Loss 2.1651 Accuracy 0.5733\n",
            "Epoch 49 Batch 600 Loss 2.1664 Accuracy 0.5731\n",
            "Epoch 49 Batch 650 Loss 2.1673 Accuracy 0.5731\n",
            "Epoch 49 Batch 700 Loss 2.1689 Accuracy 0.5730\n",
            "Epoch 49 Batch 750 Loss 2.1695 Accuracy 0.5730\n",
            "Epoch 49 Batch 800 Loss 2.1708 Accuracy 0.5730\n",
            "Epoch 49 Batch 850 Loss 2.1728 Accuracy 0.5726\n",
            "discarded batch 883\n",
            "Epoch 49 Batch 900 Loss 2.1734 Accuracy 0.5724\n",
            "Epoch 49 Batch 950 Loss 2.1764 Accuracy 0.5721\n",
            "Epoch 49 Loss 2.1762 Accuracy 0.5721\n",
            "Time taken for 1 epoch: 23.82539176940918 secs\n",
            "\n",
            "Epoch 50 Batch 0 Loss 2.1625 Accuracy 0.5776\n",
            "Epoch 50 Batch 50 Loss 2.1521 Accuracy 0.5725\n",
            "Epoch 50 Batch 100 Loss 2.1495 Accuracy 0.5747\n",
            "Epoch 50 Batch 150 Loss 2.1407 Accuracy 0.5751\n",
            "Epoch 50 Batch 200 Loss 2.1509 Accuracy 0.5734\n",
            "Epoch 50 Batch 250 Loss 2.1468 Accuracy 0.5739\n",
            "Epoch 50 Batch 300 Loss 2.1476 Accuracy 0.5737\n",
            "Epoch 50 Batch 350 Loss 2.1522 Accuracy 0.5737\n",
            "Epoch 50 Batch 400 Loss 2.1564 Accuracy 0.5732\n",
            "Epoch 50 Batch 450 Loss 2.1571 Accuracy 0.5726\n",
            "discarded batch 490\n",
            "Epoch 50 Batch 500 Loss 2.1597 Accuracy 0.5724\n",
            "Epoch 50 Batch 550 Loss 2.1647 Accuracy 0.5722\n",
            "Epoch 50 Batch 600 Loss 2.1686 Accuracy 0.5719\n",
            "Epoch 50 Batch 650 Loss 2.1679 Accuracy 0.5719\n",
            "Epoch 50 Batch 700 Loss 2.1686 Accuracy 0.5722\n",
            "Epoch 50 Batch 750 Loss 2.1688 Accuracy 0.5721\n",
            "Epoch 50 Batch 800 Loss 2.1700 Accuracy 0.5722\n",
            "Epoch 50 Batch 850 Loss 2.1715 Accuracy 0.5721\n",
            "Epoch 50 Batch 900 Loss 2.1716 Accuracy 0.5722\n",
            "Epoch 50 Batch 950 Loss 2.1717 Accuracy 0.5720\n",
            "Saving checkpoint for epoch 50 at ./checkpoints/train/ckpt-10\n",
            "Epoch 50 Loss 2.1716 Accuracy 0.5720\n",
            "Time taken for 1 epoch: 24.272865772247314 secs\n",
            "\n",
            "Epoch 51 Batch 0 Loss 2.0070 Accuracy 0.5776\n",
            "Epoch 51 Batch 50 Loss 2.1329 Accuracy 0.5764\n",
            "Epoch 51 Batch 100 Loss 2.1440 Accuracy 0.5746\n",
            "Epoch 51 Batch 150 Loss 2.1357 Accuracy 0.5749\n",
            "Epoch 51 Batch 200 Loss 2.1446 Accuracy 0.5736\n",
            "Epoch 51 Batch 250 Loss 2.1394 Accuracy 0.5750\n",
            "Epoch 51 Batch 300 Loss 2.1469 Accuracy 0.5747\n",
            "Epoch 51 Batch 350 Loss 2.1491 Accuracy 0.5744\n",
            "Epoch 51 Batch 400 Loss 2.1560 Accuracy 0.5741\n",
            "Epoch 51 Batch 450 Loss 2.1583 Accuracy 0.5732\n",
            "Epoch 51 Batch 500 Loss 2.1591 Accuracy 0.5734\n",
            "Epoch 51 Batch 550 Loss 2.1614 Accuracy 0.5729\n",
            "Epoch 51 Batch 600 Loss 2.1617 Accuracy 0.5727\n",
            "Epoch 51 Batch 650 Loss 2.1636 Accuracy 0.5725\n",
            "Epoch 51 Batch 700 Loss 2.1646 Accuracy 0.5721\n",
            "Epoch 51 Batch 750 Loss 2.1646 Accuracy 0.5719\n",
            "Epoch 51 Batch 800 Loss 2.1656 Accuracy 0.5719\n",
            "discarded batch 845\n",
            "Epoch 51 Batch 850 Loss 2.1664 Accuracy 0.5719\n",
            "Epoch 51 Batch 900 Loss 2.1656 Accuracy 0.5719\n",
            "Epoch 51 Batch 950 Loss 2.1666 Accuracy 0.5719\n",
            "Epoch 51 Loss 2.1667 Accuracy 0.5719\n",
            "Time taken for 1 epoch: 23.82990336418152 secs\n",
            "\n",
            "Epoch 51 VALIDATION: Loss 2.2379 Accuracy 0.5743\n",
            "\n",
            "Epoch 52 Batch 0 Loss 2.1425 Accuracy 0.5974\n",
            "Epoch 52 Batch 50 Loss 2.1336 Accuracy 0.5767\n",
            "Epoch 52 Batch 100 Loss 2.1423 Accuracy 0.5730\n",
            "Epoch 52 Batch 150 Loss 2.1479 Accuracy 0.5718\n",
            "discarded batch 151\n",
            "Epoch 52 Batch 200 Loss 2.1521 Accuracy 0.5725\n",
            "Epoch 52 Batch 250 Loss 2.1525 Accuracy 0.5727\n",
            "Epoch 52 Batch 300 Loss 2.1548 Accuracy 0.5734\n",
            "Epoch 52 Batch 350 Loss 2.1524 Accuracy 0.5738\n",
            "Epoch 52 Batch 400 Loss 2.1502 Accuracy 0.5740\n",
            "Epoch 52 Batch 450 Loss 2.1550 Accuracy 0.5736\n",
            "Epoch 52 Batch 500 Loss 2.1555 Accuracy 0.5734\n",
            "Epoch 52 Batch 550 Loss 2.1543 Accuracy 0.5735\n",
            "Epoch 52 Batch 600 Loss 2.1552 Accuracy 0.5734\n",
            "Epoch 52 Batch 650 Loss 2.1549 Accuracy 0.5735\n",
            "Epoch 52 Batch 700 Loss 2.1566 Accuracy 0.5732\n",
            "Epoch 52 Batch 750 Loss 2.1567 Accuracy 0.5732\n",
            "Epoch 52 Batch 800 Loss 2.1580 Accuracy 0.5729\n",
            "Epoch 52 Batch 850 Loss 2.1581 Accuracy 0.5729\n",
            "Epoch 52 Batch 900 Loss 2.1598 Accuracy 0.5729\n",
            "Epoch 52 Batch 950 Loss 2.1601 Accuracy 0.5729\n",
            "Epoch 52 Loss 2.1600 Accuracy 0.5730\n",
            "Time taken for 1 epoch: 23.857678413391113 secs\n",
            "\n",
            "Epoch 53 Batch 0 Loss 2.1907 Accuracy 0.5809\n",
            "Epoch 53 Batch 50 Loss 2.1355 Accuracy 0.5783\n",
            "Epoch 53 Batch 100 Loss 2.1388 Accuracy 0.5775\n",
            "discarded batch 143\n",
            "Epoch 53 Batch 150 Loss 2.1372 Accuracy 0.5769\n",
            "Epoch 53 Batch 200 Loss 2.1342 Accuracy 0.5772\n",
            "Epoch 53 Batch 250 Loss 2.1378 Accuracy 0.5761\n",
            "Epoch 53 Batch 300 Loss 2.1415 Accuracy 0.5754\n",
            "Epoch 53 Batch 350 Loss 2.1440 Accuracy 0.5752\n",
            "Epoch 53 Batch 400 Loss 2.1438 Accuracy 0.5749\n",
            "Epoch 53 Batch 450 Loss 2.1407 Accuracy 0.5750\n",
            "Epoch 53 Batch 500 Loss 2.1437 Accuracy 0.5746\n",
            "Epoch 53 Batch 550 Loss 2.1464 Accuracy 0.5745\n",
            "Epoch 53 Batch 600 Loss 2.1492 Accuracy 0.5744\n",
            "Epoch 53 Batch 650 Loss 2.1533 Accuracy 0.5741\n",
            "Epoch 53 Batch 700 Loss 2.1524 Accuracy 0.5742\n",
            "Epoch 53 Batch 750 Loss 2.1518 Accuracy 0.5745\n",
            "Epoch 53 Batch 800 Loss 2.1512 Accuracy 0.5742\n",
            "Epoch 53 Batch 850 Loss 2.1523 Accuracy 0.5741\n",
            "Epoch 53 Batch 900 Loss 2.1539 Accuracy 0.5739\n",
            "Epoch 53 Batch 950 Loss 2.1548 Accuracy 0.5737\n",
            "Epoch 53 Loss 2.1547 Accuracy 0.5737\n",
            "Time taken for 1 epoch: 23.853980541229248 secs\n",
            "\n",
            "Epoch 54 Batch 0 Loss 2.3069 Accuracy 0.5545\n",
            "Epoch 54 Batch 50 Loss 2.1534 Accuracy 0.5763\n",
            "Epoch 54 Batch 100 Loss 2.1578 Accuracy 0.5755\n",
            "Epoch 54 Batch 150 Loss 2.1481 Accuracy 0.5760\n",
            "Epoch 54 Batch 200 Loss 2.1408 Accuracy 0.5765\n",
            "Epoch 54 Batch 250 Loss 2.1419 Accuracy 0.5766\n",
            "Epoch 54 Batch 300 Loss 2.1437 Accuracy 0.5763\n",
            "Epoch 54 Batch 350 Loss 2.1399 Accuracy 0.5767\n",
            "discarded batch 392\n",
            "Epoch 54 Batch 400 Loss 2.1403 Accuracy 0.5762\n",
            "Epoch 54 Batch 450 Loss 2.1405 Accuracy 0.5762\n",
            "Epoch 54 Batch 500 Loss 2.1406 Accuracy 0.5758\n",
            "Epoch 54 Batch 550 Loss 2.1390 Accuracy 0.5758\n",
            "Epoch 54 Batch 600 Loss 2.1407 Accuracy 0.5755\n",
            "Epoch 54 Batch 650 Loss 2.1404 Accuracy 0.5755\n",
            "Epoch 54 Batch 700 Loss 2.1415 Accuracy 0.5753\n",
            "Epoch 54 Batch 750 Loss 2.1415 Accuracy 0.5751\n",
            "Epoch 54 Batch 800 Loss 2.1446 Accuracy 0.5748\n",
            "Epoch 54 Batch 850 Loss 2.1457 Accuracy 0.5748\n",
            "Epoch 54 Batch 900 Loss 2.1481 Accuracy 0.5743\n",
            "Epoch 54 Batch 950 Loss 2.1487 Accuracy 0.5744\n",
            "Epoch 54 Loss 2.1491 Accuracy 0.5744\n",
            "Time taken for 1 epoch: 23.81434917449951 secs\n",
            "\n",
            "Epoch 55 Batch 0 Loss 2.1465 Accuracy 0.5908\n",
            "Epoch 55 Batch 50 Loss 2.1068 Accuracy 0.5743\n",
            "Epoch 55 Batch 100 Loss 2.1216 Accuracy 0.5746\n",
            "Epoch 55 Batch 150 Loss 2.1352 Accuracy 0.5747\n",
            "Epoch 55 Batch 200 Loss 2.1339 Accuracy 0.5747\n",
            "Epoch 55 Batch 250 Loss 2.1319 Accuracy 0.5757\n",
            "Epoch 55 Batch 300 Loss 2.1332 Accuracy 0.5754\n",
            "Epoch 55 Batch 350 Loss 2.1350 Accuracy 0.5753\n",
            "Epoch 55 Batch 400 Loss 2.1356 Accuracy 0.5754\n",
            "Epoch 55 Batch 450 Loss 2.1362 Accuracy 0.5750\n",
            "Epoch 55 Batch 500 Loss 2.1357 Accuracy 0.5758\n",
            "Epoch 55 Batch 550 Loss 2.1390 Accuracy 0.5756\n",
            "Epoch 55 Batch 600 Loss 2.1389 Accuracy 0.5755\n",
            "Epoch 55 Batch 650 Loss 2.1401 Accuracy 0.5755\n",
            "Epoch 55 Batch 700 Loss 2.1408 Accuracy 0.5754\n",
            "Epoch 55 Batch 750 Loss 2.1405 Accuracy 0.5751\n",
            "discarded batch 781\n",
            "Epoch 55 Batch 800 Loss 2.1419 Accuracy 0.5748\n",
            "Epoch 55 Batch 850 Loss 2.1435 Accuracy 0.5747\n",
            "Epoch 55 Batch 900 Loss 2.1439 Accuracy 0.5746\n",
            "Epoch 55 Batch 950 Loss 2.1446 Accuracy 0.5745\n",
            "Saving checkpoint for epoch 55 at ./checkpoints/train/ckpt-11\n",
            "Epoch 55 Loss 2.1448 Accuracy 0.5745\n",
            "Time taken for 1 epoch: 24.238280773162842 secs\n",
            "\n",
            "Epoch 56 Batch 0 Loss 2.1105 Accuracy 0.5809\n",
            "Epoch 56 Batch 50 Loss 2.1405 Accuracy 0.5741\n",
            "Epoch 56 Batch 100 Loss 2.1403 Accuracy 0.5744\n",
            "Epoch 56 Batch 150 Loss 2.1399 Accuracy 0.5737\n",
            "discarded batch 189\n",
            "Epoch 56 Batch 200 Loss 2.1320 Accuracy 0.5751\n",
            "Epoch 56 Batch 250 Loss 2.1330 Accuracy 0.5749\n",
            "Epoch 56 Batch 300 Loss 2.1315 Accuracy 0.5748\n",
            "Epoch 56 Batch 350 Loss 2.1340 Accuracy 0.5748\n",
            "Epoch 56 Batch 400 Loss 2.1343 Accuracy 0.5756\n",
            "Epoch 56 Batch 450 Loss 2.1380 Accuracy 0.5750\n",
            "Epoch 56 Batch 500 Loss 2.1398 Accuracy 0.5747\n",
            "Epoch 56 Batch 550 Loss 2.1376 Accuracy 0.5751\n",
            "Epoch 56 Batch 600 Loss 2.1361 Accuracy 0.5752\n",
            "Epoch 56 Batch 650 Loss 2.1383 Accuracy 0.5756\n",
            "Epoch 56 Batch 700 Loss 2.1380 Accuracy 0.5757\n",
            "Epoch 56 Batch 750 Loss 2.1372 Accuracy 0.5761\n",
            "Epoch 56 Batch 800 Loss 2.1373 Accuracy 0.5764\n",
            "Epoch 56 Batch 850 Loss 2.1399 Accuracy 0.5762\n",
            "Epoch 56 Batch 900 Loss 2.1404 Accuracy 0.5761\n",
            "Epoch 56 Batch 950 Loss 2.1403 Accuracy 0.5761\n",
            "Epoch 56 Loss 2.1403 Accuracy 0.5762\n",
            "Time taken for 1 epoch: 23.883880138397217 secs\n",
            "\n",
            "Epoch 56 VALIDATION: Loss 2.2458 Accuracy 0.5752\n",
            "\n",
            "Epoch 57 Batch 0 Loss 1.9805 Accuracy 0.5974\n",
            "Epoch 57 Batch 50 Loss 2.1194 Accuracy 0.5822\n",
            "Epoch 57 Batch 100 Loss 2.1310 Accuracy 0.5781\n",
            "Epoch 57 Batch 150 Loss 2.1473 Accuracy 0.5757\n",
            "Epoch 57 Batch 200 Loss 2.1411 Accuracy 0.5757\n",
            "Epoch 57 Batch 250 Loss 2.1395 Accuracy 0.5758\n",
            "Epoch 57 Batch 300 Loss 2.1422 Accuracy 0.5756\n",
            "Epoch 57 Batch 350 Loss 2.1371 Accuracy 0.5757\n",
            "Epoch 57 Batch 400 Loss 2.1382 Accuracy 0.5758\n",
            "Epoch 57 Batch 450 Loss 2.1388 Accuracy 0.5757\n",
            "Epoch 57 Batch 500 Loss 2.1352 Accuracy 0.5762\n",
            "Epoch 57 Batch 550 Loss 2.1360 Accuracy 0.5763\n",
            "discarded batch 574\n",
            "Epoch 57 Batch 600 Loss 2.1343 Accuracy 0.5762\n",
            "Epoch 57 Batch 650 Loss 2.1339 Accuracy 0.5761\n",
            "Epoch 57 Batch 700 Loss 2.1355 Accuracy 0.5759\n",
            "Epoch 57 Batch 750 Loss 2.1339 Accuracy 0.5759\n",
            "Epoch 57 Batch 800 Loss 2.1343 Accuracy 0.5758\n",
            "Epoch 57 Batch 850 Loss 2.1331 Accuracy 0.5759\n",
            "Epoch 57 Batch 900 Loss 2.1341 Accuracy 0.5757\n",
            "Epoch 57 Batch 950 Loss 2.1364 Accuracy 0.5757\n",
            "Epoch 57 Loss 2.1367 Accuracy 0.5757\n",
            "Time taken for 1 epoch: 23.79392433166504 secs\n",
            "\n",
            "Epoch 58 Batch 0 Loss 2.0844 Accuracy 0.5809\n",
            "Epoch 58 Batch 50 Loss 2.1302 Accuracy 0.5763\n",
            "Epoch 58 Batch 100 Loss 2.1205 Accuracy 0.5771\n",
            "Epoch 58 Batch 150 Loss 2.1148 Accuracy 0.5769\n",
            "Epoch 58 Batch 200 Loss 2.1167 Accuracy 0.5773\n",
            "discarded batch 206\n",
            "Epoch 58 Batch 250 Loss 2.1185 Accuracy 0.5778\n",
            "Epoch 58 Batch 300 Loss 2.1209 Accuracy 0.5774\n",
            "Epoch 58 Batch 350 Loss 2.1225 Accuracy 0.5771\n",
            "Epoch 58 Batch 400 Loss 2.1251 Accuracy 0.5761\n",
            "Epoch 58 Batch 450 Loss 2.1251 Accuracy 0.5763\n",
            "Epoch 58 Batch 500 Loss 2.1253 Accuracy 0.5769\n",
            "Epoch 58 Batch 550 Loss 2.1247 Accuracy 0.5772\n",
            "Epoch 58 Batch 600 Loss 2.1254 Accuracy 0.5772\n",
            "Epoch 58 Batch 650 Loss 2.1268 Accuracy 0.5770\n",
            "Epoch 58 Batch 700 Loss 2.1290 Accuracy 0.5770\n",
            "Epoch 58 Batch 750 Loss 2.1308 Accuracy 0.5768\n",
            "Epoch 58 Batch 800 Loss 2.1320 Accuracy 0.5767\n",
            "Epoch 58 Batch 850 Loss 2.1309 Accuracy 0.5766\n",
            "Epoch 58 Batch 900 Loss 2.1311 Accuracy 0.5768\n",
            "Epoch 58 Batch 950 Loss 2.1313 Accuracy 0.5767\n",
            "Epoch 58 Loss 2.1312 Accuracy 0.5767\n",
            "Time taken for 1 epoch: 23.592555046081543 secs\n",
            "\n",
            "Epoch 59 Batch 0 Loss 1.9204 Accuracy 0.6040\n",
            "Epoch 59 Batch 50 Loss 2.1247 Accuracy 0.5739\n",
            "Epoch 59 Batch 100 Loss 2.1093 Accuracy 0.5767\n",
            "Epoch 59 Batch 150 Loss 2.1128 Accuracy 0.5775\n",
            "Epoch 59 Batch 200 Loss 2.1154 Accuracy 0.5775\n",
            "Epoch 59 Batch 250 Loss 2.1237 Accuracy 0.5771\n",
            "Epoch 59 Batch 300 Loss 2.1233 Accuracy 0.5769\n",
            "discarded batch 306\n",
            "Epoch 59 Batch 350 Loss 2.1208 Accuracy 0.5771\n",
            "Epoch 59 Batch 400 Loss 2.1234 Accuracy 0.5767\n",
            "Epoch 59 Batch 450 Loss 2.1233 Accuracy 0.5771\n",
            "Epoch 59 Batch 500 Loss 2.1256 Accuracy 0.5767\n",
            "Epoch 59 Batch 550 Loss 2.1244 Accuracy 0.5765\n",
            "Epoch 59 Batch 600 Loss 2.1247 Accuracy 0.5767\n",
            "Epoch 59 Batch 650 Loss 2.1268 Accuracy 0.5768\n",
            "Epoch 59 Batch 700 Loss 2.1273 Accuracy 0.5769\n",
            "Epoch 59 Batch 750 Loss 2.1271 Accuracy 0.5771\n",
            "Epoch 59 Batch 800 Loss 2.1251 Accuracy 0.5773\n",
            "Epoch 59 Batch 850 Loss 2.1244 Accuracy 0.5775\n",
            "Epoch 59 Batch 900 Loss 2.1243 Accuracy 0.5776\n",
            "Epoch 59 Batch 950 Loss 2.1262 Accuracy 0.5775\n",
            "Epoch 59 Loss 2.1261 Accuracy 0.5775\n",
            "Time taken for 1 epoch: 23.88849425315857 secs\n",
            "\n",
            "Epoch 60 Batch 0 Loss 2.0672 Accuracy 0.5974\n",
            "Epoch 60 Batch 50 Loss 2.1370 Accuracy 0.5731\n",
            "Epoch 60 Batch 100 Loss 2.1244 Accuracy 0.5767\n",
            "Epoch 60 Batch 150 Loss 2.1125 Accuracy 0.5782\n",
            "Epoch 60 Batch 200 Loss 2.1071 Accuracy 0.5789\n",
            "Epoch 60 Batch 250 Loss 2.1059 Accuracy 0.5787\n",
            "Epoch 60 Batch 300 Loss 2.1062 Accuracy 0.5791\n",
            "Epoch 60 Batch 350 Loss 2.1071 Accuracy 0.5789\n",
            "Epoch 60 Batch 400 Loss 2.1064 Accuracy 0.5795\n",
            "Epoch 60 Batch 450 Loss 2.1083 Accuracy 0.5794\n",
            "Epoch 60 Batch 500 Loss 2.1069 Accuracy 0.5797\n",
            "Epoch 60 Batch 550 Loss 2.1084 Accuracy 0.5799\n",
            "Epoch 60 Batch 600 Loss 2.1093 Accuracy 0.5796\n",
            "Epoch 60 Batch 650 Loss 2.1114 Accuracy 0.5790\n",
            "Epoch 60 Batch 700 Loss 2.1122 Accuracy 0.5790\n",
            "Epoch 60 Batch 750 Loss 2.1138 Accuracy 0.5787\n",
            "Epoch 60 Batch 800 Loss 2.1178 Accuracy 0.5785\n",
            "discarded batch 850\n",
            "Epoch 60 Batch 900 Loss 2.1180 Accuracy 0.5786\n",
            "Epoch 60 Batch 950 Loss 2.1197 Accuracy 0.5786\n",
            "Saving checkpoint for epoch 60 at ./checkpoints/train/ckpt-12\n",
            "Epoch 60 Loss 2.1200 Accuracy 0.5786\n",
            "Time taken for 1 epoch: 24.31124210357666 secs\n",
            "\n",
            "Epoch 61 Batch 0 Loss 2.0552 Accuracy 0.5809\n",
            "Epoch 61 Batch 50 Loss 2.0963 Accuracy 0.5803\n",
            "Epoch 61 Batch 100 Loss 2.0961 Accuracy 0.5802\n",
            "Epoch 61 Batch 150 Loss 2.0963 Accuracy 0.5811\n",
            "discarded batch 198\n",
            "Epoch 61 Batch 200 Loss 2.0993 Accuracy 0.5799\n",
            "Epoch 61 Batch 250 Loss 2.1001 Accuracy 0.5800\n",
            "Epoch 61 Batch 300 Loss 2.1051 Accuracy 0.5794\n",
            "Epoch 61 Batch 350 Loss 2.1058 Accuracy 0.5794\n",
            "Epoch 61 Batch 400 Loss 2.1081 Accuracy 0.5795\n",
            "Epoch 61 Batch 450 Loss 2.1111 Accuracy 0.5794\n",
            "Epoch 61 Batch 500 Loss 2.1101 Accuracy 0.5793\n",
            "Epoch 61 Batch 550 Loss 2.1117 Accuracy 0.5788\n",
            "Epoch 61 Batch 600 Loss 2.1100 Accuracy 0.5788\n",
            "Epoch 61 Batch 650 Loss 2.1121 Accuracy 0.5786\n",
            "Epoch 61 Batch 700 Loss 2.1133 Accuracy 0.5786\n",
            "Epoch 61 Batch 750 Loss 2.1131 Accuracy 0.5789\n",
            "Epoch 61 Batch 800 Loss 2.1140 Accuracy 0.5789\n",
            "Epoch 61 Batch 850 Loss 2.1156 Accuracy 0.5788\n",
            "Epoch 61 Batch 900 Loss 2.1156 Accuracy 0.5788\n",
            "Epoch 61 Batch 950 Loss 2.1150 Accuracy 0.5788\n",
            "Epoch 61 Loss 2.1152 Accuracy 0.5787\n",
            "Time taken for 1 epoch: 23.991891145706177 secs\n",
            "\n",
            "Epoch 61 VALIDATION: Loss 2.2336 Accuracy 0.5772\n",
            "\n",
            "Epoch 62 Batch 0 Loss 1.9991 Accuracy 0.5809\n",
            "Epoch 62 Batch 50 Loss 2.0900 Accuracy 0.5797\n",
            "Epoch 62 Batch 100 Loss 2.0778 Accuracy 0.5814\n",
            "Epoch 62 Batch 150 Loss 2.0881 Accuracy 0.5801\n",
            "Epoch 62 Batch 200 Loss 2.0906 Accuracy 0.5792\n",
            "Epoch 62 Batch 250 Loss 2.0952 Accuracy 0.5784\n",
            "Epoch 62 Batch 300 Loss 2.1005 Accuracy 0.5784\n",
            "Epoch 62 Batch 350 Loss 2.0954 Accuracy 0.5795\n",
            "Epoch 62 Batch 400 Loss 2.0976 Accuracy 0.5797\n",
            "Epoch 62 Batch 450 Loss 2.1014 Accuracy 0.5794\n",
            "Epoch 62 Batch 500 Loss 2.1014 Accuracy 0.5792\n",
            "Epoch 62 Batch 550 Loss 2.0988 Accuracy 0.5796\n",
            "Epoch 62 Batch 600 Loss 2.1005 Accuracy 0.5796\n",
            "Epoch 62 Batch 650 Loss 2.1003 Accuracy 0.5799\n",
            "Epoch 62 Batch 700 Loss 2.1040 Accuracy 0.5795\n",
            "Epoch 62 Batch 750 Loss 2.1049 Accuracy 0.5795\n",
            "Epoch 62 Batch 800 Loss 2.1075 Accuracy 0.5796\n",
            "discarded batch 849\n",
            "Epoch 62 Batch 850 Loss 2.1097 Accuracy 0.5793\n",
            "Epoch 62 Batch 900 Loss 2.1116 Accuracy 0.5791\n",
            "Epoch 62 Batch 950 Loss 2.1115 Accuracy 0.5792\n",
            "Epoch 62 Loss 2.1116 Accuracy 0.5792\n",
            "Time taken for 1 epoch: 24.08389639854431 secs\n",
            "\n",
            "Epoch 63 Batch 0 Loss 1.9348 Accuracy 0.5842\n",
            "Epoch 63 Batch 50 Loss 2.0908 Accuracy 0.5771\n",
            "Epoch 63 Batch 100 Loss 2.1029 Accuracy 0.5754\n",
            "Epoch 63 Batch 150 Loss 2.0927 Accuracy 0.5778\n",
            "Epoch 63 Batch 200 Loss 2.0832 Accuracy 0.5795\n",
            "Epoch 63 Batch 250 Loss 2.0855 Accuracy 0.5794\n",
            "discarded batch 254\n",
            "Epoch 63 Batch 300 Loss 2.0898 Accuracy 0.5797\n",
            "Epoch 63 Batch 350 Loss 2.0919 Accuracy 0.5801\n",
            "Epoch 63 Batch 400 Loss 2.0952 Accuracy 0.5798\n",
            "Epoch 63 Batch 450 Loss 2.0969 Accuracy 0.5796\n",
            "Epoch 63 Batch 500 Loss 2.0984 Accuracy 0.5798\n",
            "Epoch 63 Batch 550 Loss 2.1008 Accuracy 0.5801\n",
            "Epoch 63 Batch 600 Loss 2.1039 Accuracy 0.5796\n",
            "Epoch 63 Batch 650 Loss 2.1045 Accuracy 0.5796\n",
            "Epoch 63 Batch 700 Loss 2.1074 Accuracy 0.5794\n",
            "Epoch 63 Batch 750 Loss 2.1087 Accuracy 0.5793\n",
            "Epoch 63 Batch 800 Loss 2.1094 Accuracy 0.5791\n",
            "Epoch 63 Batch 850 Loss 2.1080 Accuracy 0.5795\n",
            "Epoch 63 Batch 900 Loss 2.1067 Accuracy 0.5797\n",
            "Epoch 63 Batch 950 Loss 2.1087 Accuracy 0.5794\n",
            "Epoch 63 Loss 2.1083 Accuracy 0.5794\n",
            "Time taken for 1 epoch: 24.035411834716797 secs\n",
            "\n",
            "Epoch 64 Batch 0 Loss 2.1421 Accuracy 0.6007\n",
            "Epoch 64 Batch 50 Loss 2.0723 Accuracy 0.5849\n",
            "Epoch 64 Batch 100 Loss 2.0763 Accuracy 0.5822\n",
            "Epoch 64 Batch 150 Loss 2.0757 Accuracy 0.5827\n",
            "Epoch 64 Batch 200 Loss 2.0809 Accuracy 0.5813\n",
            "Epoch 64 Batch 250 Loss 2.0807 Accuracy 0.5816\n",
            "Epoch 64 Batch 300 Loss 2.0855 Accuracy 0.5815\n",
            "Epoch 64 Batch 350 Loss 2.0906 Accuracy 0.5809\n",
            "Epoch 64 Batch 400 Loss 2.0901 Accuracy 0.5812\n",
            "Epoch 64 Batch 450 Loss 2.0932 Accuracy 0.5809\n",
            "Epoch 64 Batch 500 Loss 2.0911 Accuracy 0.5812\n",
            "Epoch 64 Batch 550 Loss 2.0948 Accuracy 0.5809\n",
            "discarded batch 595\n",
            "Epoch 64 Batch 600 Loss 2.0962 Accuracy 0.5808\n",
            "Epoch 64 Batch 650 Loss 2.0972 Accuracy 0.5806\n",
            "Epoch 64 Batch 700 Loss 2.0976 Accuracy 0.5808\n",
            "Epoch 64 Batch 750 Loss 2.0988 Accuracy 0.5808\n",
            "Epoch 64 Batch 800 Loss 2.1001 Accuracy 0.5806\n",
            "Epoch 64 Batch 850 Loss 2.1012 Accuracy 0.5805\n",
            "Epoch 64 Batch 900 Loss 2.1018 Accuracy 0.5805\n",
            "Epoch 64 Batch 950 Loss 2.1018 Accuracy 0.5806\n",
            "Epoch 64 Loss 2.1018 Accuracy 0.5805\n",
            "Time taken for 1 epoch: 24.12851572036743 secs\n",
            "\n",
            "Epoch 65 Batch 0 Loss 2.0011 Accuracy 0.5974\n",
            "Epoch 65 Batch 50 Loss 2.0830 Accuracy 0.5834\n",
            "Epoch 65 Batch 100 Loss 2.0902 Accuracy 0.5825\n",
            "Epoch 65 Batch 150 Loss 2.0865 Accuracy 0.5825\n",
            "Epoch 65 Batch 200 Loss 2.0880 Accuracy 0.5818\n",
            "Epoch 65 Batch 250 Loss 2.0896 Accuracy 0.5813\n",
            "Epoch 65 Batch 300 Loss 2.0877 Accuracy 0.5814\n",
            "Epoch 65 Batch 350 Loss 2.0880 Accuracy 0.5817\n",
            "discarded batch 361\n",
            "Epoch 65 Batch 400 Loss 2.0913 Accuracy 0.5815\n",
            "Epoch 65 Batch 450 Loss 2.0944 Accuracy 0.5809\n",
            "Epoch 65 Batch 500 Loss 2.0940 Accuracy 0.5810\n",
            "Epoch 65 Batch 550 Loss 2.0949 Accuracy 0.5811\n",
            "Epoch 65 Batch 600 Loss 2.0938 Accuracy 0.5816\n",
            "Epoch 65 Batch 650 Loss 2.0930 Accuracy 0.5817\n",
            "Epoch 65 Batch 700 Loss 2.0953 Accuracy 0.5813\n",
            "Epoch 65 Batch 750 Loss 2.0959 Accuracy 0.5815\n",
            "Epoch 65 Batch 800 Loss 2.0961 Accuracy 0.5816\n",
            "Epoch 65 Batch 850 Loss 2.0961 Accuracy 0.5816\n",
            "Epoch 65 Batch 900 Loss 2.0977 Accuracy 0.5814\n",
            "Epoch 65 Batch 950 Loss 2.0986 Accuracy 0.5815\n",
            "Saving checkpoint for epoch 65 at ./checkpoints/train/ckpt-13\n",
            "Epoch 65 Loss 2.0984 Accuracy 0.5815\n",
            "Time taken for 1 epoch: 24.196345806121826 secs\n",
            "\n",
            "Epoch 66 Batch 0 Loss 2.1466 Accuracy 0.5875\n",
            "Epoch 66 Batch 50 Loss 2.0790 Accuracy 0.5836\n",
            "Epoch 66 Batch 100 Loss 2.1036 Accuracy 0.5810\n",
            "Epoch 66 Batch 150 Loss 2.0974 Accuracy 0.5805\n",
            "Epoch 66 Batch 200 Loss 2.0908 Accuracy 0.5812\n",
            "Epoch 66 Batch 250 Loss 2.0922 Accuracy 0.5805\n",
            "Epoch 66 Batch 300 Loss 2.0897 Accuracy 0.5807\n",
            "Epoch 66 Batch 350 Loss 2.0874 Accuracy 0.5811\n",
            "Epoch 66 Batch 400 Loss 2.0886 Accuracy 0.5811\n",
            "Epoch 66 Batch 450 Loss 2.0879 Accuracy 0.5812\n",
            "Epoch 66 Batch 500 Loss 2.0885 Accuracy 0.5814\n",
            "Epoch 66 Batch 550 Loss 2.0892 Accuracy 0.5813\n",
            "Epoch 66 Batch 600 Loss 2.0901 Accuracy 0.5814\n",
            "discarded batch 610\n",
            "Epoch 66 Batch 650 Loss 2.0905 Accuracy 0.5815\n",
            "Epoch 66 Batch 700 Loss 2.0928 Accuracy 0.5811\n",
            "Epoch 66 Batch 750 Loss 2.0945 Accuracy 0.5809\n",
            "Epoch 66 Batch 800 Loss 2.0961 Accuracy 0.5807\n",
            "Epoch 66 Batch 850 Loss 2.0966 Accuracy 0.5806\n",
            "Epoch 66 Batch 900 Loss 2.0962 Accuracy 0.5805\n",
            "Epoch 66 Batch 950 Loss 2.0960 Accuracy 0.5807\n",
            "Epoch 66 Loss 2.0960 Accuracy 0.5807\n",
            "Time taken for 1 epoch: 24.195157051086426 secs\n",
            "\n",
            "Epoch 66 VALIDATION: Loss 2.2328 Accuracy 0.5776\n",
            "\n",
            "Epoch 67 Batch 0 Loss 2.1128 Accuracy 0.5941\n",
            "Epoch 67 Batch 50 Loss 2.0837 Accuracy 0.5811\n",
            "Epoch 67 Batch 100 Loss 2.0746 Accuracy 0.5820\n",
            "discarded batch 129\n",
            "Epoch 67 Batch 150 Loss 2.0779 Accuracy 0.5801\n",
            "Epoch 67 Batch 200 Loss 2.0790 Accuracy 0.5800\n",
            "Epoch 67 Batch 250 Loss 2.0774 Accuracy 0.5810\n",
            "Epoch 67 Batch 300 Loss 2.0814 Accuracy 0.5809\n",
            "Epoch 67 Batch 350 Loss 2.0797 Accuracy 0.5813\n",
            "Epoch 67 Batch 400 Loss 2.0827 Accuracy 0.5812\n",
            "Epoch 67 Batch 450 Loss 2.0817 Accuracy 0.5816\n",
            "Epoch 67 Batch 500 Loss 2.0844 Accuracy 0.5817\n",
            "Epoch 67 Batch 550 Loss 2.0856 Accuracy 0.5820\n",
            "Epoch 67 Batch 600 Loss 2.0850 Accuracy 0.5819\n",
            "Epoch 67 Batch 650 Loss 2.0855 Accuracy 0.5820\n",
            "Epoch 67 Batch 700 Loss 2.0882 Accuracy 0.5817\n",
            "Epoch 67 Batch 750 Loss 2.0887 Accuracy 0.5817\n",
            "Epoch 67 Batch 800 Loss 2.0878 Accuracy 0.5819\n",
            "Epoch 67 Batch 850 Loss 2.0880 Accuracy 0.5820\n",
            "Epoch 67 Batch 900 Loss 2.0892 Accuracy 0.5820\n",
            "Epoch 67 Batch 950 Loss 2.0909 Accuracy 0.5816\n",
            "Epoch 67 Loss 2.0910 Accuracy 0.5816\n",
            "Time taken for 1 epoch: 23.78703498840332 secs\n",
            "\n",
            "Epoch 68 Batch 0 Loss 1.8824 Accuracy 0.6205\n",
            "Epoch 68 Batch 50 Loss 2.0344 Accuracy 0.5853\n",
            "discarded batch 67\n",
            "Epoch 68 Batch 100 Loss 2.0505 Accuracy 0.5849\n",
            "Epoch 68 Batch 150 Loss 2.0467 Accuracy 0.5852\n",
            "Epoch 68 Batch 200 Loss 2.0530 Accuracy 0.5838\n",
            "Epoch 68 Batch 250 Loss 2.0617 Accuracy 0.5838\n",
            "Epoch 68 Batch 300 Loss 2.0641 Accuracy 0.5839\n",
            "Epoch 68 Batch 350 Loss 2.0666 Accuracy 0.5839\n",
            "Epoch 68 Batch 400 Loss 2.0712 Accuracy 0.5832\n",
            "Epoch 68 Batch 450 Loss 2.0727 Accuracy 0.5832\n",
            "Epoch 68 Batch 500 Loss 2.0737 Accuracy 0.5833\n",
            "Epoch 68 Batch 550 Loss 2.0774 Accuracy 0.5829\n",
            "Epoch 68 Batch 600 Loss 2.0792 Accuracy 0.5830\n",
            "Epoch 68 Batch 650 Loss 2.0814 Accuracy 0.5828\n",
            "Epoch 68 Batch 700 Loss 2.0821 Accuracy 0.5826\n",
            "Epoch 68 Batch 750 Loss 2.0826 Accuracy 0.5824\n",
            "Epoch 68 Batch 800 Loss 2.0845 Accuracy 0.5820\n",
            "Epoch 68 Batch 850 Loss 2.0844 Accuracy 0.5820\n",
            "Epoch 68 Batch 900 Loss 2.0850 Accuracy 0.5821\n",
            "Epoch 68 Batch 950 Loss 2.0864 Accuracy 0.5818\n",
            "Epoch 68 Loss 2.0866 Accuracy 0.5818\n",
            "Time taken for 1 epoch: 24.001314401626587 secs\n",
            "\n",
            "Epoch 69 Batch 0 Loss 1.9806 Accuracy 0.5743\n",
            "Epoch 69 Batch 50 Loss 2.0863 Accuracy 0.5792\n",
            "Epoch 69 Batch 100 Loss 2.0845 Accuracy 0.5803\n",
            "Epoch 69 Batch 150 Loss 2.0780 Accuracy 0.5822\n",
            "Epoch 69 Batch 200 Loss 2.0808 Accuracy 0.5818\n",
            "Epoch 69 Batch 250 Loss 2.0789 Accuracy 0.5821\n",
            "Epoch 69 Batch 300 Loss 2.0768 Accuracy 0.5822\n",
            "Epoch 69 Batch 350 Loss 2.0771 Accuracy 0.5819\n",
            "Epoch 69 Batch 400 Loss 2.0785 Accuracy 0.5821\n",
            "Epoch 69 Batch 450 Loss 2.0785 Accuracy 0.5823\n",
            "Epoch 69 Batch 500 Loss 2.0799 Accuracy 0.5822\n",
            "Epoch 69 Batch 550 Loss 2.0780 Accuracy 0.5826\n",
            "Epoch 69 Batch 600 Loss 2.0786 Accuracy 0.5826\n",
            "Epoch 69 Batch 650 Loss 2.0788 Accuracy 0.5826\n",
            "Epoch 69 Batch 700 Loss 2.0793 Accuracy 0.5826\n",
            "Epoch 69 Batch 750 Loss 2.0793 Accuracy 0.5825\n",
            "Epoch 69 Batch 800 Loss 2.0790 Accuracy 0.5827\n",
            "discarded batch 831\n",
            "Epoch 69 Batch 850 Loss 2.0794 Accuracy 0.5828\n",
            "Epoch 69 Batch 900 Loss 2.0822 Accuracy 0.5826\n",
            "Epoch 69 Batch 950 Loss 2.0831 Accuracy 0.5825\n",
            "Epoch 69 Loss 2.0833 Accuracy 0.5824\n",
            "Time taken for 1 epoch: 24.222774267196655 secs\n",
            "\n",
            "Epoch 70 Batch 0 Loss 1.9388 Accuracy 0.5842\n",
            "Epoch 70 Batch 50 Loss 2.0738 Accuracy 0.5822\n",
            "Epoch 70 Batch 100 Loss 2.0637 Accuracy 0.5829\n",
            "Epoch 70 Batch 150 Loss 2.0613 Accuracy 0.5838\n",
            "Epoch 70 Batch 200 Loss 2.0567 Accuracy 0.5847\n",
            "Epoch 70 Batch 250 Loss 2.0609 Accuracy 0.5843\n",
            "Epoch 70 Batch 300 Loss 2.0655 Accuracy 0.5845\n",
            "Epoch 70 Batch 350 Loss 2.0678 Accuracy 0.5845\n",
            "Epoch 70 Batch 400 Loss 2.0666 Accuracy 0.5846\n",
            "Epoch 70 Batch 450 Loss 2.0682 Accuracy 0.5844\n",
            "Epoch 70 Batch 500 Loss 2.0688 Accuracy 0.5844\n",
            "Epoch 70 Batch 550 Loss 2.0695 Accuracy 0.5843\n",
            "Epoch 70 Batch 600 Loss 2.0725 Accuracy 0.5841\n",
            "Epoch 70 Batch 650 Loss 2.0734 Accuracy 0.5840\n",
            "Epoch 70 Batch 700 Loss 2.0745 Accuracy 0.5837\n",
            "Epoch 70 Batch 750 Loss 2.0751 Accuracy 0.5834\n",
            "discarded batch 774\n",
            "Epoch 70 Batch 800 Loss 2.0763 Accuracy 0.5832\n",
            "Epoch 70 Batch 850 Loss 2.0765 Accuracy 0.5833\n",
            "Epoch 70 Batch 900 Loss 2.0774 Accuracy 0.5834\n",
            "Epoch 70 Batch 950 Loss 2.0781 Accuracy 0.5834\n",
            "Saving checkpoint for epoch 70 at ./checkpoints/train/ckpt-14\n",
            "Epoch 70 Loss 2.0781 Accuracy 0.5833\n",
            "Time taken for 1 epoch: 24.63690447807312 secs\n",
            "\n",
            "Epoch 71 Batch 0 Loss 1.9610 Accuracy 0.5941\n",
            "Epoch 71 Batch 50 Loss 2.0475 Accuracy 0.5838\n",
            "Epoch 71 Batch 100 Loss 2.0462 Accuracy 0.5858\n",
            "Epoch 71 Batch 150 Loss 2.0571 Accuracy 0.5851\n",
            "Epoch 71 Batch 200 Loss 2.0603 Accuracy 0.5836\n",
            "Epoch 71 Batch 250 Loss 2.0616 Accuracy 0.5841\n",
            "Epoch 71 Batch 300 Loss 2.0629 Accuracy 0.5838\n",
            "Epoch 71 Batch 350 Loss 2.0631 Accuracy 0.5842\n",
            "Epoch 71 Batch 400 Loss 2.0613 Accuracy 0.5849\n",
            "discarded batch 417\n",
            "Epoch 71 Batch 450 Loss 2.0620 Accuracy 0.5848\n",
            "Epoch 71 Batch 500 Loss 2.0610 Accuracy 0.5849\n",
            "Epoch 71 Batch 550 Loss 2.0603 Accuracy 0.5849\n",
            "Epoch 71 Batch 600 Loss 2.0635 Accuracy 0.5850\n",
            "Epoch 71 Batch 650 Loss 2.0680 Accuracy 0.5843\n",
            "Epoch 71 Batch 700 Loss 2.0695 Accuracy 0.5840\n",
            "Epoch 71 Batch 750 Loss 2.0715 Accuracy 0.5841\n",
            "Epoch 71 Batch 800 Loss 2.0719 Accuracy 0.5844\n",
            "Epoch 71 Batch 850 Loss 2.0736 Accuracy 0.5844\n",
            "Epoch 71 Batch 900 Loss 2.0760 Accuracy 0.5840\n",
            "Epoch 71 Batch 950 Loss 2.0767 Accuracy 0.5838\n",
            "Epoch 71 Loss 2.0762 Accuracy 0.5839\n",
            "Time taken for 1 epoch: 24.13587260246277 secs\n",
            "\n",
            "Epoch 71 VALIDATION: Loss 2.2177 Accuracy 0.5787\n",
            "\n",
            "Epoch 72 Batch 0 Loss 2.0300 Accuracy 0.6073\n",
            "Epoch 72 Batch 50 Loss 2.0438 Accuracy 0.5864\n",
            "discarded batch 65\n",
            "Epoch 72 Batch 100 Loss 2.0360 Accuracy 0.5875\n",
            "Epoch 72 Batch 150 Loss 2.0498 Accuracy 0.5855\n",
            "Epoch 72 Batch 200 Loss 2.0533 Accuracy 0.5851\n",
            "Epoch 72 Batch 250 Loss 2.0506 Accuracy 0.5854\n",
            "Epoch 72 Batch 300 Loss 2.0494 Accuracy 0.5854\n",
            "Epoch 72 Batch 350 Loss 2.0519 Accuracy 0.5853\n",
            "Epoch 72 Batch 400 Loss 2.0534 Accuracy 0.5854\n",
            "Epoch 72 Batch 450 Loss 2.0515 Accuracy 0.5857\n",
            "Epoch 72 Batch 500 Loss 2.0548 Accuracy 0.5854\n",
            "Epoch 72 Batch 550 Loss 2.0574 Accuracy 0.5851\n",
            "Epoch 72 Batch 600 Loss 2.0594 Accuracy 0.5849\n",
            "Epoch 72 Batch 650 Loss 2.0599 Accuracy 0.5849\n",
            "Epoch 72 Batch 700 Loss 2.0637 Accuracy 0.5849\n",
            "Epoch 72 Batch 750 Loss 2.0646 Accuracy 0.5848\n",
            "Epoch 72 Batch 800 Loss 2.0657 Accuracy 0.5845\n",
            "Epoch 72 Batch 850 Loss 2.0670 Accuracy 0.5843\n",
            "Epoch 72 Batch 900 Loss 2.0696 Accuracy 0.5841\n",
            "Epoch 72 Batch 950 Loss 2.0704 Accuracy 0.5843\n",
            "Epoch 72 Loss 2.0704 Accuracy 0.5842\n",
            "Time taken for 1 epoch: 23.958345413208008 secs\n",
            "\n",
            "Epoch 73 Batch 0 Loss 2.2984 Accuracy 0.5479\n",
            "Epoch 73 Batch 50 Loss 2.0361 Accuracy 0.5854\n",
            "Epoch 73 Batch 100 Loss 2.0397 Accuracy 0.5861\n",
            "Epoch 73 Batch 150 Loss 2.0445 Accuracy 0.5865\n",
            "Epoch 73 Batch 200 Loss 2.0484 Accuracy 0.5862\n",
            "Epoch 73 Batch 250 Loss 2.0501 Accuracy 0.5863\n",
            "Epoch 73 Batch 300 Loss 2.0558 Accuracy 0.5852\n",
            "Epoch 73 Batch 350 Loss 2.0586 Accuracy 0.5849\n",
            "Epoch 73 Batch 400 Loss 2.0582 Accuracy 0.5845\n",
            "Epoch 73 Batch 450 Loss 2.0586 Accuracy 0.5847\n",
            "Epoch 73 Batch 500 Loss 2.0631 Accuracy 0.5847\n",
            "Epoch 73 Batch 550 Loss 2.0622 Accuracy 0.5847\n",
            "Epoch 73 Batch 600 Loss 2.0627 Accuracy 0.5843\n",
            "Epoch 73 Batch 650 Loss 2.0635 Accuracy 0.5843\n",
            "Epoch 73 Batch 700 Loss 2.0653 Accuracy 0.5842\n",
            "Epoch 73 Batch 750 Loss 2.0633 Accuracy 0.5845\n",
            "discarded batch 763\n",
            "Epoch 73 Batch 800 Loss 2.0650 Accuracy 0.5845\n",
            "Epoch 73 Batch 850 Loss 2.0645 Accuracy 0.5846\n",
            "Epoch 73 Batch 900 Loss 2.0653 Accuracy 0.5846\n",
            "Epoch 73 Batch 950 Loss 2.0661 Accuracy 0.5847\n",
            "Epoch 73 Loss 2.0659 Accuracy 0.5847\n",
            "Time taken for 1 epoch: 23.888068437576294 secs\n",
            "\n",
            "Epoch 74 Batch 0 Loss 2.0113 Accuracy 0.5875\n",
            "Epoch 74 Batch 50 Loss 2.0145 Accuracy 0.5891\n",
            "Epoch 74 Batch 100 Loss 2.0273 Accuracy 0.5875\n",
            "Epoch 74 Batch 150 Loss 2.0500 Accuracy 0.5848\n",
            "Epoch 74 Batch 200 Loss 2.0609 Accuracy 0.5840\n",
            "Epoch 74 Batch 250 Loss 2.0557 Accuracy 0.5843\n",
            "Epoch 74 Batch 300 Loss 2.0522 Accuracy 0.5850\n",
            "Epoch 74 Batch 350 Loss 2.0550 Accuracy 0.5847\n",
            "Epoch 74 Batch 400 Loss 2.0553 Accuracy 0.5848\n",
            "Epoch 74 Batch 450 Loss 2.0524 Accuracy 0.5855\n",
            "Epoch 74 Batch 500 Loss 2.0542 Accuracy 0.5855\n",
            "Epoch 74 Batch 550 Loss 2.0560 Accuracy 0.5856\n",
            "discarded batch 586\n",
            "Epoch 74 Batch 600 Loss 2.0559 Accuracy 0.5858\n",
            "Epoch 74 Batch 650 Loss 2.0572 Accuracy 0.5854\n",
            "Epoch 74 Batch 700 Loss 2.0577 Accuracy 0.5852\n",
            "Epoch 74 Batch 750 Loss 2.0578 Accuracy 0.5854\n",
            "Epoch 74 Batch 800 Loss 2.0589 Accuracy 0.5853\n",
            "Epoch 74 Batch 850 Loss 2.0602 Accuracy 0.5850\n",
            "Epoch 74 Batch 900 Loss 2.0610 Accuracy 0.5851\n",
            "Epoch 74 Batch 950 Loss 2.0621 Accuracy 0.5851\n",
            "Epoch 74 Loss 2.0624 Accuracy 0.5850\n",
            "Time taken for 1 epoch: 23.930126905441284 secs\n",
            "\n",
            "Epoch 75 Batch 0 Loss 2.1052 Accuracy 0.5743\n",
            "Epoch 75 Batch 50 Loss 2.0442 Accuracy 0.5851\n",
            "Epoch 75 Batch 100 Loss 2.0444 Accuracy 0.5845\n",
            "Epoch 75 Batch 150 Loss 2.0448 Accuracy 0.5849\n",
            "Epoch 75 Batch 200 Loss 2.0499 Accuracy 0.5854\n",
            "Epoch 75 Batch 250 Loss 2.0517 Accuracy 0.5864\n",
            "discarded batch 258\n",
            "Epoch 75 Batch 300 Loss 2.0522 Accuracy 0.5864\n",
            "Epoch 75 Batch 350 Loss 2.0510 Accuracy 0.5866\n",
            "Epoch 75 Batch 400 Loss 2.0528 Accuracy 0.5865\n",
            "Epoch 75 Batch 450 Loss 2.0512 Accuracy 0.5867\n",
            "Epoch 75 Batch 500 Loss 2.0518 Accuracy 0.5865\n",
            "Epoch 75 Batch 550 Loss 2.0535 Accuracy 0.5863\n",
            "Epoch 75 Batch 600 Loss 2.0541 Accuracy 0.5861\n",
            "Epoch 75 Batch 650 Loss 2.0544 Accuracy 0.5862\n",
            "Epoch 75 Batch 700 Loss 2.0546 Accuracy 0.5861\n",
            "Epoch 75 Batch 750 Loss 2.0527 Accuracy 0.5861\n",
            "Epoch 75 Batch 800 Loss 2.0536 Accuracy 0.5862\n",
            "Epoch 75 Batch 850 Loss 2.0539 Accuracy 0.5861\n",
            "Epoch 75 Batch 900 Loss 2.0569 Accuracy 0.5858\n",
            "Epoch 75 Batch 950 Loss 2.0588 Accuracy 0.5857\n",
            "Saving checkpoint for epoch 75 at ./checkpoints/train/ckpt-15\n",
            "Epoch 75 Loss 2.0588 Accuracy 0.5858\n",
            "Time taken for 1 epoch: 24.70638418197632 secs\n",
            "\n",
            "Epoch 76 Batch 0 Loss 2.0927 Accuracy 0.5611\n",
            "discarded batch 44\n",
            "Epoch 76 Batch 50 Loss 2.0071 Accuracy 0.5938\n",
            "Epoch 76 Batch 100 Loss 2.0350 Accuracy 0.5898\n",
            "Epoch 76 Batch 150 Loss 2.0369 Accuracy 0.5888\n",
            "Epoch 76 Batch 200 Loss 2.0405 Accuracy 0.5885\n",
            "Epoch 76 Batch 250 Loss 2.0390 Accuracy 0.5882\n",
            "Epoch 76 Batch 300 Loss 2.0434 Accuracy 0.5878\n",
            "Epoch 76 Batch 350 Loss 2.0460 Accuracy 0.5870\n",
            "Epoch 76 Batch 400 Loss 2.0470 Accuracy 0.5868\n",
            "Epoch 76 Batch 450 Loss 2.0486 Accuracy 0.5865\n",
            "Epoch 76 Batch 500 Loss 2.0492 Accuracy 0.5863\n",
            "Epoch 76 Batch 550 Loss 2.0503 Accuracy 0.5859\n",
            "Epoch 76 Batch 600 Loss 2.0502 Accuracy 0.5858\n",
            "Epoch 76 Batch 650 Loss 2.0507 Accuracy 0.5861\n",
            "Epoch 76 Batch 700 Loss 2.0504 Accuracy 0.5864\n",
            "Epoch 76 Batch 750 Loss 2.0509 Accuracy 0.5861\n",
            "Epoch 76 Batch 800 Loss 2.0519 Accuracy 0.5862\n",
            "Epoch 76 Batch 850 Loss 2.0523 Accuracy 0.5861\n",
            "Epoch 76 Batch 900 Loss 2.0532 Accuracy 0.5862\n",
            "Epoch 76 Batch 950 Loss 2.0561 Accuracy 0.5861\n",
            "Epoch 76 Loss 2.0560 Accuracy 0.5861\n",
            "Time taken for 1 epoch: 24.516141653060913 secs\n",
            "\n",
            "Epoch 76 VALIDATION: Loss 2.2236 Accuracy 0.5776\n",
            "\n",
            "Epoch 77 Batch 0 Loss 2.0041 Accuracy 0.6040\n",
            "Epoch 77 Batch 50 Loss 2.0012 Accuracy 0.5924\n",
            "Epoch 77 Batch 100 Loss 2.0199 Accuracy 0.5908\n",
            "Epoch 77 Batch 150 Loss 2.0285 Accuracy 0.5887\n",
            "Epoch 77 Batch 200 Loss 2.0361 Accuracy 0.5875\n",
            "Epoch 77 Batch 250 Loss 2.0396 Accuracy 0.5879\n",
            "Epoch 77 Batch 300 Loss 2.0379 Accuracy 0.5879\n",
            "Epoch 77 Batch 350 Loss 2.0402 Accuracy 0.5878\n",
            "Epoch 77 Batch 400 Loss 2.0414 Accuracy 0.5877\n",
            "Epoch 77 Batch 450 Loss 2.0464 Accuracy 0.5870\n",
            "Epoch 77 Batch 500 Loss 2.0464 Accuracy 0.5873\n",
            "Epoch 77 Batch 550 Loss 2.0458 Accuracy 0.5872\n",
            "Epoch 77 Batch 600 Loss 2.0469 Accuracy 0.5871\n",
            "Epoch 77 Batch 650 Loss 2.0479 Accuracy 0.5868\n",
            "Epoch 77 Batch 700 Loss 2.0468 Accuracy 0.5870\n",
            "Epoch 77 Batch 750 Loss 2.0481 Accuracy 0.5870\n",
            "discarded batch 769\n",
            "Epoch 77 Batch 800 Loss 2.0504 Accuracy 0.5868\n",
            "Epoch 77 Batch 850 Loss 2.0512 Accuracy 0.5867\n",
            "Epoch 77 Batch 900 Loss 2.0520 Accuracy 0.5866\n",
            "Epoch 77 Batch 950 Loss 2.0520 Accuracy 0.5866\n",
            "Epoch 77 Loss 2.0520 Accuracy 0.5866\n",
            "Time taken for 1 epoch: 24.39171314239502 secs\n",
            "\n",
            "Epoch 78 Batch 0 Loss 1.9585 Accuracy 0.6007\n",
            "Epoch 78 Batch 50 Loss 2.0213 Accuracy 0.5880\n",
            "Epoch 78 Batch 100 Loss 2.0228 Accuracy 0.5881\n",
            "Epoch 78 Batch 150 Loss 2.0273 Accuracy 0.5875\n",
            "Epoch 78 Batch 200 Loss 2.0306 Accuracy 0.5881\n",
            "Epoch 78 Batch 250 Loss 2.0308 Accuracy 0.5878\n",
            "Epoch 78 Batch 300 Loss 2.0372 Accuracy 0.5869\n",
            "Epoch 78 Batch 350 Loss 2.0407 Accuracy 0.5867\n",
            "Epoch 78 Batch 400 Loss 2.0413 Accuracy 0.5870\n",
            "Epoch 78 Batch 450 Loss 2.0412 Accuracy 0.5873\n",
            "Epoch 78 Batch 500 Loss 2.0413 Accuracy 0.5873\n",
            "Epoch 78 Batch 550 Loss 2.0413 Accuracy 0.5876\n",
            "Epoch 78 Batch 600 Loss 2.0408 Accuracy 0.5877\n",
            "Epoch 78 Batch 650 Loss 2.0414 Accuracy 0.5876\n",
            "Epoch 78 Batch 700 Loss 2.0416 Accuracy 0.5876\n",
            "discarded batch 709\n",
            "Epoch 78 Batch 750 Loss 2.0422 Accuracy 0.5876\n",
            "Epoch 78 Batch 800 Loss 2.0448 Accuracy 0.5873\n",
            "Epoch 78 Batch 850 Loss 2.0458 Accuracy 0.5872\n",
            "Epoch 78 Batch 900 Loss 2.0466 Accuracy 0.5870\n",
            "Epoch 78 Batch 950 Loss 2.0476 Accuracy 0.5871\n",
            "Epoch 78 Loss 2.0477 Accuracy 0.5871\n",
            "Time taken for 1 epoch: 24.464659929275513 secs\n",
            "\n",
            "Epoch 79 Batch 0 Loss 2.1382 Accuracy 0.5710\n",
            "Epoch 79 Batch 50 Loss 2.0464 Accuracy 0.5840\n",
            "Epoch 79 Batch 100 Loss 2.0390 Accuracy 0.5882\n",
            "Epoch 79 Batch 150 Loss 2.0335 Accuracy 0.5867\n",
            "Epoch 79 Batch 200 Loss 2.0344 Accuracy 0.5852\n",
            "Epoch 79 Batch 250 Loss 2.0328 Accuracy 0.5854\n",
            "Epoch 79 Batch 300 Loss 2.0322 Accuracy 0.5861\n",
            "Epoch 79 Batch 350 Loss 2.0338 Accuracy 0.5867\n",
            "Epoch 79 Batch 400 Loss 2.0335 Accuracy 0.5869\n",
            "Epoch 79 Batch 450 Loss 2.0379 Accuracy 0.5866\n",
            "Epoch 79 Batch 500 Loss 2.0401 Accuracy 0.5864\n",
            "Epoch 79 Batch 550 Loss 2.0400 Accuracy 0.5868\n",
            "Epoch 79 Batch 600 Loss 2.0384 Accuracy 0.5872\n",
            "Epoch 79 Batch 650 Loss 2.0409 Accuracy 0.5871\n",
            "discarded batch 694\n",
            "Epoch 79 Batch 700 Loss 2.0421 Accuracy 0.5870\n",
            "Epoch 79 Batch 750 Loss 2.0419 Accuracy 0.5871\n",
            "Epoch 79 Batch 800 Loss 2.0414 Accuracy 0.5872\n",
            "Epoch 79 Batch 850 Loss 2.0435 Accuracy 0.5870\n",
            "Epoch 79 Batch 900 Loss 2.0447 Accuracy 0.5870\n",
            "Epoch 79 Batch 950 Loss 2.0446 Accuracy 0.5871\n",
            "Epoch 79 Loss 2.0446 Accuracy 0.5871\n",
            "Time taken for 1 epoch: 24.55464458465576 secs\n",
            "\n",
            "Epoch 80 Batch 0 Loss 1.9687 Accuracy 0.6007\n",
            "Epoch 80 Batch 50 Loss 2.0561 Accuracy 0.5889\n",
            "Epoch 80 Batch 100 Loss 2.0401 Accuracy 0.5900\n",
            "Epoch 80 Batch 150 Loss 2.0295 Accuracy 0.5904\n",
            "Epoch 80 Batch 200 Loss 2.0310 Accuracy 0.5900\n",
            "Epoch 80 Batch 250 Loss 2.0333 Accuracy 0.5890\n",
            "Epoch 80 Batch 300 Loss 2.0326 Accuracy 0.5899\n",
            "Epoch 80 Batch 350 Loss 2.0303 Accuracy 0.5904\n",
            "Epoch 80 Batch 400 Loss 2.0299 Accuracy 0.5904\n",
            "Epoch 80 Batch 450 Loss 2.0311 Accuracy 0.5905\n",
            "Epoch 80 Batch 500 Loss 2.0339 Accuracy 0.5897\n",
            "Epoch 80 Batch 550 Loss 2.0345 Accuracy 0.5895\n",
            "Epoch 80 Batch 600 Loss 2.0354 Accuracy 0.5894\n",
            "Epoch 80 Batch 650 Loss 2.0373 Accuracy 0.5889\n",
            "discarded batch 679\n",
            "Epoch 80 Batch 700 Loss 2.0394 Accuracy 0.5886\n",
            "Epoch 80 Batch 750 Loss 2.0396 Accuracy 0.5885\n",
            "Epoch 80 Batch 800 Loss 2.0407 Accuracy 0.5883\n",
            "Epoch 80 Batch 850 Loss 2.0403 Accuracy 0.5884\n",
            "Epoch 80 Batch 900 Loss 2.0407 Accuracy 0.5883\n",
            "Epoch 80 Batch 950 Loss 2.0394 Accuracy 0.5887\n",
            "Saving checkpoint for epoch 80 at ./checkpoints/train/ckpt-16\n",
            "Epoch 80 Loss 2.0397 Accuracy 0.5887\n",
            "Time taken for 1 epoch: 24.901607990264893 secs\n",
            "\n",
            "Epoch 81 Batch 0 Loss 2.0747 Accuracy 0.5776\n",
            "Epoch 81 Batch 50 Loss 2.0147 Accuracy 0.5898\n",
            "Epoch 81 Batch 100 Loss 2.0277 Accuracy 0.5895\n",
            "Epoch 81 Batch 150 Loss 2.0321 Accuracy 0.5895\n",
            "Epoch 81 Batch 200 Loss 2.0164 Accuracy 0.5911\n",
            "Epoch 81 Batch 250 Loss 2.0240 Accuracy 0.5905\n",
            "Epoch 81 Batch 300 Loss 2.0276 Accuracy 0.5904\n",
            "discarded batch 336\n",
            "Epoch 81 Batch 350 Loss 2.0272 Accuracy 0.5902\n",
            "Epoch 81 Batch 400 Loss 2.0322 Accuracy 0.5895\n",
            "Epoch 81 Batch 450 Loss 2.0332 Accuracy 0.5896\n",
            "Epoch 81 Batch 500 Loss 2.0350 Accuracy 0.5896\n",
            "Epoch 81 Batch 550 Loss 2.0374 Accuracy 0.5893\n",
            "Epoch 81 Batch 600 Loss 2.0363 Accuracy 0.5893\n",
            "Epoch 81 Batch 650 Loss 2.0365 Accuracy 0.5893\n",
            "Epoch 81 Batch 700 Loss 2.0371 Accuracy 0.5892\n",
            "Epoch 81 Batch 750 Loss 2.0370 Accuracy 0.5889\n",
            "Epoch 81 Batch 800 Loss 2.0365 Accuracy 0.5891\n",
            "Epoch 81 Batch 850 Loss 2.0379 Accuracy 0.5889\n",
            "Epoch 81 Batch 900 Loss 2.0383 Accuracy 0.5888\n",
            "Epoch 81 Batch 950 Loss 2.0389 Accuracy 0.5886\n",
            "Epoch 81 Loss 2.0384 Accuracy 0.5887\n",
            "Time taken for 1 epoch: 24.400362253189087 secs\n",
            "\n",
            "Epoch 81 VALIDATION: Loss 2.2183 Accuracy 0.5816\n",
            "\n",
            "Epoch 82 Batch 0 Loss 2.2309 Accuracy 0.5446\n",
            "Epoch 82 Batch 50 Loss 2.0067 Accuracy 0.5943\n",
            "Epoch 82 Batch 100 Loss 2.0181 Accuracy 0.5931\n",
            "Epoch 82 Batch 150 Loss 2.0108 Accuracy 0.5946\n",
            "discarded batch 157\n",
            "Epoch 82 Batch 200 Loss 2.0145 Accuracy 0.5933\n",
            "Epoch 82 Batch 250 Loss 2.0154 Accuracy 0.5925\n",
            "Epoch 82 Batch 300 Loss 2.0189 Accuracy 0.5920\n",
            "Epoch 82 Batch 350 Loss 2.0214 Accuracy 0.5908\n",
            "Epoch 82 Batch 400 Loss 2.0209 Accuracy 0.5912\n",
            "Epoch 82 Batch 450 Loss 2.0252 Accuracy 0.5906\n",
            "Epoch 82 Batch 500 Loss 2.0262 Accuracy 0.5900\n",
            "Epoch 82 Batch 550 Loss 2.0288 Accuracy 0.5898\n",
            "Epoch 82 Batch 600 Loss 2.0300 Accuracy 0.5895\n",
            "Epoch 82 Batch 650 Loss 2.0314 Accuracy 0.5891\n",
            "Epoch 82 Batch 700 Loss 2.0322 Accuracy 0.5891\n",
            "Epoch 82 Batch 750 Loss 2.0314 Accuracy 0.5890\n",
            "Epoch 82 Batch 800 Loss 2.0315 Accuracy 0.5891\n",
            "Epoch 82 Batch 850 Loss 2.0319 Accuracy 0.5893\n",
            "Epoch 82 Batch 900 Loss 2.0322 Accuracy 0.5893\n",
            "Epoch 82 Batch 950 Loss 2.0335 Accuracy 0.5891\n",
            "Epoch 82 Loss 2.0333 Accuracy 0.5891\n",
            "Time taken for 1 epoch: 24.617244958877563 secs\n",
            "\n",
            "Epoch 83 Batch 0 Loss 2.2032 Accuracy 0.5545\n",
            "Epoch 83 Batch 50 Loss 2.0103 Accuracy 0.5907\n",
            "Epoch 83 Batch 100 Loss 2.0171 Accuracy 0.5906\n",
            "Epoch 83 Batch 150 Loss 2.0135 Accuracy 0.5918\n",
            "Epoch 83 Batch 200 Loss 2.0068 Accuracy 0.5921\n",
            "Epoch 83 Batch 250 Loss 2.0106 Accuracy 0.5913\n",
            "Epoch 83 Batch 300 Loss 2.0142 Accuracy 0.5912\n",
            "Epoch 83 Batch 350 Loss 2.0175 Accuracy 0.5913\n",
            "Epoch 83 Batch 400 Loss 2.0171 Accuracy 0.5918\n",
            "Epoch 83 Batch 450 Loss 2.0178 Accuracy 0.5912\n",
            "Epoch 83 Batch 500 Loss 2.0187 Accuracy 0.5910\n",
            "Epoch 83 Batch 550 Loss 2.0185 Accuracy 0.5912\n",
            "Epoch 83 Batch 600 Loss 2.0193 Accuracy 0.5908\n",
            "Epoch 83 Batch 650 Loss 2.0247 Accuracy 0.5902\n",
            "Epoch 83 Batch 700 Loss 2.0240 Accuracy 0.5902\n",
            "Epoch 83 Batch 750 Loss 2.0254 Accuracy 0.5897\n",
            "Epoch 83 Batch 800 Loss 2.0253 Accuracy 0.5898\n",
            "Epoch 83 Batch 850 Loss 2.0264 Accuracy 0.5896\n",
            "Epoch 83 Batch 900 Loss 2.0285 Accuracy 0.5896\n",
            "discarded batch 943\n",
            "Epoch 83 Batch 950 Loss 2.0298 Accuracy 0.5894\n",
            "Epoch 83 Loss 2.0299 Accuracy 0.5894\n",
            "Time taken for 1 epoch: 23.867706775665283 secs\n",
            "\n",
            "Epoch 84 Batch 0 Loss 1.9216 Accuracy 0.5545\n",
            "Epoch 84 Batch 50 Loss 1.9867 Accuracy 0.5952\n",
            "discarded batch 72\n",
            "Epoch 84 Batch 100 Loss 1.9882 Accuracy 0.5939\n",
            "Epoch 84 Batch 150 Loss 1.9980 Accuracy 0.5919\n",
            "Epoch 84 Batch 200 Loss 2.0049 Accuracy 0.5901\n",
            "Epoch 84 Batch 250 Loss 2.0095 Accuracy 0.5890\n",
            "Epoch 84 Batch 300 Loss 2.0073 Accuracy 0.5893\n",
            "Epoch 84 Batch 350 Loss 2.0094 Accuracy 0.5894\n",
            "Epoch 84 Batch 400 Loss 2.0136 Accuracy 0.5890\n",
            "Epoch 84 Batch 450 Loss 2.0161 Accuracy 0.5895\n",
            "Epoch 84 Batch 500 Loss 2.0162 Accuracy 0.5900\n",
            "Epoch 84 Batch 550 Loss 2.0176 Accuracy 0.5899\n",
            "Epoch 84 Batch 600 Loss 2.0188 Accuracy 0.5898\n",
            "Epoch 84 Batch 650 Loss 2.0209 Accuracy 0.5899\n",
            "Epoch 84 Batch 700 Loss 2.0206 Accuracy 0.5901\n",
            "Epoch 84 Batch 750 Loss 2.0212 Accuracy 0.5901\n",
            "Epoch 84 Batch 800 Loss 2.0221 Accuracy 0.5898\n",
            "Epoch 84 Batch 850 Loss 2.0232 Accuracy 0.5898\n",
            "Epoch 84 Batch 900 Loss 2.0243 Accuracy 0.5898\n",
            "Epoch 84 Batch 950 Loss 2.0260 Accuracy 0.5897\n",
            "Epoch 84 Loss 2.0264 Accuracy 0.5897\n",
            "Time taken for 1 epoch: 24.013752937316895 secs\n",
            "\n",
            "Epoch 85 Batch 0 Loss 2.0528 Accuracy 0.5743\n",
            "Epoch 85 Batch 50 Loss 1.9772 Accuracy 0.5962\n",
            "discarded batch 56\n",
            "Epoch 85 Batch 100 Loss 1.9867 Accuracy 0.5949\n",
            "Epoch 85 Batch 150 Loss 1.9892 Accuracy 0.5947\n",
            "Epoch 85 Batch 200 Loss 1.9870 Accuracy 0.5947\n",
            "Epoch 85 Batch 250 Loss 1.9960 Accuracy 0.5936\n",
            "Epoch 85 Batch 300 Loss 2.0008 Accuracy 0.5933\n",
            "Epoch 85 Batch 350 Loss 2.0033 Accuracy 0.5922\n",
            "Epoch 85 Batch 400 Loss 2.0038 Accuracy 0.5924\n",
            "Epoch 85 Batch 450 Loss 2.0104 Accuracy 0.5915\n",
            "Epoch 85 Batch 500 Loss 2.0118 Accuracy 0.5914\n",
            "Epoch 85 Batch 550 Loss 2.0141 Accuracy 0.5913\n",
            "Epoch 85 Batch 600 Loss 2.0139 Accuracy 0.5917\n",
            "Epoch 85 Batch 650 Loss 2.0149 Accuracy 0.5916\n",
            "Epoch 85 Batch 700 Loss 2.0175 Accuracy 0.5917\n",
            "Epoch 85 Batch 750 Loss 2.0182 Accuracy 0.5919\n",
            "Epoch 85 Batch 800 Loss 2.0207 Accuracy 0.5915\n",
            "Epoch 85 Batch 850 Loss 2.0218 Accuracy 0.5914\n",
            "Epoch 85 Batch 900 Loss 2.0224 Accuracy 0.5912\n",
            "Epoch 85 Batch 950 Loss 2.0233 Accuracy 0.5911\n",
            "Saving checkpoint for epoch 85 at ./checkpoints/train/ckpt-17\n",
            "Epoch 85 Loss 2.0234 Accuracy 0.5911\n",
            "Time taken for 1 epoch: 24.272379875183105 secs\n",
            "\n",
            "Epoch 86 Batch 0 Loss 2.2926 Accuracy 0.5710\n",
            "Epoch 86 Batch 50 Loss 2.0204 Accuracy 0.5917\n",
            "Epoch 86 Batch 100 Loss 2.0057 Accuracy 0.5925\n",
            "Epoch 86 Batch 150 Loss 2.0080 Accuracy 0.5920\n",
            "Epoch 86 Batch 200 Loss 2.0069 Accuracy 0.5928\n",
            "Epoch 86 Batch 250 Loss 2.0142 Accuracy 0.5911\n",
            "Epoch 86 Batch 300 Loss 2.0168 Accuracy 0.5913\n",
            "Epoch 86 Batch 350 Loss 2.0146 Accuracy 0.5920\n",
            "Epoch 86 Batch 400 Loss 2.0158 Accuracy 0.5917\n",
            "Epoch 86 Batch 450 Loss 2.0152 Accuracy 0.5918\n",
            "Epoch 86 Batch 500 Loss 2.0163 Accuracy 0.5914\n",
            "Epoch 86 Batch 550 Loss 2.0148 Accuracy 0.5916\n",
            "Epoch 86 Batch 600 Loss 2.0158 Accuracy 0.5912\n",
            "Epoch 86 Batch 650 Loss 2.0153 Accuracy 0.5911\n",
            "discarded batch 688\n",
            "Epoch 86 Batch 700 Loss 2.0176 Accuracy 0.5906\n",
            "Epoch 86 Batch 750 Loss 2.0170 Accuracy 0.5906\n",
            "Epoch 86 Batch 800 Loss 2.0174 Accuracy 0.5905\n",
            "Epoch 86 Batch 850 Loss 2.0203 Accuracy 0.5906\n",
            "Epoch 86 Batch 900 Loss 2.0204 Accuracy 0.5906\n",
            "Epoch 86 Batch 950 Loss 2.0198 Accuracy 0.5906\n",
            "Epoch 86 Loss 2.0198 Accuracy 0.5906\n",
            "Time taken for 1 epoch: 23.858546495437622 secs\n",
            "\n",
            "Epoch 86 VALIDATION: Loss 2.2119 Accuracy 0.5801\n",
            "\n",
            "Epoch 87 Batch 0 Loss 1.8428 Accuracy 0.6238\n",
            "Epoch 87 Batch 50 Loss 2.0092 Accuracy 0.5909\n",
            "Epoch 87 Batch 100 Loss 2.0127 Accuracy 0.5909\n",
            "Epoch 87 Batch 150 Loss 2.0161 Accuracy 0.5911\n",
            "discarded batch 189\n",
            "Epoch 87 Batch 200 Loss 2.0135 Accuracy 0.5911\n",
            "Epoch 87 Batch 250 Loss 2.0146 Accuracy 0.5908\n",
            "Epoch 87 Batch 300 Loss 2.0106 Accuracy 0.5911\n",
            "Epoch 87 Batch 350 Loss 2.0112 Accuracy 0.5919\n",
            "Epoch 87 Batch 400 Loss 2.0134 Accuracy 0.5915\n",
            "Epoch 87 Batch 450 Loss 2.0128 Accuracy 0.5918\n",
            "Epoch 87 Batch 500 Loss 2.0110 Accuracy 0.5918\n",
            "Epoch 87 Batch 550 Loss 2.0134 Accuracy 0.5914\n",
            "Epoch 87 Batch 600 Loss 2.0138 Accuracy 0.5911\n",
            "Epoch 87 Batch 650 Loss 2.0144 Accuracy 0.5914\n",
            "Epoch 87 Batch 700 Loss 2.0159 Accuracy 0.5914\n",
            "Epoch 87 Batch 750 Loss 2.0179 Accuracy 0.5913\n",
            "Epoch 87 Batch 800 Loss 2.0179 Accuracy 0.5911\n",
            "Epoch 87 Batch 850 Loss 2.0178 Accuracy 0.5911\n",
            "Epoch 87 Batch 900 Loss 2.0168 Accuracy 0.5911\n",
            "Epoch 87 Batch 950 Loss 2.0180 Accuracy 0.5911\n",
            "Epoch 87 Loss 2.0182 Accuracy 0.5911\n",
            "Time taken for 1 epoch: 23.889222145080566 secs\n",
            "\n",
            "Epoch 88 Batch 0 Loss 2.0122 Accuracy 0.5842\n",
            "Epoch 88 Batch 50 Loss 1.9949 Accuracy 0.5940\n",
            "Epoch 88 Batch 100 Loss 1.9787 Accuracy 0.5932\n",
            "discarded batch 126\n",
            "Epoch 88 Batch 150 Loss 1.9830 Accuracy 0.5930\n",
            "Epoch 88 Batch 200 Loss 1.9902 Accuracy 0.5926\n",
            "Epoch 88 Batch 250 Loss 1.9907 Accuracy 0.5926\n",
            "Epoch 88 Batch 300 Loss 1.9939 Accuracy 0.5928\n",
            "Epoch 88 Batch 350 Loss 1.9938 Accuracy 0.5930\n",
            "Epoch 88 Batch 400 Loss 1.9961 Accuracy 0.5924\n",
            "Epoch 88 Batch 450 Loss 1.9977 Accuracy 0.5925\n",
            "Epoch 88 Batch 500 Loss 1.9979 Accuracy 0.5924\n",
            "Epoch 88 Batch 550 Loss 2.0007 Accuracy 0.5919\n",
            "Epoch 88 Batch 600 Loss 2.0030 Accuracy 0.5918\n",
            "Epoch 88 Batch 650 Loss 2.0038 Accuracy 0.5917\n",
            "Epoch 88 Batch 700 Loss 2.0065 Accuracy 0.5915\n",
            "Epoch 88 Batch 750 Loss 2.0079 Accuracy 0.5915\n",
            "Epoch 88 Batch 800 Loss 2.0089 Accuracy 0.5917\n",
            "Epoch 88 Batch 850 Loss 2.0086 Accuracy 0.5917\n",
            "Epoch 88 Batch 900 Loss 2.0092 Accuracy 0.5918\n",
            "Epoch 88 Batch 950 Loss 2.0136 Accuracy 0.5915\n",
            "Epoch 88 Loss 2.0140 Accuracy 0.5915\n",
            "Time taken for 1 epoch: 24.11673855781555 secs\n",
            "\n",
            "Epoch 89 Batch 0 Loss 1.9556 Accuracy 0.6238\n",
            "Epoch 89 Batch 50 Loss 1.9845 Accuracy 0.5966\n",
            "Epoch 89 Batch 100 Loss 1.9756 Accuracy 0.5955\n",
            "Epoch 89 Batch 150 Loss 1.9881 Accuracy 0.5953\n",
            "Epoch 89 Batch 200 Loss 1.9876 Accuracy 0.5953\n",
            "Epoch 89 Batch 250 Loss 1.9869 Accuracy 0.5952\n",
            "Epoch 89 Batch 300 Loss 1.9853 Accuracy 0.5955\n",
            "Epoch 89 Batch 350 Loss 1.9867 Accuracy 0.5950\n",
            "Epoch 89 Batch 400 Loss 1.9892 Accuracy 0.5946\n",
            "Epoch 89 Batch 450 Loss 1.9936 Accuracy 0.5946\n",
            "Epoch 89 Batch 500 Loss 1.9937 Accuracy 0.5948\n",
            "Epoch 89 Batch 550 Loss 1.9976 Accuracy 0.5941\n",
            "discarded batch 598\n",
            "Epoch 89 Batch 600 Loss 2.0010 Accuracy 0.5937\n",
            "Epoch 89 Batch 650 Loss 2.0022 Accuracy 0.5937\n",
            "Epoch 89 Batch 700 Loss 2.0025 Accuracy 0.5936\n",
            "Epoch 89 Batch 750 Loss 2.0039 Accuracy 0.5932\n",
            "Epoch 89 Batch 800 Loss 2.0053 Accuracy 0.5931\n",
            "Epoch 89 Batch 850 Loss 2.0075 Accuracy 0.5928\n",
            "Epoch 89 Batch 900 Loss 2.0076 Accuracy 0.5927\n",
            "Epoch 89 Batch 950 Loss 2.0095 Accuracy 0.5924\n",
            "Epoch 89 Loss 2.0094 Accuracy 0.5924\n",
            "Time taken for 1 epoch: 24.218218326568604 secs\n",
            "\n",
            "Epoch 90 Batch 0 Loss 2.0961 Accuracy 0.5512\n",
            "Epoch 90 Batch 50 Loss 1.9669 Accuracy 0.5962\n",
            "Epoch 90 Batch 100 Loss 1.9787 Accuracy 0.5945\n",
            "discarded batch 146\n",
            "Epoch 90 Batch 150 Loss 1.9825 Accuracy 0.5939\n",
            "Epoch 90 Batch 200 Loss 1.9818 Accuracy 0.5943\n",
            "Epoch 90 Batch 250 Loss 1.9821 Accuracy 0.5945\n",
            "Epoch 90 Batch 300 Loss 1.9844 Accuracy 0.5945\n",
            "Epoch 90 Batch 350 Loss 1.9878 Accuracy 0.5940\n",
            "Epoch 90 Batch 400 Loss 1.9910 Accuracy 0.5939\n",
            "Epoch 90 Batch 450 Loss 1.9932 Accuracy 0.5936\n",
            "Epoch 90 Batch 500 Loss 1.9951 Accuracy 0.5933\n",
            "Epoch 90 Batch 550 Loss 1.9923 Accuracy 0.5934\n",
            "Epoch 90 Batch 600 Loss 1.9968 Accuracy 0.5928\n",
            "Epoch 90 Batch 650 Loss 1.9973 Accuracy 0.5927\n",
            "Epoch 90 Batch 700 Loss 1.9990 Accuracy 0.5924\n",
            "Epoch 90 Batch 750 Loss 2.0002 Accuracy 0.5926\n",
            "Epoch 90 Batch 800 Loss 2.0012 Accuracy 0.5927\n",
            "Epoch 90 Batch 850 Loss 2.0032 Accuracy 0.5925\n",
            "Epoch 90 Batch 900 Loss 2.0057 Accuracy 0.5920\n",
            "Epoch 90 Batch 950 Loss 2.0067 Accuracy 0.5921\n",
            "Saving checkpoint for epoch 90 at ./checkpoints/train/ckpt-18\n",
            "Epoch 90 Loss 2.0069 Accuracy 0.5921\n",
            "Time taken for 1 epoch: 24.931668758392334 secs\n",
            "\n",
            "Epoch 91 Batch 0 Loss 1.9677 Accuracy 0.5908\n",
            "Epoch 91 Batch 50 Loss 1.9835 Accuracy 0.5933\n",
            "Epoch 91 Batch 100 Loss 1.9790 Accuracy 0.5939\n",
            "Epoch 91 Batch 150 Loss 1.9812 Accuracy 0.5934\n",
            "Epoch 91 Batch 200 Loss 1.9791 Accuracy 0.5942\n",
            "Epoch 91 Batch 250 Loss 1.9807 Accuracy 0.5946\n",
            "Epoch 91 Batch 300 Loss 1.9786 Accuracy 0.5950\n",
            "Epoch 91 Batch 350 Loss 1.9855 Accuracy 0.5944\n",
            "discarded batch 351\n",
            "Epoch 91 Batch 400 Loss 1.9883 Accuracy 0.5943\n",
            "Epoch 91 Batch 450 Loss 1.9895 Accuracy 0.5943\n",
            "Epoch 91 Batch 500 Loss 1.9906 Accuracy 0.5943\n",
            "Epoch 91 Batch 550 Loss 1.9924 Accuracy 0.5943\n",
            "Epoch 91 Batch 600 Loss 1.9955 Accuracy 0.5938\n",
            "Epoch 91 Batch 650 Loss 1.9964 Accuracy 0.5936\n",
            "Epoch 91 Batch 700 Loss 1.9984 Accuracy 0.5931\n",
            "Epoch 91 Batch 750 Loss 1.9996 Accuracy 0.5929\n",
            "Epoch 91 Batch 800 Loss 2.0008 Accuracy 0.5928\n",
            "Epoch 91 Batch 850 Loss 2.0026 Accuracy 0.5926\n",
            "Epoch 91 Batch 900 Loss 2.0027 Accuracy 0.5926\n",
            "Epoch 91 Batch 950 Loss 2.0029 Accuracy 0.5928\n",
            "Epoch 91 Loss 2.0027 Accuracy 0.5928\n",
            "Time taken for 1 epoch: 24.237043857574463 secs\n",
            "\n",
            "Epoch 91 VALIDATION: Loss 2.2252 Accuracy 0.5799\n",
            "\n",
            "Epoch 92 Batch 0 Loss 1.9210 Accuracy 0.6304\n",
            "Epoch 92 Batch 50 Loss 1.9607 Accuracy 0.5992\n",
            "Epoch 92 Batch 100 Loss 1.9712 Accuracy 0.5961\n",
            "Epoch 92 Batch 150 Loss 1.9676 Accuracy 0.5955\n",
            "Epoch 92 Batch 200 Loss 1.9731 Accuracy 0.5954\n",
            "Epoch 92 Batch 250 Loss 1.9770 Accuracy 0.5949\n",
            "Epoch 92 Batch 300 Loss 1.9779 Accuracy 0.5952\n",
            "Epoch 92 Batch 350 Loss 1.9787 Accuracy 0.5951\n",
            "Epoch 92 Batch 400 Loss 1.9805 Accuracy 0.5952\n",
            "Epoch 92 Batch 450 Loss 1.9840 Accuracy 0.5945\n",
            "Epoch 92 Batch 500 Loss 1.9872 Accuracy 0.5944\n",
            "Epoch 92 Batch 550 Loss 1.9895 Accuracy 0.5942\n",
            "Epoch 92 Batch 600 Loss 1.9919 Accuracy 0.5939\n",
            "discarded batch 633\n",
            "Epoch 92 Batch 650 Loss 1.9934 Accuracy 0.5938\n",
            "Epoch 92 Batch 700 Loss 1.9946 Accuracy 0.5935\n",
            "Epoch 92 Batch 750 Loss 1.9958 Accuracy 0.5933\n",
            "Epoch 92 Batch 800 Loss 1.9980 Accuracy 0.5931\n",
            "Epoch 92 Batch 850 Loss 1.9986 Accuracy 0.5930\n",
            "Epoch 92 Batch 900 Loss 1.9985 Accuracy 0.5931\n",
            "Epoch 92 Batch 950 Loss 2.0003 Accuracy 0.5930\n",
            "Epoch 92 Loss 2.0002 Accuracy 0.5931\n",
            "Time taken for 1 epoch: 24.022064447402954 secs\n",
            "\n",
            "Epoch 93 Batch 0 Loss 1.9133 Accuracy 0.6271\n",
            "Epoch 93 Batch 50 Loss 1.9793 Accuracy 0.5946\n",
            "Epoch 93 Batch 100 Loss 1.9821 Accuracy 0.5941\n",
            "Epoch 93 Batch 150 Loss 1.9799 Accuracy 0.5944\n",
            "discarded batch 157\n",
            "Epoch 93 Batch 200 Loss 1.9809 Accuracy 0.5950\n",
            "Epoch 93 Batch 250 Loss 1.9816 Accuracy 0.5943\n",
            "Epoch 93 Batch 300 Loss 1.9809 Accuracy 0.5943\n",
            "Epoch 93 Batch 350 Loss 1.9872 Accuracy 0.5938\n",
            "Epoch 93 Batch 400 Loss 1.9892 Accuracy 0.5934\n",
            "Epoch 93 Batch 450 Loss 1.9902 Accuracy 0.5935\n",
            "Epoch 93 Batch 500 Loss 1.9927 Accuracy 0.5933\n",
            "Epoch 93 Batch 550 Loss 1.9923 Accuracy 0.5932\n",
            "Epoch 93 Batch 600 Loss 1.9929 Accuracy 0.5931\n",
            "Epoch 93 Batch 650 Loss 1.9960 Accuracy 0.5928\n",
            "Epoch 93 Batch 700 Loss 1.9962 Accuracy 0.5927\n",
            "Epoch 93 Batch 750 Loss 1.9964 Accuracy 0.5931\n",
            "Epoch 93 Batch 800 Loss 1.9969 Accuracy 0.5932\n",
            "Epoch 93 Batch 850 Loss 1.9961 Accuracy 0.5934\n",
            "Epoch 93 Batch 900 Loss 1.9979 Accuracy 0.5933\n",
            "Epoch 93 Batch 950 Loss 1.9983 Accuracy 0.5932\n",
            "Epoch 93 Loss 1.9982 Accuracy 0.5932\n",
            "Time taken for 1 epoch: 24.491589307785034 secs\n",
            "\n",
            "Epoch 94 Batch 0 Loss 1.9242 Accuracy 0.6106\n",
            "Epoch 94 Batch 50 Loss 1.9824 Accuracy 0.5974\n",
            "Epoch 94 Batch 100 Loss 1.9876 Accuracy 0.5957\n",
            "Epoch 94 Batch 150 Loss 1.9899 Accuracy 0.5959\n",
            "Epoch 94 Batch 200 Loss 1.9899 Accuracy 0.5949\n",
            "Epoch 94 Batch 250 Loss 1.9907 Accuracy 0.5946\n",
            "Epoch 94 Batch 300 Loss 1.9890 Accuracy 0.5945\n",
            "Epoch 94 Batch 350 Loss 1.9882 Accuracy 0.5945\n",
            "Epoch 94 Batch 400 Loss 1.9906 Accuracy 0.5944\n",
            "Epoch 94 Batch 450 Loss 1.9881 Accuracy 0.5947\n",
            "Epoch 94 Batch 500 Loss 1.9859 Accuracy 0.5948\n",
            "discarded batch 549\n",
            "Epoch 94 Batch 550 Loss 1.9884 Accuracy 0.5946\n",
            "Epoch 94 Batch 600 Loss 1.9882 Accuracy 0.5947\n",
            "Epoch 94 Batch 650 Loss 1.9900 Accuracy 0.5946\n",
            "Epoch 94 Batch 700 Loss 1.9901 Accuracy 0.5944\n",
            "Epoch 94 Batch 750 Loss 1.9906 Accuracy 0.5944\n",
            "Epoch 94 Batch 800 Loss 1.9919 Accuracy 0.5943\n",
            "Epoch 94 Batch 850 Loss 1.9927 Accuracy 0.5943\n",
            "Epoch 94 Batch 900 Loss 1.9935 Accuracy 0.5943\n",
            "Epoch 94 Batch 950 Loss 1.9942 Accuracy 0.5942\n",
            "Epoch 94 Loss 1.9943 Accuracy 0.5942\n",
            "Time taken for 1 epoch: 24.757721424102783 secs\n",
            "\n",
            "Epoch 95 Batch 0 Loss 2.0470 Accuracy 0.5545\n",
            "Epoch 95 Batch 50 Loss 1.9929 Accuracy 0.5928\n",
            "Epoch 95 Batch 100 Loss 1.9836 Accuracy 0.5939\n",
            "Epoch 95 Batch 150 Loss 1.9808 Accuracy 0.5948\n",
            "Epoch 95 Batch 200 Loss 1.9814 Accuracy 0.5951\n",
            "Epoch 95 Batch 250 Loss 1.9788 Accuracy 0.5953\n",
            "Epoch 95 Batch 300 Loss 1.9829 Accuracy 0.5948\n",
            "Epoch 95 Batch 350 Loss 1.9831 Accuracy 0.5949\n",
            "Epoch 95 Batch 400 Loss 1.9827 Accuracy 0.5954\n",
            "discarded batch 433\n",
            "Epoch 95 Batch 450 Loss 1.9831 Accuracy 0.5950\n",
            "Epoch 95 Batch 500 Loss 1.9854 Accuracy 0.5947\n",
            "Epoch 95 Batch 550 Loss 1.9836 Accuracy 0.5952\n",
            "Epoch 95 Batch 600 Loss 1.9817 Accuracy 0.5956\n",
            "Epoch 95 Batch 650 Loss 1.9832 Accuracy 0.5955\n",
            "Epoch 95 Batch 700 Loss 1.9837 Accuracy 0.5951\n",
            "Epoch 95 Batch 750 Loss 1.9837 Accuracy 0.5952\n",
            "Epoch 95 Batch 800 Loss 1.9863 Accuracy 0.5948\n",
            "Epoch 95 Batch 850 Loss 1.9881 Accuracy 0.5948\n",
            "Epoch 95 Batch 900 Loss 1.9894 Accuracy 0.5948\n",
            "Epoch 95 Batch 950 Loss 1.9903 Accuracy 0.5948\n",
            "Saving checkpoint for epoch 95 at ./checkpoints/train/ckpt-19\n",
            "Epoch 95 Loss 1.9900 Accuracy 0.5948\n",
            "Time taken for 1 epoch: 25.406641244888306 secs\n",
            "\n",
            "Epoch 96 Batch 0 Loss 1.9844 Accuracy 0.6040\n",
            "Epoch 96 Batch 50 Loss 1.9599 Accuracy 0.5952\n",
            "Epoch 96 Batch 100 Loss 1.9546 Accuracy 0.5967\n",
            "Epoch 96 Batch 150 Loss 1.9596 Accuracy 0.5967\n",
            "Epoch 96 Batch 200 Loss 1.9610 Accuracy 0.5964\n",
            "Epoch 96 Batch 250 Loss 1.9635 Accuracy 0.5963\n",
            "Epoch 96 Batch 300 Loss 1.9660 Accuracy 0.5959\n",
            "Epoch 96 Batch 350 Loss 1.9702 Accuracy 0.5962\n",
            "Epoch 96 Batch 400 Loss 1.9754 Accuracy 0.5955\n",
            "Epoch 96 Batch 450 Loss 1.9775 Accuracy 0.5949\n",
            "Epoch 96 Batch 500 Loss 1.9811 Accuracy 0.5943\n",
            "Epoch 96 Batch 550 Loss 1.9847 Accuracy 0.5938\n",
            "discarded batch 561\n",
            "Epoch 96 Batch 600 Loss 1.9863 Accuracy 0.5939\n",
            "Epoch 96 Batch 650 Loss 1.9887 Accuracy 0.5938\n",
            "Epoch 96 Batch 700 Loss 1.9895 Accuracy 0.5939\n",
            "Epoch 96 Batch 750 Loss 1.9894 Accuracy 0.5940\n",
            "Epoch 96 Batch 800 Loss 1.9900 Accuracy 0.5940\n",
            "Epoch 96 Batch 850 Loss 1.9903 Accuracy 0.5940\n",
            "Epoch 96 Batch 900 Loss 1.9896 Accuracy 0.5941\n",
            "Epoch 96 Batch 950 Loss 1.9894 Accuracy 0.5942\n",
            "Epoch 96 Loss 1.9896 Accuracy 0.5942\n",
            "Time taken for 1 epoch: 24.576195240020752 secs\n",
            "\n",
            "Epoch 96 VALIDATION: Loss 2.2169 Accuracy 0.5820\n",
            "\n",
            "Epoch 97 Batch 0 Loss 1.9113 Accuracy 0.5974\n",
            "Epoch 97 Batch 50 Loss 1.9389 Accuracy 0.5979\n",
            "Epoch 97 Batch 100 Loss 1.9403 Accuracy 0.5987\n",
            "Epoch 97 Batch 150 Loss 1.9463 Accuracy 0.5983\n",
            "Epoch 97 Batch 200 Loss 1.9554 Accuracy 0.5983\n",
            "Epoch 97 Batch 250 Loss 1.9580 Accuracy 0.5981\n",
            "Epoch 97 Batch 300 Loss 1.9613 Accuracy 0.5971\n",
            "Epoch 97 Batch 350 Loss 1.9669 Accuracy 0.5966\n",
            "Epoch 97 Batch 400 Loss 1.9704 Accuracy 0.5961\n",
            "Epoch 97 Batch 450 Loss 1.9707 Accuracy 0.5965\n",
            "Epoch 97 Batch 500 Loss 1.9737 Accuracy 0.5964\n",
            "Epoch 97 Batch 550 Loss 1.9746 Accuracy 0.5963\n",
            "Epoch 97 Batch 600 Loss 1.9765 Accuracy 0.5961\n",
            "discarded batch 642\n",
            "Epoch 97 Batch 650 Loss 1.9803 Accuracy 0.5957\n",
            "Epoch 97 Batch 700 Loss 1.9820 Accuracy 0.5955\n",
            "Epoch 97 Batch 750 Loss 1.9838 Accuracy 0.5951\n",
            "Epoch 97 Batch 800 Loss 1.9849 Accuracy 0.5951\n",
            "Epoch 97 Batch 850 Loss 1.9849 Accuracy 0.5953\n",
            "Epoch 97 Batch 900 Loss 1.9860 Accuracy 0.5953\n",
            "Epoch 97 Batch 950 Loss 1.9873 Accuracy 0.5951\n",
            "Epoch 97 Loss 1.9870 Accuracy 0.5951\n",
            "Time taken for 1 epoch: 24.66003942489624 secs\n",
            "\n",
            "Epoch 98 Batch 0 Loss 1.8843 Accuracy 0.6304\n",
            "Epoch 98 Batch 50 Loss 1.9361 Accuracy 0.5987\n",
            "Epoch 98 Batch 100 Loss 1.9339 Accuracy 0.5991\n",
            "Epoch 98 Batch 150 Loss 1.9395 Accuracy 0.5990\n",
            "Epoch 98 Batch 200 Loss 1.9410 Accuracy 0.5991\n",
            "Epoch 98 Batch 250 Loss 1.9539 Accuracy 0.5976\n",
            "Epoch 98 Batch 300 Loss 1.9589 Accuracy 0.5973\n",
            "discarded batch 310\n",
            "Epoch 98 Batch 350 Loss 1.9611 Accuracy 0.5974\n",
            "Epoch 98 Batch 400 Loss 1.9610 Accuracy 0.5977\n",
            "Epoch 98 Batch 450 Loss 1.9632 Accuracy 0.5974\n",
            "Epoch 98 Batch 500 Loss 1.9683 Accuracy 0.5971\n",
            "Epoch 98 Batch 550 Loss 1.9703 Accuracy 0.5969\n",
            "Epoch 98 Batch 600 Loss 1.9743 Accuracy 0.5965\n",
            "Epoch 98 Batch 650 Loss 1.9738 Accuracy 0.5964\n",
            "Epoch 98 Batch 700 Loss 1.9744 Accuracy 0.5962\n",
            "Epoch 98 Batch 750 Loss 1.9747 Accuracy 0.5960\n",
            "Epoch 98 Batch 800 Loss 1.9769 Accuracy 0.5957\n",
            "Epoch 98 Batch 850 Loss 1.9787 Accuracy 0.5956\n",
            "Epoch 98 Batch 900 Loss 1.9802 Accuracy 0.5957\n",
            "Epoch 98 Batch 950 Loss 1.9815 Accuracy 0.5955\n",
            "Epoch 98 Loss 1.9816 Accuracy 0.5955\n",
            "Time taken for 1 epoch: 24.735826015472412 secs\n",
            "\n",
            "Epoch 99 Batch 0 Loss 1.8978 Accuracy 0.6139\n",
            "Epoch 99 Batch 50 Loss 1.9500 Accuracy 0.5991\n",
            "Epoch 99 Batch 100 Loss 1.9555 Accuracy 0.5985\n",
            "Epoch 99 Batch 150 Loss 1.9573 Accuracy 0.5992\n",
            "Epoch 99 Batch 200 Loss 1.9551 Accuracy 0.5996\n",
            "Epoch 99 Batch 250 Loss 1.9620 Accuracy 0.5993\n",
            "Epoch 99 Batch 300 Loss 1.9601 Accuracy 0.5995\n",
            "Epoch 99 Batch 350 Loss 1.9597 Accuracy 0.5994\n",
            "discarded batch 399\n",
            "Epoch 99 Batch 400 Loss 1.9629 Accuracy 0.5990\n",
            "Epoch 99 Batch 450 Loss 1.9636 Accuracy 0.5992\n",
            "Epoch 99 Batch 500 Loss 1.9649 Accuracy 0.5987\n",
            "Epoch 99 Batch 550 Loss 1.9666 Accuracy 0.5980\n",
            "Epoch 99 Batch 600 Loss 1.9698 Accuracy 0.5975\n",
            "Epoch 99 Batch 650 Loss 1.9692 Accuracy 0.5976\n",
            "Epoch 99 Batch 700 Loss 1.9733 Accuracy 0.5973\n",
            "Epoch 99 Batch 750 Loss 1.9750 Accuracy 0.5970\n",
            "Epoch 99 Batch 800 Loss 1.9763 Accuracy 0.5968\n",
            "Epoch 99 Batch 850 Loss 1.9771 Accuracy 0.5967\n",
            "Epoch 99 Batch 900 Loss 1.9784 Accuracy 0.5964\n",
            "Epoch 99 Batch 950 Loss 1.9788 Accuracy 0.5965\n",
            "Epoch 99 Loss 1.9791 Accuracy 0.5964\n",
            "Time taken for 1 epoch: 24.941410541534424 secs\n",
            "\n",
            "Epoch 100 Batch 0 Loss 2.0043 Accuracy 0.5710\n",
            "Epoch 100 Batch 50 Loss 1.9771 Accuracy 0.5973\n",
            "discarded batch 59\n",
            "Epoch 100 Batch 100 Loss 1.9759 Accuracy 0.5952\n",
            "Epoch 100 Batch 150 Loss 1.9628 Accuracy 0.5970\n",
            "Epoch 100 Batch 200 Loss 1.9629 Accuracy 0.5964\n",
            "Epoch 100 Batch 250 Loss 1.9580 Accuracy 0.5978\n",
            "Epoch 100 Batch 300 Loss 1.9606 Accuracy 0.5978\n",
            "Epoch 100 Batch 350 Loss 1.9609 Accuracy 0.5975\n",
            "Epoch 100 Batch 400 Loss 1.9632 Accuracy 0.5970\n",
            "Epoch 100 Batch 450 Loss 1.9640 Accuracy 0.5970\n",
            "Epoch 100 Batch 500 Loss 1.9653 Accuracy 0.5967\n",
            "Epoch 100 Batch 550 Loss 1.9670 Accuracy 0.5968\n",
            "Epoch 100 Batch 600 Loss 1.9688 Accuracy 0.5963\n",
            "Epoch 100 Batch 650 Loss 1.9703 Accuracy 0.5962\n",
            "Epoch 100 Batch 700 Loss 1.9748 Accuracy 0.5957\n",
            "Epoch 100 Batch 750 Loss 1.9741 Accuracy 0.5957\n",
            "Epoch 100 Batch 800 Loss 1.9729 Accuracy 0.5959\n",
            "Epoch 100 Batch 850 Loss 1.9752 Accuracy 0.5957\n",
            "Epoch 100 Batch 900 Loss 1.9758 Accuracy 0.5958\n",
            "Epoch 100 Batch 950 Loss 1.9767 Accuracy 0.5959\n",
            "Saving checkpoint for epoch 100 at ./checkpoints/train/ckpt-20\n",
            "Epoch 100 Loss 1.9768 Accuracy 0.5959\n",
            "Time taken for 1 epoch: 25.178515672683716 secs\n",
            "\n",
            "Epoch 101 Batch 0 Loss 2.0031 Accuracy 0.5710\n",
            "Epoch 101 Batch 50 Loss 1.9504 Accuracy 0.5987\n",
            "Epoch 101 Batch 100 Loss 1.9591 Accuracy 0.5976\n",
            "Epoch 101 Batch 150 Loss 1.9625 Accuracy 0.5982\n",
            "Epoch 101 Batch 200 Loss 1.9630 Accuracy 0.5984\n",
            "Epoch 101 Batch 250 Loss 1.9653 Accuracy 0.5977\n",
            "Epoch 101 Batch 300 Loss 1.9621 Accuracy 0.5984\n",
            "Epoch 101 Batch 350 Loss 1.9624 Accuracy 0.5984\n",
            "Epoch 101 Batch 400 Loss 1.9639 Accuracy 0.5986\n",
            "Epoch 101 Batch 450 Loss 1.9624 Accuracy 0.5984\n",
            "Epoch 101 Batch 500 Loss 1.9656 Accuracy 0.5981\n",
            "Epoch 101 Batch 550 Loss 1.9675 Accuracy 0.5975\n",
            "discarded batch 585\n",
            "Epoch 101 Batch 600 Loss 1.9681 Accuracy 0.5976\n",
            "Epoch 101 Batch 650 Loss 1.9696 Accuracy 0.5970\n",
            "Epoch 101 Batch 700 Loss 1.9711 Accuracy 0.5968\n",
            "Epoch 101 Batch 750 Loss 1.9729 Accuracy 0.5966\n",
            "Epoch 101 Batch 800 Loss 1.9712 Accuracy 0.5968\n",
            "Epoch 101 Batch 850 Loss 1.9716 Accuracy 0.5968\n",
            "Epoch 101 Batch 900 Loss 1.9715 Accuracy 0.5968\n",
            "Epoch 101 Batch 950 Loss 1.9726 Accuracy 0.5964\n",
            "Epoch 101 Loss 1.9734 Accuracy 0.5963\n",
            "Time taken for 1 epoch: 24.205421447753906 secs\n",
            "\n",
            "Epoch 101 VALIDATION: Loss 2.2016 Accuracy 0.5838\n",
            "\n",
            "Epoch 102 Batch 0 Loss 1.9626 Accuracy 0.5908\n",
            "Epoch 102 Batch 50 Loss 1.9134 Accuracy 0.6005\n",
            "Epoch 102 Batch 100 Loss 1.9294 Accuracy 0.5990\n",
            "Epoch 102 Batch 150 Loss 1.9339 Accuracy 0.5989\n",
            "Epoch 102 Batch 200 Loss 1.9347 Accuracy 0.5995\n",
            "Epoch 102 Batch 250 Loss 1.9372 Accuracy 0.5989\n",
            "Epoch 102 Batch 300 Loss 1.9429 Accuracy 0.5982\n",
            "Epoch 102 Batch 350 Loss 1.9513 Accuracy 0.5973\n",
            "Epoch 102 Batch 400 Loss 1.9516 Accuracy 0.5977\n",
            "Epoch 102 Batch 450 Loss 1.9536 Accuracy 0.5976\n",
            "Epoch 102 Batch 500 Loss 1.9560 Accuracy 0.5978\n",
            "Epoch 102 Batch 550 Loss 1.9583 Accuracy 0.5976\n",
            "discarded batch 560\n",
            "Epoch 102 Batch 600 Loss 1.9601 Accuracy 0.5973\n",
            "Epoch 102 Batch 650 Loss 1.9627 Accuracy 0.5967\n",
            "Epoch 102 Batch 700 Loss 1.9644 Accuracy 0.5964\n",
            "Epoch 102 Batch 750 Loss 1.9649 Accuracy 0.5965\n",
            "Epoch 102 Batch 800 Loss 1.9674 Accuracy 0.5963\n",
            "Epoch 102 Batch 850 Loss 1.9699 Accuracy 0.5959\n",
            "Epoch 102 Batch 900 Loss 1.9690 Accuracy 0.5961\n",
            "Epoch 102 Batch 950 Loss 1.9691 Accuracy 0.5963\n",
            "Epoch 102 Loss 1.9693 Accuracy 0.5963\n",
            "Time taken for 1 epoch: 24.148600339889526 secs\n",
            "\n",
            "Epoch 103 Batch 0 Loss 2.0783 Accuracy 0.6139\n",
            "Epoch 103 Batch 50 Loss 1.9474 Accuracy 0.6009\n",
            "Epoch 103 Batch 100 Loss 1.9430 Accuracy 0.5982\n",
            "Epoch 103 Batch 150 Loss 1.9448 Accuracy 0.5998\n",
            "Epoch 103 Batch 200 Loss 1.9516 Accuracy 0.5983\n",
            "Epoch 103 Batch 250 Loss 1.9531 Accuracy 0.5982\n",
            "Epoch 103 Batch 300 Loss 1.9502 Accuracy 0.5992\n",
            "Epoch 103 Batch 350 Loss 1.9527 Accuracy 0.5991\n",
            "Epoch 103 Batch 400 Loss 1.9518 Accuracy 0.5992\n",
            "Epoch 103 Batch 450 Loss 1.9542 Accuracy 0.5989\n",
            "Epoch 103 Batch 500 Loss 1.9556 Accuracy 0.5987\n",
            "Epoch 103 Batch 550 Loss 1.9557 Accuracy 0.5989\n",
            "Epoch 103 Batch 600 Loss 1.9570 Accuracy 0.5988\n",
            "Epoch 103 Batch 650 Loss 1.9596 Accuracy 0.5984\n",
            "Epoch 103 Batch 700 Loss 1.9589 Accuracy 0.5986\n",
            "discarded batch 715\n",
            "Epoch 103 Batch 750 Loss 1.9623 Accuracy 0.5982\n",
            "Epoch 103 Batch 800 Loss 1.9633 Accuracy 0.5981\n",
            "Epoch 103 Batch 850 Loss 1.9646 Accuracy 0.5981\n",
            "Epoch 103 Batch 900 Loss 1.9670 Accuracy 0.5979\n",
            "Epoch 103 Batch 950 Loss 1.9698 Accuracy 0.5975\n",
            "Epoch 103 Loss 1.9696 Accuracy 0.5976\n",
            "Time taken for 1 epoch: 24.278196334838867 secs\n",
            "\n",
            "Epoch 104 Batch 0 Loss 2.1368 Accuracy 0.5677\n",
            "Epoch 104 Batch 50 Loss 1.9635 Accuracy 0.5977\n",
            "Epoch 104 Batch 100 Loss 1.9536 Accuracy 0.5978\n",
            "Epoch 104 Batch 150 Loss 1.9550 Accuracy 0.5972\n",
            "Epoch 104 Batch 200 Loss 1.9539 Accuracy 0.5965\n",
            "Epoch 104 Batch 250 Loss 1.9541 Accuracy 0.5974\n",
            "Epoch 104 Batch 300 Loss 1.9531 Accuracy 0.5976\n",
            "Epoch 104 Batch 350 Loss 1.9512 Accuracy 0.5978\n",
            "Epoch 104 Batch 400 Loss 1.9534 Accuracy 0.5980\n",
            "Epoch 104 Batch 450 Loss 1.9565 Accuracy 0.5978\n",
            "Epoch 104 Batch 500 Loss 1.9573 Accuracy 0.5980\n",
            "Epoch 104 Batch 550 Loss 1.9576 Accuracy 0.5979\n",
            "Epoch 104 Batch 600 Loss 1.9585 Accuracy 0.5977\n",
            "discarded batch 617\n",
            "Epoch 104 Batch 650 Loss 1.9607 Accuracy 0.5977\n",
            "Epoch 104 Batch 700 Loss 1.9605 Accuracy 0.5977\n",
            "Epoch 104 Batch 750 Loss 1.9614 Accuracy 0.5979\n",
            "Epoch 104 Batch 800 Loss 1.9632 Accuracy 0.5979\n",
            "Epoch 104 Batch 850 Loss 1.9639 Accuracy 0.5981\n",
            "Epoch 104 Batch 900 Loss 1.9647 Accuracy 0.5981\n",
            "Epoch 104 Batch 950 Loss 1.9655 Accuracy 0.5980\n",
            "Epoch 104 Loss 1.9654 Accuracy 0.5980\n",
            "Time taken for 1 epoch: 24.52622699737549 secs\n",
            "\n",
            "Epoch 105 Batch 0 Loss 2.0421 Accuracy 0.5710\n",
            "Epoch 105 Batch 50 Loss 1.9150 Accuracy 0.6035\n",
            "Epoch 105 Batch 100 Loss 1.9590 Accuracy 0.5990\n",
            "Epoch 105 Batch 150 Loss 1.9570 Accuracy 0.5988\n",
            "Epoch 105 Batch 200 Loss 1.9594 Accuracy 0.5980\n",
            "Epoch 105 Batch 250 Loss 1.9630 Accuracy 0.5974\n",
            "Epoch 105 Batch 300 Loss 1.9642 Accuracy 0.5973\n",
            "Epoch 105 Batch 350 Loss 1.9622 Accuracy 0.5977\n",
            "Epoch 105 Batch 400 Loss 1.9593 Accuracy 0.5979\n",
            "Epoch 105 Batch 450 Loss 1.9599 Accuracy 0.5983\n",
            "Epoch 105 Batch 500 Loss 1.9597 Accuracy 0.5982\n",
            "Epoch 105 Batch 550 Loss 1.9612 Accuracy 0.5979\n",
            "Epoch 105 Batch 600 Loss 1.9619 Accuracy 0.5977\n",
            "Epoch 105 Batch 650 Loss 1.9637 Accuracy 0.5978\n",
            "Epoch 105 Batch 700 Loss 1.9651 Accuracy 0.5976\n",
            "Epoch 105 Batch 750 Loss 1.9648 Accuracy 0.5977\n",
            "Epoch 105 Batch 800 Loss 1.9636 Accuracy 0.5981\n",
            "Epoch 105 Batch 850 Loss 1.9611 Accuracy 0.5983\n",
            "Epoch 105 Batch 900 Loss 1.9614 Accuracy 0.5982\n",
            "discarded batch 929\n",
            "Epoch 105 Batch 950 Loss 1.9621 Accuracy 0.5981\n",
            "Saving checkpoint for epoch 105 at ./checkpoints/train/ckpt-21\n",
            "Epoch 105 Loss 1.9622 Accuracy 0.5980\n",
            "Time taken for 1 epoch: 25.069164037704468 secs\n",
            "\n",
            "Epoch 106 Batch 0 Loss 1.9443 Accuracy 0.6040\n",
            "Epoch 106 Batch 50 Loss 1.9141 Accuracy 0.6027\n",
            "Epoch 106 Batch 100 Loss 1.9173 Accuracy 0.6028\n",
            "Epoch 106 Batch 150 Loss 1.9341 Accuracy 0.5998\n",
            "Epoch 106 Batch 200 Loss 1.9337 Accuracy 0.6001\n",
            "Epoch 106 Batch 250 Loss 1.9402 Accuracy 0.6001\n",
            "Epoch 106 Batch 300 Loss 1.9404 Accuracy 0.6005\n",
            "Epoch 106 Batch 350 Loss 1.9429 Accuracy 0.6002\n",
            "Epoch 106 Batch 400 Loss 1.9463 Accuracy 0.5995\n",
            "Epoch 106 Batch 450 Loss 1.9454 Accuracy 0.5998\n",
            "Epoch 106 Batch 500 Loss 1.9482 Accuracy 0.5997\n",
            "Epoch 106 Batch 550 Loss 1.9517 Accuracy 0.5997\n",
            "Epoch 106 Batch 600 Loss 1.9536 Accuracy 0.5994\n",
            "Epoch 106 Batch 650 Loss 1.9554 Accuracy 0.5991\n",
            "Epoch 106 Batch 700 Loss 1.9543 Accuracy 0.5992\n",
            "Epoch 106 Batch 750 Loss 1.9556 Accuracy 0.5991\n",
            "discarded batch 765\n",
            "Epoch 106 Batch 800 Loss 1.9578 Accuracy 0.5988\n",
            "Epoch 106 Batch 850 Loss 1.9574 Accuracy 0.5987\n",
            "Epoch 106 Batch 900 Loss 1.9582 Accuracy 0.5987\n",
            "Epoch 106 Batch 950 Loss 1.9581 Accuracy 0.5988\n",
            "Epoch 106 Loss 1.9584 Accuracy 0.5988\n",
            "Time taken for 1 epoch: 24.570907592773438 secs\n",
            "\n",
            "Epoch 106 VALIDATION: Loss 2.2063 Accuracy 0.5841\n",
            "\n",
            "Epoch 107 Batch 0 Loss 2.0289 Accuracy 0.6007\n",
            "Epoch 107 Batch 50 Loss 1.9528 Accuracy 0.6001\n",
            "Epoch 107 Batch 100 Loss 1.9451 Accuracy 0.6001\n",
            "Epoch 107 Batch 150 Loss 1.9483 Accuracy 0.5995\n",
            "Epoch 107 Batch 200 Loss 1.9509 Accuracy 0.5996\n",
            "Epoch 107 Batch 250 Loss 1.9521 Accuracy 0.5987\n",
            "Epoch 107 Batch 300 Loss 1.9504 Accuracy 0.5993\n",
            "Epoch 107 Batch 350 Loss 1.9526 Accuracy 0.5988\n",
            "discarded batch 373\n",
            "Epoch 107 Batch 400 Loss 1.9522 Accuracy 0.5992\n",
            "Epoch 107 Batch 450 Loss 1.9551 Accuracy 0.5991\n",
            "Epoch 107 Batch 500 Loss 1.9551 Accuracy 0.5989\n",
            "Epoch 107 Batch 550 Loss 1.9548 Accuracy 0.5992\n",
            "Epoch 107 Batch 600 Loss 1.9558 Accuracy 0.5990\n",
            "Epoch 107 Batch 650 Loss 1.9562 Accuracy 0.5989\n",
            "Epoch 107 Batch 700 Loss 1.9563 Accuracy 0.5991\n",
            "Epoch 107 Batch 750 Loss 1.9559 Accuracy 0.5989\n",
            "Epoch 107 Batch 800 Loss 1.9559 Accuracy 0.5989\n",
            "Epoch 107 Batch 850 Loss 1.9564 Accuracy 0.5990\n",
            "Epoch 107 Batch 900 Loss 1.9558 Accuracy 0.5991\n",
            "Epoch 107 Batch 950 Loss 1.9571 Accuracy 0.5993\n",
            "Epoch 107 Loss 1.9570 Accuracy 0.5993\n",
            "Time taken for 1 epoch: 24.88031506538391 secs\n",
            "\n",
            "Epoch 108 Batch 0 Loss 2.1273 Accuracy 0.5710\n",
            "Epoch 108 Batch 50 Loss 1.9330 Accuracy 0.6009\n",
            "Epoch 108 Batch 100 Loss 1.9272 Accuracy 0.6003\n",
            "Epoch 108 Batch 150 Loss 1.9271 Accuracy 0.6010\n",
            "Epoch 108 Batch 200 Loss 1.9305 Accuracy 0.6005\n",
            "discarded batch 205\n",
            "Epoch 108 Batch 250 Loss 1.9365 Accuracy 0.5995\n",
            "Epoch 108 Batch 300 Loss 1.9334 Accuracy 0.5998\n",
            "Epoch 108 Batch 350 Loss 1.9367 Accuracy 0.5994\n",
            "Epoch 108 Batch 400 Loss 1.9376 Accuracy 0.5992\n",
            "Epoch 108 Batch 450 Loss 1.9388 Accuracy 0.5994\n",
            "Epoch 108 Batch 500 Loss 1.9421 Accuracy 0.5994\n",
            "Epoch 108 Batch 550 Loss 1.9461 Accuracy 0.5990\n",
            "Epoch 108 Batch 600 Loss 1.9481 Accuracy 0.5992\n",
            "Epoch 108 Batch 650 Loss 1.9481 Accuracy 0.5996\n",
            "Epoch 108 Batch 700 Loss 1.9484 Accuracy 0.5996\n",
            "Epoch 108 Batch 750 Loss 1.9497 Accuracy 0.5996\n",
            "Epoch 108 Batch 800 Loss 1.9506 Accuracy 0.5996\n",
            "Epoch 108 Batch 850 Loss 1.9498 Accuracy 0.5997\n",
            "Epoch 108 Batch 900 Loss 1.9514 Accuracy 0.5995\n",
            "Epoch 108 Batch 950 Loss 1.9532 Accuracy 0.5992\n",
            "Epoch 108 Loss 1.9531 Accuracy 0.5992\n",
            "Time taken for 1 epoch: 24.357545614242554 secs\n",
            "\n",
            "Epoch 109 Batch 0 Loss 1.8746 Accuracy 0.5974\n",
            "Epoch 109 Batch 50 Loss 1.9262 Accuracy 0.6014\n",
            "Epoch 109 Batch 100 Loss 1.9270 Accuracy 0.6021\n",
            "Epoch 109 Batch 150 Loss 1.9371 Accuracy 0.6008\n",
            "Epoch 109 Batch 200 Loss 1.9361 Accuracy 0.6010\n",
            "Epoch 109 Batch 250 Loss 1.9344 Accuracy 0.6007\n",
            "Epoch 109 Batch 300 Loss 1.9353 Accuracy 0.6005\n",
            "Epoch 109 Batch 350 Loss 1.9373 Accuracy 0.6004\n",
            "Epoch 109 Batch 400 Loss 1.9428 Accuracy 0.5995\n",
            "discarded batch 441\n",
            "Epoch 109 Batch 450 Loss 1.9442 Accuracy 0.5995\n",
            "Epoch 109 Batch 500 Loss 1.9460 Accuracy 0.5994\n",
            "Epoch 109 Batch 550 Loss 1.9460 Accuracy 0.5995\n",
            "Epoch 109 Batch 600 Loss 1.9443 Accuracy 0.5998\n",
            "Epoch 109 Batch 650 Loss 1.9440 Accuracy 0.5998\n",
            "Epoch 109 Batch 700 Loss 1.9444 Accuracy 0.5999\n",
            "Epoch 109 Batch 750 Loss 1.9471 Accuracy 0.5995\n",
            "Epoch 109 Batch 800 Loss 1.9477 Accuracy 0.5994\n",
            "Epoch 109 Batch 850 Loss 1.9478 Accuracy 0.5995\n",
            "Epoch 109 Batch 900 Loss 1.9485 Accuracy 0.5996\n",
            "Epoch 109 Batch 950 Loss 1.9494 Accuracy 0.5993\n",
            "Epoch 109 Loss 1.9496 Accuracy 0.5994\n",
            "Time taken for 1 epoch: 24.01532745361328 secs\n",
            "\n",
            "Epoch 110 Batch 0 Loss 2.7029 Accuracy 0.5578\n",
            "Epoch 110 Batch 50 Loss 1.9392 Accuracy 0.5998\n",
            "Epoch 110 Batch 100 Loss 1.9413 Accuracy 0.5999\n",
            "Epoch 110 Batch 150 Loss 1.9432 Accuracy 0.5995\n",
            "Epoch 110 Batch 200 Loss 1.9348 Accuracy 0.6013\n",
            "Epoch 110 Batch 250 Loss 1.9358 Accuracy 0.6017\n",
            "Epoch 110 Batch 300 Loss 1.9382 Accuracy 0.6010\n",
            "Epoch 110 Batch 350 Loss 1.9415 Accuracy 0.6007\n",
            "Epoch 110 Batch 400 Loss 1.9429 Accuracy 0.6003\n",
            "Epoch 110 Batch 450 Loss 1.9435 Accuracy 0.6006\n",
            "discarded batch 489\n",
            "Epoch 110 Batch 500 Loss 1.9407 Accuracy 0.6008\n",
            "Epoch 110 Batch 550 Loss 1.9422 Accuracy 0.6008\n",
            "Epoch 110 Batch 600 Loss 1.9417 Accuracy 0.6010\n",
            "Epoch 110 Batch 650 Loss 1.9421 Accuracy 0.6007\n",
            "Epoch 110 Batch 700 Loss 1.9433 Accuracy 0.6004\n",
            "Epoch 110 Batch 750 Loss 1.9457 Accuracy 0.6000\n",
            "Epoch 110 Batch 800 Loss 1.9466 Accuracy 0.6000\n",
            "Epoch 110 Batch 850 Loss 1.9486 Accuracy 0.5996\n",
            "Epoch 110 Batch 900 Loss 1.9485 Accuracy 0.5996\n",
            "Epoch 110 Batch 950 Loss 1.9488 Accuracy 0.5997\n",
            "Saving checkpoint for epoch 110 at ./checkpoints/train/ckpt-22\n",
            "Epoch 110 Loss 1.9486 Accuracy 0.5997\n",
            "Time taken for 1 epoch: 24.42822027206421 secs\n",
            "\n",
            "Epoch 111 Batch 0 Loss 1.7784 Accuracy 0.6073\n",
            "Epoch 111 Batch 50 Loss 1.9248 Accuracy 0.6014\n",
            "Epoch 111 Batch 100 Loss 1.9235 Accuracy 0.6018\n",
            "Epoch 111 Batch 150 Loss 1.9278 Accuracy 0.6006\n",
            "Epoch 111 Batch 200 Loss 1.9230 Accuracy 0.6020\n",
            "Epoch 111 Batch 250 Loss 1.9255 Accuracy 0.6021\n",
            "Epoch 111 Batch 300 Loss 1.9288 Accuracy 0.6022\n",
            "Epoch 111 Batch 350 Loss 1.9273 Accuracy 0.6025\n",
            "Epoch 111 Batch 400 Loss 1.9328 Accuracy 0.6016\n",
            "discarded batch 413\n",
            "Epoch 111 Batch 450 Loss 1.9354 Accuracy 0.6016\n",
            "Epoch 111 Batch 500 Loss 1.9356 Accuracy 0.6020\n",
            "Epoch 111 Batch 550 Loss 1.9353 Accuracy 0.6019\n",
            "Epoch 111 Batch 600 Loss 1.9356 Accuracy 0.6018\n",
            "Epoch 111 Batch 650 Loss 1.9372 Accuracy 0.6014\n",
            "Epoch 111 Batch 700 Loss 1.9393 Accuracy 0.6012\n",
            "Epoch 111 Batch 750 Loss 1.9402 Accuracy 0.6012\n",
            "Epoch 111 Batch 800 Loss 1.9411 Accuracy 0.6012\n",
            "Epoch 111 Batch 850 Loss 1.9410 Accuracy 0.6011\n",
            "Epoch 111 Batch 900 Loss 1.9415 Accuracy 0.6011\n",
            "Epoch 111 Batch 950 Loss 1.9441 Accuracy 0.6010\n",
            "Epoch 111 Loss 1.9442 Accuracy 0.6009\n",
            "Time taken for 1 epoch: 23.980015993118286 secs\n",
            "\n",
            "Epoch 111 VALIDATION: Loss 2.2043 Accuracy 0.5856\n",
            "\n",
            "Epoch 112 Batch 0 Loss 1.9139 Accuracy 0.6172\n",
            "Epoch 112 Batch 50 Loss 1.9229 Accuracy 0.6069\n",
            "Epoch 112 Batch 100 Loss 1.9174 Accuracy 0.6052\n",
            "Epoch 112 Batch 150 Loss 1.9162 Accuracy 0.6062\n",
            "Epoch 112 Batch 200 Loss 1.9245 Accuracy 0.6038\n",
            "Epoch 112 Batch 250 Loss 1.9244 Accuracy 0.6034\n",
            "Epoch 112 Batch 300 Loss 1.9244 Accuracy 0.6029\n",
            "Epoch 112 Batch 350 Loss 1.9250 Accuracy 0.6025\n",
            "Epoch 112 Batch 400 Loss 1.9247 Accuracy 0.6021\n",
            "Epoch 112 Batch 450 Loss 1.9241 Accuracy 0.6021\n",
            "Epoch 112 Batch 500 Loss 1.9246 Accuracy 0.6022\n",
            "Epoch 112 Batch 550 Loss 1.9273 Accuracy 0.6018\n",
            "Epoch 112 Batch 600 Loss 1.9296 Accuracy 0.6016\n",
            "Epoch 112 Batch 650 Loss 1.9301 Accuracy 0.6016\n",
            "Epoch 112 Batch 700 Loss 1.9315 Accuracy 0.6014\n",
            "Epoch 112 Batch 750 Loss 1.9331 Accuracy 0.6013\n",
            "Epoch 112 Batch 800 Loss 1.9348 Accuracy 0.6010\n",
            "Epoch 112 Batch 850 Loss 1.9367 Accuracy 0.6009\n",
            "Epoch 112 Batch 900 Loss 1.9375 Accuracy 0.6009\n",
            "discarded batch 945\n",
            "Epoch 112 Batch 950 Loss 1.9396 Accuracy 0.6009\n",
            "Epoch 112 Loss 1.9398 Accuracy 0.6009\n",
            "Time taken for 1 epoch: 24.139383554458618 secs\n",
            "\n",
            "Epoch 113 Batch 0 Loss 1.8538 Accuracy 0.6205\n",
            "Epoch 113 Batch 50 Loss 1.9226 Accuracy 0.6029\n",
            "Epoch 113 Batch 100 Loss 1.9153 Accuracy 0.6035\n",
            "Epoch 113 Batch 150 Loss 1.9129 Accuracy 0.6035\n",
            "Epoch 113 Batch 200 Loss 1.9106 Accuracy 0.6041\n",
            "Epoch 113 Batch 250 Loss 1.9171 Accuracy 0.6030\n",
            "Epoch 113 Batch 300 Loss 1.9206 Accuracy 0.6024\n",
            "Epoch 113 Batch 350 Loss 1.9218 Accuracy 0.6019\n",
            "Epoch 113 Batch 400 Loss 1.9252 Accuracy 0.6015\n",
            "Epoch 113 Batch 450 Loss 1.9276 Accuracy 0.6014\n",
            "discarded batch 490\n",
            "Epoch 113 Batch 500 Loss 1.9288 Accuracy 0.6011\n",
            "Epoch 113 Batch 550 Loss 1.9292 Accuracy 0.6010\n",
            "Epoch 113 Batch 600 Loss 1.9294 Accuracy 0.6010\n",
            "Epoch 113 Batch 650 Loss 1.9308 Accuracy 0.6012\n",
            "Epoch 113 Batch 700 Loss 1.9331 Accuracy 0.6010\n",
            "Epoch 113 Batch 750 Loss 1.9351 Accuracy 0.6010\n",
            "Epoch 113 Batch 800 Loss 1.9374 Accuracy 0.6008\n",
            "Epoch 113 Batch 850 Loss 1.9371 Accuracy 0.6011\n",
            "Epoch 113 Batch 900 Loss 1.9395 Accuracy 0.6007\n",
            "Epoch 113 Batch 950 Loss 1.9403 Accuracy 0.6007\n",
            "Epoch 113 Loss 1.9405 Accuracy 0.6007\n",
            "Time taken for 1 epoch: 24.003883361816406 secs\n",
            "\n",
            "Epoch 114 Batch 0 Loss 1.9049 Accuracy 0.6007\n",
            "Epoch 114 Batch 50 Loss 1.9323 Accuracy 0.5984\n",
            "Epoch 114 Batch 100 Loss 1.9332 Accuracy 0.5994\n",
            "Epoch 114 Batch 150 Loss 1.9256 Accuracy 0.6013\n",
            "Epoch 114 Batch 200 Loss 1.9226 Accuracy 0.6017\n",
            "Epoch 114 Batch 250 Loss 1.9248 Accuracy 0.6016\n",
            "Epoch 114 Batch 300 Loss 1.9258 Accuracy 0.6010\n",
            "Epoch 114 Batch 350 Loss 1.9268 Accuracy 0.6008\n",
            "Epoch 114 Batch 400 Loss 1.9288 Accuracy 0.6007\n",
            "discarded batch 428\n",
            "Epoch 114 Batch 450 Loss 1.9295 Accuracy 0.6007\n",
            "Epoch 114 Batch 500 Loss 1.9302 Accuracy 0.6009\n",
            "Epoch 114 Batch 550 Loss 1.9296 Accuracy 0.6014\n",
            "Epoch 114 Batch 600 Loss 1.9323 Accuracy 0.6012\n",
            "Epoch 114 Batch 650 Loss 1.9329 Accuracy 0.6012\n",
            "Epoch 114 Batch 700 Loss 1.9325 Accuracy 0.6013\n",
            "Epoch 114 Batch 750 Loss 1.9327 Accuracy 0.6015\n",
            "Epoch 114 Batch 800 Loss 1.9338 Accuracy 0.6012\n",
            "Epoch 114 Batch 850 Loss 1.9352 Accuracy 0.6011\n",
            "Epoch 114 Batch 900 Loss 1.9353 Accuracy 0.6009\n",
            "Epoch 114 Batch 950 Loss 1.9358 Accuracy 0.6011\n",
            "Epoch 114 Loss 1.9358 Accuracy 0.6011\n",
            "Time taken for 1 epoch: 23.97265100479126 secs\n",
            "\n",
            "Epoch 115 Batch 0 Loss 2.0606 Accuracy 0.5710\n",
            "Epoch 115 Batch 50 Loss 1.8961 Accuracy 0.6088\n",
            "Epoch 115 Batch 100 Loss 1.9093 Accuracy 0.6046\n",
            "Epoch 115 Batch 150 Loss 1.9125 Accuracy 0.6037\n",
            "Epoch 115 Batch 200 Loss 1.9155 Accuracy 0.6025\n",
            "Epoch 115 Batch 250 Loss 1.9183 Accuracy 0.6029\n",
            "Epoch 115 Batch 300 Loss 1.9197 Accuracy 0.6030\n",
            "Epoch 115 Batch 350 Loss 1.9252 Accuracy 0.6028\n",
            "Epoch 115 Batch 400 Loss 1.9278 Accuracy 0.6021\n",
            "Epoch 115 Batch 450 Loss 1.9273 Accuracy 0.6027\n",
            "Epoch 115 Batch 500 Loss 1.9303 Accuracy 0.6024\n",
            "Epoch 115 Batch 550 Loss 1.9305 Accuracy 0.6022\n",
            "Epoch 115 Batch 600 Loss 1.9311 Accuracy 0.6020\n",
            "Epoch 115 Batch 650 Loss 1.9286 Accuracy 0.6023\n",
            "Epoch 115 Batch 700 Loss 1.9278 Accuracy 0.6027\n",
            "Epoch 115 Batch 750 Loss 1.9273 Accuracy 0.6026\n",
            "discarded batch 799\n",
            "Epoch 115 Batch 800 Loss 1.9290 Accuracy 0.6022\n",
            "Epoch 115 Batch 850 Loss 1.9302 Accuracy 0.6022\n",
            "Epoch 115 Batch 900 Loss 1.9303 Accuracy 0.6023\n",
            "Epoch 115 Batch 950 Loss 1.9323 Accuracy 0.6020\n",
            "Saving checkpoint for epoch 115 at ./checkpoints/train/ckpt-23\n",
            "Epoch 115 Loss 1.9330 Accuracy 0.6020\n",
            "Time taken for 1 epoch: 24.478697538375854 secs\n",
            "\n",
            "Epoch 116 Batch 0 Loss 1.8581 Accuracy 0.6238\n",
            "discarded batch 37\n",
            "Epoch 116 Batch 50 Loss 1.9230 Accuracy 0.6032\n",
            "Epoch 116 Batch 100 Loss 1.9071 Accuracy 0.6054\n",
            "Epoch 116 Batch 150 Loss 1.9066 Accuracy 0.6060\n",
            "Epoch 116 Batch 200 Loss 1.9135 Accuracy 0.6050\n",
            "Epoch 116 Batch 250 Loss 1.9170 Accuracy 0.6033\n",
            "Epoch 116 Batch 300 Loss 1.9197 Accuracy 0.6032\n",
            "Epoch 116 Batch 350 Loss 1.9238 Accuracy 0.6028\n",
            "Epoch 116 Batch 400 Loss 1.9239 Accuracy 0.6022\n",
            "Epoch 116 Batch 450 Loss 1.9257 Accuracy 0.6019\n",
            "Epoch 116 Batch 500 Loss 1.9264 Accuracy 0.6021\n",
            "Epoch 116 Batch 550 Loss 1.9239 Accuracy 0.6023\n",
            "Epoch 116 Batch 600 Loss 1.9243 Accuracy 0.6026\n",
            "Epoch 116 Batch 650 Loss 1.9276 Accuracy 0.6024\n",
            "Epoch 116 Batch 700 Loss 1.9296 Accuracy 0.6022\n",
            "Epoch 116 Batch 750 Loss 1.9288 Accuracy 0.6023\n",
            "Epoch 116 Batch 800 Loss 1.9290 Accuracy 0.6025\n",
            "Epoch 116 Batch 850 Loss 1.9299 Accuracy 0.6025\n",
            "Epoch 116 Batch 900 Loss 1.9301 Accuracy 0.6026\n",
            "Epoch 116 Batch 950 Loss 1.9319 Accuracy 0.6024\n",
            "Epoch 116 Loss 1.9320 Accuracy 0.6024\n",
            "Time taken for 1 epoch: 23.996769189834595 secs\n",
            "\n",
            "Epoch 116 VALIDATION: Loss 2.2094 Accuracy 0.5826\n",
            "\n",
            "Epoch 117 Batch 0 Loss 1.8020 Accuracy 0.6337\n",
            "Epoch 117 Batch 50 Loss 1.9150 Accuracy 0.6072\n",
            "Epoch 117 Batch 100 Loss 1.9257 Accuracy 0.6046\n",
            "Epoch 117 Batch 150 Loss 1.9136 Accuracy 0.6052\n",
            "Epoch 117 Batch 200 Loss 1.9092 Accuracy 0.6053\n",
            "Epoch 117 Batch 250 Loss 1.9134 Accuracy 0.6045\n",
            "Epoch 117 Batch 300 Loss 1.9146 Accuracy 0.6045\n",
            "Epoch 117 Batch 350 Loss 1.9157 Accuracy 0.6038\n",
            "Epoch 117 Batch 400 Loss 1.9162 Accuracy 0.6039\n",
            "Epoch 117 Batch 450 Loss 1.9175 Accuracy 0.6033\n",
            "Epoch 117 Batch 500 Loss 1.9206 Accuracy 0.6028\n",
            "Epoch 117 Batch 550 Loss 1.9213 Accuracy 0.6027\n",
            "discarded batch 562\n",
            "Epoch 117 Batch 600 Loss 1.9229 Accuracy 0.6027\n",
            "Epoch 117 Batch 650 Loss 1.9247 Accuracy 0.6025\n",
            "Epoch 117 Batch 700 Loss 1.9251 Accuracy 0.6025\n",
            "Epoch 117 Batch 750 Loss 1.9252 Accuracy 0.6025\n",
            "Epoch 117 Batch 800 Loss 1.9257 Accuracy 0.6023\n",
            "Epoch 117 Batch 850 Loss 1.9268 Accuracy 0.6024\n",
            "Epoch 117 Batch 900 Loss 1.9270 Accuracy 0.6024\n",
            "Epoch 117 Batch 950 Loss 1.9284 Accuracy 0.6024\n",
            "Epoch 117 Loss 1.9283 Accuracy 0.6024\n",
            "Time taken for 1 epoch: 24.49775528907776 secs\n",
            "\n",
            "Epoch 118 Batch 0 Loss 1.8059 Accuracy 0.6172\n",
            "Epoch 118 Batch 50 Loss 1.9060 Accuracy 0.6056\n",
            "Epoch 118 Batch 100 Loss 1.9046 Accuracy 0.6053\n",
            "Epoch 118 Batch 150 Loss 1.9167 Accuracy 0.6049\n",
            "Epoch 118 Batch 200 Loss 1.9106 Accuracy 0.6044\n",
            "Epoch 118 Batch 250 Loss 1.9107 Accuracy 0.6042\n",
            "Epoch 118 Batch 300 Loss 1.9151 Accuracy 0.6039\n",
            "Epoch 118 Batch 350 Loss 1.9135 Accuracy 0.6046\n",
            "Epoch 118 Batch 400 Loss 1.9144 Accuracy 0.6043\n",
            "Epoch 118 Batch 450 Loss 1.9142 Accuracy 0.6041\n",
            "Epoch 118 Batch 500 Loss 1.9144 Accuracy 0.6039\n",
            "Epoch 118 Batch 550 Loss 1.9163 Accuracy 0.6036\n",
            "Epoch 118 Batch 600 Loss 1.9170 Accuracy 0.6037\n",
            "discarded batch 647\n",
            "Epoch 118 Batch 650 Loss 1.9197 Accuracy 0.6032\n",
            "Epoch 118 Batch 700 Loss 1.9195 Accuracy 0.6035\n",
            "Epoch 118 Batch 750 Loss 1.9215 Accuracy 0.6032\n",
            "Epoch 118 Batch 800 Loss 1.9215 Accuracy 0.6033\n",
            "Epoch 118 Batch 850 Loss 1.9227 Accuracy 0.6032\n",
            "Epoch 118 Batch 900 Loss 1.9242 Accuracy 0.6031\n",
            "Epoch 118 Batch 950 Loss 1.9241 Accuracy 0.6033\n",
            "Epoch 118 Loss 1.9242 Accuracy 0.6033\n",
            "Time taken for 1 epoch: 24.662481546401978 secs\n",
            "\n",
            "Epoch 119 Batch 0 Loss 1.8906 Accuracy 0.6403\n",
            "Epoch 119 Batch 50 Loss 1.8931 Accuracy 0.6073\n",
            "Epoch 119 Batch 100 Loss 1.8933 Accuracy 0.6071\n",
            "Epoch 119 Batch 150 Loss 1.9042 Accuracy 0.6050\n",
            "Epoch 119 Batch 200 Loss 1.9087 Accuracy 0.6042\n",
            "Epoch 119 Batch 250 Loss 1.9032 Accuracy 0.6046\n",
            "Epoch 119 Batch 300 Loss 1.9051 Accuracy 0.6042\n",
            "Epoch 119 Batch 350 Loss 1.9072 Accuracy 0.6041\n",
            "discarded batch 370\n",
            "Epoch 119 Batch 400 Loss 1.9081 Accuracy 0.6041\n",
            "Epoch 119 Batch 450 Loss 1.9100 Accuracy 0.6040\n",
            "Epoch 119 Batch 500 Loss 1.9146 Accuracy 0.6036\n",
            "Epoch 119 Batch 550 Loss 1.9165 Accuracy 0.6033\n",
            "Epoch 119 Batch 600 Loss 1.9169 Accuracy 0.6033\n",
            "Epoch 119 Batch 650 Loss 1.9177 Accuracy 0.6033\n",
            "Epoch 119 Batch 700 Loss 1.9182 Accuracy 0.6037\n",
            "Epoch 119 Batch 750 Loss 1.9179 Accuracy 0.6037\n",
            "Epoch 119 Batch 800 Loss 1.9190 Accuracy 0.6036\n",
            "Epoch 119 Batch 850 Loss 1.9197 Accuracy 0.6037\n",
            "Epoch 119 Batch 900 Loss 1.9210 Accuracy 0.6035\n",
            "Epoch 119 Batch 950 Loss 1.9230 Accuracy 0.6031\n",
            "Epoch 119 Loss 1.9228 Accuracy 0.6031\n",
            "Time taken for 1 epoch: 23.928089141845703 secs\n",
            "\n",
            "Epoch 120 Batch 0 Loss 1.8622 Accuracy 0.6469\n",
            "Epoch 120 Batch 50 Loss 1.8977 Accuracy 0.6059\n",
            "Epoch 120 Batch 100 Loss 1.9038 Accuracy 0.6036\n",
            "Epoch 120 Batch 150 Loss 1.9108 Accuracy 0.6024\n",
            "Epoch 120 Batch 200 Loss 1.9088 Accuracy 0.6033\n",
            "Epoch 120 Batch 250 Loss 1.9065 Accuracy 0.6041\n",
            "Epoch 120 Batch 300 Loss 1.9075 Accuracy 0.6045\n",
            "Epoch 120 Batch 350 Loss 1.9070 Accuracy 0.6046\n",
            "Epoch 120 Batch 400 Loss 1.9066 Accuracy 0.6047\n",
            "Epoch 120 Batch 450 Loss 1.9078 Accuracy 0.6044\n",
            "Epoch 120 Batch 500 Loss 1.9096 Accuracy 0.6042\n",
            "Epoch 120 Batch 550 Loss 1.9070 Accuracy 0.6043\n",
            "Epoch 120 Batch 600 Loss 1.9101 Accuracy 0.6039\n",
            "Epoch 120 Batch 650 Loss 1.9124 Accuracy 0.6038\n",
            "discarded batch 663\n",
            "Epoch 120 Batch 700 Loss 1.9132 Accuracy 0.6038\n",
            "Epoch 120 Batch 750 Loss 1.9135 Accuracy 0.6038\n",
            "Epoch 120 Batch 800 Loss 1.9159 Accuracy 0.6036\n",
            "Epoch 120 Batch 850 Loss 1.9163 Accuracy 0.6037\n",
            "Epoch 120 Batch 900 Loss 1.9180 Accuracy 0.6036\n",
            "Epoch 120 Batch 950 Loss 1.9194 Accuracy 0.6035\n",
            "Saving checkpoint for epoch 120 at ./checkpoints/train/ckpt-24\n",
            "Epoch 120 Loss 1.9195 Accuracy 0.6035\n",
            "Time taken for 1 epoch: 24.761022090911865 secs\n",
            "\n",
            "Epoch 121 Batch 0 Loss 2.0686 Accuracy 0.5875\n",
            "Epoch 121 Batch 50 Loss 1.8817 Accuracy 0.6036\n",
            "Epoch 121 Batch 100 Loss 1.8850 Accuracy 0.6057\n",
            "Epoch 121 Batch 150 Loss 1.8886 Accuracy 0.6064\n",
            "Epoch 121 Batch 200 Loss 1.8971 Accuracy 0.6063\n",
            "Epoch 121 Batch 250 Loss 1.9049 Accuracy 0.6057\n",
            "Epoch 121 Batch 300 Loss 1.9078 Accuracy 0.6053\n",
            "Epoch 121 Batch 350 Loss 1.9071 Accuracy 0.6055\n",
            "Epoch 121 Batch 400 Loss 1.9071 Accuracy 0.6049\n",
            "Epoch 121 Batch 450 Loss 1.9098 Accuracy 0.6046\n",
            "Epoch 121 Batch 500 Loss 1.9096 Accuracy 0.6048\n",
            "Epoch 121 Batch 550 Loss 1.9108 Accuracy 0.6047\n",
            "Epoch 121 Batch 600 Loss 1.9107 Accuracy 0.6046\n",
            "Epoch 121 Batch 650 Loss 1.9104 Accuracy 0.6047\n",
            "Epoch 121 Batch 700 Loss 1.9104 Accuracy 0.6048\n",
            "Epoch 121 Batch 750 Loss 1.9111 Accuracy 0.6045\n",
            "Epoch 121 Batch 800 Loss 1.9115 Accuracy 0.6047\n",
            "Epoch 121 Batch 850 Loss 1.9143 Accuracy 0.6046\n",
            "discarded batch 891\n",
            "Epoch 121 Batch 900 Loss 1.9158 Accuracy 0.6043\n",
            "Epoch 121 Batch 950 Loss 1.9160 Accuracy 0.6043\n",
            "Epoch 121 Loss 1.9161 Accuracy 0.6043\n",
            "Time taken for 1 epoch: 24.178186416625977 secs\n",
            "\n",
            "Epoch 121 VALIDATION: Loss 2.2271 Accuracy 0.5857\n",
            "\n",
            "Epoch 122 Batch 0 Loss 1.9109 Accuracy 0.6238\n",
            "Epoch 122 Batch 50 Loss 1.8782 Accuracy 0.6062\n",
            "Epoch 122 Batch 100 Loss 1.8999 Accuracy 0.6058\n",
            "Epoch 122 Batch 150 Loss 1.8972 Accuracy 0.6051\n",
            "Epoch 122 Batch 200 Loss 1.8977 Accuracy 0.6049\n",
            "Epoch 122 Batch 250 Loss 1.9001 Accuracy 0.6049\n",
            "Epoch 122 Batch 300 Loss 1.8985 Accuracy 0.6053\n",
            "Epoch 122 Batch 350 Loss 1.8961 Accuracy 0.6061\n",
            "Epoch 122 Batch 400 Loss 1.8998 Accuracy 0.6061\n",
            "discarded batch 421\n",
            "Epoch 122 Batch 450 Loss 1.9017 Accuracy 0.6058\n",
            "Epoch 122 Batch 500 Loss 1.9041 Accuracy 0.6057\n",
            "Epoch 122 Batch 550 Loss 1.9047 Accuracy 0.6055\n",
            "Epoch 122 Batch 600 Loss 1.9056 Accuracy 0.6051\n",
            "Epoch 122 Batch 650 Loss 1.9070 Accuracy 0.6050\n",
            "Epoch 122 Batch 700 Loss 1.9079 Accuracy 0.6048\n",
            "Epoch 122 Batch 750 Loss 1.9094 Accuracy 0.6045\n",
            "Epoch 122 Batch 800 Loss 1.9116 Accuracy 0.6045\n",
            "Epoch 122 Batch 850 Loss 1.9117 Accuracy 0.6045\n",
            "Epoch 122 Batch 900 Loss 1.9135 Accuracy 0.6041\n",
            "Epoch 122 Batch 950 Loss 1.9138 Accuracy 0.6041\n",
            "Epoch 122 Loss 1.9136 Accuracy 0.6042\n",
            "Time taken for 1 epoch: 24.377290964126587 secs\n",
            "\n",
            "Epoch 123 Batch 0 Loss 1.8605 Accuracy 0.6304\n",
            "Epoch 123 Batch 50 Loss 1.8554 Accuracy 0.6096\n",
            "Epoch 123 Batch 100 Loss 1.8684 Accuracy 0.6080\n",
            "Epoch 123 Batch 150 Loss 1.8774 Accuracy 0.6079\n",
            "Epoch 123 Batch 200 Loss 1.8807 Accuracy 0.6071\n",
            "Epoch 123 Batch 250 Loss 1.8876 Accuracy 0.6064\n",
            "discarded batch 289\n",
            "Epoch 123 Batch 300 Loss 1.8911 Accuracy 0.6062\n",
            "Epoch 123 Batch 350 Loss 1.8889 Accuracy 0.6068\n",
            "Epoch 123 Batch 400 Loss 1.8937 Accuracy 0.6058\n",
            "Epoch 123 Batch 450 Loss 1.8943 Accuracy 0.6056\n",
            "Epoch 123 Batch 500 Loss 1.8976 Accuracy 0.6047\n",
            "Epoch 123 Batch 550 Loss 1.9006 Accuracy 0.6043\n",
            "Epoch 123 Batch 600 Loss 1.9021 Accuracy 0.6046\n",
            "Epoch 123 Batch 650 Loss 1.9024 Accuracy 0.6046\n",
            "Epoch 123 Batch 700 Loss 1.9045 Accuracy 0.6047\n",
            "Epoch 123 Batch 750 Loss 1.9071 Accuracy 0.6045\n",
            "Epoch 123 Batch 800 Loss 1.9072 Accuracy 0.6045\n",
            "Epoch 123 Batch 850 Loss 1.9092 Accuracy 0.6044\n",
            "Epoch 123 Batch 900 Loss 1.9093 Accuracy 0.6047\n",
            "Epoch 123 Batch 950 Loss 1.9105 Accuracy 0.6047\n",
            "Epoch 123 Loss 1.9106 Accuracy 0.6047\n",
            "Time taken for 1 epoch: 24.61701464653015 secs\n",
            "\n",
            "Epoch 124 Batch 0 Loss 1.7946 Accuracy 0.6139\n",
            "Epoch 124 Batch 50 Loss 1.8490 Accuracy 0.6154\n",
            "Epoch 124 Batch 100 Loss 1.8682 Accuracy 0.6114\n",
            "Epoch 124 Batch 150 Loss 1.8682 Accuracy 0.6113\n",
            "Epoch 124 Batch 200 Loss 1.8769 Accuracy 0.6101\n",
            "Epoch 124 Batch 250 Loss 1.8851 Accuracy 0.6088\n",
            "Epoch 124 Batch 300 Loss 1.8879 Accuracy 0.6087\n",
            "Epoch 124 Batch 350 Loss 1.8901 Accuracy 0.6084\n",
            "Epoch 124 Batch 400 Loss 1.8943 Accuracy 0.6075\n",
            "Epoch 124 Batch 450 Loss 1.8986 Accuracy 0.6068\n",
            "Epoch 124 Batch 500 Loss 1.8978 Accuracy 0.6068\n",
            "Epoch 124 Batch 550 Loss 1.8998 Accuracy 0.6065\n",
            "Epoch 124 Batch 600 Loss 1.9018 Accuracy 0.6063\n",
            "Epoch 124 Batch 650 Loss 1.9046 Accuracy 0.6060\n",
            "Epoch 124 Batch 700 Loss 1.9070 Accuracy 0.6055\n",
            "Epoch 124 Batch 750 Loss 1.9083 Accuracy 0.6053\n",
            "Epoch 124 Batch 800 Loss 1.9096 Accuracy 0.6052\n",
            "Epoch 124 Batch 850 Loss 1.9083 Accuracy 0.6053\n",
            "Epoch 124 Batch 900 Loss 1.9083 Accuracy 0.6054\n",
            "Epoch 124 Batch 950 Loss 1.9098 Accuracy 0.6052\n",
            "discarded batch 953\n",
            "Epoch 124 Loss 1.9100 Accuracy 0.6052\n",
            "Time taken for 1 epoch: 24.5568585395813 secs\n",
            "\n",
            "Epoch 125 Batch 0 Loss 2.0877 Accuracy 0.6073\n",
            "Epoch 125 Batch 50 Loss 1.9027 Accuracy 0.6045\n",
            "Epoch 125 Batch 100 Loss 1.8830 Accuracy 0.6070\n",
            "Epoch 125 Batch 150 Loss 1.8777 Accuracy 0.6083\n",
            "Epoch 125 Batch 200 Loss 1.8832 Accuracy 0.6072\n",
            "Epoch 125 Batch 250 Loss 1.8880 Accuracy 0.6073\n",
            "Epoch 125 Batch 300 Loss 1.8879 Accuracy 0.6073\n",
            "Epoch 125 Batch 350 Loss 1.8915 Accuracy 0.6071\n",
            "Epoch 125 Batch 400 Loss 1.8930 Accuracy 0.6070\n",
            "Epoch 125 Batch 450 Loss 1.8960 Accuracy 0.6062\n",
            "Epoch 125 Batch 500 Loss 1.8970 Accuracy 0.6064\n",
            "Epoch 125 Batch 550 Loss 1.8966 Accuracy 0.6068\n",
            "Epoch 125 Batch 600 Loss 1.8986 Accuracy 0.6069\n",
            "Epoch 125 Batch 650 Loss 1.9008 Accuracy 0.6065\n",
            "Epoch 125 Batch 700 Loss 1.9007 Accuracy 0.6064\n",
            "discarded batch 729\n",
            "Epoch 125 Batch 750 Loss 1.9024 Accuracy 0.6062\n",
            "Epoch 125 Batch 800 Loss 1.9028 Accuracy 0.6060\n",
            "Epoch 125 Batch 850 Loss 1.9048 Accuracy 0.6056\n",
            "Epoch 125 Batch 900 Loss 1.9063 Accuracy 0.6055\n",
            "Epoch 125 Batch 950 Loss 1.9074 Accuracy 0.6053\n",
            "Saving checkpoint for epoch 125 at ./checkpoints/train/ckpt-25\n",
            "Epoch 125 Loss 1.9076 Accuracy 0.6053\n",
            "Time taken for 1 epoch: 25.03435730934143 secs\n",
            "\n",
            "Epoch 126 Batch 0 Loss 1.8780 Accuracy 0.5875\n",
            "Epoch 126 Batch 50 Loss 1.8934 Accuracy 0.6027\n",
            "Epoch 126 Batch 100 Loss 1.8850 Accuracy 0.6057\n",
            "Epoch 126 Batch 150 Loss 1.8850 Accuracy 0.6071\n",
            "Epoch 126 Batch 200 Loss 1.8927 Accuracy 0.6059\n",
            "Epoch 126 Batch 250 Loss 1.8914 Accuracy 0.6064\n",
            "discarded batch 278\n",
            "Epoch 126 Batch 300 Loss 1.8965 Accuracy 0.6056\n",
            "Epoch 126 Batch 350 Loss 1.8963 Accuracy 0.6056\n",
            "Epoch 126 Batch 400 Loss 1.8950 Accuracy 0.6064\n",
            "Epoch 126 Batch 450 Loss 1.8983 Accuracy 0.6057\n",
            "Epoch 126 Batch 500 Loss 1.8972 Accuracy 0.6060\n",
            "Epoch 126 Batch 550 Loss 1.8981 Accuracy 0.6061\n",
            "Epoch 126 Batch 600 Loss 1.8975 Accuracy 0.6060\n",
            "Epoch 126 Batch 650 Loss 1.8985 Accuracy 0.6061\n",
            "Epoch 126 Batch 700 Loss 1.8988 Accuracy 0.6061\n",
            "Epoch 126 Batch 750 Loss 1.9005 Accuracy 0.6060\n",
            "Epoch 126 Batch 800 Loss 1.9025 Accuracy 0.6059\n",
            "Epoch 126 Batch 850 Loss 1.9032 Accuracy 0.6059\n",
            "Epoch 126 Batch 900 Loss 1.9048 Accuracy 0.6057\n",
            "Epoch 126 Batch 950 Loss 1.9045 Accuracy 0.6057\n",
            "Epoch 126 Loss 1.9044 Accuracy 0.6057\n",
            "Time taken for 1 epoch: 24.514314889907837 secs\n",
            "\n",
            "Epoch 126 VALIDATION: Loss 2.2176 Accuracy 0.5871\n",
            "\n",
            "Epoch 127 Batch 0 Loss 1.9528 Accuracy 0.6007\n",
            "Epoch 127 Batch 50 Loss 1.8753 Accuracy 0.6031\n",
            "Epoch 127 Batch 100 Loss 1.8770 Accuracy 0.6057\n",
            "Epoch 127 Batch 150 Loss 1.8881 Accuracy 0.6064\n",
            "Epoch 127 Batch 200 Loss 1.8874 Accuracy 0.6055\n",
            "Epoch 127 Batch 250 Loss 1.8870 Accuracy 0.6057\n",
            "Epoch 127 Batch 300 Loss 1.8831 Accuracy 0.6063\n",
            "Epoch 127 Batch 350 Loss 1.8830 Accuracy 0.6070\n",
            "discarded batch 395\n",
            "Epoch 127 Batch 400 Loss 1.8823 Accuracy 0.6074\n",
            "Epoch 127 Batch 450 Loss 1.8833 Accuracy 0.6074\n",
            "Epoch 127 Batch 500 Loss 1.8873 Accuracy 0.6068\n",
            "Epoch 127 Batch 550 Loss 1.8897 Accuracy 0.6067\n",
            "Epoch 127 Batch 600 Loss 1.8925 Accuracy 0.6066\n",
            "Epoch 127 Batch 650 Loss 1.8938 Accuracy 0.6063\n",
            "Epoch 127 Batch 700 Loss 1.8960 Accuracy 0.6063\n",
            "Epoch 127 Batch 750 Loss 1.8977 Accuracy 0.6063\n",
            "Epoch 127 Batch 800 Loss 1.8976 Accuracy 0.6062\n",
            "Epoch 127 Batch 850 Loss 1.9005 Accuracy 0.6057\n",
            "Epoch 127 Batch 900 Loss 1.9010 Accuracy 0.6059\n",
            "Epoch 127 Batch 950 Loss 1.9021 Accuracy 0.6058\n",
            "Epoch 127 Loss 1.9025 Accuracy 0.6058\n",
            "Time taken for 1 epoch: 24.00157642364502 secs\n",
            "\n",
            "Epoch 128 Batch 0 Loss 1.9020 Accuracy 0.6106\n",
            "Epoch 128 Batch 50 Loss 1.8681 Accuracy 0.6087\n",
            "Epoch 128 Batch 100 Loss 1.8706 Accuracy 0.6093\n",
            "Epoch 128 Batch 150 Loss 1.8860 Accuracy 0.6079\n",
            "discarded batch 191\n",
            "Epoch 128 Batch 200 Loss 1.8848 Accuracy 0.6069\n",
            "Epoch 128 Batch 250 Loss 1.8869 Accuracy 0.6068\n",
            "Epoch 128 Batch 300 Loss 1.8879 Accuracy 0.6061\n",
            "Epoch 128 Batch 350 Loss 1.8894 Accuracy 0.6058\n",
            "Epoch 128 Batch 400 Loss 1.8895 Accuracy 0.6056\n",
            "Epoch 128 Batch 450 Loss 1.8879 Accuracy 0.6056\n",
            "Epoch 128 Batch 500 Loss 1.8891 Accuracy 0.6059\n",
            "Epoch 128 Batch 550 Loss 1.8878 Accuracy 0.6065\n",
            "Epoch 128 Batch 600 Loss 1.8917 Accuracy 0.6061\n",
            "Epoch 128 Batch 650 Loss 1.8946 Accuracy 0.6061\n",
            "Epoch 128 Batch 700 Loss 1.8947 Accuracy 0.6062\n",
            "Epoch 128 Batch 750 Loss 1.8959 Accuracy 0.6061\n",
            "Epoch 128 Batch 800 Loss 1.8971 Accuracy 0.6062\n",
            "Epoch 128 Batch 850 Loss 1.8965 Accuracy 0.6062\n",
            "Epoch 128 Batch 900 Loss 1.8969 Accuracy 0.6061\n",
            "Epoch 128 Batch 950 Loss 1.8969 Accuracy 0.6064\n",
            "Epoch 128 Loss 1.8971 Accuracy 0.6064\n",
            "Time taken for 1 epoch: 24.057328939437866 secs\n",
            "\n",
            "Epoch 129 Batch 0 Loss 1.7687 Accuracy 0.6172\n",
            "Epoch 129 Batch 50 Loss 1.8707 Accuracy 0.6117\n",
            "Epoch 129 Batch 100 Loss 1.8709 Accuracy 0.6100\n",
            "Epoch 129 Batch 150 Loss 1.8685 Accuracy 0.6091\n",
            "Epoch 129 Batch 200 Loss 1.8735 Accuracy 0.6093\n",
            "Epoch 129 Batch 250 Loss 1.8747 Accuracy 0.6096\n",
            "Epoch 129 Batch 300 Loss 1.8793 Accuracy 0.6090\n",
            "Epoch 129 Batch 350 Loss 1.8833 Accuracy 0.6084\n",
            "Epoch 129 Batch 400 Loss 1.8835 Accuracy 0.6084\n",
            "Epoch 129 Batch 450 Loss 1.8850 Accuracy 0.6081\n",
            "Epoch 129 Batch 500 Loss 1.8827 Accuracy 0.6085\n",
            "Epoch 129 Batch 550 Loss 1.8818 Accuracy 0.6080\n",
            "Epoch 129 Batch 600 Loss 1.8829 Accuracy 0.6076\n",
            "Epoch 129 Batch 650 Loss 1.8855 Accuracy 0.6074\n",
            "Epoch 129 Batch 700 Loss 1.8874 Accuracy 0.6074\n",
            "Epoch 129 Batch 750 Loss 1.8889 Accuracy 0.6075\n",
            "Epoch 129 Batch 800 Loss 1.8904 Accuracy 0.6073\n",
            "discarded batch 849\n",
            "Epoch 129 Batch 850 Loss 1.8919 Accuracy 0.6070\n",
            "Epoch 129 Batch 900 Loss 1.8934 Accuracy 0.6068\n",
            "Epoch 129 Batch 950 Loss 1.8939 Accuracy 0.6068\n",
            "Epoch 129 Loss 1.8939 Accuracy 0.6067\n",
            "Time taken for 1 epoch: 24.010361671447754 secs\n",
            "\n",
            "Epoch 130 Batch 0 Loss 2.0381 Accuracy 0.5875\n",
            "Epoch 130 Batch 50 Loss 1.8797 Accuracy 0.6116\n",
            "Epoch 130 Batch 100 Loss 1.8755 Accuracy 0.6095\n",
            "Epoch 130 Batch 150 Loss 1.8742 Accuracy 0.6088\n",
            "Epoch 130 Batch 200 Loss 1.8774 Accuracy 0.6082\n",
            "Epoch 130 Batch 250 Loss 1.8745 Accuracy 0.6086\n",
            "Epoch 130 Batch 300 Loss 1.8746 Accuracy 0.6083\n",
            "Epoch 130 Batch 350 Loss 1.8748 Accuracy 0.6083\n",
            "Epoch 130 Batch 400 Loss 1.8767 Accuracy 0.6082\n",
            "Epoch 130 Batch 450 Loss 1.8794 Accuracy 0.6080\n",
            "Epoch 130 Batch 500 Loss 1.8810 Accuracy 0.6079\n",
            "Epoch 130 Batch 550 Loss 1.8829 Accuracy 0.6076\n",
            "Epoch 130 Batch 600 Loss 1.8847 Accuracy 0.6075\n",
            "Epoch 130 Batch 650 Loss 1.8860 Accuracy 0.6072\n",
            "Epoch 130 Batch 700 Loss 1.8905 Accuracy 0.6064\n",
            "discarded batch 750\n",
            "Epoch 130 Batch 800 Loss 1.8898 Accuracy 0.6066\n",
            "Epoch 130 Batch 850 Loss 1.8902 Accuracy 0.6066\n",
            "Epoch 130 Batch 900 Loss 1.8913 Accuracy 0.6066\n",
            "Epoch 130 Batch 950 Loss 1.8932 Accuracy 0.6066\n",
            "Saving checkpoint for epoch 130 at ./checkpoints/train/ckpt-26\n",
            "Epoch 130 Loss 1.8933 Accuracy 0.6066\n",
            "Time taken for 1 epoch: 24.3593168258667 secs\n",
            "\n",
            "Epoch 131 Batch 0 Loss 1.9491 Accuracy 0.5875\n",
            "Epoch 131 Batch 50 Loss 1.8387 Accuracy 0.6135\n",
            "Epoch 131 Batch 100 Loss 1.8354 Accuracy 0.6120\n",
            "Epoch 131 Batch 150 Loss 1.8555 Accuracy 0.6115\n",
            "Epoch 131 Batch 200 Loss 1.8662 Accuracy 0.6094\n",
            "Epoch 131 Batch 250 Loss 1.8695 Accuracy 0.6086\n",
            "Epoch 131 Batch 300 Loss 1.8758 Accuracy 0.6083\n",
            "Epoch 131 Batch 350 Loss 1.8745 Accuracy 0.6087\n",
            "Epoch 131 Batch 400 Loss 1.8776 Accuracy 0.6090\n",
            "Epoch 131 Batch 450 Loss 1.8790 Accuracy 0.6087\n",
            "Epoch 131 Batch 500 Loss 1.8799 Accuracy 0.6087\n",
            "Epoch 131 Batch 550 Loss 1.8812 Accuracy 0.6089\n",
            "discarded batch 596\n",
            "Epoch 131 Batch 600 Loss 1.8812 Accuracy 0.6090\n",
            "Epoch 131 Batch 650 Loss 1.8842 Accuracy 0.6087\n",
            "Epoch 131 Batch 700 Loss 1.8848 Accuracy 0.6084\n",
            "Epoch 131 Batch 750 Loss 1.8862 Accuracy 0.6083\n",
            "Epoch 131 Batch 800 Loss 1.8879 Accuracy 0.6080\n",
            "Epoch 131 Batch 850 Loss 1.8892 Accuracy 0.6077\n",
            "Epoch 131 Batch 900 Loss 1.8914 Accuracy 0.6073\n",
            "Epoch 131 Batch 950 Loss 1.8923 Accuracy 0.6072\n",
            "Epoch 131 Loss 1.8926 Accuracy 0.6072\n",
            "Time taken for 1 epoch: 24.113332509994507 secs\n",
            "\n",
            "Epoch 131 VALIDATION: Loss 2.2252 Accuracy 0.5857\n",
            "\n",
            "Epoch 132 Batch 0 Loss 1.7987 Accuracy 0.6271\n",
            "Epoch 132 Batch 50 Loss 1.8468 Accuracy 0.6130\n",
            "Epoch 132 Batch 100 Loss 1.8622 Accuracy 0.6130\n",
            "Epoch 132 Batch 150 Loss 1.8550 Accuracy 0.6136\n",
            "Epoch 132 Batch 200 Loss 1.8596 Accuracy 0.6127\n",
            "Epoch 132 Batch 250 Loss 1.8603 Accuracy 0.6118\n",
            "Epoch 132 Batch 300 Loss 1.8619 Accuracy 0.6115\n",
            "Epoch 132 Batch 350 Loss 1.8659 Accuracy 0.6101\n",
            "Epoch 132 Batch 400 Loss 1.8704 Accuracy 0.6100\n",
            "Epoch 132 Batch 450 Loss 1.8711 Accuracy 0.6097\n",
            "Epoch 132 Batch 500 Loss 1.8725 Accuracy 0.6092\n",
            "Epoch 132 Batch 550 Loss 1.8748 Accuracy 0.6090\n",
            "Epoch 132 Batch 600 Loss 1.8768 Accuracy 0.6089\n",
            "Epoch 132 Batch 650 Loss 1.8787 Accuracy 0.6087\n",
            "discarded batch 694\n",
            "Epoch 132 Batch 700 Loss 1.8827 Accuracy 0.6083\n",
            "Epoch 132 Batch 750 Loss 1.8845 Accuracy 0.6081\n",
            "Epoch 132 Batch 800 Loss 1.8860 Accuracy 0.6078\n",
            "Epoch 132 Batch 850 Loss 1.8859 Accuracy 0.6079\n",
            "Epoch 132 Batch 900 Loss 1.8881 Accuracy 0.6077\n",
            "Epoch 132 Batch 950 Loss 1.8893 Accuracy 0.6076\n",
            "Epoch 132 Loss 1.8899 Accuracy 0.6075\n",
            "Time taken for 1 epoch: 24.253545999526978 secs\n",
            "\n",
            "Epoch 133 Batch 0 Loss 1.8699 Accuracy 0.5974\n",
            "Epoch 133 Batch 50 Loss 1.8598 Accuracy 0.6139\n",
            "Epoch 133 Batch 100 Loss 1.8532 Accuracy 0.6130\n",
            "Epoch 133 Batch 150 Loss 1.8655 Accuracy 0.6116\n",
            "Epoch 133 Batch 200 Loss 1.8664 Accuracy 0.6120\n",
            "Epoch 133 Batch 250 Loss 1.8743 Accuracy 0.6109\n",
            "Epoch 133 Batch 300 Loss 1.8781 Accuracy 0.6103\n",
            "Epoch 133 Batch 350 Loss 1.8794 Accuracy 0.6104\n",
            "Epoch 133 Batch 400 Loss 1.8808 Accuracy 0.6097\n",
            "Epoch 133 Batch 450 Loss 1.8834 Accuracy 0.6095\n",
            "discarded batch 474\n",
            "Epoch 133 Batch 500 Loss 1.8833 Accuracy 0.6095\n",
            "Epoch 133 Batch 550 Loss 1.8813 Accuracy 0.6096\n",
            "Epoch 133 Batch 600 Loss 1.8825 Accuracy 0.6092\n",
            "Epoch 133 Batch 650 Loss 1.8835 Accuracy 0.6089\n",
            "Epoch 133 Batch 700 Loss 1.8830 Accuracy 0.6090\n",
            "Epoch 133 Batch 750 Loss 1.8811 Accuracy 0.6091\n",
            "Epoch 133 Batch 800 Loss 1.8821 Accuracy 0.6088\n",
            "Epoch 133 Batch 850 Loss 1.8837 Accuracy 0.6085\n",
            "Epoch 133 Batch 900 Loss 1.8845 Accuracy 0.6085\n",
            "Epoch 133 Batch 950 Loss 1.8862 Accuracy 0.6083\n",
            "Epoch 133 Loss 1.8861 Accuracy 0.6083\n",
            "Time taken for 1 epoch: 24.217975616455078 secs\n",
            "\n",
            "Epoch 134 Batch 0 Loss 2.1420 Accuracy 0.6172\n",
            "Epoch 134 Batch 50 Loss 1.8454 Accuracy 0.6125\n",
            "Epoch 134 Batch 100 Loss 1.8568 Accuracy 0.6107\n",
            "Epoch 134 Batch 150 Loss 1.8582 Accuracy 0.6103\n",
            "Epoch 134 Batch 200 Loss 1.8664 Accuracy 0.6100\n",
            "discarded batch 240\n",
            "Epoch 134 Batch 250 Loss 1.8660 Accuracy 0.6098\n",
            "Epoch 134 Batch 300 Loss 1.8684 Accuracy 0.6097\n",
            "Epoch 134 Batch 350 Loss 1.8712 Accuracy 0.6097\n",
            "Epoch 134 Batch 400 Loss 1.8712 Accuracy 0.6103\n",
            "Epoch 134 Batch 450 Loss 1.8734 Accuracy 0.6094\n",
            "Epoch 134 Batch 500 Loss 1.8721 Accuracy 0.6093\n",
            "Epoch 134 Batch 550 Loss 1.8755 Accuracy 0.6091\n",
            "Epoch 134 Batch 600 Loss 1.8738 Accuracy 0.6093\n",
            "Epoch 134 Batch 650 Loss 1.8748 Accuracy 0.6094\n",
            "Epoch 134 Batch 700 Loss 1.8785 Accuracy 0.6088\n",
            "Epoch 134 Batch 750 Loss 1.8789 Accuracy 0.6086\n",
            "Epoch 134 Batch 800 Loss 1.8800 Accuracy 0.6086\n",
            "Epoch 134 Batch 850 Loss 1.8802 Accuracy 0.6086\n",
            "Epoch 134 Batch 900 Loss 1.8820 Accuracy 0.6086\n",
            "Epoch 134 Batch 950 Loss 1.8844 Accuracy 0.6083\n",
            "Epoch 134 Loss 1.8846 Accuracy 0.6083\n",
            "Time taken for 1 epoch: 23.765869140625 secs\n",
            "\n",
            "Epoch 135 Batch 0 Loss 1.8378 Accuracy 0.6007\n",
            "Epoch 135 Batch 50 Loss 1.8360 Accuracy 0.6163\n",
            "Epoch 135 Batch 100 Loss 1.8696 Accuracy 0.6126\n",
            "Epoch 135 Batch 150 Loss 1.8677 Accuracy 0.6119\n",
            "Epoch 135 Batch 200 Loss 1.8700 Accuracy 0.6102\n",
            "Epoch 135 Batch 250 Loss 1.8678 Accuracy 0.6100\n",
            "Epoch 135 Batch 300 Loss 1.8679 Accuracy 0.6096\n",
            "Epoch 135 Batch 350 Loss 1.8713 Accuracy 0.6090\n",
            "Epoch 135 Batch 400 Loss 1.8746 Accuracy 0.6093\n",
            "Epoch 135 Batch 450 Loss 1.8744 Accuracy 0.6097\n",
            "Epoch 135 Batch 500 Loss 1.8732 Accuracy 0.6098\n",
            "Epoch 135 Batch 550 Loss 1.8751 Accuracy 0.6096\n",
            "Epoch 135 Batch 600 Loss 1.8759 Accuracy 0.6096\n",
            "Epoch 135 Batch 650 Loss 1.8782 Accuracy 0.6093\n",
            "Epoch 135 Batch 700 Loss 1.8785 Accuracy 0.6095\n",
            "Epoch 135 Batch 750 Loss 1.8789 Accuracy 0.6094\n",
            "discarded batch 756\n",
            "Epoch 135 Batch 800 Loss 1.8791 Accuracy 0.6096\n",
            "Epoch 135 Batch 850 Loss 1.8812 Accuracy 0.6092\n",
            "Epoch 135 Batch 900 Loss 1.8816 Accuracy 0.6092\n",
            "Epoch 135 Batch 950 Loss 1.8810 Accuracy 0.6092\n",
            "Saving checkpoint for epoch 135 at ./checkpoints/train/ckpt-27\n",
            "Epoch 135 Loss 1.8808 Accuracy 0.6092\n",
            "Time taken for 1 epoch: 24.31731677055359 secs\n",
            "\n",
            "Epoch 136 Batch 0 Loss 1.9050 Accuracy 0.6139\n",
            "Epoch 136 Batch 50 Loss 1.8661 Accuracy 0.6111\n",
            "Epoch 136 Batch 100 Loss 1.8650 Accuracy 0.6112\n",
            "discarded batch 144\n",
            "Epoch 136 Batch 150 Loss 1.8591 Accuracy 0.6107\n",
            "Epoch 136 Batch 200 Loss 1.8622 Accuracy 0.6097\n",
            "Epoch 136 Batch 250 Loss 1.8637 Accuracy 0.6092\n",
            "Epoch 136 Batch 300 Loss 1.8660 Accuracy 0.6089\n",
            "Epoch 136 Batch 350 Loss 1.8656 Accuracy 0.6091\n",
            "Epoch 136 Batch 400 Loss 1.8630 Accuracy 0.6093\n",
            "Epoch 136 Batch 450 Loss 1.8657 Accuracy 0.6092\n",
            "Epoch 136 Batch 500 Loss 1.8680 Accuracy 0.6090\n",
            "Epoch 136 Batch 550 Loss 1.8697 Accuracy 0.6090\n",
            "Epoch 136 Batch 600 Loss 1.8709 Accuracy 0.6089\n",
            "Epoch 136 Batch 650 Loss 1.8719 Accuracy 0.6088\n",
            "Epoch 136 Batch 700 Loss 1.8729 Accuracy 0.6087\n",
            "Epoch 136 Batch 750 Loss 1.8742 Accuracy 0.6085\n",
            "Epoch 136 Batch 800 Loss 1.8760 Accuracy 0.6088\n",
            "Epoch 136 Batch 850 Loss 1.8776 Accuracy 0.6087\n",
            "Epoch 136 Batch 900 Loss 1.8782 Accuracy 0.6087\n",
            "Epoch 136 Batch 950 Loss 1.8791 Accuracy 0.6088\n",
            "Epoch 136 Loss 1.8791 Accuracy 0.6088\n",
            "Time taken for 1 epoch: 24.040534496307373 secs\n",
            "\n",
            "Epoch 136 VALIDATION: Loss 2.2270 Accuracy 0.5863\n",
            "\n",
            "Epoch 137 Batch 0 Loss 1.7636 Accuracy 0.6172\n",
            "Epoch 137 Batch 50 Loss 1.8465 Accuracy 0.6119\n",
            "Epoch 137 Batch 100 Loss 1.8568 Accuracy 0.6129\n",
            "Epoch 137 Batch 150 Loss 1.8565 Accuracy 0.6124\n",
            "Epoch 137 Batch 200 Loss 1.8614 Accuracy 0.6115\n",
            "Epoch 137 Batch 250 Loss 1.8634 Accuracy 0.6115\n",
            "Epoch 137 Batch 300 Loss 1.8627 Accuracy 0.6114\n",
            "Epoch 137 Batch 350 Loss 1.8625 Accuracy 0.6112\n",
            "Epoch 137 Batch 400 Loss 1.8663 Accuracy 0.6104\n",
            "Epoch 137 Batch 450 Loss 1.8663 Accuracy 0.6107\n",
            "Epoch 137 Batch 500 Loss 1.8663 Accuracy 0.6105\n",
            "Epoch 137 Batch 550 Loss 1.8693 Accuracy 0.6104\n",
            "Epoch 137 Batch 600 Loss 1.8698 Accuracy 0.6105\n",
            "Epoch 137 Batch 650 Loss 1.8708 Accuracy 0.6102\n",
            "Epoch 137 Batch 700 Loss 1.8718 Accuracy 0.6102\n",
            "Epoch 137 Batch 750 Loss 1.8730 Accuracy 0.6100\n",
            "Epoch 137 Batch 800 Loss 1.8750 Accuracy 0.6097\n",
            "Epoch 137 Batch 850 Loss 1.8754 Accuracy 0.6097\n",
            "Epoch 137 Batch 900 Loss 1.8762 Accuracy 0.6095\n",
            "discarded batch 925\n",
            "Epoch 137 Batch 950 Loss 1.8761 Accuracy 0.6096\n",
            "Epoch 137 Loss 1.8765 Accuracy 0.6096\n",
            "Time taken for 1 epoch: 24.037028312683105 secs\n",
            "\n",
            "Epoch 138 Batch 0 Loss 1.9126 Accuracy 0.6205\n",
            "Epoch 138 Batch 50 Loss 1.8486 Accuracy 0.6111\n",
            "Epoch 138 Batch 100 Loss 1.8390 Accuracy 0.6141\n",
            "Epoch 138 Batch 150 Loss 1.8443 Accuracy 0.6133\n",
            "Epoch 138 Batch 200 Loss 1.8557 Accuracy 0.6118\n",
            "Epoch 138 Batch 250 Loss 1.8553 Accuracy 0.6111\n",
            "Epoch 138 Batch 300 Loss 1.8538 Accuracy 0.6114\n",
            "Epoch 138 Batch 350 Loss 1.8559 Accuracy 0.6117\n",
            "Epoch 138 Batch 400 Loss 1.8558 Accuracy 0.6119\n",
            "Epoch 138 Batch 450 Loss 1.8583 Accuracy 0.6114\n",
            "Epoch 138 Batch 500 Loss 1.8621 Accuracy 0.6113\n",
            "Epoch 138 Batch 550 Loss 1.8642 Accuracy 0.6109\n",
            "Epoch 138 Batch 600 Loss 1.8664 Accuracy 0.6105\n",
            "Epoch 138 Batch 650 Loss 1.8653 Accuracy 0.6109\n",
            "discarded batch 669\n",
            "Epoch 138 Batch 700 Loss 1.8680 Accuracy 0.6104\n",
            "Epoch 138 Batch 750 Loss 1.8686 Accuracy 0.6103\n",
            "Epoch 138 Batch 800 Loss 1.8709 Accuracy 0.6103\n",
            "Epoch 138 Batch 850 Loss 1.8717 Accuracy 0.6102\n",
            "Epoch 138 Batch 900 Loss 1.8727 Accuracy 0.6101\n",
            "Epoch 138 Batch 950 Loss 1.8728 Accuracy 0.6100\n",
            "Epoch 138 Loss 1.8728 Accuracy 0.6100\n",
            "Time taken for 1 epoch: 24.274263381958008 secs\n",
            "\n",
            "Epoch 139 Batch 0 Loss 1.7215 Accuracy 0.6469\n",
            "Epoch 139 Batch 50 Loss 1.8455 Accuracy 0.6144\n",
            "Epoch 139 Batch 100 Loss 1.8352 Accuracy 0.6149\n",
            "Epoch 139 Batch 150 Loss 1.8438 Accuracy 0.6132\n",
            "Epoch 139 Batch 200 Loss 1.8424 Accuracy 0.6131\n",
            "Epoch 139 Batch 250 Loss 1.8438 Accuracy 0.6130\n",
            "Epoch 139 Batch 300 Loss 1.8482 Accuracy 0.6123\n",
            "Epoch 139 Batch 350 Loss 1.8518 Accuracy 0.6116\n",
            "Epoch 139 Batch 400 Loss 1.8568 Accuracy 0.6110\n",
            "Epoch 139 Batch 450 Loss 1.8604 Accuracy 0.6109\n",
            "Epoch 139 Batch 500 Loss 1.8625 Accuracy 0.6109\n",
            "discarded batch 533\n",
            "Epoch 139 Batch 550 Loss 1.8636 Accuracy 0.6108\n",
            "Epoch 139 Batch 600 Loss 1.8647 Accuracy 0.6107\n",
            "Epoch 139 Batch 650 Loss 1.8649 Accuracy 0.6108\n",
            "Epoch 139 Batch 700 Loss 1.8665 Accuracy 0.6107\n",
            "Epoch 139 Batch 750 Loss 1.8665 Accuracy 0.6108\n",
            "Epoch 139 Batch 800 Loss 1.8671 Accuracy 0.6109\n",
            "Epoch 139 Batch 850 Loss 1.8682 Accuracy 0.6107\n",
            "Epoch 139 Batch 900 Loss 1.8694 Accuracy 0.6106\n",
            "Epoch 139 Batch 950 Loss 1.8715 Accuracy 0.6105\n",
            "Epoch 139 Loss 1.8716 Accuracy 0.6105\n",
            "Time taken for 1 epoch: 23.99242901802063 secs\n",
            "\n",
            "Epoch 140 Batch 0 Loss 1.7981 Accuracy 0.6007\n",
            "Epoch 140 Batch 50 Loss 1.8373 Accuracy 0.6155\n",
            "Epoch 140 Batch 100 Loss 1.8400 Accuracy 0.6149\n",
            "Epoch 140 Batch 150 Loss 1.8373 Accuracy 0.6144\n",
            "Epoch 140 Batch 200 Loss 1.8394 Accuracy 0.6133\n",
            "Epoch 140 Batch 250 Loss 1.8500 Accuracy 0.6124\n",
            "Epoch 140 Batch 300 Loss 1.8496 Accuracy 0.6125\n",
            "Epoch 140 Batch 350 Loss 1.8532 Accuracy 0.6117\n",
            "discarded batch 374\n",
            "Epoch 140 Batch 400 Loss 1.8553 Accuracy 0.6113\n",
            "Epoch 140 Batch 450 Loss 1.8554 Accuracy 0.6112\n",
            "Epoch 140 Batch 500 Loss 1.8573 Accuracy 0.6110\n",
            "Epoch 140 Batch 550 Loss 1.8602 Accuracy 0.6106\n",
            "Epoch 140 Batch 600 Loss 1.8614 Accuracy 0.6102\n",
            "Epoch 140 Batch 650 Loss 1.8605 Accuracy 0.6105\n",
            "Epoch 140 Batch 700 Loss 1.8629 Accuracy 0.6105\n",
            "Epoch 140 Batch 750 Loss 1.8646 Accuracy 0.6105\n",
            "Epoch 140 Batch 800 Loss 1.8644 Accuracy 0.6104\n",
            "Epoch 140 Batch 850 Loss 1.8666 Accuracy 0.6105\n",
            "Epoch 140 Batch 900 Loss 1.8670 Accuracy 0.6104\n",
            "Epoch 140 Batch 950 Loss 1.8672 Accuracy 0.6103\n",
            "Saving checkpoint for epoch 140 at ./checkpoints/train/ckpt-28\n",
            "Epoch 140 Loss 1.8673 Accuracy 0.6103\n",
            "Time taken for 1 epoch: 24.21019411087036 secs\n",
            "\n",
            "Epoch 141 Batch 0 Loss 1.8857 Accuracy 0.6304\n",
            "Epoch 141 Batch 50 Loss 1.8298 Accuracy 0.6115\n",
            "Epoch 141 Batch 100 Loss 1.8423 Accuracy 0.6101\n",
            "Epoch 141 Batch 150 Loss 1.8419 Accuracy 0.6106\n",
            "Epoch 141 Batch 200 Loss 1.8508 Accuracy 0.6106\n",
            "Epoch 141 Batch 250 Loss 1.8504 Accuracy 0.6115\n",
            "Epoch 141 Batch 300 Loss 1.8517 Accuracy 0.6113\n",
            "Epoch 141 Batch 350 Loss 1.8488 Accuracy 0.6113\n",
            "Epoch 141 Batch 400 Loss 1.8505 Accuracy 0.6116\n",
            "discarded batch 448\n",
            "Epoch 141 Batch 450 Loss 1.8503 Accuracy 0.6116\n",
            "Epoch 141 Batch 500 Loss 1.8527 Accuracy 0.6114\n",
            "Epoch 141 Batch 550 Loss 1.8558 Accuracy 0.6114\n",
            "Epoch 141 Batch 600 Loss 1.8577 Accuracy 0.6112\n",
            "Epoch 141 Batch 650 Loss 1.8597 Accuracy 0.6111\n",
            "Epoch 141 Batch 700 Loss 1.8592 Accuracy 0.6109\n",
            "Epoch 141 Batch 750 Loss 1.8603 Accuracy 0.6108\n",
            "Epoch 141 Batch 800 Loss 1.8633 Accuracy 0.6105\n",
            "Epoch 141 Batch 850 Loss 1.8647 Accuracy 0.6106\n",
            "Epoch 141 Batch 900 Loss 1.8650 Accuracy 0.6105\n",
            "Epoch 141 Batch 950 Loss 1.8662 Accuracy 0.6104\n",
            "Epoch 141 Loss 1.8664 Accuracy 0.6104\n",
            "Time taken for 1 epoch: 23.90222477912903 secs\n",
            "\n",
            "Epoch 141 VALIDATION: Loss 2.2191 Accuracy 0.5890\n",
            "\n",
            "Epoch 142 Batch 0 Loss 1.7759 Accuracy 0.6106\n",
            "Epoch 142 Batch 50 Loss 1.8168 Accuracy 0.6165\n",
            "Epoch 142 Batch 100 Loss 1.8210 Accuracy 0.6157\n",
            "Epoch 142 Batch 150 Loss 1.8306 Accuracy 0.6143\n",
            "Epoch 142 Batch 200 Loss 1.8415 Accuracy 0.6122\n",
            "discarded batch 242\n",
            "Epoch 142 Batch 250 Loss 1.8448 Accuracy 0.6120\n",
            "Epoch 142 Batch 300 Loss 1.8497 Accuracy 0.6113\n",
            "Epoch 142 Batch 350 Loss 1.8506 Accuracy 0.6115\n",
            "Epoch 142 Batch 400 Loss 1.8543 Accuracy 0.6113\n",
            "Epoch 142 Batch 450 Loss 1.8554 Accuracy 0.6113\n",
            "Epoch 142 Batch 500 Loss 1.8585 Accuracy 0.6111\n",
            "Epoch 142 Batch 550 Loss 1.8562 Accuracy 0.6114\n",
            "Epoch 142 Batch 600 Loss 1.8583 Accuracy 0.6112\n",
            "Epoch 142 Batch 650 Loss 1.8598 Accuracy 0.6107\n",
            "Epoch 142 Batch 700 Loss 1.8615 Accuracy 0.6107\n",
            "Epoch 142 Batch 750 Loss 1.8607 Accuracy 0.6108\n",
            "Epoch 142 Batch 800 Loss 1.8609 Accuracy 0.6108\n",
            "Epoch 142 Batch 850 Loss 1.8611 Accuracy 0.6108\n",
            "Epoch 142 Batch 900 Loss 1.8614 Accuracy 0.6107\n",
            "Epoch 142 Batch 950 Loss 1.8633 Accuracy 0.6107\n",
            "Epoch 142 Loss 1.8631 Accuracy 0.6107\n",
            "Time taken for 1 epoch: 23.858657121658325 secs\n",
            "\n",
            "Epoch 143 Batch 0 Loss 1.8055 Accuracy 0.6271\n",
            "Epoch 143 Batch 50 Loss 1.8186 Accuracy 0.6071\n",
            "Epoch 143 Batch 100 Loss 1.8369 Accuracy 0.6101\n",
            "Epoch 143 Batch 150 Loss 1.8386 Accuracy 0.6105\n",
            "Epoch 143 Batch 200 Loss 1.8347 Accuracy 0.6125\n",
            "Epoch 143 Batch 250 Loss 1.8352 Accuracy 0.6128\n",
            "discarded batch 273\n",
            "Epoch 143 Batch 300 Loss 1.8375 Accuracy 0.6127\n",
            "Epoch 143 Batch 350 Loss 1.8436 Accuracy 0.6124\n",
            "Epoch 143 Batch 400 Loss 1.8458 Accuracy 0.6119\n",
            "Epoch 143 Batch 450 Loss 1.8460 Accuracy 0.6118\n",
            "Epoch 143 Batch 500 Loss 1.8510 Accuracy 0.6110\n",
            "Epoch 143 Batch 550 Loss 1.8535 Accuracy 0.6107\n",
            "Epoch 143 Batch 600 Loss 1.8531 Accuracy 0.6108\n",
            "Epoch 143 Batch 650 Loss 1.8550 Accuracy 0.6106\n",
            "Epoch 143 Batch 700 Loss 1.8559 Accuracy 0.6105\n",
            "Epoch 143 Batch 750 Loss 1.8577 Accuracy 0.6107\n",
            "Epoch 143 Batch 800 Loss 1.8592 Accuracy 0.6107\n",
            "Epoch 143 Batch 850 Loss 1.8599 Accuracy 0.6107\n",
            "Epoch 143 Batch 900 Loss 1.8603 Accuracy 0.6107\n",
            "Epoch 143 Batch 950 Loss 1.8612 Accuracy 0.6106\n",
            "Epoch 143 Loss 1.8610 Accuracy 0.6107\n",
            "Time taken for 1 epoch: 23.87811017036438 secs\n",
            "\n",
            "Epoch 144 Batch 0 Loss 2.0799 Accuracy 0.6073\n",
            "Epoch 144 Batch 50 Loss 1.8409 Accuracy 0.6161\n",
            "Epoch 144 Batch 100 Loss 1.8294 Accuracy 0.6148\n",
            "Epoch 144 Batch 150 Loss 1.8273 Accuracy 0.6145\n",
            "Epoch 144 Batch 200 Loss 1.8393 Accuracy 0.6128\n",
            "discarded batch 223\n",
            "Epoch 144 Batch 250 Loss 1.8395 Accuracy 0.6134\n",
            "Epoch 144 Batch 300 Loss 1.8380 Accuracy 0.6134\n",
            "Epoch 144 Batch 350 Loss 1.8427 Accuracy 0.6131\n",
            "Epoch 144 Batch 400 Loss 1.8454 Accuracy 0.6127\n",
            "Epoch 144 Batch 450 Loss 1.8465 Accuracy 0.6123\n",
            "Epoch 144 Batch 500 Loss 1.8476 Accuracy 0.6121\n",
            "Epoch 144 Batch 550 Loss 1.8496 Accuracy 0.6116\n",
            "Epoch 144 Batch 600 Loss 1.8516 Accuracy 0.6116\n",
            "Epoch 144 Batch 650 Loss 1.8507 Accuracy 0.6116\n",
            "Epoch 144 Batch 700 Loss 1.8519 Accuracy 0.6113\n",
            "Epoch 144 Batch 750 Loss 1.8522 Accuracy 0.6115\n",
            "Epoch 144 Batch 800 Loss 1.8553 Accuracy 0.6111\n",
            "Epoch 144 Batch 850 Loss 1.8564 Accuracy 0.6111\n",
            "Epoch 144 Batch 900 Loss 1.8574 Accuracy 0.6111\n",
            "Epoch 144 Batch 950 Loss 1.8593 Accuracy 0.6111\n",
            "Epoch 144 Loss 1.8592 Accuracy 0.6111\n",
            "Time taken for 1 epoch: 23.91197896003723 secs\n",
            "\n",
            "Epoch 145 Batch 0 Loss 2.0502 Accuracy 0.6238\n",
            "Epoch 145 Batch 50 Loss 1.8271 Accuracy 0.6122\n",
            "discarded batch 98\n",
            "Epoch 145 Batch 100 Loss 1.8306 Accuracy 0.6130\n",
            "Epoch 145 Batch 150 Loss 1.8361 Accuracy 0.6130\n",
            "Epoch 145 Batch 200 Loss 1.8374 Accuracy 0.6134\n",
            "Epoch 145 Batch 250 Loss 1.8412 Accuracy 0.6130\n",
            "Epoch 145 Batch 300 Loss 1.8403 Accuracy 0.6136\n",
            "Epoch 145 Batch 350 Loss 1.8388 Accuracy 0.6140\n",
            "Epoch 145 Batch 400 Loss 1.8422 Accuracy 0.6139\n",
            "Epoch 145 Batch 450 Loss 1.8442 Accuracy 0.6137\n",
            "Epoch 145 Batch 500 Loss 1.8495 Accuracy 0.6131\n",
            "Epoch 145 Batch 550 Loss 1.8477 Accuracy 0.6134\n",
            "Epoch 145 Batch 600 Loss 1.8487 Accuracy 0.6133\n",
            "Epoch 145 Batch 650 Loss 1.8501 Accuracy 0.6129\n",
            "Epoch 145 Batch 700 Loss 1.8500 Accuracy 0.6127\n",
            "Epoch 145 Batch 750 Loss 1.8517 Accuracy 0.6126\n",
            "Epoch 145 Batch 800 Loss 1.8506 Accuracy 0.6125\n",
            "Epoch 145 Batch 850 Loss 1.8518 Accuracy 0.6125\n",
            "Epoch 145 Batch 900 Loss 1.8549 Accuracy 0.6121\n",
            "Epoch 145 Batch 950 Loss 1.8554 Accuracy 0.6120\n",
            "Saving checkpoint for epoch 145 at ./checkpoints/train/ckpt-29\n",
            "Epoch 145 Loss 1.8557 Accuracy 0.6120\n",
            "Time taken for 1 epoch: 24.678513765335083 secs\n",
            "\n",
            "Epoch 146 Batch 0 Loss 1.7445 Accuracy 0.6469\n",
            "Epoch 146 Batch 50 Loss 1.8266 Accuracy 0.6147\n",
            "Epoch 146 Batch 100 Loss 1.8268 Accuracy 0.6145\n",
            "Epoch 146 Batch 150 Loss 1.8253 Accuracy 0.6146\n",
            "Epoch 146 Batch 200 Loss 1.8317 Accuracy 0.6143\n",
            "Epoch 146 Batch 250 Loss 1.8337 Accuracy 0.6137\n",
            "Epoch 146 Batch 300 Loss 1.8376 Accuracy 0.6137\n",
            "discarded batch 328\n",
            "Epoch 146 Batch 350 Loss 1.8394 Accuracy 0.6135\n",
            "Epoch 146 Batch 400 Loss 1.8405 Accuracy 0.6133\n",
            "Epoch 146 Batch 450 Loss 1.8417 Accuracy 0.6131\n",
            "Epoch 146 Batch 500 Loss 1.8426 Accuracy 0.6131\n",
            "Epoch 146 Batch 550 Loss 1.8412 Accuracy 0.6134\n",
            "Epoch 146 Batch 600 Loss 1.8411 Accuracy 0.6134\n",
            "Epoch 146 Batch 650 Loss 1.8438 Accuracy 0.6132\n",
            "Epoch 146 Batch 700 Loss 1.8454 Accuracy 0.6127\n",
            "Epoch 146 Batch 750 Loss 1.8481 Accuracy 0.6121\n",
            "Epoch 146 Batch 800 Loss 1.8500 Accuracy 0.6118\n",
            "Epoch 146 Batch 850 Loss 1.8534 Accuracy 0.6116\n",
            "Epoch 146 Batch 900 Loss 1.8541 Accuracy 0.6116\n",
            "Epoch 146 Batch 950 Loss 1.8542 Accuracy 0.6118\n",
            "Epoch 146 Loss 1.8543 Accuracy 0.6118\n",
            "Time taken for 1 epoch: 23.99699091911316 secs\n",
            "\n",
            "Epoch 146 VALIDATION: Loss 2.2405 Accuracy 0.5860\n",
            "\n",
            "Epoch 147 Batch 0 Loss 1.7104 Accuracy 0.6436\n",
            "Epoch 147 Batch 50 Loss 1.8023 Accuracy 0.6182\n",
            "Epoch 147 Batch 100 Loss 1.8203 Accuracy 0.6172\n",
            "Epoch 147 Batch 150 Loss 1.8248 Accuracy 0.6167\n",
            "Epoch 147 Batch 200 Loss 1.8322 Accuracy 0.6153\n",
            "Epoch 147 Batch 250 Loss 1.8363 Accuracy 0.6148\n",
            "Epoch 147 Batch 300 Loss 1.8352 Accuracy 0.6147\n",
            "Epoch 147 Batch 350 Loss 1.8360 Accuracy 0.6146\n",
            "Epoch 147 Batch 400 Loss 1.8388 Accuracy 0.6146\n",
            "discarded batch 406\n",
            "Epoch 147 Batch 450 Loss 1.8402 Accuracy 0.6142\n",
            "Epoch 147 Batch 500 Loss 1.8416 Accuracy 0.6142\n",
            "Epoch 147 Batch 550 Loss 1.8420 Accuracy 0.6142\n",
            "Epoch 147 Batch 600 Loss 1.8439 Accuracy 0.6139\n",
            "Epoch 147 Batch 650 Loss 1.8469 Accuracy 0.6138\n",
            "Epoch 147 Batch 700 Loss 1.8475 Accuracy 0.6139\n",
            "Epoch 147 Batch 750 Loss 1.8477 Accuracy 0.6138\n",
            "Epoch 147 Batch 800 Loss 1.8474 Accuracy 0.6137\n",
            "Epoch 147 Batch 850 Loss 1.8496 Accuracy 0.6133\n",
            "Epoch 147 Batch 900 Loss 1.8495 Accuracy 0.6130\n",
            "Epoch 147 Batch 950 Loss 1.8508 Accuracy 0.6130\n",
            "Epoch 147 Loss 1.8511 Accuracy 0.6130\n",
            "Time taken for 1 epoch: 24.16597867012024 secs\n",
            "\n",
            "Epoch 148 Batch 0 Loss 1.8123 Accuracy 0.6106\n",
            "Epoch 148 Batch 50 Loss 1.8212 Accuracy 0.6173\n",
            "Epoch 148 Batch 100 Loss 1.8249 Accuracy 0.6153\n",
            "Epoch 148 Batch 150 Loss 1.8343 Accuracy 0.6146\n",
            "Epoch 148 Batch 200 Loss 1.8390 Accuracy 0.6130\n",
            "Epoch 148 Batch 250 Loss 1.8418 Accuracy 0.6133\n",
            "Epoch 148 Batch 300 Loss 1.8394 Accuracy 0.6135\n",
            "Epoch 148 Batch 350 Loss 1.8402 Accuracy 0.6136\n",
            "Epoch 148 Batch 400 Loss 1.8449 Accuracy 0.6130\n",
            "Epoch 148 Batch 450 Loss 1.8463 Accuracy 0.6130\n",
            "Epoch 148 Batch 500 Loss 1.8480 Accuracy 0.6128\n",
            "Epoch 148 Batch 550 Loss 1.8471 Accuracy 0.6128\n",
            "Epoch 148 Batch 600 Loss 1.8484 Accuracy 0.6125\n",
            "Epoch 148 Batch 650 Loss 1.8505 Accuracy 0.6124\n",
            "Epoch 148 Batch 700 Loss 1.8505 Accuracy 0.6125\n",
            "Epoch 148 Batch 750 Loss 1.8485 Accuracy 0.6129\n",
            "Epoch 148 Batch 800 Loss 1.8485 Accuracy 0.6130\n",
            "Epoch 148 Batch 850 Loss 1.8495 Accuracy 0.6128\n",
            "discarded batch 854\n",
            "Epoch 148 Batch 900 Loss 1.8496 Accuracy 0.6127\n",
            "Epoch 148 Batch 950 Loss 1.8498 Accuracy 0.6127\n",
            "Epoch 148 Loss 1.8496 Accuracy 0.6129\n",
            "Time taken for 1 epoch: 24.431415796279907 secs\n",
            "\n",
            "Epoch 149 Batch 0 Loss 1.6888 Accuracy 0.6469\n",
            "Epoch 149 Batch 50 Loss 1.8459 Accuracy 0.6130\n",
            "Epoch 149 Batch 100 Loss 1.8321 Accuracy 0.6142\n",
            "Epoch 149 Batch 150 Loss 1.8288 Accuracy 0.6137\n",
            "discarded batch 177\n",
            "Epoch 149 Batch 200 Loss 1.8287 Accuracy 0.6154\n",
            "Epoch 149 Batch 250 Loss 1.8295 Accuracy 0.6152\n",
            "Epoch 149 Batch 300 Loss 1.8291 Accuracy 0.6142\n",
            "Epoch 149 Batch 350 Loss 1.8290 Accuracy 0.6141\n",
            "Epoch 149 Batch 400 Loss 1.8320 Accuracy 0.6136\n",
            "Epoch 149 Batch 450 Loss 1.8343 Accuracy 0.6133\n",
            "Epoch 149 Batch 500 Loss 1.8369 Accuracy 0.6130\n",
            "Epoch 149 Batch 550 Loss 1.8392 Accuracy 0.6124\n",
            "Epoch 149 Batch 600 Loss 1.8427 Accuracy 0.6121\n",
            "Epoch 149 Batch 650 Loss 1.8431 Accuracy 0.6122\n",
            "Epoch 149 Batch 700 Loss 1.8434 Accuracy 0.6124\n",
            "Epoch 149 Batch 750 Loss 1.8434 Accuracy 0.6125\n",
            "Epoch 149 Batch 800 Loss 1.8452 Accuracy 0.6124\n",
            "Epoch 149 Batch 850 Loss 1.8461 Accuracy 0.6125\n",
            "Epoch 149 Batch 900 Loss 1.8468 Accuracy 0.6126\n",
            "Epoch 149 Batch 950 Loss 1.8470 Accuracy 0.6127\n",
            "Epoch 149 Loss 1.8475 Accuracy 0.6126\n",
            "Time taken for 1 epoch: 24.307880640029907 secs\n",
            "\n",
            "Epoch 150 Batch 0 Loss 1.8000 Accuracy 0.6172\n",
            "Epoch 150 Batch 50 Loss 1.7885 Accuracy 0.6185\n",
            "Epoch 150 Batch 100 Loss 1.8067 Accuracy 0.6155\n",
            "Epoch 150 Batch 150 Loss 1.8150 Accuracy 0.6153\n",
            "Epoch 150 Batch 200 Loss 1.8212 Accuracy 0.6148\n",
            "Epoch 150 Batch 250 Loss 1.8268 Accuracy 0.6147\n",
            "Epoch 150 Batch 300 Loss 1.8312 Accuracy 0.6147\n",
            "Epoch 150 Batch 350 Loss 1.8319 Accuracy 0.6145\n",
            "Epoch 150 Batch 400 Loss 1.8326 Accuracy 0.6145\n",
            "Epoch 150 Batch 450 Loss 1.8338 Accuracy 0.6141\n",
            "Epoch 150 Batch 500 Loss 1.8334 Accuracy 0.6139\n",
            "Epoch 150 Batch 550 Loss 1.8344 Accuracy 0.6140\n",
            "Epoch 150 Batch 600 Loss 1.8334 Accuracy 0.6142\n",
            "Epoch 150 Batch 650 Loss 1.8362 Accuracy 0.6136\n",
            "Epoch 150 Batch 700 Loss 1.8354 Accuracy 0.6138\n",
            "Epoch 150 Batch 750 Loss 1.8349 Accuracy 0.6142\n",
            "Epoch 150 Batch 800 Loss 1.8371 Accuracy 0.6139\n",
            "Epoch 150 Batch 850 Loss 1.8399 Accuracy 0.6136\n",
            "Epoch 150 Batch 900 Loss 1.8420 Accuracy 0.6134\n",
            "discarded batch 949\n",
            "Epoch 150 Batch 950 Loss 1.8429 Accuracy 0.6133\n",
            "Saving checkpoint for epoch 150 at ./checkpoints/train/ckpt-30\n",
            "Epoch 150 Loss 1.8432 Accuracy 0.6132\n",
            "Time taken for 1 epoch: 24.698548793792725 secs\n",
            "\n",
            "Epoch 151 Batch 0 Loss 2.0160 Accuracy 0.5974\n",
            "Epoch 151 Batch 50 Loss 1.7985 Accuracy 0.6170\n",
            "Epoch 151 Batch 100 Loss 1.8123 Accuracy 0.6138\n",
            "Epoch 151 Batch 150 Loss 1.8129 Accuracy 0.6149\n",
            "Epoch 151 Batch 200 Loss 1.8176 Accuracy 0.6145\n",
            "Epoch 151 Batch 250 Loss 1.8170 Accuracy 0.6148\n",
            "Epoch 151 Batch 300 Loss 1.8169 Accuracy 0.6148\n",
            "Epoch 151 Batch 350 Loss 1.8278 Accuracy 0.6135\n",
            "Epoch 151 Batch 400 Loss 1.8290 Accuracy 0.6136\n",
            "Epoch 151 Batch 450 Loss 1.8297 Accuracy 0.6139\n",
            "Epoch 151 Batch 500 Loss 1.8287 Accuracy 0.6141\n",
            "Epoch 151 Batch 550 Loss 1.8314 Accuracy 0.6138\n",
            "Epoch 151 Batch 600 Loss 1.8342 Accuracy 0.6137\n",
            "Epoch 151 Batch 650 Loss 1.8345 Accuracy 0.6140\n",
            "Epoch 151 Batch 700 Loss 1.8354 Accuracy 0.6138\n",
            "Epoch 151 Batch 750 Loss 1.8379 Accuracy 0.6137\n",
            "discarded batch 764\n",
            "Epoch 151 Batch 800 Loss 1.8398 Accuracy 0.6134\n",
            "Epoch 151 Batch 850 Loss 1.8420 Accuracy 0.6132\n",
            "Epoch 151 Batch 900 Loss 1.8417 Accuracy 0.6133\n",
            "Epoch 151 Batch 950 Loss 1.8418 Accuracy 0.6133\n",
            "Epoch 151 Loss 1.8416 Accuracy 0.6133\n",
            "Time taken for 1 epoch: 24.621986627578735 secs\n",
            "\n",
            "Epoch 151 VALIDATION: Loss 2.2244 Accuracy 0.5884\n",
            "\n",
            "Epoch 152 Batch 0 Loss 1.8146 Accuracy 0.5875\n",
            "Epoch 152 Batch 50 Loss 1.7864 Accuracy 0.6189\n",
            "Epoch 152 Batch 100 Loss 1.7995 Accuracy 0.6189\n",
            "Epoch 152 Batch 150 Loss 1.8070 Accuracy 0.6167\n",
            "Epoch 152 Batch 200 Loss 1.8213 Accuracy 0.6153\n",
            "Epoch 152 Batch 250 Loss 1.8240 Accuracy 0.6148\n",
            "Epoch 152 Batch 300 Loss 1.8232 Accuracy 0.6148\n",
            "Epoch 152 Batch 350 Loss 1.8245 Accuracy 0.6151\n",
            "Epoch 152 Batch 400 Loss 1.8234 Accuracy 0.6149\n",
            "Epoch 152 Batch 450 Loss 1.8253 Accuracy 0.6148\n",
            "Epoch 152 Batch 500 Loss 1.8290 Accuracy 0.6141\n",
            "Epoch 152 Batch 550 Loss 1.8311 Accuracy 0.6140\n",
            "Epoch 152 Batch 600 Loss 1.8311 Accuracy 0.6139\n",
            "Epoch 152 Batch 650 Loss 1.8310 Accuracy 0.6139\n",
            "Epoch 152 Batch 700 Loss 1.8338 Accuracy 0.6138\n",
            "Epoch 152 Batch 750 Loss 1.8372 Accuracy 0.6134\n",
            "Epoch 152 Batch 800 Loss 1.8398 Accuracy 0.6130\n",
            "discarded batch 821\n",
            "Epoch 152 Batch 850 Loss 1.8401 Accuracy 0.6132\n",
            "Epoch 152 Batch 900 Loss 1.8401 Accuracy 0.6132\n",
            "Epoch 152 Batch 950 Loss 1.8400 Accuracy 0.6133\n",
            "Epoch 152 Loss 1.8402 Accuracy 0.6132\n",
            "Time taken for 1 epoch: 24.803361177444458 secs\n",
            "\n",
            "Epoch 153 Batch 0 Loss 1.7840 Accuracy 0.6337\n",
            "Epoch 153 Batch 50 Loss 1.8196 Accuracy 0.6185\n",
            "Epoch 153 Batch 100 Loss 1.8259 Accuracy 0.6167\n",
            "discarded batch 142\n",
            "Epoch 153 Batch 150 Loss 1.8275 Accuracy 0.6166\n",
            "Epoch 153 Batch 200 Loss 1.8197 Accuracy 0.6179\n",
            "Epoch 153 Batch 250 Loss 1.8165 Accuracy 0.6177\n",
            "Epoch 153 Batch 300 Loss 1.8219 Accuracy 0.6170\n",
            "Epoch 153 Batch 350 Loss 1.8207 Accuracy 0.6167\n",
            "Epoch 153 Batch 400 Loss 1.8237 Accuracy 0.6164\n",
            "Epoch 153 Batch 450 Loss 1.8250 Accuracy 0.6164\n",
            "Epoch 153 Batch 500 Loss 1.8253 Accuracy 0.6162\n",
            "Epoch 153 Batch 550 Loss 1.8259 Accuracy 0.6161\n",
            "Epoch 153 Batch 600 Loss 1.8290 Accuracy 0.6156\n",
            "Epoch 153 Batch 650 Loss 1.8290 Accuracy 0.6156\n",
            "Epoch 153 Batch 700 Loss 1.8298 Accuracy 0.6154\n",
            "Epoch 153 Batch 750 Loss 1.8319 Accuracy 0.6153\n",
            "Epoch 153 Batch 800 Loss 1.8339 Accuracy 0.6152\n",
            "Epoch 153 Batch 850 Loss 1.8337 Accuracy 0.6152\n",
            "Epoch 153 Batch 900 Loss 1.8365 Accuracy 0.6148\n",
            "Epoch 153 Batch 950 Loss 1.8371 Accuracy 0.6146\n",
            "Epoch 153 Loss 1.8372 Accuracy 0.6146\n",
            "Time taken for 1 epoch: 24.705608367919922 secs\n",
            "\n",
            "Epoch 154 Batch 0 Loss 1.7791 Accuracy 0.5908\n",
            "Epoch 154 Batch 50 Loss 1.8022 Accuracy 0.6148\n",
            "Epoch 154 Batch 100 Loss 1.8222 Accuracy 0.6135\n",
            "Epoch 154 Batch 150 Loss 1.8178 Accuracy 0.6144\n",
            "Epoch 154 Batch 200 Loss 1.8162 Accuracy 0.6151\n",
            "Epoch 154 Batch 250 Loss 1.8178 Accuracy 0.6153\n",
            "discarded batch 296\n",
            "Epoch 154 Batch 300 Loss 1.8193 Accuracy 0.6156\n",
            "Epoch 154 Batch 350 Loss 1.8226 Accuracy 0.6150\n",
            "Epoch 154 Batch 400 Loss 1.8234 Accuracy 0.6156\n",
            "Epoch 154 Batch 450 Loss 1.8255 Accuracy 0.6150\n",
            "Epoch 154 Batch 500 Loss 1.8233 Accuracy 0.6155\n",
            "Epoch 154 Batch 550 Loss 1.8246 Accuracy 0.6153\n",
            "Epoch 154 Batch 600 Loss 1.8240 Accuracy 0.6156\n",
            "Epoch 154 Batch 650 Loss 1.8250 Accuracy 0.6156\n",
            "Epoch 154 Batch 700 Loss 1.8274 Accuracy 0.6152\n",
            "Epoch 154 Batch 750 Loss 1.8281 Accuracy 0.6151\n",
            "Epoch 154 Batch 800 Loss 1.8306 Accuracy 0.6150\n",
            "Epoch 154 Batch 850 Loss 1.8316 Accuracy 0.6149\n",
            "Epoch 154 Batch 900 Loss 1.8336 Accuracy 0.6149\n",
            "Epoch 154 Batch 950 Loss 1.8338 Accuracy 0.6148\n",
            "Epoch 154 Loss 1.8341 Accuracy 0.6148\n",
            "Time taken for 1 epoch: 24.773455381393433 secs\n",
            "\n",
            "Epoch 155 Batch 0 Loss 1.7203 Accuracy 0.6238\n",
            "discarded batch 21\n",
            "Epoch 155 Batch 50 Loss 1.8191 Accuracy 0.6144\n",
            "Epoch 155 Batch 100 Loss 1.8027 Accuracy 0.6162\n",
            "Epoch 155 Batch 150 Loss 1.8177 Accuracy 0.6129\n",
            "Epoch 155 Batch 200 Loss 1.8197 Accuracy 0.6141\n",
            "Epoch 155 Batch 250 Loss 1.8187 Accuracy 0.6147\n",
            "Epoch 155 Batch 300 Loss 1.8202 Accuracy 0.6151\n",
            "Epoch 155 Batch 350 Loss 1.8220 Accuracy 0.6151\n",
            "Epoch 155 Batch 400 Loss 1.8209 Accuracy 0.6152\n",
            "Epoch 155 Batch 450 Loss 1.8211 Accuracy 0.6150\n",
            "Epoch 155 Batch 500 Loss 1.8221 Accuracy 0.6151\n",
            "Epoch 155 Batch 550 Loss 1.8242 Accuracy 0.6150\n",
            "Epoch 155 Batch 600 Loss 1.8273 Accuracy 0.6147\n",
            "Epoch 155 Batch 650 Loss 1.8278 Accuracy 0.6148\n",
            "Epoch 155 Batch 700 Loss 1.8279 Accuracy 0.6147\n",
            "Epoch 155 Batch 750 Loss 1.8289 Accuracy 0.6147\n",
            "Epoch 155 Batch 800 Loss 1.8292 Accuracy 0.6147\n",
            "Epoch 155 Batch 850 Loss 1.8300 Accuracy 0.6148\n",
            "Epoch 155 Batch 900 Loss 1.8317 Accuracy 0.6146\n",
            "Epoch 155 Batch 950 Loss 1.8319 Accuracy 0.6146\n",
            "Saving checkpoint for epoch 155 at ./checkpoints/train/ckpt-31\n",
            "Epoch 155 Loss 1.8317 Accuracy 0.6145\n",
            "Time taken for 1 epoch: 25.231878995895386 secs\n",
            "\n",
            "Epoch 156 Batch 0 Loss 1.7624 Accuracy 0.6238\n",
            "Epoch 156 Batch 50 Loss 1.8071 Accuracy 0.6171\n",
            "Epoch 156 Batch 100 Loss 1.7969 Accuracy 0.6185\n",
            "Epoch 156 Batch 150 Loss 1.7989 Accuracy 0.6179\n",
            "Epoch 156 Batch 200 Loss 1.8006 Accuracy 0.6172\n",
            "Epoch 156 Batch 250 Loss 1.8021 Accuracy 0.6173\n",
            "Epoch 156 Batch 300 Loss 1.8031 Accuracy 0.6176\n",
            "Epoch 156 Batch 350 Loss 1.8053 Accuracy 0.6171\n",
            "Epoch 156 Batch 400 Loss 1.8093 Accuracy 0.6168\n",
            "Epoch 156 Batch 450 Loss 1.8138 Accuracy 0.6165\n",
            "Epoch 156 Batch 500 Loss 1.8151 Accuracy 0.6164\n",
            "Epoch 156 Batch 550 Loss 1.8158 Accuracy 0.6165\n",
            "Epoch 156 Batch 600 Loss 1.8194 Accuracy 0.6159\n",
            "Epoch 156 Batch 650 Loss 1.8222 Accuracy 0.6158\n",
            "Epoch 156 Batch 700 Loss 1.8244 Accuracy 0.6154\n",
            "Epoch 156 Batch 750 Loss 1.8248 Accuracy 0.6154\n",
            "Epoch 156 Batch 800 Loss 1.8251 Accuracy 0.6154\n",
            "discarded batch 848\n",
            "Epoch 156 Batch 850 Loss 1.8277 Accuracy 0.6152\n",
            "Epoch 156 Batch 900 Loss 1.8275 Accuracy 0.6153\n",
            "Epoch 156 Batch 950 Loss 1.8293 Accuracy 0.6150\n",
            "Epoch 156 Loss 1.8292 Accuracy 0.6150\n",
            "Time taken for 1 epoch: 24.892746448516846 secs\n",
            "\n",
            "Epoch 156 VALIDATION: Loss 2.2381 Accuracy 0.5864\n",
            "\n",
            "Epoch 157 Batch 0 Loss 1.6843 Accuracy 0.6304\n",
            "Epoch 157 Batch 50 Loss 1.7536 Accuracy 0.6256\n",
            "Epoch 157 Batch 100 Loss 1.7937 Accuracy 0.6210\n",
            "discarded batch 141\n",
            "Epoch 157 Batch 150 Loss 1.7997 Accuracy 0.6192\n",
            "Epoch 157 Batch 200 Loss 1.8003 Accuracy 0.6184\n",
            "Epoch 157 Batch 250 Loss 1.8063 Accuracy 0.6174\n",
            "Epoch 157 Batch 300 Loss 1.8095 Accuracy 0.6169\n",
            "Epoch 157 Batch 350 Loss 1.8094 Accuracy 0.6171\n",
            "Epoch 157 Batch 400 Loss 1.8097 Accuracy 0.6173\n",
            "Epoch 157 Batch 450 Loss 1.8122 Accuracy 0.6171\n",
            "Epoch 157 Batch 500 Loss 1.8163 Accuracy 0.6169\n",
            "Epoch 157 Batch 550 Loss 1.8175 Accuracy 0.6168\n",
            "Epoch 157 Batch 600 Loss 1.8205 Accuracy 0.6166\n",
            "Epoch 157 Batch 650 Loss 1.8202 Accuracy 0.6168\n",
            "Epoch 157 Batch 700 Loss 1.8221 Accuracy 0.6167\n",
            "Epoch 157 Batch 750 Loss 1.8229 Accuracy 0.6165\n",
            "Epoch 157 Batch 800 Loss 1.8243 Accuracy 0.6163\n",
            "Epoch 157 Batch 850 Loss 1.8267 Accuracy 0.6159\n",
            "Epoch 157 Batch 900 Loss 1.8271 Accuracy 0.6157\n",
            "Epoch 157 Batch 950 Loss 1.8282 Accuracy 0.6156\n",
            "Epoch 157 Loss 1.8287 Accuracy 0.6156\n",
            "Time taken for 1 epoch: 25.349819660186768 secs\n",
            "\n",
            "Epoch 158 Batch 0 Loss 1.6487 Accuracy 0.6403\n",
            "Epoch 158 Batch 50 Loss 1.8008 Accuracy 0.6170\n",
            "discarded batch 95\n",
            "Epoch 158 Batch 100 Loss 1.7931 Accuracy 0.6186\n",
            "Epoch 158 Batch 150 Loss 1.8009 Accuracy 0.6171\n",
            "Epoch 158 Batch 200 Loss 1.8019 Accuracy 0.6169\n",
            "Epoch 158 Batch 250 Loss 1.8069 Accuracy 0.6173\n",
            "Epoch 158 Batch 300 Loss 1.8065 Accuracy 0.6174\n",
            "Epoch 158 Batch 350 Loss 1.8089 Accuracy 0.6174\n",
            "Epoch 158 Batch 400 Loss 1.8111 Accuracy 0.6169\n",
            "Epoch 158 Batch 450 Loss 1.8121 Accuracy 0.6169\n",
            "Epoch 158 Batch 500 Loss 1.8119 Accuracy 0.6167\n",
            "Epoch 158 Batch 550 Loss 1.8122 Accuracy 0.6164\n",
            "Epoch 158 Batch 600 Loss 1.8145 Accuracy 0.6163\n",
            "Epoch 158 Batch 650 Loss 1.8183 Accuracy 0.6158\n",
            "Epoch 158 Batch 700 Loss 1.8209 Accuracy 0.6155\n",
            "Epoch 158 Batch 750 Loss 1.8235 Accuracy 0.6151\n",
            "Epoch 158 Batch 800 Loss 1.8257 Accuracy 0.6147\n",
            "Epoch 158 Batch 850 Loss 1.8268 Accuracy 0.6146\n",
            "Epoch 158 Batch 900 Loss 1.8281 Accuracy 0.6144\n",
            "Epoch 158 Batch 950 Loss 1.8284 Accuracy 0.6146\n",
            "Epoch 158 Loss 1.8281 Accuracy 0.6147\n",
            "Time taken for 1 epoch: 25.411253452301025 secs\n",
            "\n",
            "Epoch 159 Batch 0 Loss 1.7413 Accuracy 0.6172\n",
            "Epoch 159 Batch 50 Loss 1.7890 Accuracy 0.6210\n",
            "Epoch 159 Batch 100 Loss 1.7879 Accuracy 0.6206\n",
            "Epoch 159 Batch 150 Loss 1.7970 Accuracy 0.6184\n",
            "Epoch 159 Batch 200 Loss 1.7982 Accuracy 0.6193\n",
            "Epoch 159 Batch 250 Loss 1.8038 Accuracy 0.6186\n",
            "Epoch 159 Batch 300 Loss 1.8077 Accuracy 0.6182\n",
            "Epoch 159 Batch 350 Loss 1.8105 Accuracy 0.6180\n",
            "Epoch 159 Batch 400 Loss 1.8099 Accuracy 0.6181\n",
            "Epoch 159 Batch 450 Loss 1.8111 Accuracy 0.6181\n",
            "discarded batch 463\n",
            "Epoch 159 Batch 500 Loss 1.8123 Accuracy 0.6177\n",
            "Epoch 159 Batch 550 Loss 1.8152 Accuracy 0.6171\n",
            "Epoch 159 Batch 600 Loss 1.8153 Accuracy 0.6172\n",
            "Epoch 159 Batch 650 Loss 1.8174 Accuracy 0.6167\n",
            "Epoch 159 Batch 700 Loss 1.8177 Accuracy 0.6167\n",
            "Epoch 159 Batch 750 Loss 1.8215 Accuracy 0.6163\n",
            "Epoch 159 Batch 800 Loss 1.8216 Accuracy 0.6162\n",
            "Epoch 159 Batch 850 Loss 1.8217 Accuracy 0.6162\n",
            "Epoch 159 Batch 900 Loss 1.8229 Accuracy 0.6161\n",
            "Epoch 159 Batch 950 Loss 1.8229 Accuracy 0.6161\n",
            "Epoch 159 Loss 1.8228 Accuracy 0.6161\n",
            "Time taken for 1 epoch: 24.99658751487732 secs\n",
            "\n",
            "Epoch 160 Batch 0 Loss 1.6168 Accuracy 0.6601\n",
            "Epoch 160 Batch 50 Loss 1.7816 Accuracy 0.6205\n",
            "Epoch 160 Batch 100 Loss 1.7863 Accuracy 0.6206\n",
            "Epoch 160 Batch 150 Loss 1.7946 Accuracy 0.6197\n",
            "Epoch 160 Batch 200 Loss 1.7985 Accuracy 0.6189\n",
            "Epoch 160 Batch 250 Loss 1.7955 Accuracy 0.6192\n",
            "Epoch 160 Batch 300 Loss 1.7990 Accuracy 0.6189\n",
            "Epoch 160 Batch 350 Loss 1.8029 Accuracy 0.6183\n",
            "Epoch 160 Batch 400 Loss 1.8019 Accuracy 0.6185\n",
            "Epoch 160 Batch 450 Loss 1.8055 Accuracy 0.6183\n",
            "Epoch 160 Batch 500 Loss 1.8077 Accuracy 0.6185\n",
            "Epoch 160 Batch 550 Loss 1.8104 Accuracy 0.6182\n",
            "Epoch 160 Batch 600 Loss 1.8126 Accuracy 0.6175\n",
            "Epoch 160 Batch 650 Loss 1.8131 Accuracy 0.6173\n",
            "Epoch 160 Batch 700 Loss 1.8146 Accuracy 0.6171\n",
            "Epoch 160 Batch 750 Loss 1.8150 Accuracy 0.6171\n",
            "Epoch 160 Batch 800 Loss 1.8149 Accuracy 0.6172\n",
            "Epoch 160 Batch 850 Loss 1.8179 Accuracy 0.6168\n",
            "Epoch 160 Batch 900 Loss 1.8192 Accuracy 0.6166\n",
            "discarded batch 932\n",
            "Epoch 160 Batch 950 Loss 1.8195 Accuracy 0.6165\n",
            "Saving checkpoint for epoch 160 at ./checkpoints/train/ckpt-32\n",
            "Epoch 160 Loss 1.8194 Accuracy 0.6166\n",
            "Time taken for 1 epoch: 25.454014539718628 secs\n",
            "\n",
            "Epoch 161 Batch 0 Loss 1.7427 Accuracy 0.6337\n",
            "Epoch 161 Batch 50 Loss 1.7972 Accuracy 0.6201\n",
            "Epoch 161 Batch 100 Loss 1.7860 Accuracy 0.6203\n",
            "Epoch 161 Batch 150 Loss 1.7891 Accuracy 0.6192\n",
            "Epoch 161 Batch 200 Loss 1.7958 Accuracy 0.6179\n",
            "Epoch 161 Batch 250 Loss 1.7982 Accuracy 0.6176\n",
            "Epoch 161 Batch 300 Loss 1.7964 Accuracy 0.6176\n",
            "Epoch 161 Batch 350 Loss 1.8006 Accuracy 0.6176\n",
            "Epoch 161 Batch 400 Loss 1.8056 Accuracy 0.6167\n",
            "Epoch 161 Batch 450 Loss 1.8084 Accuracy 0.6164\n",
            "Epoch 161 Batch 500 Loss 1.8106 Accuracy 0.6166\n",
            "Epoch 161 Batch 550 Loss 1.8105 Accuracy 0.6166\n",
            "Epoch 161 Batch 600 Loss 1.8136 Accuracy 0.6162\n",
            "Epoch 161 Batch 650 Loss 1.8141 Accuracy 0.6163\n",
            "Epoch 161 Batch 700 Loss 1.8150 Accuracy 0.6163\n",
            "Epoch 161 Batch 750 Loss 1.8169 Accuracy 0.6162\n",
            "Epoch 161 Batch 800 Loss 1.8184 Accuracy 0.6160\n",
            "Epoch 161 Batch 850 Loss 1.8182 Accuracy 0.6164\n",
            "discarded batch 895\n",
            "Epoch 161 Batch 900 Loss 1.8183 Accuracy 0.6164\n",
            "Epoch 161 Batch 950 Loss 1.8187 Accuracy 0.6164\n",
            "Epoch 161 Loss 1.8187 Accuracy 0.6164\n",
            "Time taken for 1 epoch: 24.428234338760376 secs\n",
            "\n",
            "Epoch 161 VALIDATION: Loss 2.2443 Accuracy 0.5857\n",
            "\n",
            "Epoch 162 Batch 0 Loss 1.8876 Accuracy 0.6073\n",
            "Epoch 162 Batch 50 Loss 1.7592 Accuracy 0.6251\n",
            "Epoch 162 Batch 100 Loss 1.7869 Accuracy 0.6211\n",
            "Epoch 162 Batch 150 Loss 1.8016 Accuracy 0.6185\n",
            "Epoch 162 Batch 200 Loss 1.8043 Accuracy 0.6181\n",
            "discarded batch 208\n",
            "Epoch 162 Batch 250 Loss 1.8017 Accuracy 0.6187\n",
            "Epoch 162 Batch 300 Loss 1.8056 Accuracy 0.6177\n",
            "Epoch 162 Batch 350 Loss 1.8045 Accuracy 0.6172\n",
            "Epoch 162 Batch 400 Loss 1.8070 Accuracy 0.6170\n",
            "Epoch 162 Batch 450 Loss 1.8085 Accuracy 0.6175\n",
            "Epoch 162 Batch 500 Loss 1.8096 Accuracy 0.6170\n",
            "Epoch 162 Batch 550 Loss 1.8087 Accuracy 0.6170\n",
            "Epoch 162 Batch 600 Loss 1.8097 Accuracy 0.6165\n",
            "Epoch 162 Batch 650 Loss 1.8094 Accuracy 0.6165\n",
            "Epoch 162 Batch 700 Loss 1.8106 Accuracy 0.6167\n",
            "Epoch 162 Batch 750 Loss 1.8121 Accuracy 0.6164\n",
            "Epoch 162 Batch 800 Loss 1.8127 Accuracy 0.6166\n",
            "Epoch 162 Batch 850 Loss 1.8143 Accuracy 0.6167\n",
            "Epoch 162 Batch 900 Loss 1.8144 Accuracy 0.6169\n",
            "Epoch 162 Batch 950 Loss 1.8162 Accuracy 0.6168\n",
            "Epoch 162 Loss 1.8159 Accuracy 0.6169\n",
            "Time taken for 1 epoch: 24.431297302246094 secs\n",
            "\n",
            "Epoch 163 Batch 0 Loss 1.7008 Accuracy 0.6436\n",
            "Epoch 163 Batch 50 Loss 1.8055 Accuracy 0.6151\n",
            "Epoch 163 Batch 100 Loss 1.7822 Accuracy 0.6202\n",
            "Epoch 163 Batch 150 Loss 1.7799 Accuracy 0.6211\n",
            "Epoch 163 Batch 200 Loss 1.7821 Accuracy 0.6206\n",
            "Epoch 163 Batch 250 Loss 1.7866 Accuracy 0.6193\n",
            "discarded batch 261\n",
            "Epoch 163 Batch 300 Loss 1.7906 Accuracy 0.6193\n",
            "Epoch 163 Batch 350 Loss 1.7948 Accuracy 0.6189\n",
            "Epoch 163 Batch 400 Loss 1.7942 Accuracy 0.6189\n",
            "Epoch 163 Batch 450 Loss 1.7951 Accuracy 0.6186\n",
            "Epoch 163 Batch 500 Loss 1.7986 Accuracy 0.6183\n",
            "Epoch 163 Batch 550 Loss 1.8005 Accuracy 0.6180\n",
            "Epoch 163 Batch 600 Loss 1.8021 Accuracy 0.6179\n",
            "Epoch 163 Batch 650 Loss 1.8059 Accuracy 0.6178\n",
            "Epoch 163 Batch 700 Loss 1.8079 Accuracy 0.6175\n",
            "Epoch 163 Batch 750 Loss 1.8094 Accuracy 0.6174\n",
            "Epoch 163 Batch 800 Loss 1.8112 Accuracy 0.6174\n",
            "Epoch 163 Batch 850 Loss 1.8116 Accuracy 0.6174\n",
            "Epoch 163 Batch 900 Loss 1.8129 Accuracy 0.6173\n",
            "Epoch 163 Batch 950 Loss 1.8133 Accuracy 0.6174\n",
            "Epoch 163 Loss 1.8137 Accuracy 0.6173\n",
            "Time taken for 1 epoch: 24.578417778015137 secs\n",
            "\n",
            "Epoch 164 Batch 0 Loss 1.7264 Accuracy 0.6205\n",
            "Epoch 164 Batch 50 Loss 1.7922 Accuracy 0.6172\n",
            "Epoch 164 Batch 100 Loss 1.7825 Accuracy 0.6194\n",
            "Epoch 164 Batch 150 Loss 1.7854 Accuracy 0.6185\n",
            "discarded batch 171\n",
            "Epoch 164 Batch 200 Loss 1.7956 Accuracy 0.6183\n",
            "Epoch 164 Batch 250 Loss 1.7974 Accuracy 0.6187\n",
            "Epoch 164 Batch 300 Loss 1.7987 Accuracy 0.6178\n",
            "Epoch 164 Batch 350 Loss 1.7982 Accuracy 0.6180\n",
            "Epoch 164 Batch 400 Loss 1.7993 Accuracy 0.6180\n",
            "Epoch 164 Batch 450 Loss 1.7966 Accuracy 0.6186\n",
            "Epoch 164 Batch 500 Loss 1.7997 Accuracy 0.6184\n",
            "Epoch 164 Batch 550 Loss 1.8010 Accuracy 0.6182\n",
            "Epoch 164 Batch 600 Loss 1.8031 Accuracy 0.6177\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjLbfLOU_pRw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#transformer.save_weights(\"./optimus_rhyme_wide100\")\n",
        "#transformer.load_weights(\"./optimus_rhyme_200\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEun8YPR8Sn6",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Generation\n",
        "\n",
        "def evaluate(inp_sentence, decoder_input):\n",
        "    inp_sentence = inp_sentence\n",
        "    encoder_input = tf.expand_dims(inp_sentence, 0)\n",
        "    \n",
        "    output = tf.expand_dims(decoder_input, 0)\n",
        "\n",
        "    terces = 0\n",
        "    for i in range(batch_len):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "            encoder_input, output)\n",
        "    \n",
        "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "        predictions, attention_weights = transformer(encoder_input, \n",
        "                                                    output,\n",
        "                                                    False,\n",
        "                                                    enc_padding_mask,\n",
        "                                                    combined_mask,\n",
        "                                                    dec_padding_mask)\n",
        "        \n",
        "        # select the last word from the seq_len dimension\n",
        "        predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "        # return the result if the predicted_id is equal to the end token\n",
        "        if predicted_id == eos:\n",
        "            terces += 1\n",
        "            if terces == 3:\n",
        "                return tf.squeeze(output, axis=0), attention_weights\n",
        "        # concatentate the predicted_id to the output which is given to the decoder\n",
        "        # as its input.\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "    return tf.squeeze(output, axis=0), attention_weights\n",
        "\n",
        "out_list = test_b[1][0]\n",
        "print(seq2str(out_list))\n",
        "txt_gen = seq2str(out_list)+\"\\n--------------------\\n\"\n",
        "offset = 75 # a tercet\n",
        "print(\"---------------------------\")\n",
        "for i in range(10): # 30 terces = cantica\n",
        "    out, att_w = evaluate([pad], out_list[-offset:])\n",
        "    out_list = out.numpy().tolist()\n",
        "    out_str = seq2str(out_list[offset:])\n",
        "    txt_gen += out_str + \"\\n\"\n",
        "    print(out_str) \n",
        "\n",
        "wandb.log({\"generated\": wandb.Table(data=txt_gen.split('\\n'), columns=[\"Text\"])})"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}