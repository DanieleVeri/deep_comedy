{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOOLYN5cMbIGDLkORlntqIR"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHRo2NsK-Rgd",
        "colab_type": "code",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "2fd9c9bb-f2ff-40d5-e264-1cfac3030a4c"
      },
      "source": [
        "#@title Import & seed\n",
        "import time\n",
        "import re\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import requests\n",
        "import collections\n",
        "import pickle\n",
        "import copy, random\n",
        "import nltk as nl\n",
        "nl.download('punkt')\n",
        "from itertools import zip_longest\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Reshape, BatchNormalization, Dense, Dropout, concatenate,\n",
        "    Embedding, LSTM, Dense, GRU, Bidirectional, Add\n",
        ")\n",
        "from tensorflow.keras.activations import elu, relu, softmax, sigmoid\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "np.random.seed(1234)\n",
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "2.3.0\n",
            "Fri Aug 21 18:56:01 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.57       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLZ0LreHzhhr",
        "colab_type": "code",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2c04181d-427f-46ad-9751-efd6535b697a"
      },
      "source": [
        "#@title Setup wandb\n",
        "!pip install wandb\n",
        "!wandb login f57cb185d23a8b60d349a4ea02278a6eee82550a\n",
        "import wandb\n",
        "wandb.init(project=\"deep_comedy\", name=\"continuous-gen-2\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/14/e7988204e4d4c9a349e73362399263b1c17f2b4d8a753864444f9eac1c92/wandb-0.9.5-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 8.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.13)\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/4c/49f899856e3a83e02bc88f2c4945aa0bda4f56b804baa9f71e6664a574a2/sentry_sdk-0.16.5-py2.py3-none-any.whl (113kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 43.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.8.1)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.352.0)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/4b/6b/01baa293090240cf0562cc5eccb69c6f5006282127f2b846fad011305c79/configparser-5.0.0-py3-none-any.whl\n",
            "Collecting watchdog>=0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/06/121302598a4fc01aca942d937f4a2c33430b7181137b35758913a8db10ad/watchdog-0.10.3.tar.gz (94kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 13.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.23.0)\n",
            "Collecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 10.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.15.0)\n",
            "Collecting gql==0.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/c4/6f/cf9a3056045518f06184e804bae89390eb706168349daa9dff8ac609962a/gql-0.2.0.tar.gz\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (5.4.8)\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/1e/a45320cab182bf1c8656107b3d4c042e659742822fc6bff150d769a984dd/GitPython-3.1.7-py3-none-any.whl (158kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 39.2MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from sentry-sdk>=0.4.0->wandb) (2020.6.20)\n",
            "Requirement already satisfied: urllib3>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from sentry-sdk>=0.4.0->wandb) (1.24.3)\n",
            "Collecting pathtools>=0.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (2.10)\n",
            "Collecting graphql-core<2,>=0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/89/00ad5e07524d8c523b14d70c685e0299a8b0de6d0727e368c41b89b7ed0b/graphql-core-1.1.tar.gz (70kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.6/dist-packages (from gql==0.2.0->wandb) (2.3)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.2MB/s \n",
            "\u001b[?25hCollecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/b0/9a/4d409a6234eb940e6a78dfdfc66156e7522262f5f2fecca07dc55915952d/smmap-3.0.4-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: watchdog, subprocess32, gql, pathtools, graphql-core\n",
            "  Building wheel for watchdog (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for watchdog: filename=watchdog-0.10.3-cp36-none-any.whl size=73870 sha256=8ab9580366207ab78d2be39e851a81b11cca11199ff6f10b8c933f718c546873\n",
            "  Stored in directory: /root/.cache/pip/wheels/a8/1d/38/2c19bb311f67cc7b4d07a2ec5ea36ab1a0a0ea50db994a5bc7\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp36-none-any.whl size=6489 sha256=75db09dfaf3a242df00f40b943b59bf100a4497386c7922a7cc603c1ca786021\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for gql (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gql: filename=gql-0.2.0-cp36-none-any.whl size=7630 sha256=d86f5014cf5895725241d15046826e646307af942d6bc9a179729da9bc4bb204\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/0e/7b/58a8a5268655b3ad74feef5aa97946f0addafb3cbb6bd2da23\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp36-none-any.whl size=8784 sha256=67064fc2145021457bfb4ed4b718c77209363b445ff55767d64bab44e1cf726b\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "  Building wheel for graphql-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for graphql-core: filename=graphql_core-1.1-cp36-none-any.whl size=104650 sha256=c3f6cd015752226fae690605c9010541bf3f6a7aaae577cafa9e5a13b429bc82\n",
            "  Stored in directory: /root/.cache/pip/wheels/45/99/d7/c424029bb0fe910c63b68dbf2aa20d3283d023042521bcd7d5\n",
            "Successfully built watchdog subprocess32 gql pathtools graphql-core\n",
            "Installing collected packages: sentry-sdk, shortuuid, configparser, pathtools, watchdog, subprocess32, graphql-core, gql, smmap, gitdb, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.7 configparser-5.0.0 docker-pycreds-0.4.0 gitdb-4.0.5 gql-0.2.0 graphql-core-1.1 pathtools-0.1.2 sentry-sdk-0.16.5 shortuuid-1.0.1 smmap-3.0.4 subprocess32-3.5.4 wandb-0.9.5 watchdog-0.10.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[32mSuccessfully logged in to Weights & Biases!\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/veri/deep_comedy\" target=\"_blank\">https://app.wandb.ai/veri/deep_comedy</a><br/>\n",
              "                Run page: <a href=\"https://app.wandb.ai/veri/deep_comedy/runs/1fh8wzzg\" target=\"_blank\">https://app.wandb.ai/veri/deep_comedy/runs/1fh8wzzg</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "W&B Run: https://app.wandb.ai/veri/deep_comedy/runs/1fh8wzzg"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onTHkWW_YOHp",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Model\n",
        "\n",
        "vocab_size = 1797\n",
        "terces_per_batch = 4\n",
        "wandb.config.num_layers = 4\n",
        "wandb.config.d_model = 128\n",
        "wandb.config.dff = 512\n",
        "wandb.config.num_heads = 16\n",
        "input_vocab_size = vocab_size\n",
        "target_vocab_size = vocab_size\n",
        "wandb.config.dropout = 0.1\n",
        "batch_len = terces_per_batch * 76\n",
        "EPOCHS = 300\n",
        "\n",
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "    return pos * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                            np.arange(d_model)[np.newaxis, :],\n",
        "                            d_model)\n",
        "    # apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    # apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "def create_padding_mask(seq):   \n",
        "    seq = tf.cast(tf.math.equal(seq, pad), tf.float32)\n",
        "    # add extra dimensions to add the padding\n",
        "    # to the attention logits.\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
        "\n",
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask  # (seq_len, seq_len)\n",
        "\n",
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    \"\"\"Calculate the attention weights.\n",
        "    q, k, v must have matching leading dimensions.\n",
        "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "    The mask has different shapes depending on its type(padding or look ahead) \n",
        "    but it must be broadcastable for addition.\n",
        "    \n",
        "    Args:\n",
        "        q: query shape == (..., seq_len_q, depth)\n",
        "        k: key shape == (..., seq_len_k, depth)\n",
        "        v: value shape == (..., seq_len_v, depth_v)\n",
        "        mask: Float tensor with shape broadcastable \n",
        "            to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "        \n",
        "    Returns:\n",
        "        output, attention_weights\n",
        "    \"\"\"\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "    # scale matmul_qk\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "    # add the mask to the scaled tensor.\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)  \n",
        "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "    # add up to 1.\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "    return output, attention_weights\n",
        "\n",
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "    ])\n",
        "\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        assert d_model % self.num_heads == 0\n",
        "        self.depth = d_model // self.num_heads\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "            \n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"Split the last dimension into (num_heads, depth).\n",
        "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "        \n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "        \n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "        \n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "        \n",
        "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "        concat_attention = tf.reshape(scaled_attention, \n",
        "                                    (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "        return output, attention_weights\n",
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "    def __call__(self, x, training, mask):\n",
        "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "        \n",
        "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "        \n",
        "        return out2\n",
        "\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "    \n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "        \n",
        "    def __call__(self, x, enc_output, training, \n",
        "            look_ahead_mask, padding_mask):\n",
        "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "        \n",
        "        attn2, attn_weights_block2 = self.mha2(\n",
        "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "        \n",
        "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
        "        \n",
        "        return out3, attn_weights_block1, attn_weights_block2\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "                maximum_position_encoding, rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)   ########## rm ??????\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
        "                                                self.d_model)\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
        "                        for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "            \n",
        "    def __call__(self, x, training, mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        # adding embedding and position encoding.\n",
        "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "        x = self.dropout(x, training=training)\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "        return x  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "                maximum_position_encoding, rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
        "                        for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "    def __call__(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "        x = self.dropout(x, training=training)\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                                look_ahead_mask, padding_mask)\n",
        "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "        \n",
        "        # x.shape == (batch_size, target_seq_len, d_model)\n",
        "        return x, attention_weights\n",
        "\n",
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
        "                target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
        "                            input_vocab_size, pe_input, rate)\n",
        "\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
        "                            target_vocab_size, pe_target, rate)\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "        \n",
        "    def __call__(self, inp, tar, training, enc_padding_mask, \n",
        "            look_ahead_mask, dec_padding_mask):\n",
        "\n",
        "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
        "        \n",
        "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "        dec_output, attention_weights = self.decoder(\n",
        "            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "        \n",
        "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "        \n",
        "        return final_output, attention_weights\n",
        "\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "        \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "        \n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "learning_rate = CustomSchedule(wandb.config.d_model)\n",
        "\n",
        "lerning_rate = 1e-3\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
        "                                     epsilon=1e-9)\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "    name='train_accuracy')\n",
        "\n",
        "val_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "    name='train_accuracy')\n",
        "\n",
        "transformer = Transformer(wandb.config.num_layers, wandb.config.d_model, \n",
        "                          wandb.config.num_heads, wandb.config.dff,\n",
        "                          input_vocab_size, target_vocab_size, \n",
        "                          pe_input=input_vocab_size, \n",
        "                          pe_target=target_vocab_size,\n",
        "                          rate=wandb.config.dropout)\n",
        "def create_masks(inp, tar):\n",
        "    # Encoder padding mask\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "    \n",
        "    # Used in the 2nd attention block in the decoder.\n",
        "    # This padding mask is used to mask the encoder outputs.\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "    \n",
        "    # Used in the 1st attention block in the decoder.\n",
        "    # It is used to pad and mask future tokens in the input received by \n",
        "    # the decoder.\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "    \n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask\n",
        "\n",
        "checkpoint_path = \"./checkpoints/train\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "# if a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print ('Latest checkpoint restored!!')\n",
        "\n",
        "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
        "# execution. The function specializes to the precise shape of the argument\n",
        "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
        "# batch sizes (the last batch is smaller), use input_signature to specify\n",
        "# more generic shapes.\n",
        "\n",
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, batch_len), dtype=tf.int64),\n",
        "    tf.TensorSpec(shape=(None, batch_len), dtype=tf.int64),\n",
        "]\n",
        "\n",
        "@tf.function()#(input_signature=train_step_signature)\n",
        "def train_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _ = transformer(inp, tar_inp, \n",
        "                                    True, \n",
        "                                    enc_padding_mask, \n",
        "                                    combined_mask, \n",
        "                                    dec_padding_mask)\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "        gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "    \n",
        "    train_loss(loss)\n",
        "    train_accuracy(tar_real, predictions)\n",
        "\n",
        "@tf.function()#(input_signature=train_step_signature)\n",
        "def val_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "    \n",
        "    predictions, _ = transformer(inp, tar_inp, \n",
        "                                False, \n",
        "                                enc_padding_mask, \n",
        "                                combined_mask, \n",
        "                                dec_padding_mask)\n",
        "    loss = loss_function(tar_real, predictions)\n",
        "    \n",
        "    val_loss(loss)\n",
        "    val_accuracy(tar_real, predictions)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GI14h2PZX70w",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "outputId": "f82922b0-39a0-4922-ecc2-8a4fc3c9a72c"
      },
      "source": [
        "#@title Preprocessing\n",
        "\n",
        "def get_hyp_lm_tercets(tercets):\n",
        "    new_tercets = []\n",
        "    for tercet in tercets:\n",
        "        new_tercets.append([])\n",
        "        for verse in tercet:\n",
        "            new_tercets[-1].append([])\n",
        "            for hyp_w in verse:\n",
        "                new_tercets[-1][-1].extend(hyp_w)\n",
        "                new_tercets[-1][-1].append('<SEP>')\n",
        "            new_tercets[-1][-1] = new_tercets[-1][-1][:-1]\n",
        "\n",
        "    return new_tercets\n",
        "\n",
        "def is_vowel(c):\n",
        "    return c in 'aeiouAEIOUàìíèéùúüòï'\n",
        "\n",
        "def unsplittable_cons():\n",
        "    u_cons = []\n",
        "    for c1 in ('b', 'c', 'd', 'f', 'g', 'p', 't', 'v'):\n",
        "        for c2 in ('l', 'r'):\n",
        "            u_cons.append(c1 + c2)\n",
        "\n",
        "    others = ['gn', 'gh', 'ch']\n",
        "    u_cons.extend(others)\n",
        "    return u_cons\n",
        "\n",
        "\n",
        "def are_cons_to_split(c1, c2):\n",
        "    to_split = ('cq', 'cn', 'lm', 'rc', 'bd', 'mb', 'mn', 'ld', 'ng', 'nd', 'tm', 'nv', 'nc', 'ft', 'nf', 'gm', 'fm', 'rv', 'fp')\n",
        "    return (c1 + c2) in to_split or (not is_vowel(c1) and (c1 == c2)) or ((c1 + c2) not in unsplittable_cons()) and (\n",
        "        (not is_vowel(c1)) and (not is_vowel(c2)) and c1 != 's')\n",
        "\n",
        "\n",
        "def is_diphthong(c1, c2):\n",
        "    return (c1 + c2) in ('ia', 'ie', 'io', 'iu', 'ua', 'ue', 'uo', 'ui', 'ai', 'ei', 'oi', 'ui', 'au', 'eu', 'ïe', 'iú', 'iù')\n",
        "\n",
        "\n",
        "def is_triphthong(c1, c2, c3):\n",
        "    return (c1 + c2 + c3) in ('iai', 'iei', 'uoi', 'uai', 'uei', 'iuo')\n",
        "\n",
        "\n",
        "def is_toned_vowel(c):\n",
        "    return c in 'àìèéùòï'\n",
        "\n",
        "def has_vowels(sy):\n",
        "    for c in sy:\n",
        "        if is_vowel(c):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def hyphenation(word):\n",
        "    \"\"\"\n",
        "    Split word in syllables\n",
        "    :param word: input string\n",
        "    :return: a list containing syllables of the word\n",
        "    \"\"\"\n",
        "    if not word or word == '':\n",
        "        return []\n",
        "    # elif len(word) == 3 and (is_vowel(word[1]) and is_vowel(word[2]) and not is_toned_vowel(word[2]) and (\n",
        "    #     not is_diphthong(word[1], word[2]) or (word[1] == 'i'))):\n",
        "    elif len(word) == 3 and (is_vowel(word[1]) and is_vowel(word[2]) and not is_toned_vowel(word[2]) and (\n",
        "        not is_diphthong(word[1], word[2]))):\n",
        "        return [word[:2]] + [word[2]]\n",
        "    elif len(word) == 3 and is_vowel(word[0]) and not is_vowel(word[1]) and is_vowel(word[2]):\n",
        "        return [word[:2]] + [word[2]]\n",
        "    elif len(word) == 3:\n",
        "        return [word]\n",
        "\n",
        "    syllables = []\n",
        "    is_done = False\n",
        "    count = 0\n",
        "    while not is_done and count <= len(word) - 1:\n",
        "        syllables.append('')\n",
        "        c = word[count]\n",
        "        while not is_vowel(c) and count < len(word) - 1:\n",
        "            syllables[-1] = syllables[-1] + c\n",
        "            count += 1\n",
        "            c = word[count]\n",
        "\n",
        "        syllables[-1] = syllables[-1] + word[count]\n",
        "\n",
        "        if count == len(word) - 1:\n",
        "            is_done = True\n",
        "        else:\n",
        "            count += 1\n",
        "\n",
        "            if count < len(word) and not is_vowel(word[count]):\n",
        "                if count == len(word) - 1:\n",
        "                    syllables[-1] += word[count]\n",
        "                    count += 1\n",
        "                elif count + 1 < len(word) and are_cons_to_split(word[count], word[count + 1]):\n",
        "                    syllables[-1] += word[count]\n",
        "                    count += 1\n",
        "                elif count + 2 < len(word) and not is_vowel(word[count + 1]) and not is_vowel(word[count + 2]) and word[\n",
        "                    count] != 's':\n",
        "                    syllables[-1] += word[count]\n",
        "                    count += 1\n",
        "            elif count < len(word):\n",
        "                if count + 1 < len(word) and is_triphthong(word[count - 1], word[count], word[count + 1]):\n",
        "                    syllables[-1] += word[count] + word[count + 1]\n",
        "                    count += 2\n",
        "                elif is_diphthong(word[count - 1], word[count]):\n",
        "                    syllables[-1] += word[count]\n",
        "                    count += 1\n",
        "\n",
        "                if count + 1 < len(word) and are_cons_to_split(word[count], word[count + 1]):\n",
        "                    syllables[-1] += word[count]\n",
        "                    count += 1\n",
        "\n",
        "            else:\n",
        "                is_done = True\n",
        "\n",
        "    if not has_vowels(syllables[-1]) and len(syllables) > 1:\n",
        "        syllables[-2] = syllables[-2] + syllables[-1]\n",
        "        syllables = syllables[:-1]\n",
        "\n",
        "    return syllables\n",
        "\n",
        "\n",
        "\n",
        "def get_dc_hyphenation(canti):\n",
        "    hyp_canti, hyp_tokens = [], []\n",
        "    for canto in canti:\n",
        "        hyp_canti.append([])\n",
        "        for verso in canto:\n",
        "            syllables = seq_hyphentation(verso)\n",
        "            hyp_canti[-1].append(syllables)\n",
        "            for syllable in syllables:\n",
        "                hyp_tokens.extend(syllable)\n",
        "\n",
        "    return hyp_canti, hyp_tokens\n",
        "\n",
        "\n",
        "def seq_hyphentation(words):\n",
        "    \"\"\"\n",
        "    Converts words in a list of strings into lists of syllables\n",
        "    :param words: a list of words (strings)\n",
        "    :return: a list of lists containing word syllables\n",
        "    \"\"\"\n",
        "    return [hyphenation(w) for w in words]\n",
        "\n",
        "\n",
        "def get_dc_cantos(filename, encoding=None):\n",
        "    # raw_data = read_words(filename=filename)\n",
        "    cantos, words, raw = [], [], []\n",
        "    with open(filename, \"r\", encoding=encoding) as f:\n",
        "        for line in f:\n",
        "            sentence = line.strip()\n",
        "            sentence = str.replace(sentence, \"\\.\", \" \\. \")\n",
        "            sentence = str.replace(sentence, \"[\", '')\n",
        "            sentence = str.replace(sentence, \"]\", '')\n",
        "            sentence = str.replace(sentence, \"-\", '')\n",
        "            sentence = str.replace(sentence, \";\", \" ; \")\n",
        "            sentence = str.replace(sentence, \",\", \" , \")\n",
        "            # sentence = str.replace(sentence, \" \\'\", '')\n",
        "            sentence = str.replace(sentence, \"\\'\", ' \\' ')\n",
        "            if len(sentence) > 1:\n",
        "                # sentence = sentence.translate(string.punctuation)\n",
        "                tokenized_sentence = nl.word_tokenize(sentence)\n",
        "                # tokenized_sentence = sentence.split()\n",
        "                tokenized_sentence = [w.lower() for w in tokenized_sentence if len(w) > 0]\n",
        "                tokenized_sentence = [w for w in tokenized_sentence if \",\" not in w]\n",
        "                tokenized_sentence = [w for w in tokenized_sentence if \".\" not in w]\n",
        "                tokenized_sentence = [w for w in tokenized_sentence if \":\" not in w]\n",
        "                tokenized_sentence = [w for w in tokenized_sentence if \";\" not in w]\n",
        "                tokenized_sentence = [w for w in tokenized_sentence if \"«\" not in w]\n",
        "                tokenized_sentence = [w for w in tokenized_sentence if \"»\" not in w]\n",
        "                # ts = []\n",
        "                ts = tokenized_sentence\n",
        "                # [ts.extend(re.split(\"(\\')\", e)) for e in tokenized_sentence]\n",
        "                tokenized_sentence = [w for w in ts if len(w) > 0]\n",
        "\n",
        "                if len(tokenized_sentence) == 2:\n",
        "                    cantos.append([])\n",
        "                    raw.append([])\n",
        "                elif len(tokenized_sentence) > 2:\n",
        "                    raw[-1].append(sentence)\n",
        "                    cantos[-1].append(tokenized_sentence)\n",
        "                    words.extend(tokenized_sentence)\n",
        "\n",
        "    return cantos, words, raw\n",
        "\n",
        "\n",
        "def create_tercets(cantos):\n",
        "    tercets = []\n",
        "    for i,canto in enumerate(cantos):\n",
        "        for v,verse in enumerate(canto):\n",
        "            if v%3 == 0:\n",
        "                tercets.append([])\n",
        "\n",
        "            tercets[-1].append(verse)\n",
        "        tercets = tercets[:-1]  # removes the last malformed tercets (only 2 verses)\n",
        "\n",
        "    return tercets\n",
        "\n",
        "def pad_list(l, pad_token, max_l_size, keep_lasts=False, pad_right=True):\n",
        "    \"\"\"\n",
        "    Adds a padding token to a list\n",
        "    inputs:\n",
        "    :param l: input list to pad.\n",
        "    :param pad_token: value to add as padding.\n",
        "    :param max_l_size: length of the new padded list to return,\n",
        "    it truncates lists longer that 'max_l_size' without adding\n",
        "    padding values.\n",
        "    :param keep_lasts: If True, preserves the max_l_size last elements\n",
        "    of a sequence (by keeping the same order).  E.g.:\n",
        "    if keep_lasts is True and max_l_size=3 [1,2,3,4] becomes [2,3,4].\n",
        "\n",
        "\n",
        "    :return: the list padded or truncated.\n",
        "    \"\"\"\n",
        "    to_pad = []\n",
        "    max_l = min(max_l_size, len(l))  # maximum len\n",
        "    l_init = len(l) - max_l if len(l) > max_l and keep_lasts else 0  # initial position where to sample from the list\n",
        "    l_end = len(l) if len(l) > max_l and keep_lasts else max_l\n",
        "    for i in range(l_init, l_end):\n",
        "        to_pad.append(l[i])\n",
        "\n",
        "    # for j in range(len(l), max_l_size):\n",
        "    #     to_pad.append(pad_token)\n",
        "    pad_tokens = [pad_token] * (max_l_size-len(l))\n",
        "    padded_l = to_pad + pad_tokens if pad_right else pad_tokens + to_pad\n",
        "\n",
        "    return padded_l\n",
        "\n",
        "\n",
        "def save_data(data, file):\n",
        "    with open(file, 'wb') as output:\n",
        "        pickle.dump(data, output, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def load_data(file):\n",
        "    with open(file, 'rb') as obj:\n",
        "        return pickle.load(obj)\n",
        "\n",
        "def print_and_write(file, s):\n",
        "    print(s)\n",
        "    file.write(s)\n",
        "\n",
        "\n",
        "class Vocabulary(object):\n",
        "    def __init__(self, vocab_size=None):\n",
        "        self.dictionary = dict()\n",
        "        self.rev_dictionary = dict()\n",
        "        self.count = []\n",
        "        self.special_tokens = []\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    def build_vocabulary_from_counts(self, count, special_tokens=[]):\n",
        "        \"\"\"\n",
        "        Sets all the attributes of the Vocabulary object.\n",
        "        :param count: a list of lists as follows: [['token', number_of_occurrences],...]\n",
        "        :param special_tokens: a list of strings. E.g. ['<EOS>', '<PAD>',...]\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "\n",
        "        dictionary = dict()\n",
        "        for word, _ in count:\n",
        "            dictionary[word] = len(dictionary)\n",
        "\n",
        "        # adding eventual special tokens to the dictionary (e.g. <EOS>,<PAD> etc..)\n",
        "        d = len(dictionary)\n",
        "        for i, token in enumerate(special_tokens):\n",
        "            dictionary[token] = d + i\n",
        "\n",
        "        self.count = count\n",
        "        self.dictionary = dictionary\n",
        "        self.rev_dictionary = dict(zip(self.dictionary.values(), self.dictionary.keys()))\n",
        "        self.special_tokens = special_tokens\n",
        "        self.vocab_size = len(dictionary)\n",
        "\n",
        "    def build_vocabulary_from_tokens(self, tokens, vocabulary_size=None, special_tokens=[]):\n",
        "        \"\"\"\n",
        "        Given a list of tokens, it sets the Vocabulary object attributes by constructing\n",
        "        a dictionary mapping each token to a unique id.\n",
        "        :param tokens: a list of strings.\n",
        "         E.g. [\"the\", \"cat\", \"is\", ... \".\", \"the\", \"house\" ,\"is\" ...].\n",
        "         NB: Here you should put all your token instances of the corpus.\n",
        "        :param vocabulary_size: The number of elements of your vocabulary. If there are more\n",
        "        than 'vocabulary_size' elements on tokens, it considers only the 'vocabulary_size'\n",
        "        most frequent ones.\n",
        "        :param special_tokens: Optional. A list of strings. Useful to add special tokens in vocabulary.\n",
        "        If you don't have any, keep it empty.\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "\n",
        "        vocabulary_size = vocabulary_size if vocabulary_size is not None else self.vocab_size\n",
        "        vocabulary_size = vocabulary_size - (len(special_tokens) + 1) if vocabulary_size else None\n",
        "        # counts occurrences of each token\n",
        "        count = [['<UNK>', -1]]\n",
        "        count.extend(collections.Counter(tokens).most_common(vocabulary_size))  # takes only the most frequent ones, if size is None takes them all\n",
        "        self.build_vocabulary_from_counts(count, special_tokens)  # actually build the vocabulary\n",
        "        self._set_unk_count(tokens)  # set the number of OOV instances\n",
        "\n",
        "    @staticmethod\n",
        "    def merge_vocabulary(vocab0, vocab1, vocabulary_size=-1):\n",
        "        \"\"\"\n",
        "        Merge two Vocabulary objects into a new one.\n",
        "        :param vocab0: first Vocabulary object\n",
        "        :param vocab1: second Vocabulary object\n",
        "        :param vocabulary_size: parameter to decide the merged vocabulary size.\n",
        "        With default value -1, all the words of both vocabularies are preserved.\n",
        "        When set to 0, the size of the vocabulary is set to the size of vocab0,\n",
        "        when set to 1 it is kept the size of vocab1.\n",
        "        :return: a new vocabulary\n",
        "        \"\"\"\n",
        "        # get size of the new vocabulary\n",
        "        vocab_size = vocab0.vocab_size + vocab1.vocab_size if vocabulary_size == -1 else vocabulary_size\n",
        "        merged_special_tokens = list(set(vocab0.special_tokens) | set(vocab1.special_tokens))\n",
        "\n",
        "        # merge the counts from the two vocabularies and then selects the most_common tokens\n",
        "        merged_counts = collections.Counter(dict(vocab0.count)) + collections.Counter(dict(vocab1.count))\n",
        "        merged_counts = merged_counts.most_common(vocab_size)\n",
        "        count = [['<UNK>', -1]]\n",
        "        count.extend(merged_counts)\n",
        "\n",
        "        # create the new vocabulary\n",
        "        merged_vocab = Vocabulary(vocab_size)\n",
        "        merged_vocab.build_vocabulary_from_counts(count, merged_special_tokens)\n",
        "        return merged_vocab\n",
        "\n",
        "    @staticmethod\n",
        "    def merge_vocabularies(vocab_list, vocab_size=None):\n",
        "        \"\"\"\n",
        "        Join a list of vocabularies into a new one.\n",
        "        :param vocab_list: a list of Vocabulary objects\n",
        "        :param vocab_size: the maximum size of the merged vocabulary.\n",
        "        :return: a vocabulary merging them all.\n",
        "        \"\"\"\n",
        "        vocab_size = vocab_size if vocab_size else sum([v.vocab_size for v in vocab_list])\n",
        "        merged_vocab = Vocabulary(vocab_size)\n",
        "        for voc in vocab_list:\n",
        "            merged_vocab = Vocabulary.merge_vocabulary(merged_vocab, voc, vocab_size)\n",
        "        return merged_vocab\n",
        "\n",
        "    def string2id(self, dataset):\n",
        "        \"\"\"\n",
        "        Converts a dataset of strings into a dataset of ids according to the object dictionary.\n",
        "        :param dataset: any string-based dataset with any nested lists.\n",
        "        :return: a new dataset, with the same shape of dataset, where each string is mapped into its\n",
        "        corresponding id associated in the dictionary (0 for unknown tokens).\n",
        "        \"\"\"\n",
        "\n",
        "        def _recursive_call(items):\n",
        "            new_items = []\n",
        "            for item in items:\n",
        "                if isinstance(item, str) or isinstance(item, int) or isinstance(item, float):\n",
        "                    new_items.append(self.word2id(item))\n",
        "                else:\n",
        "                    new_items.append(_recursive_call(item))\n",
        "            return new_items\n",
        "\n",
        "        return _recursive_call(dataset)\n",
        "\n",
        "    def id2string(self, dataset):\n",
        "        \"\"\"\n",
        "        Converts a dataset of integer ids into a dataset of string according to the reverse dictionary.\n",
        "        :param dataset: any int-based dataset with any nested lists. Allowed types are int, np.int32, np.int64.\n",
        "        :return: a new dataset, with the same shape of dataset, where each token is mapped into its\n",
        "        corresponding string associated in the reverse dictionary.\n",
        "        \"\"\"\n",
        "        def _recursive_call(items):\n",
        "            new_items = []\n",
        "            for item in items:\n",
        "                if isinstance(item, int) or isinstance(item, np.int) or isinstance(item, np.int32) or isinstance(item, np.int64):\n",
        "                    new_items.append(self.id2word(item))\n",
        "                else:\n",
        "                    new_items.append(_recursive_call(item))\n",
        "            return new_items\n",
        "\n",
        "        return _recursive_call(dataset)\n",
        "\n",
        "    def word2id(self, item):\n",
        "        \"\"\"\n",
        "        Maps a string token to its corresponding id.\n",
        "        :param item: a string.\n",
        "        :return: If the token belongs to the vocabulary, it returns an integer id > 0, otherwise\n",
        "        it returns the value associated to the unknown symbol, that is typically 0.\n",
        "        \"\"\"\n",
        "        return self.dictionary[item] if item in self.dictionary else self.dictionary['<UNK>']\n",
        "\n",
        "    def id2word(self, token_id):\n",
        "        \"\"\"\n",
        "        Maps an integer token to its corresponding string.\n",
        "        :param token_id: an integer.\n",
        "        :return: If the id belongs to the vocabulary, it returns the string\n",
        "        associated to it, otherwise it returns the string associated\n",
        "        to the unknown symbol, that is '<UNK>'.\n",
        "        \"\"\"\n",
        "\n",
        "        return self.rev_dictionary[token_id] if token_id in self.rev_dictionary else self.rev_dictionary[self.dictionary['<UNK>']]\n",
        "\n",
        "    def get_unk_count(self):\n",
        "        return self.count[0][1]\n",
        "\n",
        "    def _set_unk_count(self, tokens):\n",
        "        \"\"\"\n",
        "        Sets the number of OOV instances in the tokens provided\n",
        "        :param tokens: a list of tokens\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "        data = list()\n",
        "        unk_count = 0\n",
        "        for word in tokens:\n",
        "            if word in self.dictionary:\n",
        "                index = self.dictionary[word]\n",
        "            else:\n",
        "                index = 0  # dictionary['<UNK>']\n",
        "                unk_count += 1\n",
        "            data.append(index)\n",
        "        self.count[0][1] = unk_count\n",
        "\n",
        "    def add_element(self, name, is_special_token=False):\n",
        "        if name not in self.dictionary:\n",
        "            self.vocab_size += 1\n",
        "            self.dictionary[name] = self.vocab_size\n",
        "            self.rev_dictionary[self.vocab_size] = name\n",
        "\n",
        "            if is_special_token:\n",
        "                self.special_tokens = list(self.special_tokens)\n",
        "                self.special_tokens.append(name)\n",
        "\n",
        "            self.count.append([name, 1])\n",
        "\n",
        "    def set_vocabulary(self, dictionary, rev_dictionary, special_tokens, vocab_size):\n",
        "        self.dictionary = dictionary,\n",
        "        self.rev_dictionary = rev_dictionary\n",
        "        self.special_tokens = special_tokens\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    @staticmethod\n",
        "    def load_vocabulary(filename):\n",
        "        return load_data(filename)\n",
        "\n",
        "    def save_vocabulary(self, filename):\n",
        "        save_data(self, filename)\n",
        "\n",
        "class SyLMDataset(object):\n",
        "    def __init__(self, config, sy_vocab=None):\n",
        "        self.config = config\n",
        "        self.vocabulary = sy_vocab\n",
        "\n",
        "        self.raw_train_x = []\n",
        "        self.raw_val_x = []\n",
        "        self.raw_test_x = []\n",
        "        self.raw_x = []\n",
        "\n",
        "        self.train_x, self.train_y = [], []\n",
        "        self.val_x, self.val_y = [], []\n",
        "        self.test_x, self.test_y = [], []\n",
        "        self.x, self.y = [], []\n",
        "\n",
        "    def initialize(self, sess):\n",
        "        pass\n",
        "\n",
        "    def load(self, sources):\n",
        "        \"\"\"\n",
        "        Extract raw texts form sources and gather them all together.\n",
        "        :param sources: a string or an iterable of strings containing the file(s)\n",
        "        to process in order to build the dataset.\n",
        "        :return: a list of raw strings.\n",
        "        \"\"\"\n",
        "        return NotImplementedError\n",
        "\n",
        "    def build(self, sources, split_size=0.8):\n",
        "        \"\"\"\n",
        "        :param sources: a string or an iterable of strings containing the file(s)\n",
        "        to process in order to build the dataset.\n",
        "        :param split_size: the size to split the dataset, set >=1.0 to not split.\n",
        "        \"\"\"\n",
        "\n",
        "        raw_x = self.load(sources)\n",
        "        # raw_x = self.tokenize([self.preprocess(ex) for ex in raw_x])  # fixme\n",
        "        # splitting data\n",
        "        self.raw_x = raw_x\n",
        "        if split_size < 1.0:\n",
        "            self.raw_train_x, self.raw_test_x = self.split(self.raw_x, train_size=split_size)\n",
        "            self.raw_train_x, self.raw_val_x = self.split(self.raw_train_x, train_size=split_size)\n",
        "        else:\n",
        "            self.raw_train_x = self.raw_x\n",
        "\n",
        "        if self.vocabulary is None:\n",
        "            # creates vocabulary\n",
        "            tokens = [item for sublist in self.raw_train_x for item in sublist]  # get tokens\n",
        "            special_tokens = (\"<GO>\", \"<PAD>\", \"<SEP>\", \"<EOS>\", \"<EOV>\")\n",
        "            self._create_vocab(tokens, special_tokens=special_tokens)\n",
        "\n",
        "        # creates x,y for train\n",
        "        self.train_x = self._build_dataset(self.raw_train_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "        self.train_y = self._build_dataset(self.raw_train_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "\n",
        "        # creates x,y for validation\n",
        "        self.val_x = self._build_dataset(self.raw_val_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "        self.val_y = self._build_dataset(self.raw_val_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "\n",
        "        # creates x,y for validation\n",
        "        self.test_x = self._build_dataset(self.raw_test_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "        self.test_y = self._build_dataset(self.raw_test_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "\n",
        "    def _create_vocab(self, tokens, special_tokens=(\"<PAD>\", \"<GO>\", \"<SEP>\", \"<EOV>\", \"<EOS>\")):\n",
        "        \"\"\"\n",
        "        Create the vocabulary. Special tokens can be added to the tokens obtained from\n",
        "        the corpus.\n",
        "        :param tokens: a list of all the tokens in the corpus. Each token is a string.\n",
        "        :param special_tokens: a list of strings.\n",
        "        \"\"\"\n",
        "        vocab = Vocabulary(vocab_size=self.config.input_vocab_size)\n",
        "        vocab.build_vocabulary_from_tokens(tokens, special_tokens=special_tokens)\n",
        "        self.vocabulary = vocab\n",
        "\n",
        "    @staticmethod\n",
        "    def split(raw_data, train_size=0.8):\n",
        "        size = math.floor(len(raw_data)*train_size)\n",
        "        return raw_data[:size], raw_data[size:]\n",
        "\n",
        "    @staticmethod\n",
        "    def preprocess(txt):\n",
        "        return txt\n",
        "\n",
        "    @staticmethod\n",
        "    def shuffle(x):\n",
        "        return random.sample(x, len(x))\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenize(txt):\n",
        "        return txt\n",
        "\n",
        "    def _build_dataset(self, raw_data, max_len=100, insert_go=True, keep_lasts=False, pad_right=True, shuffle=True):\n",
        "        \"\"\"\n",
        "        Converts all the tokens in e1_raw_data by mapping each token with its corresponding\n",
        "        value in the dictionary. In case of token not in the dictionary, they are assigned to\n",
        "        a specific id. Each sequence is padded up to the seq_max_len setup in the config.\n",
        "\n",
        "        :param raw_data: list of sequences, each sequence is a list of tokens (strings).\n",
        "        :param max_len: max length of a sequence, crop longer and pad smaller ones.\n",
        "        :param insert_go: True to insert <GO>, False otherwise.\n",
        "        :param keep_lasts: True to truncate initial elements of a sequence.\n",
        "        :param pad_right: pad to the right (default value True), otherwise pads to left.\n",
        "        :param shuffle: Optional. If True data are shuffled.\n",
        "        :return: A list of sequences where each token in each sequence is an int id.\n",
        "        \"\"\"\n",
        "        dataset = []\n",
        "        for sentence in raw_data:\n",
        "            sentence_ids = [self.vocabulary.word2id(\"<GO>\")] if insert_go else []\n",
        "            sentence_ids.extend([self.vocabulary.word2id(w) for w in sentence])\n",
        "            sentence_ids.append(self.vocabulary.word2id(\"<EOS>\"))\n",
        "            sentence_ids = pad_list(sentence_ids, self.vocabulary.word2id(\"<PAD>\"), max_len, keep_lasts=keep_lasts, pad_right=pad_right)\n",
        "\n",
        "            dataset.append(sentence_ids)\n",
        "\n",
        "        if shuffle:\n",
        "            return random.sample(dataset, len(dataset))\n",
        "        else:\n",
        "            return dataset\n",
        "\n",
        "    def get_batches(self, batch_size=32, split_sel='train'):\n",
        "        \"\"\"\n",
        "        Iterator over the training set. Useful method to run experiments.\n",
        "        :param batch_size: size of the mini_batch\n",
        "        :return: input and target.\n",
        "        \"\"\"\n",
        "        if split_sel == 'train':\n",
        "            x, y = self.train_x, self.train_y\n",
        "        elif split_sel == 'val':\n",
        "            x, y = self.val_x, self.val_y\n",
        "        else:\n",
        "            x, y = self.test_x, self.test_y\n",
        "        \n",
        "        i = 0#random.randint(0, batch_size)\n",
        "        batches = []\n",
        "        eov = self.vocabulary.word2id(\"<EOV>\")\n",
        "        go = self.vocabulary.word2id(\"<GO>\")\n",
        "        # prepare batches\n",
        "        while i < len(x):\n",
        "            j = 0\n",
        "            batch_x, batch_y = [], []\n",
        "            while j < batch_size and i+j<len(x):\n",
        "                for c in x[i+j]:\n",
        "                  batch_x.append(c)\n",
        "                batch_x.append(eov)\n",
        "                for c in y[i+j]:\n",
        "                  batch_y.append(c)\n",
        "                batch_y.append(eov)\n",
        "                j += 1\n",
        "            i += batch_size\n",
        "            batches.append((batch_x, batch_y))\n",
        "\n",
        "        # supply\n",
        "        i = 0\n",
        "        while i < len(batches):\n",
        "            yield batches[i][0], batches[i][1]\n",
        "            i += 1\n",
        "\n",
        "class DanteSyLMDataset(SyLMDataset):\n",
        "    def __init__(self, config, sy_vocab=None):\n",
        "        \"\"\"\n",
        "        Class to create a dataset from Dante Alighieri's Divine Comedy.\n",
        "        :param config: a Config object\n",
        "        :param sy_vocab: (optional) a Vocabulary object where tokens of the dictionary\n",
        "        are syllables. If None, the vocabulary is create automatically from the source.\n",
        "        \"\"\"\n",
        "        super().__init__(config, sy_vocab)\n",
        "\n",
        "    def load(self, sources):\n",
        "        \"\"\"\n",
        "        Load examples from dataset\n",
        "        :param sources: data filepath.\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        canti, _, raw = get_dc_cantos(filename=sources)  # get raw data from file\n",
        "        canti, tokens = get_dc_hyphenation(canti)  # converts each\n",
        "\n",
        "        tercets = create_tercets(canti)\n",
        "        tercets = get_hyp_lm_tercets(tercets)\n",
        "\n",
        "        x = []\n",
        "        for tercet in tercets:\n",
        "            x.append([])\n",
        "            for verse in tercet:\n",
        "                x[-1].extend(verse)\n",
        "                x[-1].append(\"<EOV>\")\n",
        "\n",
        "        #x = self.shuffle(x)\n",
        "        return x\n",
        "\n",
        "def seq2str(seq):\n",
        "    def output2string(batch, rev_vocabulary, special_tokens, end_of_tokens):\n",
        "        to_print = ''\n",
        "        for token in batch:\n",
        "            if token in special_tokens:\n",
        "                to_print += ' '\n",
        "            elif end_of_tokens and token in end_of_tokens:\n",
        "                to_print += '\\n'\n",
        "            elif token in rev_vocabulary:\n",
        "                to_print += rev_vocabulary[token]\n",
        "            else:\n",
        "                to_print += '<UNK>'\n",
        "        return to_print\n",
        "\n",
        "    return output2string(seq, poetry_sy_lm_dataset.vocabulary.rev_dictionary,\n",
        "      special_tokens=[poetry_sy_lm_dataset.vocabulary.word2id(\"<PAD>\"), 0, poetry_sy_lm_dataset.vocabulary.word2id(\"<SEP>\"),\n",
        "                      poetry_sy_lm_dataset.vocabulary.word2id(\"<GO>\"), poetry_sy_lm_dataset.vocabulary.word2id(\"<EOS>\")],\n",
        "      end_of_tokens=[poetry_sy_lm_dataset.vocabulary.word2id(\"<EOV>\")])\n",
        "\n",
        "class cnfg:\n",
        "  vocab_size = 1884\n",
        "  input_vocab_size = 1884\n",
        "  sentence_max_len = 75\n",
        "\n",
        "config = cnfg()\n",
        "poetry_sy_lm_dataset = DanteSyLMDataset(config, sy_vocab=None)\n",
        "url = \"https://gitlab.com/zugo91/nlgpoetry/-/raw/release/data/la_divina_commedia.txt\"\n",
        "response = requests.get(url)\n",
        "response.encoding = 'ISO-8859-1'\n",
        "fi = open(\"divcom.txt\",\"w\")\n",
        "fi.write(response.text)\n",
        "fi.close()\n",
        "data_path = os.path.join(os.getcwd(), \"divcom.txt\")  # dataset location, here just the name of the source file\n",
        "poetry_sy_lm_dataset.build(data_path, split_size=0.9)  # actual creation of  vocabulary (if not provided) and dataset\n",
        "print(\"Train size: \" + str(len(poetry_sy_lm_dataset.train_y)))\n",
        "print(\"Val size: \" + str(len(poetry_sy_lm_dataset.val_y)))\n",
        "print(\"Test size: \" + str(len(poetry_sy_lm_dataset.test_y)))\n",
        "\n",
        "eov = poetry_sy_lm_dataset.vocabulary.word2id(\"<EOV>\")\n",
        "pad = poetry_sy_lm_dataset.vocabulary.word2id(\"<PAD>\")\n",
        "go = poetry_sy_lm_dataset.vocabulary.word2id(\"<GO>\")\n",
        "eos = poetry_sy_lm_dataset.vocabulary.word2id(\"<EOS>\")\n",
        "\n",
        "batches = [b for b in poetry_sy_lm_dataset.get_batches(terces_per_batch)]\n",
        "print(batches[0][0])\n",
        "print(batches[0][1])\n",
        "print(len(batches[0][0]))\n",
        "val_b = [b for b in poetry_sy_lm_dataset.get_batches(terces_per_batch, split_sel='val')]\n",
        "print(val_b[0][0])\n",
        "print(val_b[0][1])\n",
        "print(len(val_b[0][0]))\n",
        "test_b = [b for b in poetry_sy_lm_dataset.get_batches(terces_per_batch, split_sel='test')]\n",
        "print(test_b[0][0])\n",
        "print(test_b[0][1])\n",
        "print(len(test_b[0][0]))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train size: 3815\n",
            "Val size: 424\n",
            "Test size: 472\n",
            "[1792, 84, 1794, 339, 198, 1794, 50, 1794, 284, 188, 1794, 6, 1794, 24, 136, 1794, 37, 14, 1796, 27, 1794, 32, 61, 509, 1794, 18, 1794, 52, 5, 1794, 404, 47, 1794, 40, 276, 30, 1796, 89, 1794, 8, 1794, 6, 683, 14, 1794, 264, 1794, 92, 5, 1794, 655, 32, 14, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 585, 1794, 78, 9, 1794, 5, 1794, 238, 1794, 162, 1794, 92, 5, 1794, 57, 1794, 12, 45, 1794, 116, 30, 1796, 4, 63, 1794, 404, 47, 1794, 404, 748, 148, 1794, 4, 1794, 5, 1203, 1794, 4, 1794, 120, 10, 1796, 7, 1794, 84, 1794, 166, 536, 1794, 32, 24, 47, 1794, 8, 1794, 455, 30, 1794, 153, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 792, 1794, 3, 1794, 57, 1794, 5, 22, 30, 1794, 7, 1794, 49, 12, 1794, 57, 1794, 64, 1794, 144, 10, 1796, 22, 1794, 18, 1794, 416, 202, 1794, 50, 1794, 154, 1794, 46, 1794, 3, 1794, 41, 1794, 37, 1794, 61, 509, 1796, 6, 150, 1794, 193, 1794, 3, 1794, 15, 124, 1794, 12, 11, 1794, 46, 1794, 3, 1794, 53, 1794, 3, 1794, 405, 1794, 3, 1794, 456, 1794, 336, 10, 1796, 1796, 1792, 41, 1794, 31, 1794, 23, 1794, 154, 1794, 32, 238, 1794, 131, 1794, 3, 1794, 41, 1794, 405, 1794, 3, 1794, 257, 716, 1796, 792, 1794, 3, 1794, 92, 5, 1794, 163, 24, 1794, 6, 1794, 138, 24, 1794, 5, 1794, 60, 1794, 293, 9, 1796, 7, 1794, 8, 1794, 25, 30, 43, 1794, 264, 1794, 337, 539, 17, 702, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796]\n",
            "[1792, 84, 1794, 339, 198, 1794, 50, 1794, 284, 188, 1794, 6, 1794, 24, 136, 1794, 37, 14, 1796, 27, 1794, 32, 61, 509, 1794, 18, 1794, 52, 5, 1794, 404, 47, 1794, 40, 276, 30, 1796, 89, 1794, 8, 1794, 6, 683, 14, 1794, 264, 1794, 92, 5, 1794, 655, 32, 14, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 585, 1794, 78, 9, 1794, 5, 1794, 238, 1794, 162, 1794, 92, 5, 1794, 57, 1794, 12, 45, 1794, 116, 30, 1796, 4, 63, 1794, 404, 47, 1794, 404, 748, 148, 1794, 4, 1794, 5, 1203, 1794, 4, 1794, 120, 10, 1796, 7, 1794, 84, 1794, 166, 536, 1794, 32, 24, 47, 1794, 8, 1794, 455, 30, 1794, 153, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 792, 1794, 3, 1794, 57, 1794, 5, 22, 30, 1794, 7, 1794, 49, 12, 1794, 57, 1794, 64, 1794, 144, 10, 1796, 22, 1794, 18, 1794, 416, 202, 1794, 50, 1794, 154, 1794, 46, 1794, 3, 1794, 41, 1794, 37, 1794, 61, 509, 1796, 6, 150, 1794, 193, 1794, 3, 1794, 15, 124, 1794, 12, 11, 1794, 46, 1794, 3, 1794, 53, 1794, 3, 1794, 405, 1794, 3, 1794, 456, 1794, 336, 10, 1796, 1796, 1792, 41, 1794, 31, 1794, 23, 1794, 154, 1794, 32, 238, 1794, 131, 1794, 3, 1794, 41, 1794, 405, 1794, 3, 1794, 257, 716, 1796, 792, 1794, 3, 1794, 92, 5, 1794, 163, 24, 1794, 6, 1794, 138, 24, 1794, 5, 1794, 60, 1794, 293, 9, 1796, 7, 1794, 8, 1794, 25, 30, 43, 1794, 264, 1794, 337, 539, 17, 702, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796]\n",
            "304\n",
            "[1792, 4, 1794, 76, 150, 1794, 70, 1794, 27, 1794, 249, 1794, 4, 1794, 424, 1794, 3, 1794, 41, 1794, 642, 5, 1796, 64, 1794, 1353, 601, 40, 23, 1794, 5, 1794, 10, 1794, 31, 1794, 27, 1794, 17, 134, 6, 1796, 7, 1794, 15, 299, 1794, 15, 61, 1794, 21, 1794, 65, 63, 1794, 486, 161, 1794, 1014, 5, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 59, 1794, 159, 6, 1794, 3, 1794, 16, 1794, 25, 48, 1794, 89, 1794, 53, 1794, 27, 24, 32, 1794, 4, 1794, 3, 1794, 203, 6, 1796, 6, 1794, 65, 63, 1794, 37, 14, 1794, 27, 164, 1794, 84, 34, 1794, 255, 189, 1796, 21, 1794, 7, 1794, 128, 22, 1794, 7, 1794, 166, 13, 1794, 56, 1794, 166, 536, 1794, 624, 6, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 22, 1794, 18, 89, 1794, 3, 1794, 16, 1794, 45, 492, 1794, 5, 42, 29, 1794, 21, 1794, 7, 1794, 41, 1794, 25, 189, 1796, 35, 1794, 18, 76, 246, 1794, 37, 63, 1794, 4, 1794, 7, 1794, 127, 1794, 3, 1794, 217, 11, 14, 1796, 6, 1794, 234, 43, 1794, 6, 573, 106, 1794, 80, 1794, 3, 1794, 5, 562, 209, 1794, 26, 189, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 8, 1794, 66, 43, 1794, 246, 1794, 13, 156, 30, 1794, 577, 36, 1794, 4, 1794, 353, 14, 1796, 119, 55, 1794, 8, 1794, 66, 381, 250, 1794, 119, 55, 1794, 3, 1794, 16, 1794, 6, 349, 1796, 5, 1794, 7, 1794, 8, 1794, 184, 1794, 32, 275, 63, 1794, 57, 1794, 133, 1794, 33, 159, 14, 1794, 153, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796]\n",
            "[1792, 4, 1794, 76, 150, 1794, 70, 1794, 27, 1794, 249, 1794, 4, 1794, 424, 1794, 3, 1794, 41, 1794, 642, 5, 1796, 64, 1794, 1353, 601, 40, 23, 1794, 5, 1794, 10, 1794, 31, 1794, 27, 1794, 17, 134, 6, 1796, 7, 1794, 15, 299, 1794, 15, 61, 1794, 21, 1794, 65, 63, 1794, 486, 161, 1794, 1014, 5, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 59, 1794, 159, 6, 1794, 3, 1794, 16, 1794, 25, 48, 1794, 89, 1794, 53, 1794, 27, 24, 32, 1794, 4, 1794, 3, 1794, 203, 6, 1796, 6, 1794, 65, 63, 1794, 37, 14, 1794, 27, 164, 1794, 84, 34, 1794, 255, 189, 1796, 21, 1794, 7, 1794, 128, 22, 1794, 7, 1794, 166, 13, 1794, 56, 1794, 166, 536, 1794, 624, 6, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 22, 1794, 18, 89, 1794, 3, 1794, 16, 1794, 45, 492, 1794, 5, 42, 29, 1794, 21, 1794, 7, 1794, 41, 1794, 25, 189, 1796, 35, 1794, 18, 76, 246, 1794, 37, 63, 1794, 4, 1794, 7, 1794, 127, 1794, 3, 1794, 217, 11, 14, 1796, 6, 1794, 234, 43, 1794, 6, 573, 106, 1794, 80, 1794, 3, 1794, 5, 562, 209, 1794, 26, 189, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 8, 1794, 66, 43, 1794, 246, 1794, 13, 156, 30, 1794, 577, 36, 1794, 4, 1794, 353, 14, 1796, 119, 55, 1794, 8, 1794, 66, 381, 250, 1794, 119, 55, 1794, 3, 1794, 16, 1794, 6, 349, 1796, 5, 1794, 7, 1794, 8, 1794, 184, 1794, 32, 275, 63, 1794, 57, 1794, 133, 1794, 33, 159, 14, 1794, 153, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796]\n",
            "304\n",
            "[1792, 40, 1794, 98, 14, 1794, 119, 30, 1794, 184, 1794, 7, 1794, 44, 1794, 38, 1794, 121, 501, 1796, 6, 66, 14, 1794, 18, 1794, 34, 1794, 247, 1794, 106, 160, 10, 1794, 301, 432, 9, 1796, 36, 1794, 60, 8, 1794, 245, 8, 1794, 255, 30, 1794, 27, 1794, 6, 1065, 501, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 49, 190, 1794, 199, 22, 9, 1794, 56, 1794, 151, 12, 1794, 81, 38, 253, 9, 1796, 15, 8, 1794, 184, 1794, 265, 39, 1794, 6, 700, 645, 1794, 34, 1794, 230, 48, 1796, 7, 1794, 74, 289, 742, 1794, 12, 44, 1794, 131, 1794, 3, 1794, 53, 1794, 3, 1794, 456, 1794, 253, 9, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 118, 1794, 94, 8, 1794, 40, 1794, 114, 43, 1794, 251, 108, 39, 1794, 50, 1794, 203, 1794, 37, 48, 1796, 5, 1794, 197, 1794, 24, 123, 1794, 13, 559, 1794, 8, 340, 218, 1794, 19, 1794, 179, 37, 1796, 46, 1794, 3, 1794, 330, 1794, 176, 440, 1794, 214, 1794, 6, 1794, 65, 75, 1794, 1353, 211, 1794, 27, 48, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 149, 14, 1794, 12, 484, 1794, 6, 1794, 293, 20, 1794, 353, 37, 1794, 4, 1794, 173, 37, 1796, 12, 26, 1794, 20, 1794, 209, 43, 1794, 21, 101, 24, 1794, 50, 8, 1794, 82, 33, 1796, 18, 1794, 8, 1794, 162, 1794, 59, 1794, 79, 1794, 18, 1794, 34, 1794, 22, 29, 1794, 58, 36, 37, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796]\n",
            "[1792, 40, 1794, 98, 14, 1794, 119, 30, 1794, 184, 1794, 7, 1794, 44, 1794, 38, 1794, 121, 501, 1796, 6, 66, 14, 1794, 18, 1794, 34, 1794, 247, 1794, 106, 160, 10, 1794, 301, 432, 9, 1796, 36, 1794, 60, 8, 1794, 245, 8, 1794, 255, 30, 1794, 27, 1794, 6, 1065, 501, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 49, 190, 1794, 199, 22, 9, 1794, 56, 1794, 151, 12, 1794, 81, 38, 253, 9, 1796, 15, 8, 1794, 184, 1794, 265, 39, 1794, 6, 700, 645, 1794, 34, 1794, 230, 48, 1796, 7, 1794, 74, 289, 742, 1794, 12, 44, 1794, 131, 1794, 3, 1794, 53, 1794, 3, 1794, 456, 1794, 253, 9, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 118, 1794, 94, 8, 1794, 40, 1794, 114, 43, 1794, 251, 108, 39, 1794, 50, 1794, 203, 1794, 37, 48, 1796, 5, 1794, 197, 1794, 24, 123, 1794, 13, 559, 1794, 8, 340, 218, 1794, 19, 1794, 179, 37, 1796, 46, 1794, 3, 1794, 330, 1794, 176, 440, 1794, 214, 1794, 6, 1794, 65, 75, 1794, 1353, 211, 1794, 27, 48, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 149, 14, 1794, 12, 484, 1794, 6, 1794, 293, 20, 1794, 353, 37, 1794, 4, 1794, 173, 37, 1796, 12, 26, 1794, 20, 1794, 209, 43, 1794, 21, 101, 24, 1794, 50, 8, 1794, 82, 33, 1796, 18, 1794, 8, 1794, 162, 1794, 59, 1794, 79, 1794, 18, 1794, 34, 1794, 22, 29, 1794, 58, 36, 37, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796]\n",
            "304\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIxbfp-lZ8Rl",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Evaluation\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "def ngrams_plagiarism(generated_text, n=4):\n",
        "    # the tokenizer is used to remove non-alphanumeric symbols\n",
        "    tokenizer = tfds.features.text.Tokenizer()\n",
        "    with open(\"divcom.txt\") as f:\n",
        "        original_text = f.read()\n",
        "    original_text = tokenizer.join(tokenizer.tokenize(original_text.lower()))\n",
        "    generated_text_tokens = tokenizer.tokenize(generated_text.lower())\n",
        "\n",
        "    total_ngrams = len(generated_text_tokens) - n + 1\n",
        "    plagiarism_counter = 0\n",
        "\n",
        "    for i in range(total_ngrams):\n",
        "        ngram = tokenizer.join(generated_text_tokens[i:i+n])\n",
        "        plagiarism_counter += 1 if ngram in original_text else 0\n",
        "    return 1 - (plagiarism_counter / total_ngrams)\n",
        "\n",
        "# coding=utf-8\n",
        "\n",
        "# Syllabification module.\n",
        "# A special thanks goes to Simona S., Italian linguist, teacher and friend, without whom this module could never exist.\n",
        "\n",
        "# This module is used both for building the dataset and for computing metrics.\n",
        "# IMPORTANT: the #, @ and § characters are used internally to correctly split syllables, the input string should not contain them.\n",
        "\n",
        "# Splits a string along word boundaries (empty spaces and punctuation marks). If synalepha is True, doesn't split\n",
        "# words which have a vowel boundary (eg. selva_oscura).\n",
        "def split_words(str, synalepha=False):\n",
        "    regex = re.compile(r\"\"\"[,.;:\"“”«»?—'`‘’\\s]*\\s+[,.;:\"“”«»?—'`‘’\\s]*\"\"\")\n",
        "    matches = regex.finditer(str)\n",
        "    indexes = [0]\n",
        "\n",
        "    for m in matches:\n",
        "        begin = (m.start() - 1) if m.start() - 1 > 0 else 0\n",
        "        end = m.end() + 1\n",
        "        if _is_split_acceptable(str[begin: end], synalepha):\n",
        "            indexes.append(begin + 1)\n",
        "\n",
        "    return [str[i:j] for i,j in zip(indexes, indexes[1:]+[None])]\n",
        "\n",
        "# Splits a single word into syllables.\n",
        "def syllabify_word(str):\n",
        "    return _perform_final_splits(_perform_initial_splits(str))\n",
        "\n",
        "# Splits a block into words and then into syllables.\n",
        "def syllabify_block(str, synalepha=False):\n",
        "    words = split_words(str, synalepha)\n",
        "    syllables = [syllabify_word(w) for w in words]\n",
        "    return \"#\".join(syllables)\n",
        "\n",
        "# Removes capitalization, punctuation marks and, optionally, diacritics (accents and dieresis).\n",
        "def prettify(str, keep_diacritics=True):\n",
        "    if keep_diacritics:\n",
        "        out = _strip_spaces(_strip_punctuaction(str.lower()))\n",
        "    else:\n",
        "        out = _strip_spaces(_strip_punctuaction(_remove_diacritics(str.lower())))\n",
        "    return out\n",
        "\n",
        "# Removes hash characters from a string.\n",
        "def strip_hashes(str):\n",
        "    return re.sub(\"#\", \"\", str)\n",
        "\n",
        "# Determines if a split between two words is acceptable, ie. if there are no synalepha nor elision (eg. \"l' amico\" should be kept together).\n",
        "# Heuristic: all apostrophes are considered a non-breakable point. This is not always the case (eg. \"perch’ i’ fu’\" should be split into \"perch’ i’\"-\"fu’).\n",
        "def _is_split_acceptable(str, synalepha=False):\n",
        "    prev = str[0]\n",
        "    next = str[len(str) - 1]\n",
        "    vowel = re.compile(r\"\"\"[AEIOUaeiouàèéìòóùÈ]\"\"\")\n",
        "    apostrophe = re.compile(r\"\"\".*['`‘’].*\"\"\")\n",
        "    newline = re.compile(r\"\"\".*\\n+.*\"\"\")\n",
        "\n",
        "    out = newline.match(str) or \\\n",
        "          not (apostrophe.match(str) and (vowel.match(prev) or vowel.match(next)))\n",
        "\n",
        "    if synalepha:\n",
        "        out = out and not (vowel.match(prev) and vowel.match(next))\n",
        "\n",
        "    return out\n",
        "\n",
        "# Removes punctuation from a string.\n",
        "def _strip_punctuaction(str):\n",
        "    return re.sub(r\"\"\"[,.;:\"“”!?«»—'`’]+\"\"\", \"\", str)\n",
        "\n",
        "# Removes diacritic marks from a string.\n",
        "def _remove_diacritics(str):\n",
        "    out = re.sub(r\"\"\"[àä]\"\"\", \"a\", str)\n",
        "    out = re.sub(r\"\"\"[èéë]\"\"\", \"e\", out)\n",
        "    out = re.sub(r\"\"\"[ìï]\"\"\", \"i\", out)\n",
        "    out = re.sub(r\"\"\"[òóö]\"\"\", \"o\", out)\n",
        "    out = re.sub(r\"\"\"[ùü]\"\"\", \"u\", out)\n",
        "    return out\n",
        "\n",
        "# Removes spaces from a string.\n",
        "def _strip_spaces(str):\n",
        "    return re.sub(r\"\"\"\\s+\"\"\", \"\", str)\n",
        "\n",
        "# Performs the first (easy and unambiguous) phase of syllabification.\n",
        "def _perform_initial_splits(str):\n",
        "    return _split_hiatus(_split_dieresis(_split_double_cons(_split_multiple_cons(str))))\n",
        "\n",
        "# Performs the second (difficult and heuristic) phase of syllabification.\n",
        "def _perform_final_splits(str):\n",
        "    cvcv = r\"\"\"(?i)([bcdfglmnpqrstvz][,.;:\"“”«»?—'`‘’\\s]*[aeiouàèéìóòùÈËÏ]+)([bcdfglmnpqrstvz]+[,.;:\"“”«»?—'`‘’\\s]*[aeiouàèéìóòùÈËÏ]+)\"\"\"\n",
        "    vcv = r\"\"\"(?i)([aeiouàèéìóòùÈËÏ]+)([bcdfglmnpqrstvz]+[,.;:\"“”«»?—'`‘’\\s]*[aeiouàèéìóòùÈËÏ]+)\"\"\"\n",
        "    vv = r\"\"\"(?i)(?<=[aeiouàèéìóòùÈËÏ])(?=[aeiouàèéìóòùÈËÏ])\"\"\"\n",
        "\n",
        "    # Split the contoid vocoid - contoid vocoid case (eg. ca-ne). Deterministic.\n",
        "    out = re.sub(cvcv, r\"\"\"\\1#\\2\"\"\", str)\n",
        "    # Split the vocoid - contoid vocoid case (eg. ae-reo). Deterministic.\n",
        "    out = re.sub(vcv, r\"\"\"\\1#\\2\"\"\", out)\n",
        "\n",
        "    # Split the vocoid - vocoid case (eg. a-iuola). Heuristic.\n",
        "    out = _clump_diphthongs(out)\n",
        "    out = re.sub(vv, r\"\"\"#\"\"\", out)\n",
        "    out = re.sub(\"§\", \"\", out)\n",
        "\n",
        "    return out\n",
        "\n",
        "# Splits double consonants (eg. al-legro)\n",
        "def _split_double_cons(str):\n",
        "    doubles = re.compile(r\"\"\"(?i)(([bcdfglmnpqrstvz])(?=\\2)|c(?=q))\"\"\")\n",
        "    return \"#\".join(doubles.sub(r\"\"\"\\1@\"\"\", str).split(\"@\"))\n",
        "\n",
        "# Splits multiple consonants, except: impure s (sc, sg, etc.), mute followed by liquide (eg. tr), digrams and trigrams.\n",
        "def _split_multiple_cons(str):\n",
        "    impures = re.compile(r\"\"\"(?i)(s(?=[bcdfghlmnpqrtvz]))\"\"\")\n",
        "    muteliquide = re.compile(r\"\"\"(?i)([bcdgpt](?=[lr]))\"\"\")\n",
        "    digrams = re.compile(r\"\"\"(?i)(g(?=li)|g(?=n[aeiou])|s(?=c[ei])|[cg](?=h[eèéiì])|[cg](?=i[aou]))\"\"\")\n",
        "    trigrams = re.compile(r\"\"\"(?i)(g(?=li[aou])|s(?=ci[aou]))\"\"\")\n",
        "    multicons = re.compile(r\"\"\"(?i)([bcdfglmnpqrstvz](?=[bcdfglmnpqrstvz]+))\"\"\")\n",
        "\n",
        "    # Preserve non admissibile splits.\n",
        "    out =\"§\".join(impures.sub(r\"\"\"\\1@\"\"\", str).split(\"@\"))\n",
        "    out = \"§\".join(muteliquide.sub(r\"\"\"\\1@\"\"\", out).split(\"@\"))\n",
        "    out = \"§\".join(digrams.sub(r\"\"\"\\1@\"\"\", out).split(\"@\"))\n",
        "    out = \"§\".join(trigrams.sub(r\"\"\"\\1@\"\"\", out).split(\"@\"))\n",
        "    # Split everything else.\n",
        "    out = \"#\".join(multicons.sub(r\"\"\"\\1@\"\"\", out).split(\"@\"))\n",
        "\n",
        "    return \"\".join(re.split(\"§\", out))\n",
        "\n",
        "# Splits dieresis.\n",
        "def _split_dieresis(str):\n",
        "    dieresis = re.compile(r\"\"\"(?i)([äëïöüËÏ](?=[aeiou])|[aeiou](?=[äëïöüËÏ]))\"\"\")\n",
        "    return \"#\".join(dieresis.sub(r\"\"\"\\1@\"\"\", str).split(\"@\"))\n",
        "\n",
        "# Splits SURE hiatuses only. Ambiguous ones are heuristically considered diphthongs.\n",
        "def _split_hiatus(str):\n",
        "    hiatus = re.compile(r\"\"\"(?i)([aeoàèòóé](?=[aeoàèòóé])|[rb]i(?=[aeou])|tri(?=[aeou])|[ìù](?=[aeiou]))\"\"\")\n",
        "    return \"#\".join(hiatus.sub(r\"\"\"\\1@\"\"\", str).split(\"@\"))\n",
        "\n",
        "# Prevents splitting of diphthongs and triphthongs.\n",
        "def _clump_diphthongs(str):\n",
        "    diphthong = r\"\"\"(?i)(i[,.;:\"“”«»?—'`‘’\\s]*[aeouàèéòóù]|u[,.;:\"“”«»?—'`‘’\\s]*[aeioàèéìòó]|[aeouàèéòóù][,.;:\"“”«»?—'`‘’\\s]*i|[aeàèé][,.;:\"“”«»?—'`‘’\\s]*u)\"\"\"\n",
        "    diphthongsep = r\"\"\"(\\{.[,.;:\"“”«»?—'`‘’\\s]*)(.\\})\"\"\"\n",
        "    triphthong = r\"\"\"(?i)(i[àèé]i|u[àòó]i|iu[òó])\"\"\"\n",
        "    triphthongsep = r\"\"\"(\\{.)(.)(.\\})\"\"\"\n",
        "\n",
        "    out = re.sub(triphthong, r\"\"\"{\\1}\"\"\", str)\n",
        "    out = re.sub(triphthongsep, r\"\"\"\\1§\\2§\\3\"\"\", out)\n",
        "    out = re.sub(diphthong, r\"\"\"{\\1}\"\"\", out)\n",
        "    out = re.sub(diphthongsep, r\"\"\"\\1§\\2\"\"\", out)\n",
        "    out = re.sub(r\"\"\"[{}]\"\"\", \"\", out)\n",
        "\n",
        "    return out\n",
        "\n",
        "# coding=utf-8\n",
        "\n",
        "# Rhyme scoring and extraction module. Exploits informations about accents, syllables and heuristics to perform\n",
        "# the difficult task of determining if two words form a rhyme.\n",
        "\n",
        "# Computes a rhyming score between two words.\n",
        "def rhyme_score(w1, w2):\n",
        "    if w1 == \"\" or w2 == \"\": # One of the two words is missing.\n",
        "        return 0\n",
        "\n",
        "    pw1 = prettify(w1, True)  # preserving accents.\n",
        "    pw2 = prettify(w2, True)\n",
        "    ppw1 = prettify(w1, False)  # removing accents.\n",
        "    ppw2 = prettify(w2, False)\n",
        "    accent1 = _locate_accent(pw1)\n",
        "    accent2 = _locate_accent(pw2)\n",
        "\n",
        "    if accent1 == 0 and accent2 == 0: # Difficult case: no accent is known. Heuristic match.\n",
        "        out = _heuristic_rhyme(w1, w2)\n",
        "    elif accent1 == accent2: # Trivial case: both accents in the same position.\n",
        "        out = _match_syllable(ppw1[accent1:], ppw2[accent1:])\n",
        "    elif accent1 != 0 and accent2 == 0: # Trivial case: accent1 known.\n",
        "        out = _match_syllable(ppw1[accent1:], ppw2[accent1:])\n",
        "    elif accent2 != 0 and accent1 == 0: # Trivial case: accent2 known.\n",
        "        out = _match_syllable(ppw1[accent2:], ppw2[accent2:])\n",
        "    else: # Trivial case: both accents are known, but in different positions.\n",
        "        out = _match_syllable(ppw1[accent1:], ppw2[accent2:])\n",
        "\n",
        "    return out\n",
        "\n",
        "# Determines if a word is tronca (accent on the last syllable). Exact cases: word ending with an accented letter (morì) or word ending with a consonant (mangiàr).\n",
        "# Heuristic: NO other words are considered tronche since the majority of Italian words are piane (accent on the second to last syllable) or sdrucciole (third to last).\n",
        "def is_tronca(word):\n",
        "    consonant = re.compile(r\"\"\"[bcdfghlmnprstvz]\"\"\")\n",
        "    accentlastsyl = re.compile(r\"\"\".*#[^#]*[àèéìóòù][^#]*\"\"\")\n",
        "    w = prettify(word, True)\n",
        "    out = False\n",
        "\n",
        "    if w == \"\": # The \"word\" was actually composed by punctuation only.\n",
        "        out = False\n",
        "    elif consonant.match(w[-1]):\n",
        "        out = True\n",
        "    else:\n",
        "        sw = syllabify_word(w)\n",
        "        if sw.count(\"#\") == 0:\n",
        "            out = True\n",
        "        elif accentlastsyl.match(sw):\n",
        "            out = True\n",
        "        else:\n",
        "            out = False\n",
        "\n",
        "    return out\n",
        "\n",
        "# Not used:\n",
        "# def _is_piana(word): # Most common case.\n",
        "#     return not (_is_tronca(word) or _is_sdrucciola(word))\n",
        "#\n",
        "# def _is_sdrucciola(word): # Detected only if the accent is marked.\n",
        "#     accentlastsyl = re.compile(r\"\"\".*[àèéìóòù].*#.*#.*\"\"\") # The accent is marked and followed by at least two hashes.\n",
        "#     return accentlastsyl.match(s.syllabify_word(s.prettify(word, True)))\n",
        "\n",
        "# Returns the accent position FROM THE END of the word (eg. mangiò -> -1, dormìre -> -3).\n",
        "# NOTE: prettification is done by the caller, since it could change accent position.\n",
        "def _locate_accent(word):\n",
        "    accent = re.compile(r\"\"\"[àèéìóòù]\"\"\")\n",
        "    match = accent.search(word)\n",
        "    if match:\n",
        "        pos = match.start()\n",
        "    else:\n",
        "        pos = len(word)\n",
        "\n",
        "    return pos - len(word)\n",
        "\n",
        "# Determines a rhyming score if the two words don't have accents.\n",
        "def _heuristic_rhyme(w1, w2):\n",
        "    pw1 = prettify(w1, False)\n",
        "    pw2 = prettify(w2, False)\n",
        "\n",
        "    sw1 = syllabify_word(pw1).split(\"#\")\n",
        "    sw2 = syllabify_word(pw2).split(\"#\")\n",
        "\n",
        "    # Approximate match:\n",
        "    if is_tronca(w1) and is_tronca(w2): # Both words are tronche: match only the last syllable from the vowel.\n",
        "        out = _match_syllable(sw1[-1], sw2[-1])\n",
        "    else: # Both words are piane: match exactly the last syllable and the last-but-one from the vowel.\n",
        "        ssw1 = \"\".join(sw1[-2:])\n",
        "        ssw2 = \"\".join(sw2[-2:])\n",
        "        out = _match_syllable(ssw1, ssw2)\n",
        "\n",
        "    return out\n",
        "\n",
        "# Computes a score based on how many letters match from the end of the two strings, up to the last vowel of the first vocoid (eg. \"men#te\" vs. \"can#te\" tries to match ente and ante, computing a score of 0.75, while \"iuo#la\" vs. \"suo#la\" tries to match ola and ola, computing a score of 1.0).\n",
        "# HEURISTIC: since no accent is known, the match is as PERMISSIVE as possible (ie. matches from the LAST vowel of a diphthong). This rhymes correctly \"quivi/sorgivi\" (while a restrictive heuristic wouldn't).\n",
        "# The computed score is the sum of all matching characters (truncated at the first difference), weighted exponentially with the distance from the putative beginning of the rhyme.\n",
        "# As such, it's a score which can scale well on different matching lengths (eg. \"più\" and \"fu\" have a score similar to \"frangente\" and \"assolutamente\"), at the expenses of not having a \"natural\" meaning which could be easier to threshold.\n",
        "def _match_syllable(s1, s2):\n",
        "    lastvowel = re.compile(r\"\"\"[aeiou](?![aeiou])\"\"\") # Inside a syllable vowels can only be together, so only the NEXT character needs to be checked.\n",
        "\n",
        "    match1 = lastvowel.search(s1)\n",
        "    match2 = lastvowel.search(s2)\n",
        "\n",
        "    if match1 and match2:\n",
        "        ss1 = s1[match1.start():]\n",
        "        ss2 = s2[match2.start():]\n",
        "\n",
        "        # maxlength = len(ss2) if len(ss1) < len(ss2) else len(ss1) # The two lengths could be different.\n",
        "        minlength = len(ss1) if len(ss1) < len(ss2) else len(ss2)\n",
        "        out = 0.0\n",
        "        a = (ss1 if len(ss1) < len(ss2) else ss2)[::-1] # reverse.\n",
        "        b = (ss2 if len(ss1) < len(ss2) else ss1)[::-1]\n",
        "\n",
        "        i = 0\n",
        "        while (i < minlength) and (a[i] == b[i]): # Iterate only over the shared part of the string.\n",
        "            out += 2.0 ** (minlength - i)\n",
        "            i += 1\n",
        "        out /= 2 ** minlength\n",
        "    else:\n",
        "        out = 0.0\n",
        "\n",
        "    return out\n",
        "\n",
        "# Metrics evaluation module.\n",
        "\n",
        "# Evaluates metrics on a string, computing each value on a per-terzina basis and then outputting the average scores.\n",
        "# If verbose, outputs the scores referred to each terzina.\n",
        "def eval_txt(string, verbose=False, synalepha=False, permissive=True, rhyme_threshold=1.0):\n",
        "    terzine = _extract_terzine(string)\n",
        "\n",
        "    avg_hendecasyllabicness = 0.0\n",
        "    avg_rhymeness = 0.0\n",
        "    last_terzina = terzine[0]\n",
        "    for terzina in terzine[1:]:\n",
        "        hendecasyllabicness = _hendecasyllabicness(terzina, synalepha, permissive)\n",
        "        tmp = \"\\n\".join(terzina.split(\"\\n\")[1:])\n",
        "\n",
        "        # In order to properly check chaining, two terzine at the time need to be considered.\n",
        "        rhymeness = _rhymeness(last_terzina + tmp, rhyme_threshold)\n",
        "        avg_hendecasyllabicness += hendecasyllabicness\n",
        "        avg_rhymeness += rhymeness\n",
        "\n",
        "        last_terzina = terzina\n",
        "\n",
        "        if verbose:\n",
        "            print()\n",
        "            print(terzina)\n",
        "            print(\"Hendecasyllabicness: {}, Rhymeness: {}\".format(hendecasyllabicness, rhymeness))\n",
        "\n",
        "    print()\n",
        "    if len(terzine) > 0:\n",
        "        # Each \"optimal\" terzina has 5 lines, the last of which is shared with the next one\n",
        "        # (therefore a file with n perfect terzine has 4n + 2 lines, due to the final stray verse and empty line).\n",
        "        avg_structuredness = (4 * len(terzine) + 2) / len(string.split(\"\\n\"))\n",
        "        avg_hendecasyllabicness /= len(terzine)\n",
        "        avg_rhymeness /= len(terzine) - 1 # The rhymes on the first terzina are not checked.\n",
        "\n",
        "        return [\"Number of putative terzine: {}\".format((len(string.split(\"\\n\")) - 1) // 4),\n",
        "            \"Number of well formed terzine: {}\".format(len(terzine)),\n",
        "            \"Average structuredness: {}\".format(avg_structuredness),\n",
        "            \"Average hendecasyllabicness: {}\".format(avg_hendecasyllabicness),\n",
        "            \"Average rhymeness: {}\".format(avg_rhymeness),\n",
        "            \"N-grams plagiarism: {}\".format(ngrams_plagiarism(string))]\n",
        "    else:\n",
        "        print(\"ERROR: no valid terzina detected.\")\n",
        "\n",
        "# Hendecasyllabicness score. For each of the four verses in input, computes a score and returns their average.\n",
        "# The score is 1.0 if a verse has 10, 11 or 12 syllables, and decreases towards 0.0 the more the number of syllables diverges.\n",
        "# Syllabification is done using Italian grammar rules, ignoring synalepha.\n",
        "def _hendecasyllabicness(str, synalepha, permissive):\n",
        "    score  = 0.0\n",
        "    lines = str.split(\"\\n\")\n",
        "    for line in lines:\n",
        "        if line != \"\":\n",
        "            # In order to avoid cheating, strip all # characters and perform syllabification according to grammar.\n",
        "            tmp = syllabify_block(strip_hashes(line), synalepha)\n",
        "            if is_tronca(split_words(line, False)[-1]):\n",
        "                target = 10\n",
        "            else:\n",
        "                target = 11\n",
        "\n",
        "            syllables = [s for s in tmp.split(\"#\") if s != \"\"]\n",
        "            if not permissive or abs(len(syllables) - target) > 1: # Tolerate 10 and 12 syllables.\n",
        "                score += 1 - abs(len(syllables) - target) / target\n",
        "            else:\n",
        "                score += 1.0\n",
        "\n",
        "    return score / 4\n",
        "\n",
        "# Rhymeness score. In order to correctly detect chaining, TWO terzine need to be passed, but the score is referred only to the second one.\n",
        "# Since a terzina formally includes the stray verse which begins the next one, the rhyming scheme to be checked is the following:\n",
        "# don't care\n",
        "# B\n",
        "# don't care\n",
        "#\n",
        "# B\n",
        "# C\n",
        "# B\n",
        "#\n",
        "# C.\n",
        "# For each of the three rhymes (BB, CC and BB) assign 1.0 if the rhyme score (computed in an encoding-agnostic way in rhymes.py) is above 1.5.\n",
        "# NOTE: due to the intrinsic difficulty of formally define a rhyme, this threshold has no clear semantic and was chosen empirically.\n",
        "def _rhymeness(str, rhyme_threshold):\n",
        "    score = 0.0\n",
        "    last_words = _extract_last_words(str)\n",
        "\n",
        "    rhymes = []\n",
        "    rhymes.append(rhyme_score(last_words[1], last_words[3]))\n",
        "    rhymes.append(rhyme_score(last_words[3], last_words[5]))\n",
        "    # rhymes.append(rhyme_score(last_words[1], last_words[5])) # Is transitivity implied?\n",
        "    rhymes.append(rhyme_score(last_words[4], last_words[6]))\n",
        "\n",
        "    for rhyme in rhymes:\n",
        "        if rhyme >= rhyme_threshold:\n",
        "            score += 1.0\n",
        "\n",
        "    return score / len(rhymes)\n",
        "\n",
        "# Extracts a list of terzine from a string, skipping malformed lines.\n",
        "# Each well formed terzina has the following structure:\n",
        "# Verse\n",
        "# Verse\n",
        "# Verse\n",
        "#\n",
        "# Verse,\n",
        "# In order to correctly handle chaining, the last verse of each terzina is also the first verse of the next one.\n",
        "def _extract_terzine(str):\n",
        "    terzinaA = re.compile(r\"\"\"([^\\n]+\\n[^\\n]+\\n[^\\n]+\\n\\n[^\\n]+\\n)\"\"\") # Case LLL L. Extract 3 + 1 lines and then skip 4 lines.\n",
        "    terzinaB = re.compile(r\"\"\"[^\\n]+\\n([^\\n]+\\n\\n[^\\n]+\\n[^\\n]+\\n[^\\n]+)\"\"\") # Case LL LLL. Ignore 1 line, extract 1 + 3 lines and then skip 3 lines. After the skip, only case A can appear.\n",
        "    skipA = re.compile(r\"\"\"[^\\n]+\\n[^\\n]+\\n[^\\n]+(\\n\\n)?\"\"\")\n",
        "    skipB = re.compile(r\"\"\"[^\\n]+\\n[^\\n]+(\\n\\n)?\"\"\")\n",
        "    out = []\n",
        "    tmp = str\n",
        "\n",
        "    m = terzinaA.search(tmp)\n",
        "    if m:\n",
        "        while m:\n",
        "            out.append(m.group(0))\n",
        "            tmp = tmp[skipA.search(tmp).end():]\n",
        "            m = terzinaA.search(tmp)\n",
        "    else:\n",
        "        m = terzinaB.search(tmp)\n",
        "        if m:\n",
        "            out.append(m.group(0)) # The regex will not capture the first line.\n",
        "            tmp = tmp[skipB.search(tmp).end():]\n",
        "            m = terzinaA.search(tmp)  # After the first skip, the case A appears.\n",
        "            while m:\n",
        "                out.append(m.group(0))\n",
        "                tmp = tmp[skipA.search(tmp).end():]\n",
        "                m = terzinaA.search(tmp)\n",
        "\n",
        "    return out\n",
        "\n",
        "# Extract the last words from each verse of a string.\n",
        "# NOTE: empty lines are skipped.\n",
        "def _extract_last_words(str):\n",
        "    lines = str.split(\"\\n\")\n",
        "\n",
        "    verses = [l for l in lines if l != \"\"]\n",
        "    words = [split_words(v, False)[-1] for v in verses]\n",
        "    out = [strip_hashes(prettify(w, True)) for w in words]\n",
        "    return out"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEun8YPR8Sn6",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Generation\n",
        "def generate():\n",
        "    def evaluate_greedy(inp_sentence, decoder_input):\n",
        "        inp_sentence = inp_sentence\n",
        "        encoder_input = tf.expand_dims(inp_sentence, 0)\n",
        "        \n",
        "        output = tf.expand_dims(decoder_input, 0)\n",
        "\n",
        "        terces = 0\n",
        "        for i in range(batch_len):\n",
        "            enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "                encoder_input, output)\n",
        "        \n",
        "            # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "            predictions, attention_weights = transformer(encoder_input, \n",
        "                                                        output,\n",
        "                                                        False,\n",
        "                                                        enc_padding_mask,\n",
        "                                                        combined_mask,\n",
        "                                                        dec_padding_mask)\n",
        "            \n",
        "            # select the last word from the seq_len dimension\n",
        "            predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "            predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "            # return the result if the predicted_id is equal to the end token\n",
        "            if predicted_id == eos:\n",
        "                terces += 1\n",
        "                if terces == 3:\n",
        "                    return tf.squeeze(output, axis=0), attention_weights\n",
        "            # concatentate the predicted_id to the output which is given to the decoder\n",
        "            # as its input.\n",
        "            output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "        return tf.squeeze(output, axis=0), attention_weights\n",
        "\n",
        "\n",
        "    def evaluate_topk(inp_sentence, decoder_input, k=5, temperature=0.5):\n",
        "        inp_sentence = inp_sentence\n",
        "        encoder_input = tf.expand_dims(inp_sentence, 0)\n",
        "        \n",
        "        output = tf.expand_dims(decoder_input, 0)\n",
        "\n",
        "        def scale(tensor):\n",
        "            tensor = tf.math.divide(\n",
        "                tf.subtract(\n",
        "                    tensor, \n",
        "                    tf.reduce_min(tensor)\n",
        "                ), \n",
        "                tf.subtract(\n",
        "                    tf.reduce_max(tensor), \n",
        "                    tf.reduce_min(tensor))\n",
        "                )\n",
        "            return tensor\n",
        "\n",
        "        tt = temperature\n",
        "        tk = k\n",
        "        terces = 0\n",
        "        for i in range(batch_len):\n",
        "            enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "                encoder_input, output)\n",
        "        \n",
        "            # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "            predictions, attention_weights = transformer(encoder_input, \n",
        "                                                        output,\n",
        "                                                        False,\n",
        "                                                        enc_padding_mask,\n",
        "                                                        combined_mask,\n",
        "                                                        dec_padding_mask)\n",
        "            \n",
        "            # select the last word from the seq_len dimension\n",
        "            predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
        "            predictions, indices = tf.math.top_k(predictions,k=k)\n",
        "            predictions /= temperature\n",
        "            #predictions = scale(predictions)\n",
        "            predictions = np.squeeze(predictions, axis=0)\n",
        "            indices = np.squeeze(indices, axis=0)\n",
        "            indices = np.squeeze(indices, axis=0)\n",
        "            predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "            predicted_id = indices[predicted_id]\n",
        "\n",
        "            # return the result if the predicted_id is equal to the end token\n",
        "            if predicted_id == eos:\n",
        "                terces += 1\n",
        "                if terces == terces_per_batch-1:\n",
        "                    return tf.squeeze(output, axis=0), attention_weights\n",
        "            # concatentate the predicted_id to the output which is given to the decoder\n",
        "            # as its input.\n",
        "            predicted_id = tf.expand_dims(predicted_id, 0)\n",
        "            predicted_id = tf.expand_dims(predicted_id, 0)\n",
        "            output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "        return tf.squeeze(output, axis=0), attention_weights\n",
        "\n",
        "\n",
        "    def plot_attention_weights(attention, sentence, result, layer):\n",
        "        fig = plt.figure(figsize=(32, 16))\n",
        "        attention = tf.squeeze(attention[layer], axis=0)\n",
        "        for head in range(attention.shape[0]):\n",
        "            ax = fig.add_subplot(2, 4, head+1)\n",
        "            # plot the attention weights\n",
        "            ax.matshow(attention[head][:-1, :], cmap='viridis')\n",
        "            fontdict = {'fontsize': 10}\n",
        "            ax.set_xticks(range(len(sentence)+2))\n",
        "            ax.set_yticks(range(len(result)))\n",
        "            ax.set_ylim(len(result)-1.5, -0.5)\n",
        "            ax.set_xticklabels(sentence, fontdict=fontdict, rotation=90)\n",
        "            ax.set_yticklabels(result, fontdict=fontdict)\n",
        "            ax.set_xlabel('Head {}'.format(head+1))\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    out_list = test_b[0][0]\n",
        "    print(seq2str(out_list)+\"---------------------------\")\n",
        "\n",
        "    offset = 75 # a tercet\n",
        "    txt_gen = seq2str(out_list[-offset:])+\"\\n\"\n",
        "    k=1\n",
        "    t=0.5\n",
        "    for i in range(32//(terces_per_batch-1)): # 30 terces = cantica\n",
        "        out, att_w = evaluate_topk([pad], out_list[-offset:], k, t)\n",
        "        aa = out.numpy().tolist()\n",
        "        '''\n",
        "        if i==0: #only once\n",
        "            plot_attention_weights(att_w, out_list, aa, 'decoder_layer1_block1')\n",
        "            plot_attention_weights(att_w, out_list, aa, 'decoder_layer2_block1')\n",
        "            plot_attention_weights(att_w, out_list, aa, 'decoder_layer3_block1')\n",
        "            plot_attention_weights(att_w, out_list, aa, 'decoder_layer4_block1')\n",
        "        '''\n",
        "        out_list = aa\n",
        "        out_str = seq2str(out_list[offset:])\n",
        "        txt_gen += out_str + \"\\n\"\n",
        "        print(out_str) \n",
        "\n",
        "    try:\n",
        "        evaluation = eval_txt(txt_gen, synalepha=True, permissive=False, rhyme_threshold=1.0)\n",
        "    except:\n",
        "        evaluation = [\"Unknow error\"]\n",
        "    wandb.log({\"generated\":\n",
        "            wandb.Html(\"k=\"+str(k)+\" t=\"+str(t)+\n",
        "                       \"<pre>\"+txt_gen+\"</pre>\"+\n",
        "                       \"<pre>\"+\"\\n\".join(evaluation)+\"</pre>\", inject=False)})"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwdCuj3HsFux",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c0b4b658-6c56-4b8d-aa6a-2dedc04ed487"
      },
      "source": [
        " #@title Train loop\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    random.shuffle(batches)\n",
        "    start = time.time()\n",
        "    \n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    \n",
        "    for (batch, (inp, tar)) in enumerate(batches):\n",
        "        if (len(inp) != batch_len or len(tar) != batch_len):\n",
        "            print(\"discarded batch\", batch)\n",
        "            continue\n",
        "        train_step(np.expand_dims(inp, axis=0), np.expand_dims(tar, axis=0))\n",
        "        \n",
        "        if batch % 50 == 0:\n",
        "            print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
        "                epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
        "        \n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        ckpt_save_path = ckpt_manager.save()\n",
        "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n",
        "\n",
        "    print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, train_loss.result(), train_accuracy.result()))\n",
        "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))\n",
        "\n",
        "    wandb.log({\n",
        "        'train_loss': train_loss.result(),\n",
        "        'train_accuracy': train_accuracy.result()\n",
        "    }, step=epoch+1)\n",
        "\n",
        "    # validation\n",
        "    if epoch % 5 == 0:\n",
        "        loss_l, acc_l = [], []\n",
        "        for (batch, (inp, tar)) in enumerate(val_b):\n",
        "            val_loss.reset_states()\n",
        "            val_accuracy.reset_states()\n",
        "            \n",
        "            if (len(inp) != batch_len or len(tar) != batch_len):\n",
        "                print(\"discarded batch\", batch)\n",
        "                continue\n",
        "\n",
        "            val_step(np.expand_dims(inp, axis=0), np.expand_dims(tar, axis=0))\n",
        "\n",
        "            loss_l.append(val_loss.result())\n",
        "            acc_l.append(val_accuracy.result())\n",
        "\n",
        "        loss_mean = sum(loss_l)/len(loss_l)\n",
        "        acc_mean = sum(acc_l)/len(acc_l)\n",
        "        print('Epoch {} VALIDATION: Loss {:.4f} Accuracy {:.4f}\\n'.format(epoch + 1, loss_mean, acc_mean))\n",
        "\n",
        "        wandb.log({\n",
        "            'val_loss': loss_mean,\n",
        "            'val_accuracy': acc_mean\n",
        "        }, step=epoch+1)\n",
        "\n",
        "    # generation\n",
        "    if epoch in [10, 25, 50, 100, 200, 299]:\n",
        "        generate()\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch 129 Batch 450 Loss 1.2428 Accuracy 0.7159\n",
            "Epoch 129 Batch 500 Loss 1.2446 Accuracy 0.7160\n",
            "discarded batch 518\n",
            "Epoch 129 Batch 550 Loss 1.2449 Accuracy 0.7158\n",
            "Epoch 129 Batch 600 Loss 1.2468 Accuracy 0.7157\n",
            "Epoch 129 Batch 650 Loss 1.2473 Accuracy 0.7154\n",
            "Epoch 129 Batch 700 Loss 1.2502 Accuracy 0.7152\n",
            "Epoch 129 Batch 750 Loss 1.2522 Accuracy 0.7148\n",
            "Epoch 129 Batch 800 Loss 1.2557 Accuracy 0.7141\n",
            "Epoch 129 Batch 850 Loss 1.2559 Accuracy 0.7141\n",
            "Epoch 129 Batch 900 Loss 1.2576 Accuracy 0.7138\n",
            "Epoch 129 Batch 950 Loss 1.2595 Accuracy 0.7134\n",
            "Epoch 129 Batch 1000 Loss 1.2613 Accuracy 0.7129\n",
            "Epoch 129 Batch 1050 Loss 1.2643 Accuracy 0.7125\n",
            "Epoch 129 Batch 1100 Loss 1.2652 Accuracy 0.7124\n",
            "Epoch 129 Loss 1.2662 Accuracy 0.7122\n",
            "Time taken for 1 epoch: 22.245359182357788 secs\n",
            "\n",
            "Epoch 130 Batch 0 Loss 1.1363 Accuracy 0.7327\n",
            "Epoch 130 Batch 50 Loss 1.2053 Accuracy 0.7212\n",
            "Epoch 130 Batch 100 Loss 1.2131 Accuracy 0.7226\n",
            "Epoch 130 Batch 150 Loss 1.2197 Accuracy 0.7207\n",
            "Epoch 130 Batch 200 Loss 1.2238 Accuracy 0.7203\n",
            "Epoch 130 Batch 250 Loss 1.2256 Accuracy 0.7189\n",
            "Epoch 130 Batch 300 Loss 1.2299 Accuracy 0.7184\n",
            "Epoch 130 Batch 350 Loss 1.2300 Accuracy 0.7182\n",
            "Epoch 130 Batch 400 Loss 1.2305 Accuracy 0.7181\n",
            "discarded batch 418\n",
            "Epoch 130 Batch 450 Loss 1.2324 Accuracy 0.7176\n",
            "Epoch 130 Batch 500 Loss 1.2350 Accuracy 0.7169\n",
            "Epoch 130 Batch 550 Loss 1.2376 Accuracy 0.7165\n",
            "Epoch 130 Batch 600 Loss 1.2402 Accuracy 0.7160\n",
            "Epoch 130 Batch 650 Loss 1.2422 Accuracy 0.7154\n",
            "Epoch 130 Batch 700 Loss 1.2451 Accuracy 0.7149\n",
            "Epoch 130 Batch 750 Loss 1.2462 Accuracy 0.7147\n",
            "Epoch 130 Batch 800 Loss 1.2469 Accuracy 0.7147\n",
            "Epoch 130 Batch 850 Loss 1.2495 Accuracy 0.7144\n",
            "Epoch 130 Batch 900 Loss 1.2514 Accuracy 0.7140\n",
            "Epoch 130 Batch 950 Loss 1.2547 Accuracy 0.7136\n",
            "Epoch 130 Batch 1000 Loss 1.2565 Accuracy 0.7134\n",
            "Epoch 130 Batch 1050 Loss 1.2579 Accuracy 0.7133\n",
            "Epoch 130 Batch 1100 Loss 1.2598 Accuracy 0.7130\n",
            "Saving checkpoint for epoch 130 at ./checkpoints/train/ckpt-26\n",
            "Epoch 130 Loss 1.2605 Accuracy 0.7130\n",
            "Time taken for 1 epoch: 22.559783935546875 secs\n",
            "\n",
            "Epoch 131 Batch 0 Loss 1.1228 Accuracy 0.7360\n",
            "Epoch 131 Batch 50 Loss 1.2001 Accuracy 0.7259\n",
            "Epoch 131 Batch 100 Loss 1.2146 Accuracy 0.7221\n",
            "Epoch 131 Batch 150 Loss 1.2083 Accuracy 0.7227\n",
            "Epoch 131 Batch 200 Loss 1.2117 Accuracy 0.7221\n",
            "Epoch 131 Batch 250 Loss 1.2150 Accuracy 0.7219\n",
            "Epoch 131 Batch 300 Loss 1.2185 Accuracy 0.7215\n",
            "Epoch 131 Batch 350 Loss 1.2224 Accuracy 0.7205\n",
            "Epoch 131 Batch 400 Loss 1.2277 Accuracy 0.7194\n",
            "Epoch 131 Batch 450 Loss 1.2299 Accuracy 0.7191\n",
            "Epoch 131 Batch 500 Loss 1.2312 Accuracy 0.7190\n",
            "Epoch 131 Batch 550 Loss 1.2342 Accuracy 0.7189\n",
            "Epoch 131 Batch 600 Loss 1.2355 Accuracy 0.7186\n",
            "Epoch 131 Batch 650 Loss 1.2388 Accuracy 0.7182\n",
            "Epoch 131 Batch 700 Loss 1.2412 Accuracy 0.7175\n",
            "Epoch 131 Batch 750 Loss 1.2446 Accuracy 0.7168\n",
            "discarded batch 767\n",
            "Epoch 131 Batch 800 Loss 1.2454 Accuracy 0.7165\n",
            "Epoch 131 Batch 850 Loss 1.2486 Accuracy 0.7161\n",
            "Epoch 131 Batch 900 Loss 1.2508 Accuracy 0.7155\n",
            "Epoch 131 Batch 950 Loss 1.2538 Accuracy 0.7148\n",
            "Epoch 131 Batch 1000 Loss 1.2558 Accuracy 0.7147\n",
            "Epoch 131 Batch 1050 Loss 1.2571 Accuracy 0.7145\n",
            "Epoch 131 Batch 1100 Loss 1.2588 Accuracy 0.7141\n",
            "Epoch 131 Loss 1.2616 Accuracy 0.7138\n",
            "Time taken for 1 epoch: 22.40792727470398 secs\n",
            "\n",
            "discarded batch 23\n",
            "Epoch 131 VALIDATION: Loss 2.5305 Accuracy 0.5774\n",
            "\n",
            "Epoch 132 Batch 0 Loss 1.1109 Accuracy 0.7360\n",
            "Epoch 132 Batch 50 Loss 1.2136 Accuracy 0.7176\n",
            "Epoch 132 Batch 100 Loss 1.2094 Accuracy 0.7211\n",
            "Epoch 132 Batch 150 Loss 1.2174 Accuracy 0.7216\n",
            "Epoch 132 Batch 200 Loss 1.2200 Accuracy 0.7209\n",
            "Epoch 132 Batch 250 Loss 1.2222 Accuracy 0.7205\n",
            "Epoch 132 Batch 300 Loss 1.2214 Accuracy 0.7202\n",
            "Epoch 132 Batch 350 Loss 1.2260 Accuracy 0.7200\n",
            "Epoch 132 Batch 400 Loss 1.2298 Accuracy 0.7186\n",
            "Epoch 132 Batch 450 Loss 1.2309 Accuracy 0.7181\n",
            "Epoch 132 Batch 500 Loss 1.2348 Accuracy 0.7174\n",
            "Epoch 132 Batch 550 Loss 1.2362 Accuracy 0.7172\n",
            "Epoch 132 Batch 600 Loss 1.2390 Accuracy 0.7171\n",
            "Epoch 132 Batch 650 Loss 1.2405 Accuracy 0.7169\n",
            "Epoch 132 Batch 700 Loss 1.2427 Accuracy 0.7162\n",
            "discarded batch 715\n",
            "Epoch 132 Batch 750 Loss 1.2446 Accuracy 0.7163\n",
            "Epoch 132 Batch 800 Loss 1.2452 Accuracy 0.7161\n",
            "Epoch 132 Batch 850 Loss 1.2475 Accuracy 0.7158\n",
            "Epoch 132 Batch 900 Loss 1.2486 Accuracy 0.7155\n",
            "Epoch 132 Batch 950 Loss 1.2499 Accuracy 0.7154\n",
            "Epoch 132 Batch 1000 Loss 1.2520 Accuracy 0.7153\n",
            "Epoch 132 Batch 1050 Loss 1.2544 Accuracy 0.7147\n",
            "Epoch 132 Batch 1100 Loss 1.2557 Accuracy 0.7146\n",
            "Epoch 132 Loss 1.2567 Accuracy 0.7143\n",
            "Time taken for 1 epoch: 22.35014820098877 secs\n",
            "\n",
            "Epoch 133 Batch 0 Loss 1.1136 Accuracy 0.7228\n",
            "Epoch 133 Batch 50 Loss 1.1951 Accuracy 0.7245\n",
            "Epoch 133 Batch 100 Loss 1.2028 Accuracy 0.7209\n",
            "Epoch 133 Batch 150 Loss 1.2123 Accuracy 0.7198\n",
            "Epoch 133 Batch 200 Loss 1.2122 Accuracy 0.7202\n",
            "discarded batch 246\n",
            "Epoch 133 Batch 250 Loss 1.2114 Accuracy 0.7206\n",
            "Epoch 133 Batch 300 Loss 1.2166 Accuracy 0.7196\n",
            "Epoch 133 Batch 350 Loss 1.2205 Accuracy 0.7194\n",
            "Epoch 133 Batch 400 Loss 1.2225 Accuracy 0.7186\n",
            "Epoch 133 Batch 450 Loss 1.2252 Accuracy 0.7182\n",
            "Epoch 133 Batch 500 Loss 1.2291 Accuracy 0.7176\n",
            "Epoch 133 Batch 550 Loss 1.2309 Accuracy 0.7174\n",
            "Epoch 133 Batch 600 Loss 1.2335 Accuracy 0.7168\n",
            "Epoch 133 Batch 650 Loss 1.2377 Accuracy 0.7160\n",
            "Epoch 133 Batch 700 Loss 1.2397 Accuracy 0.7156\n",
            "Epoch 133 Batch 750 Loss 1.2426 Accuracy 0.7152\n",
            "Epoch 133 Batch 800 Loss 1.2452 Accuracy 0.7149\n",
            "Epoch 133 Batch 850 Loss 1.2471 Accuracy 0.7145\n",
            "Epoch 133 Batch 900 Loss 1.2487 Accuracy 0.7142\n",
            "Epoch 133 Batch 950 Loss 1.2497 Accuracy 0.7144\n",
            "Epoch 133 Batch 1000 Loss 1.2516 Accuracy 0.7141\n",
            "Epoch 133 Batch 1050 Loss 1.2528 Accuracy 0.7140\n",
            "Epoch 133 Batch 1100 Loss 1.2536 Accuracy 0.7140\n",
            "Epoch 133 Loss 1.2547 Accuracy 0.7139\n",
            "Time taken for 1 epoch: 22.304900884628296 secs\n",
            "\n",
            "Epoch 134 Batch 0 Loss 1.1101 Accuracy 0.7459\n",
            "Epoch 134 Batch 50 Loss 1.1949 Accuracy 0.7255\n",
            "Epoch 134 Batch 100 Loss 1.1934 Accuracy 0.7250\n",
            "Epoch 134 Batch 150 Loss 1.2070 Accuracy 0.7222\n",
            "Epoch 134 Batch 200 Loss 1.2099 Accuracy 0.7220\n",
            "Epoch 134 Batch 250 Loss 1.2178 Accuracy 0.7206\n",
            "Epoch 134 Batch 300 Loss 1.2203 Accuracy 0.7209\n",
            "Epoch 134 Batch 350 Loss 1.2237 Accuracy 0.7196\n",
            "Epoch 134 Batch 400 Loss 1.2258 Accuracy 0.7189\n",
            "discarded batch 412\n",
            "Epoch 134 Batch 450 Loss 1.2293 Accuracy 0.7185\n",
            "Epoch 134 Batch 500 Loss 1.2289 Accuracy 0.7185\n",
            "Epoch 134 Batch 550 Loss 1.2317 Accuracy 0.7181\n",
            "Epoch 134 Batch 600 Loss 1.2340 Accuracy 0.7178\n",
            "Epoch 134 Batch 650 Loss 1.2372 Accuracy 0.7174\n",
            "Epoch 134 Batch 700 Loss 1.2384 Accuracy 0.7175\n",
            "Epoch 134 Batch 750 Loss 1.2401 Accuracy 0.7171\n",
            "Epoch 134 Batch 800 Loss 1.2412 Accuracy 0.7168\n",
            "Epoch 134 Batch 850 Loss 1.2423 Accuracy 0.7164\n",
            "Epoch 134 Batch 900 Loss 1.2433 Accuracy 0.7164\n",
            "Epoch 134 Batch 950 Loss 1.2453 Accuracy 0.7162\n",
            "Epoch 134 Batch 1000 Loss 1.2485 Accuracy 0.7156\n",
            "Epoch 134 Batch 1050 Loss 1.2508 Accuracy 0.7154\n",
            "Epoch 134 Batch 1100 Loss 1.2530 Accuracy 0.7150\n",
            "Epoch 134 Loss 1.2538 Accuracy 0.7149\n",
            "Time taken for 1 epoch: 22.43684673309326 secs\n",
            "\n",
            "Epoch 135 Batch 0 Loss 1.2976 Accuracy 0.7030\n",
            "Epoch 135 Batch 50 Loss 1.2124 Accuracy 0.7232\n",
            "Epoch 135 Batch 100 Loss 1.2060 Accuracy 0.7230\n",
            "Epoch 135 Batch 150 Loss 1.2114 Accuracy 0.7214\n",
            "Epoch 135 Batch 200 Loss 1.2193 Accuracy 0.7208\n",
            "Epoch 135 Batch 250 Loss 1.2173 Accuracy 0.7204\n",
            "Epoch 135 Batch 300 Loss 1.2184 Accuracy 0.7202\n",
            "Epoch 135 Batch 350 Loss 1.2212 Accuracy 0.7197\n",
            "Epoch 135 Batch 400 Loss 1.2230 Accuracy 0.7194\n",
            "Epoch 135 Batch 450 Loss 1.2269 Accuracy 0.7191\n",
            "Epoch 135 Batch 500 Loss 1.2293 Accuracy 0.7186\n",
            "Epoch 135 Batch 550 Loss 1.2318 Accuracy 0.7182\n",
            "Epoch 135 Batch 600 Loss 1.2348 Accuracy 0.7179\n",
            "discarded batch 610\n",
            "Epoch 135 Batch 650 Loss 1.2375 Accuracy 0.7176\n",
            "Epoch 135 Batch 700 Loss 1.2392 Accuracy 0.7172\n",
            "Epoch 135 Batch 750 Loss 1.2405 Accuracy 0.7171\n",
            "Epoch 135 Batch 800 Loss 1.2427 Accuracy 0.7165\n",
            "Epoch 135 Batch 850 Loss 1.2435 Accuracy 0.7164\n",
            "Epoch 135 Batch 900 Loss 1.2446 Accuracy 0.7160\n",
            "Epoch 135 Batch 950 Loss 1.2466 Accuracy 0.7158\n",
            "Epoch 135 Batch 1000 Loss 1.2473 Accuracy 0.7156\n",
            "Epoch 135 Batch 1050 Loss 1.2484 Accuracy 0.7153\n",
            "Epoch 135 Batch 1100 Loss 1.2493 Accuracy 0.7154\n",
            "Saving checkpoint for epoch 135 at ./checkpoints/train/ckpt-27\n",
            "Epoch 135 Loss 1.2510 Accuracy 0.7152\n",
            "Time taken for 1 epoch: 22.743748903274536 secs\n",
            "\n",
            "Epoch 136 Batch 0 Loss 1.1635 Accuracy 0.7063\n",
            "Epoch 136 Batch 50 Loss 1.1997 Accuracy 0.7251\n",
            "Epoch 136 Batch 100 Loss 1.2017 Accuracy 0.7234\n",
            "Epoch 136 Batch 150 Loss 1.2062 Accuracy 0.7223\n",
            "Epoch 136 Batch 200 Loss 1.2084 Accuracy 0.7221\n",
            "Epoch 136 Batch 250 Loss 1.2099 Accuracy 0.7215\n",
            "Epoch 136 Batch 300 Loss 1.2123 Accuracy 0.7213\n",
            "Epoch 136 Batch 350 Loss 1.2147 Accuracy 0.7206\n",
            "Epoch 136 Batch 400 Loss 1.2158 Accuracy 0.7207\n",
            "Epoch 136 Batch 450 Loss 1.2197 Accuracy 0.7198\n",
            "Epoch 136 Batch 500 Loss 1.2223 Accuracy 0.7191\n",
            "Epoch 136 Batch 550 Loss 1.2228 Accuracy 0.7189\n",
            "Epoch 136 Batch 600 Loss 1.2246 Accuracy 0.7186\n",
            "discarded batch 627\n",
            "Epoch 136 Batch 650 Loss 1.2274 Accuracy 0.7179\n",
            "Epoch 136 Batch 700 Loss 1.2298 Accuracy 0.7174\n",
            "Epoch 136 Batch 750 Loss 1.2309 Accuracy 0.7172\n",
            "Epoch 136 Batch 800 Loss 1.2334 Accuracy 0.7166\n",
            "Epoch 136 Batch 850 Loss 1.2359 Accuracy 0.7165\n",
            "Epoch 136 Batch 900 Loss 1.2400 Accuracy 0.7162\n",
            "Epoch 136 Batch 950 Loss 1.2425 Accuracy 0.7158\n",
            "Epoch 136 Batch 1000 Loss 1.2438 Accuracy 0.7157\n",
            "Epoch 136 Batch 1050 Loss 1.2454 Accuracy 0.7155\n",
            "Epoch 136 Batch 1100 Loss 1.2476 Accuracy 0.7152\n",
            "Epoch 136 Loss 1.2484 Accuracy 0.7150\n",
            "Time taken for 1 epoch: 22.311092376708984 secs\n",
            "\n",
            "discarded batch 23\n",
            "Epoch 136 VALIDATION: Loss 2.5439 Accuracy 0.5824\n",
            "\n",
            "Epoch 137 Batch 0 Loss 1.1356 Accuracy 0.7360\n",
            "Epoch 137 Batch 50 Loss 1.1930 Accuracy 0.7235\n",
            "Epoch 137 Batch 100 Loss 1.2075 Accuracy 0.7197\n",
            "Epoch 137 Batch 150 Loss 1.2123 Accuracy 0.7204\n",
            "Epoch 137 Batch 200 Loss 1.2122 Accuracy 0.7201\n",
            "Epoch 137 Batch 250 Loss 1.2155 Accuracy 0.7203\n",
            "Epoch 137 Batch 300 Loss 1.2155 Accuracy 0.7202\n",
            "Epoch 137 Batch 350 Loss 1.2173 Accuracy 0.7200\n",
            "Epoch 137 Batch 400 Loss 1.2213 Accuracy 0.7195\n",
            "Epoch 137 Batch 450 Loss 1.2255 Accuracy 0.7189\n",
            "Epoch 137 Batch 500 Loss 1.2270 Accuracy 0.7188\n",
            "Epoch 137 Batch 550 Loss 1.2280 Accuracy 0.7184\n",
            "Epoch 137 Batch 600 Loss 1.2302 Accuracy 0.7179\n",
            "Epoch 137 Batch 650 Loss 1.2354 Accuracy 0.7173\n",
            "Epoch 137 Batch 700 Loss 1.2367 Accuracy 0.7168\n",
            "Epoch 137 Batch 750 Loss 1.2388 Accuracy 0.7166\n",
            "Epoch 137 Batch 800 Loss 1.2387 Accuracy 0.7166\n",
            "Epoch 137 Batch 850 Loss 1.2403 Accuracy 0.7164\n",
            "Epoch 137 Batch 900 Loss 1.2424 Accuracy 0.7161\n",
            "discarded batch 903\n",
            "Epoch 137 Batch 950 Loss 1.2439 Accuracy 0.7159\n",
            "Epoch 137 Batch 1000 Loss 1.2452 Accuracy 0.7157\n",
            "Epoch 137 Batch 1050 Loss 1.2467 Accuracy 0.7154\n",
            "Epoch 137 Batch 1100 Loss 1.2481 Accuracy 0.7152\n",
            "Epoch 137 Loss 1.2487 Accuracy 0.7150\n",
            "Time taken for 1 epoch: 22.27259087562561 secs\n",
            "\n",
            "Epoch 138 Batch 0 Loss 1.1824 Accuracy 0.6931\n",
            "Epoch 138 Batch 50 Loss 1.1836 Accuracy 0.7248\n",
            "Epoch 138 Batch 100 Loss 1.1901 Accuracy 0.7245\n",
            "Epoch 138 Batch 150 Loss 1.1878 Accuracy 0.7240\n",
            "Epoch 138 Batch 200 Loss 1.1902 Accuracy 0.7234\n",
            "Epoch 138 Batch 250 Loss 1.1910 Accuracy 0.7243\n",
            "Epoch 138 Batch 300 Loss 1.1999 Accuracy 0.7233\n",
            "Epoch 138 Batch 350 Loss 1.2063 Accuracy 0.7219\n",
            "Epoch 138 Batch 400 Loss 1.2100 Accuracy 0.7211\n",
            "Epoch 138 Batch 450 Loss 1.2164 Accuracy 0.7198\n",
            "Epoch 138 Batch 500 Loss 1.2205 Accuracy 0.7194\n",
            "Epoch 138 Batch 550 Loss 1.2231 Accuracy 0.7193\n",
            "Epoch 138 Batch 600 Loss 1.2256 Accuracy 0.7190\n",
            "Epoch 138 Batch 650 Loss 1.2263 Accuracy 0.7189\n",
            "Epoch 138 Batch 700 Loss 1.2290 Accuracy 0.7188\n",
            "Epoch 138 Batch 750 Loss 1.2308 Accuracy 0.7184\n",
            "Epoch 138 Batch 800 Loss 1.2325 Accuracy 0.7179\n",
            "discarded batch 814\n",
            "Epoch 138 Batch 850 Loss 1.2353 Accuracy 0.7173\n",
            "Epoch 138 Batch 900 Loss 1.2378 Accuracy 0.7168\n",
            "Epoch 138 Batch 950 Loss 1.2387 Accuracy 0.7168\n",
            "Epoch 138 Batch 1000 Loss 1.2403 Accuracy 0.7167\n",
            "Epoch 138 Batch 1050 Loss 1.2425 Accuracy 0.7162\n",
            "Epoch 138 Batch 1100 Loss 1.2442 Accuracy 0.7160\n",
            "Epoch 138 Loss 1.2456 Accuracy 0.7158\n",
            "Time taken for 1 epoch: 22.311556100845337 secs\n",
            "\n",
            "Epoch 139 Batch 0 Loss 1.1006 Accuracy 0.7360\n",
            "Epoch 139 Batch 50 Loss 1.1956 Accuracy 0.7206\n",
            "Epoch 139 Batch 100 Loss 1.1927 Accuracy 0.7233\n",
            "Epoch 139 Batch 150 Loss 1.1955 Accuracy 0.7228\n",
            "Epoch 139 Batch 200 Loss 1.1980 Accuracy 0.7221\n",
            "Epoch 139 Batch 250 Loss 1.2032 Accuracy 0.7215\n",
            "Epoch 139 Batch 300 Loss 1.2046 Accuracy 0.7212\n",
            "Epoch 139 Batch 350 Loss 1.2064 Accuracy 0.7222\n",
            "Epoch 139 Batch 400 Loss 1.2092 Accuracy 0.7216\n",
            "Epoch 139 Batch 450 Loss 1.2146 Accuracy 0.7206\n",
            "Epoch 139 Batch 500 Loss 1.2178 Accuracy 0.7200\n",
            "discarded batch 526\n",
            "Epoch 139 Batch 550 Loss 1.2182 Accuracy 0.7200\n",
            "Epoch 139 Batch 600 Loss 1.2213 Accuracy 0.7194\n",
            "Epoch 139 Batch 650 Loss 1.2234 Accuracy 0.7191\n",
            "Epoch 139 Batch 700 Loss 1.2254 Accuracy 0.7187\n",
            "Epoch 139 Batch 750 Loss 1.2276 Accuracy 0.7185\n",
            "Epoch 139 Batch 800 Loss 1.2287 Accuracy 0.7181\n",
            "Epoch 139 Batch 850 Loss 1.2314 Accuracy 0.7177\n",
            "Epoch 139 Batch 900 Loss 1.2329 Accuracy 0.7176\n",
            "Epoch 139 Batch 950 Loss 1.2353 Accuracy 0.7172\n",
            "Epoch 139 Batch 1000 Loss 1.2369 Accuracy 0.7169\n",
            "Epoch 139 Batch 1050 Loss 1.2382 Accuracy 0.7167\n",
            "Epoch 139 Batch 1100 Loss 1.2411 Accuracy 0.7164\n",
            "Epoch 139 Loss 1.2422 Accuracy 0.7164\n",
            "Time taken for 1 epoch: 22.19356632232666 secs\n",
            "\n",
            "Epoch 140 Batch 0 Loss 1.1935 Accuracy 0.7162\n",
            "Epoch 140 Batch 50 Loss 1.1911 Accuracy 0.7210\n",
            "Epoch 140 Batch 100 Loss 1.1852 Accuracy 0.7232\n",
            "Epoch 140 Batch 150 Loss 1.1946 Accuracy 0.7224\n",
            "Epoch 140 Batch 200 Loss 1.1953 Accuracy 0.7227\n",
            "Epoch 140 Batch 250 Loss 1.1991 Accuracy 0.7217\n",
            "Epoch 140 Batch 300 Loss 1.2011 Accuracy 0.7220\n",
            "Epoch 140 Batch 350 Loss 1.2036 Accuracy 0.7217\n",
            "Epoch 140 Batch 400 Loss 1.2079 Accuracy 0.7213\n",
            "Epoch 140 Batch 450 Loss 1.2101 Accuracy 0.7217\n",
            "Epoch 140 Batch 500 Loss 1.2132 Accuracy 0.7210\n",
            "Epoch 140 Batch 550 Loss 1.2157 Accuracy 0.7210\n",
            "Epoch 140 Batch 600 Loss 1.2197 Accuracy 0.7203\n",
            "Epoch 140 Batch 650 Loss 1.2226 Accuracy 0.7197\n",
            "discarded batch 685\n",
            "Epoch 140 Batch 700 Loss 1.2231 Accuracy 0.7197\n",
            "Epoch 140 Batch 750 Loss 1.2257 Accuracy 0.7190\n",
            "Epoch 140 Batch 800 Loss 1.2272 Accuracy 0.7190\n",
            "Epoch 140 Batch 850 Loss 1.2303 Accuracy 0.7183\n",
            "Epoch 140 Batch 900 Loss 1.2328 Accuracy 0.7179\n",
            "Epoch 140 Batch 950 Loss 1.2351 Accuracy 0.7175\n",
            "Epoch 140 Batch 1000 Loss 1.2368 Accuracy 0.7170\n",
            "Epoch 140 Batch 1050 Loss 1.2390 Accuracy 0.7167\n",
            "Epoch 140 Batch 1100 Loss 1.2399 Accuracy 0.7164\n",
            "Saving checkpoint for epoch 140 at ./checkpoints/train/ckpt-28\n",
            "Epoch 140 Loss 1.2412 Accuracy 0.7163\n",
            "Time taken for 1 epoch: 22.404890298843384 secs\n",
            "\n",
            "Epoch 141 Batch 0 Loss 1.2316 Accuracy 0.7096\n",
            "Epoch 141 Batch 50 Loss 1.2024 Accuracy 0.7232\n",
            "Epoch 141 Batch 100 Loss 1.1937 Accuracy 0.7227\n",
            "Epoch 141 Batch 150 Loss 1.1923 Accuracy 0.7228\n",
            "Epoch 141 Batch 200 Loss 1.2023 Accuracy 0.7213\n",
            "Epoch 141 Batch 250 Loss 1.2019 Accuracy 0.7211\n",
            "Epoch 141 Batch 300 Loss 1.2066 Accuracy 0.7208\n",
            "Epoch 141 Batch 350 Loss 1.2093 Accuracy 0.7204\n",
            "Epoch 141 Batch 400 Loss 1.2132 Accuracy 0.7194\n",
            "Epoch 141 Batch 450 Loss 1.2147 Accuracy 0.7196\n",
            "Epoch 141 Batch 500 Loss 1.2159 Accuracy 0.7194\n",
            "Epoch 141 Batch 550 Loss 1.2172 Accuracy 0.7190\n",
            "Epoch 141 Batch 600 Loss 1.2201 Accuracy 0.7188\n",
            "Epoch 141 Batch 650 Loss 1.2211 Accuracy 0.7183\n",
            "Epoch 141 Batch 700 Loss 1.2239 Accuracy 0.7179\n",
            "Epoch 141 Batch 750 Loss 1.2271 Accuracy 0.7177\n",
            "Epoch 141 Batch 800 Loss 1.2293 Accuracy 0.7174\n",
            "discarded batch 843\n",
            "Epoch 141 Batch 850 Loss 1.2300 Accuracy 0.7173\n",
            "Epoch 141 Batch 900 Loss 1.2317 Accuracy 0.7171\n",
            "Epoch 141 Batch 950 Loss 1.2321 Accuracy 0.7170\n",
            "Epoch 141 Batch 1000 Loss 1.2345 Accuracy 0.7166\n",
            "Epoch 141 Batch 1050 Loss 1.2368 Accuracy 0.7166\n",
            "Epoch 141 Batch 1100 Loss 1.2383 Accuracy 0.7163\n",
            "Epoch 141 Loss 1.2394 Accuracy 0.7162\n",
            "Time taken for 1 epoch: 22.22024631500244 secs\n",
            "\n",
            "discarded batch 23\n",
            "Epoch 141 VALIDATION: Loss 2.5545 Accuracy 0.5770\n",
            "\n",
            "Epoch 142 Batch 0 Loss 1.1521 Accuracy 0.7129\n",
            "Epoch 142 Batch 50 Loss 1.1698 Accuracy 0.7251\n",
            "Epoch 142 Batch 100 Loss 1.1842 Accuracy 0.7251\n",
            "Epoch 142 Batch 150 Loss 1.1806 Accuracy 0.7266\n",
            "Epoch 142 Batch 200 Loss 1.1840 Accuracy 0.7253\n",
            "Epoch 142 Batch 250 Loss 1.1910 Accuracy 0.7241\n",
            "Epoch 142 Batch 300 Loss 1.1937 Accuracy 0.7240\n",
            "Epoch 142 Batch 350 Loss 1.1946 Accuracy 0.7236\n",
            "discarded batch 373\n",
            "Epoch 142 Batch 400 Loss 1.1982 Accuracy 0.7233\n",
            "Epoch 142 Batch 450 Loss 1.2013 Accuracy 0.7229\n",
            "Epoch 142 Batch 500 Loss 1.2044 Accuracy 0.7223\n",
            "Epoch 142 Batch 550 Loss 1.2076 Accuracy 0.7216\n",
            "Epoch 142 Batch 600 Loss 1.2097 Accuracy 0.7211\n",
            "Epoch 142 Batch 650 Loss 1.2116 Accuracy 0.7207\n",
            "Epoch 142 Batch 700 Loss 1.2132 Accuracy 0.7204\n",
            "Epoch 142 Batch 750 Loss 1.2173 Accuracy 0.7197\n",
            "Epoch 142 Batch 800 Loss 1.2200 Accuracy 0.7191\n",
            "Epoch 142 Batch 850 Loss 1.2230 Accuracy 0.7187\n",
            "Epoch 142 Batch 900 Loss 1.2252 Accuracy 0.7184\n",
            "Epoch 142 Batch 950 Loss 1.2281 Accuracy 0.7181\n",
            "Epoch 142 Batch 1000 Loss 1.2309 Accuracy 0.7175\n",
            "Epoch 142 Batch 1050 Loss 1.2321 Accuracy 0.7173\n",
            "Epoch 142 Batch 1100 Loss 1.2344 Accuracy 0.7167\n",
            "Epoch 142 Loss 1.2360 Accuracy 0.7166\n",
            "Time taken for 1 epoch: 22.14063334465027 secs\n",
            "\n",
            "Epoch 143 Batch 0 Loss 1.2013 Accuracy 0.7129\n",
            "Epoch 143 Batch 50 Loss 1.1664 Accuracy 0.7287\n",
            "Epoch 143 Batch 100 Loss 1.1830 Accuracy 0.7255\n",
            "Epoch 143 Batch 150 Loss 1.1872 Accuracy 0.7255\n",
            "Epoch 143 Batch 200 Loss 1.1900 Accuracy 0.7254\n",
            "Epoch 143 Batch 250 Loss 1.1943 Accuracy 0.7243\n",
            "Epoch 143 Batch 300 Loss 1.1999 Accuracy 0.7232\n",
            "Epoch 143 Batch 350 Loss 1.2027 Accuracy 0.7224\n",
            "Epoch 143 Batch 400 Loss 1.2050 Accuracy 0.7218\n",
            "Epoch 143 Batch 450 Loss 1.2075 Accuracy 0.7215\n",
            "Epoch 143 Batch 500 Loss 1.2110 Accuracy 0.7205\n",
            "Epoch 143 Batch 550 Loss 1.2148 Accuracy 0.7200\n",
            "discarded batch 593\n",
            "Epoch 143 Batch 600 Loss 1.2147 Accuracy 0.7199\n",
            "Epoch 143 Batch 650 Loss 1.2157 Accuracy 0.7196\n",
            "Epoch 143 Batch 700 Loss 1.2178 Accuracy 0.7196\n",
            "Epoch 143 Batch 750 Loss 1.2196 Accuracy 0.7193\n",
            "Epoch 143 Batch 800 Loss 1.2207 Accuracy 0.7190\n",
            "Epoch 143 Batch 850 Loss 1.2230 Accuracy 0.7185\n",
            "Epoch 143 Batch 900 Loss 1.2239 Accuracy 0.7183\n",
            "Epoch 143 Batch 950 Loss 1.2259 Accuracy 0.7180\n",
            "Epoch 143 Batch 1000 Loss 1.2266 Accuracy 0.7181\n",
            "Epoch 143 Batch 1050 Loss 1.2287 Accuracy 0.7179\n",
            "Epoch 143 Batch 1100 Loss 1.2311 Accuracy 0.7176\n",
            "Epoch 143 Loss 1.2319 Accuracy 0.7175\n",
            "Time taken for 1 epoch: 22.15611433982849 secs\n",
            "\n",
            "Epoch 144 Batch 0 Loss 1.2505 Accuracy 0.7261\n",
            "Epoch 144 Batch 50 Loss 1.1782 Accuracy 0.7303\n",
            "Epoch 144 Batch 100 Loss 1.1749 Accuracy 0.7283\n",
            "Epoch 144 Batch 150 Loss 1.1745 Accuracy 0.7276\n",
            "Epoch 144 Batch 200 Loss 1.1799 Accuracy 0.7265\n",
            "Epoch 144 Batch 250 Loss 1.1806 Accuracy 0.7267\n",
            "discarded batch 251\n",
            "Epoch 144 Batch 300 Loss 1.1875 Accuracy 0.7250\n",
            "Epoch 144 Batch 350 Loss 1.1921 Accuracy 0.7240\n",
            "Epoch 144 Batch 400 Loss 1.1936 Accuracy 0.7238\n",
            "Epoch 144 Batch 450 Loss 1.1970 Accuracy 0.7231\n",
            "Epoch 144 Batch 500 Loss 1.2006 Accuracy 0.7225\n",
            "Epoch 144 Batch 550 Loss 1.2037 Accuracy 0.7224\n",
            "Epoch 144 Batch 600 Loss 1.2066 Accuracy 0.7222\n",
            "Epoch 144 Batch 650 Loss 1.2078 Accuracy 0.7219\n",
            "Epoch 144 Batch 700 Loss 1.2115 Accuracy 0.7214\n",
            "Epoch 144 Batch 750 Loss 1.2142 Accuracy 0.7208\n",
            "Epoch 144 Batch 800 Loss 1.2156 Accuracy 0.7208\n",
            "Epoch 144 Batch 850 Loss 1.2194 Accuracy 0.7203\n",
            "Epoch 144 Batch 900 Loss 1.2216 Accuracy 0.7199\n",
            "Epoch 144 Batch 950 Loss 1.2238 Accuracy 0.7196\n",
            "Epoch 144 Batch 1000 Loss 1.2252 Accuracy 0.7193\n",
            "Epoch 144 Batch 1050 Loss 1.2277 Accuracy 0.7190\n",
            "Epoch 144 Batch 1100 Loss 1.2306 Accuracy 0.7184\n",
            "Epoch 144 Loss 1.2318 Accuracy 0.7182\n",
            "Time taken for 1 epoch: 22.15541696548462 secs\n",
            "\n",
            "Epoch 145 Batch 0 Loss 1.1106 Accuracy 0.7558\n",
            "Epoch 145 Batch 50 Loss 1.1995 Accuracy 0.7246\n",
            "Epoch 145 Batch 100 Loss 1.1957 Accuracy 0.7270\n",
            "Epoch 145 Batch 150 Loss 1.1866 Accuracy 0.7275\n",
            "Epoch 145 Batch 200 Loss 1.1874 Accuracy 0.7267\n",
            "Epoch 145 Batch 250 Loss 1.1882 Accuracy 0.7265\n",
            "Epoch 145 Batch 300 Loss 1.1923 Accuracy 0.7255\n",
            "discarded batch 344\n",
            "Epoch 145 Batch 350 Loss 1.1944 Accuracy 0.7248\n",
            "Epoch 145 Batch 400 Loss 1.1996 Accuracy 0.7236\n",
            "Epoch 145 Batch 450 Loss 1.2010 Accuracy 0.7235\n",
            "Epoch 145 Batch 500 Loss 1.2029 Accuracy 0.7230\n",
            "Epoch 145 Batch 550 Loss 1.2066 Accuracy 0.7227\n",
            "Epoch 145 Batch 600 Loss 1.2088 Accuracy 0.7224\n",
            "Epoch 145 Batch 650 Loss 1.2097 Accuracy 0.7223\n",
            "Epoch 145 Batch 700 Loss 1.2126 Accuracy 0.7219\n",
            "Epoch 145 Batch 750 Loss 1.2135 Accuracy 0.7216\n",
            "Epoch 145 Batch 800 Loss 1.2150 Accuracy 0.7212\n",
            "Epoch 145 Batch 850 Loss 1.2166 Accuracy 0.7210\n",
            "Epoch 145 Batch 900 Loss 1.2185 Accuracy 0.7208\n",
            "Epoch 145 Batch 950 Loss 1.2199 Accuracy 0.7205\n",
            "Epoch 145 Batch 1000 Loss 1.2214 Accuracy 0.7200\n",
            "Epoch 145 Batch 1050 Loss 1.2230 Accuracy 0.7197\n",
            "Epoch 145 Batch 1100 Loss 1.2245 Accuracy 0.7194\n",
            "Saving checkpoint for epoch 145 at ./checkpoints/train/ckpt-29\n",
            "Epoch 145 Loss 1.2259 Accuracy 0.7191\n",
            "Time taken for 1 epoch: 22.458476781845093 secs\n",
            "\n",
            "Epoch 146 Batch 0 Loss 1.2232 Accuracy 0.7261\n",
            "Epoch 146 Batch 50 Loss 1.1662 Accuracy 0.7316\n",
            "Epoch 146 Batch 100 Loss 1.1732 Accuracy 0.7299\n",
            "Epoch 146 Batch 150 Loss 1.1748 Accuracy 0.7291\n",
            "Epoch 146 Batch 200 Loss 1.1808 Accuracy 0.7270\n",
            "Epoch 146 Batch 250 Loss 1.1880 Accuracy 0.7252\n",
            "Epoch 146 Batch 300 Loss 1.1907 Accuracy 0.7246\n",
            "Epoch 146 Batch 350 Loss 1.1933 Accuracy 0.7242\n",
            "Epoch 146 Batch 400 Loss 1.1950 Accuracy 0.7236\n",
            "Epoch 146 Batch 450 Loss 1.1967 Accuracy 0.7231\n",
            "Epoch 146 Batch 500 Loss 1.2014 Accuracy 0.7222\n",
            "Epoch 146 Batch 550 Loss 1.2053 Accuracy 0.7214\n",
            "Epoch 146 Batch 600 Loss 1.2069 Accuracy 0.7213\n",
            "Epoch 146 Batch 650 Loss 1.2069 Accuracy 0.7213\n",
            "Epoch 146 Batch 700 Loss 1.2095 Accuracy 0.7210\n",
            "Epoch 146 Batch 750 Loss 1.2109 Accuracy 0.7210\n",
            "Epoch 146 Batch 800 Loss 1.2132 Accuracy 0.7204\n",
            "Epoch 146 Batch 850 Loss 1.2150 Accuracy 0.7198\n",
            "Epoch 146 Batch 900 Loss 1.2170 Accuracy 0.7198\n",
            "Epoch 146 Batch 950 Loss 1.2189 Accuracy 0.7196\n",
            "discarded batch 992\n",
            "Epoch 146 Batch 1000 Loss 1.2206 Accuracy 0.7193\n",
            "Epoch 146 Batch 1050 Loss 1.2224 Accuracy 0.7190\n",
            "Epoch 146 Batch 1100 Loss 1.2238 Accuracy 0.7188\n",
            "Epoch 146 Loss 1.2246 Accuracy 0.7187\n",
            "Time taken for 1 epoch: 22.157474040985107 secs\n",
            "\n",
            "discarded batch 23\n",
            "Epoch 146 VALIDATION: Loss 2.5744 Accuracy 0.5730\n",
            "\n",
            "Epoch 147 Batch 0 Loss 1.1357 Accuracy 0.7393\n",
            "Epoch 147 Batch 50 Loss 1.1782 Accuracy 0.7234\n",
            "Epoch 147 Batch 100 Loss 1.1763 Accuracy 0.7247\n",
            "Epoch 147 Batch 150 Loss 1.1748 Accuracy 0.7252\n",
            "Epoch 147 Batch 200 Loss 1.1790 Accuracy 0.7251\n",
            "Epoch 147 Batch 250 Loss 1.1806 Accuracy 0.7247\n",
            "Epoch 147 Batch 300 Loss 1.1850 Accuracy 0.7247\n",
            "Epoch 147 Batch 350 Loss 1.1864 Accuracy 0.7243\n",
            "Epoch 147 Batch 400 Loss 1.1931 Accuracy 0.7233\n",
            "Epoch 147 Batch 450 Loss 1.1959 Accuracy 0.7231\n",
            "Epoch 147 Batch 500 Loss 1.1988 Accuracy 0.7223\n",
            "Epoch 147 Batch 550 Loss 1.2016 Accuracy 0.7221\n",
            "Epoch 147 Batch 600 Loss 1.2042 Accuracy 0.7217\n",
            "Epoch 147 Batch 650 Loss 1.2068 Accuracy 0.7211\n",
            "Epoch 147 Batch 700 Loss 1.2083 Accuracy 0.7209\n",
            "Epoch 147 Batch 750 Loss 1.2100 Accuracy 0.7204\n",
            "Epoch 147 Batch 800 Loss 1.2116 Accuracy 0.7202\n",
            "Epoch 147 Batch 850 Loss 1.2136 Accuracy 0.7198\n",
            "Epoch 147 Batch 900 Loss 1.2164 Accuracy 0.7194\n",
            "Epoch 147 Batch 950 Loss 1.2202 Accuracy 0.7190\n",
            "discarded batch 963\n",
            "Epoch 147 Batch 1000 Loss 1.2213 Accuracy 0.7189\n",
            "Epoch 147 Batch 1050 Loss 1.2228 Accuracy 0.7187\n",
            "Epoch 147 Batch 1100 Loss 1.2238 Accuracy 0.7184\n",
            "Epoch 147 Loss 1.2247 Accuracy 0.7184\n",
            "Time taken for 1 epoch: 22.219977855682373 secs\n",
            "\n",
            "Epoch 148 Batch 0 Loss 1.1913 Accuracy 0.7228\n",
            "Epoch 148 Batch 50 Loss 1.1803 Accuracy 0.7254\n",
            "Epoch 148 Batch 100 Loss 1.1787 Accuracy 0.7273\n",
            "Epoch 148 Batch 150 Loss 1.1774 Accuracy 0.7278\n",
            "Epoch 148 Batch 200 Loss 1.1817 Accuracy 0.7274\n",
            "Epoch 148 Batch 250 Loss 1.1855 Accuracy 0.7261\n",
            "Epoch 148 Batch 300 Loss 1.1896 Accuracy 0.7252\n",
            "Epoch 148 Batch 350 Loss 1.1939 Accuracy 0.7245\n",
            "Epoch 148 Batch 400 Loss 1.1945 Accuracy 0.7245\n",
            "Epoch 148 Batch 450 Loss 1.1972 Accuracy 0.7238\n",
            "Epoch 148 Batch 500 Loss 1.2012 Accuracy 0.7231\n",
            "Epoch 148 Batch 550 Loss 1.2017 Accuracy 0.7232\n",
            "discarded batch 567\n",
            "Epoch 148 Batch 600 Loss 1.2028 Accuracy 0.7228\n",
            "Epoch 148 Batch 650 Loss 1.2049 Accuracy 0.7224\n",
            "Epoch 148 Batch 700 Loss 1.2072 Accuracy 0.7220\n",
            "Epoch 148 Batch 750 Loss 1.2099 Accuracy 0.7215\n",
            "Epoch 148 Batch 800 Loss 1.2125 Accuracy 0.7210\n",
            "Epoch 148 Batch 850 Loss 1.2156 Accuracy 0.7204\n",
            "Epoch 148 Batch 900 Loss 1.2173 Accuracy 0.7201\n",
            "Epoch 148 Batch 950 Loss 1.2187 Accuracy 0.7200\n",
            "Epoch 148 Batch 1000 Loss 1.2203 Accuracy 0.7199\n",
            "Epoch 148 Batch 1050 Loss 1.2212 Accuracy 0.7198\n",
            "Epoch 148 Batch 1100 Loss 1.2225 Accuracy 0.7195\n",
            "Epoch 148 Loss 1.2232 Accuracy 0.7196\n",
            "Time taken for 1 epoch: 22.167239665985107 secs\n",
            "\n",
            "Epoch 149 Batch 0 Loss 1.0908 Accuracy 0.7129\n",
            "Epoch 149 Batch 50 Loss 1.1682 Accuracy 0.7259\n",
            "Epoch 149 Batch 100 Loss 1.1730 Accuracy 0.7254\n",
            "Epoch 149 Batch 150 Loss 1.1748 Accuracy 0.7261\n",
            "Epoch 149 Batch 200 Loss 1.1774 Accuracy 0.7258\n",
            "Epoch 149 Batch 250 Loss 1.1806 Accuracy 0.7260\n",
            "Epoch 149 Batch 300 Loss 1.1868 Accuracy 0.7251\n",
            "Epoch 149 Batch 350 Loss 1.1890 Accuracy 0.7243\n",
            "Epoch 149 Batch 400 Loss 1.1917 Accuracy 0.7238\n",
            "Epoch 149 Batch 450 Loss 1.1931 Accuracy 0.7239\n",
            "Epoch 149 Batch 500 Loss 1.1967 Accuracy 0.7235\n",
            "Epoch 149 Batch 550 Loss 1.1986 Accuracy 0.7227\n",
            "discarded batch 598\n",
            "Epoch 149 Batch 600 Loss 1.2014 Accuracy 0.7223\n",
            "Epoch 149 Batch 650 Loss 1.2037 Accuracy 0.7219\n",
            "Epoch 149 Batch 700 Loss 1.2047 Accuracy 0.7219\n",
            "Epoch 149 Batch 750 Loss 1.2085 Accuracy 0.7213\n",
            "Epoch 149 Batch 800 Loss 1.2112 Accuracy 0.7210\n",
            "Epoch 149 Batch 850 Loss 1.2123 Accuracy 0.7211\n",
            "Epoch 149 Batch 900 Loss 1.2131 Accuracy 0.7208\n",
            "Epoch 149 Batch 950 Loss 1.2142 Accuracy 0.7205\n",
            "Epoch 149 Batch 1000 Loss 1.2156 Accuracy 0.7201\n",
            "Epoch 149 Batch 1050 Loss 1.2183 Accuracy 0.7196\n",
            "Epoch 149 Batch 1100 Loss 1.2203 Accuracy 0.7193\n",
            "Epoch 149 Loss 1.2209 Accuracy 0.7193\n",
            "Time taken for 1 epoch: 22.527833461761475 secs\n",
            "\n",
            "Epoch 150 Batch 0 Loss 1.3363 Accuracy 0.7195\n",
            "Epoch 150 Batch 50 Loss 1.1746 Accuracy 0.7276\n",
            "Epoch 150 Batch 100 Loss 1.1702 Accuracy 0.7278\n",
            "Epoch 150 Batch 150 Loss 1.1775 Accuracy 0.7265\n",
            "Epoch 150 Batch 200 Loss 1.1780 Accuracy 0.7266\n",
            "Epoch 150 Batch 250 Loss 1.1778 Accuracy 0.7264\n",
            "Epoch 150 Batch 300 Loss 1.1820 Accuracy 0.7255\n",
            "Epoch 150 Batch 350 Loss 1.1821 Accuracy 0.7256\n",
            "Epoch 150 Batch 400 Loss 1.1889 Accuracy 0.7243\n",
            "Epoch 150 Batch 450 Loss 1.1911 Accuracy 0.7242\n",
            "Epoch 150 Batch 500 Loss 1.1933 Accuracy 0.7237\n",
            "Epoch 150 Batch 550 Loss 1.1955 Accuracy 0.7232\n",
            "Epoch 150 Batch 600 Loss 1.1980 Accuracy 0.7228\n",
            "discarded batch 620\n",
            "Epoch 150 Batch 650 Loss 1.2021 Accuracy 0.7221\n",
            "Epoch 150 Batch 700 Loss 1.2039 Accuracy 0.7220\n",
            "Epoch 150 Batch 750 Loss 1.2066 Accuracy 0.7215\n",
            "Epoch 150 Batch 800 Loss 1.2073 Accuracy 0.7214\n",
            "Epoch 150 Batch 850 Loss 1.2098 Accuracy 0.7210\n",
            "Epoch 150 Batch 900 Loss 1.2111 Accuracy 0.7208\n",
            "Epoch 150 Batch 950 Loss 1.2122 Accuracy 0.7204\n",
            "Epoch 150 Batch 1000 Loss 1.2149 Accuracy 0.7200\n",
            "Epoch 150 Batch 1050 Loss 1.2164 Accuracy 0.7196\n",
            "Epoch 150 Batch 1100 Loss 1.2181 Accuracy 0.7195\n",
            "Saving checkpoint for epoch 150 at ./checkpoints/train/ckpt-30\n",
            "Epoch 150 Loss 1.2192 Accuracy 0.7192\n",
            "Time taken for 1 epoch: 22.470319509506226 secs\n",
            "\n",
            "Epoch 151 Batch 0 Loss 1.2924 Accuracy 0.7129\n",
            "Epoch 151 Batch 50 Loss 1.1636 Accuracy 0.7290\n",
            "Epoch 151 Batch 100 Loss 1.1656 Accuracy 0.7300\n",
            "Epoch 151 Batch 150 Loss 1.1673 Accuracy 0.7281\n",
            "Epoch 151 Batch 200 Loss 1.1735 Accuracy 0.7280\n",
            "Epoch 151 Batch 250 Loss 1.1769 Accuracy 0.7272\n",
            "Epoch 151 Batch 300 Loss 1.1799 Accuracy 0.7264\n",
            "Epoch 151 Batch 350 Loss 1.1821 Accuracy 0.7257\n",
            "Epoch 151 Batch 400 Loss 1.1862 Accuracy 0.7250\n",
            "Epoch 151 Batch 450 Loss 1.1874 Accuracy 0.7250\n",
            "Epoch 151 Batch 500 Loss 1.1870 Accuracy 0.7250\n",
            "Epoch 151 Batch 550 Loss 1.1917 Accuracy 0.7239\n",
            "Epoch 151 Batch 600 Loss 1.1934 Accuracy 0.7237\n",
            "Epoch 151 Batch 650 Loss 1.1950 Accuracy 0.7232\n",
            "Epoch 151 Batch 700 Loss 1.1967 Accuracy 0.7228\n",
            "discarded batch 711\n",
            "Epoch 151 Batch 750 Loss 1.1995 Accuracy 0.7225\n",
            "Epoch 151 Batch 800 Loss 1.2008 Accuracy 0.7222\n",
            "Epoch 151 Batch 850 Loss 1.2042 Accuracy 0.7219\n",
            "Epoch 151 Batch 900 Loss 1.2065 Accuracy 0.7216\n",
            "Epoch 151 Batch 950 Loss 1.2093 Accuracy 0.7211\n",
            "Epoch 151 Batch 1000 Loss 1.2101 Accuracy 0.7211\n",
            "Epoch 151 Batch 1050 Loss 1.2128 Accuracy 0.7209\n",
            "Epoch 151 Batch 1100 Loss 1.2147 Accuracy 0.7206\n",
            "Epoch 151 Loss 1.2161 Accuracy 0.7204\n",
            "Time taken for 1 epoch: 22.234376430511475 secs\n",
            "\n",
            "discarded batch 23\n",
            "Epoch 151 VALIDATION: Loss 2.6105 Accuracy 0.5724\n",
            "\n",
            "Epoch 152 Batch 0 Loss 1.4297 Accuracy 0.6799\n",
            "Epoch 152 Batch 50 Loss 1.1429 Accuracy 0.7340\n",
            "Epoch 152 Batch 100 Loss 1.1510 Accuracy 0.7325\n",
            "Epoch 152 Batch 150 Loss 1.1644 Accuracy 0.7296\n",
            "Epoch 152 Batch 200 Loss 1.1668 Accuracy 0.7291\n",
            "discarded batch 250\n",
            "Epoch 152 Batch 300 Loss 1.1781 Accuracy 0.7279\n",
            "Epoch 152 Batch 350 Loss 1.1844 Accuracy 0.7269\n",
            "Epoch 152 Batch 400 Loss 1.1888 Accuracy 0.7256\n",
            "Epoch 152 Batch 450 Loss 1.1895 Accuracy 0.7254\n",
            "Epoch 152 Batch 500 Loss 1.1914 Accuracy 0.7248\n",
            "Epoch 152 Batch 550 Loss 1.1924 Accuracy 0.7242\n",
            "Epoch 152 Batch 600 Loss 1.1943 Accuracy 0.7239\n",
            "Epoch 152 Batch 650 Loss 1.1962 Accuracy 0.7237\n",
            "Epoch 152 Batch 700 Loss 1.1994 Accuracy 0.7231\n",
            "Epoch 152 Batch 750 Loss 1.2016 Accuracy 0.7228\n",
            "Epoch 152 Batch 800 Loss 1.2042 Accuracy 0.7224\n",
            "Epoch 152 Batch 850 Loss 1.2063 Accuracy 0.7222\n",
            "Epoch 152 Batch 900 Loss 1.2085 Accuracy 0.7217\n",
            "Epoch 152 Batch 950 Loss 1.2112 Accuracy 0.7213\n",
            "Epoch 152 Batch 1000 Loss 1.2130 Accuracy 0.7210\n",
            "Epoch 152 Batch 1050 Loss 1.2141 Accuracy 0.7208\n",
            "Epoch 152 Batch 1100 Loss 1.2147 Accuracy 0.7208\n",
            "Epoch 152 Loss 1.2168 Accuracy 0.7204\n",
            "Time taken for 1 epoch: 22.248982429504395 secs\n",
            "\n",
            "Epoch 153 Batch 0 Loss 1.3285 Accuracy 0.7228\n",
            "Epoch 153 Batch 50 Loss 1.1723 Accuracy 0.7266\n",
            "Epoch 153 Batch 100 Loss 1.1725 Accuracy 0.7274\n",
            "Epoch 153 Batch 150 Loss 1.1703 Accuracy 0.7267\n",
            "Epoch 153 Batch 200 Loss 1.1700 Accuracy 0.7270\n",
            "Epoch 153 Batch 250 Loss 1.1674 Accuracy 0.7280\n",
            "Epoch 153 Batch 300 Loss 1.1736 Accuracy 0.7277\n",
            "discarded batch 338\n",
            "Epoch 153 Batch 350 Loss 1.1751 Accuracy 0.7277\n",
            "Epoch 153 Batch 400 Loss 1.1779 Accuracy 0.7273\n",
            "Epoch 153 Batch 450 Loss 1.1832 Accuracy 0.7262\n",
            "Epoch 153 Batch 500 Loss 1.1870 Accuracy 0.7252\n",
            "Epoch 153 Batch 550 Loss 1.1912 Accuracy 0.7245\n",
            "Epoch 153 Batch 600 Loss 1.1943 Accuracy 0.7241\n",
            "Epoch 153 Batch 650 Loss 1.1966 Accuracy 0.7243\n",
            "Epoch 153 Batch 700 Loss 1.1990 Accuracy 0.7237\n",
            "Epoch 153 Batch 750 Loss 1.2030 Accuracy 0.7232\n",
            "Epoch 153 Batch 800 Loss 1.2041 Accuracy 0.7227\n",
            "Epoch 153 Batch 850 Loss 1.2054 Accuracy 0.7227\n",
            "Epoch 153 Batch 900 Loss 1.2073 Accuracy 0.7226\n",
            "Epoch 153 Batch 950 Loss 1.2084 Accuracy 0.7224\n",
            "Epoch 153 Batch 1000 Loss 1.2098 Accuracy 0.7222\n",
            "Epoch 153 Batch 1050 Loss 1.2115 Accuracy 0.7219\n",
            "Epoch 153 Batch 1100 Loss 1.2135 Accuracy 0.7218\n",
            "Epoch 153 Loss 1.2141 Accuracy 0.7216\n",
            "Time taken for 1 epoch: 22.13340473175049 secs\n",
            "\n",
            "Epoch 154 Batch 0 Loss 1.1734 Accuracy 0.7063\n",
            "discarded batch 39\n",
            "Epoch 154 Batch 50 Loss 1.1528 Accuracy 0.7299\n",
            "Epoch 154 Batch 100 Loss 1.1570 Accuracy 0.7301\n",
            "Epoch 154 Batch 150 Loss 1.1595 Accuracy 0.7293\n",
            "Epoch 154 Batch 200 Loss 1.1629 Accuracy 0.7295\n",
            "Epoch 154 Batch 250 Loss 1.1724 Accuracy 0.7281\n",
            "Epoch 154 Batch 300 Loss 1.1739 Accuracy 0.7276\n",
            "Epoch 154 Batch 350 Loss 1.1760 Accuracy 0.7269\n",
            "Epoch 154 Batch 400 Loss 1.1820 Accuracy 0.7260\n",
            "Epoch 154 Batch 450 Loss 1.1847 Accuracy 0.7255\n",
            "Epoch 154 Batch 500 Loss 1.1839 Accuracy 0.7258\n",
            "Epoch 154 Batch 550 Loss 1.1856 Accuracy 0.7254\n",
            "Epoch 154 Batch 600 Loss 1.1872 Accuracy 0.7252\n",
            "Epoch 154 Batch 650 Loss 1.1890 Accuracy 0.7251\n",
            "Epoch 154 Batch 700 Loss 1.1924 Accuracy 0.7246\n",
            "Epoch 154 Batch 750 Loss 1.1955 Accuracy 0.7242\n",
            "Epoch 154 Batch 800 Loss 1.1978 Accuracy 0.7238\n",
            "Epoch 154 Batch 850 Loss 1.2008 Accuracy 0.7234\n",
            "Epoch 154 Batch 900 Loss 1.2028 Accuracy 0.7231\n",
            "Epoch 154 Batch 950 Loss 1.2047 Accuracy 0.7228\n",
            "Epoch 154 Batch 1000 Loss 1.2066 Accuracy 0.7224\n",
            "Epoch 154 Batch 1050 Loss 1.2095 Accuracy 0.7218\n",
            "Epoch 154 Batch 1100 Loss 1.2111 Accuracy 0.7215\n",
            "Epoch 154 Loss 1.2122 Accuracy 0.7212\n",
            "Time taken for 1 epoch: 22.17475938796997 secs\n",
            "\n",
            "Epoch 155 Batch 0 Loss 1.2262 Accuracy 0.7492\n",
            "Epoch 155 Batch 50 Loss 1.1579 Accuracy 0.7314\n",
            "Epoch 155 Batch 100 Loss 1.1583 Accuracy 0.7315\n",
            "Epoch 155 Batch 150 Loss 1.1674 Accuracy 0.7294\n",
            "Epoch 155 Batch 200 Loss 1.1637 Accuracy 0.7292\n",
            "Epoch 155 Batch 250 Loss 1.1681 Accuracy 0.7286\n",
            "Epoch 155 Batch 300 Loss 1.1733 Accuracy 0.7277\n",
            "Epoch 155 Batch 350 Loss 1.1760 Accuracy 0.7272\n",
            "discarded batch 362\n",
            "Epoch 155 Batch 400 Loss 1.1773 Accuracy 0.7270\n",
            "Epoch 155 Batch 450 Loss 1.1798 Accuracy 0.7261\n",
            "Epoch 155 Batch 500 Loss 1.1843 Accuracy 0.7253\n",
            "Epoch 155 Batch 550 Loss 1.1880 Accuracy 0.7247\n",
            "Epoch 155 Batch 600 Loss 1.1895 Accuracy 0.7242\n",
            "Epoch 155 Batch 650 Loss 1.1921 Accuracy 0.7239\n",
            "Epoch 155 Batch 700 Loss 1.1947 Accuracy 0.7235\n",
            "Epoch 155 Batch 750 Loss 1.1958 Accuracy 0.7234\n",
            "Epoch 155 Batch 800 Loss 1.1976 Accuracy 0.7233\n",
            "Epoch 155 Batch 850 Loss 1.2004 Accuracy 0.7229\n",
            "Epoch 155 Batch 900 Loss 1.2025 Accuracy 0.7225\n",
            "Epoch 155 Batch 950 Loss 1.2045 Accuracy 0.7221\n",
            "Epoch 155 Batch 1000 Loss 1.2060 Accuracy 0.7219\n",
            "Epoch 155 Batch 1050 Loss 1.2078 Accuracy 0.7216\n",
            "Epoch 155 Batch 1100 Loss 1.2098 Accuracy 0.7214\n",
            "Saving checkpoint for epoch 155 at ./checkpoints/train/ckpt-31\n",
            "Epoch 155 Loss 1.2105 Accuracy 0.7213\n",
            "Time taken for 1 epoch: 22.375169277191162 secs\n",
            "\n",
            "Epoch 156 Batch 0 Loss 1.1047 Accuracy 0.7426\n",
            "Epoch 156 Batch 50 Loss 1.1404 Accuracy 0.7325\n",
            "Epoch 156 Batch 100 Loss 1.1528 Accuracy 0.7320\n",
            "Epoch 156 Batch 150 Loss 1.1564 Accuracy 0.7313\n",
            "Epoch 156 Batch 200 Loss 1.1613 Accuracy 0.7296\n",
            "Epoch 156 Batch 250 Loss 1.1652 Accuracy 0.7288\n",
            "Epoch 156 Batch 300 Loss 1.1701 Accuracy 0.7282\n",
            "Epoch 156 Batch 350 Loss 1.1695 Accuracy 0.7279\n",
            "Epoch 156 Batch 400 Loss 1.1735 Accuracy 0.7270\n",
            "Epoch 156 Batch 450 Loss 1.1751 Accuracy 0.7264\n",
            "Epoch 156 Batch 500 Loss 1.1797 Accuracy 0.7259\n",
            "Epoch 156 Batch 550 Loss 1.1830 Accuracy 0.7251\n",
            "Epoch 156 Batch 600 Loss 1.1871 Accuracy 0.7247\n",
            "Epoch 156 Batch 650 Loss 1.1899 Accuracy 0.7244\n",
            "Epoch 156 Batch 700 Loss 1.1922 Accuracy 0.7242\n",
            "Epoch 156 Batch 750 Loss 1.1930 Accuracy 0.7241\n",
            "Epoch 156 Batch 800 Loss 1.1961 Accuracy 0.7236\n",
            "Epoch 156 Batch 850 Loss 1.1983 Accuracy 0.7234\n",
            "Epoch 156 Batch 900 Loss 1.2008 Accuracy 0.7228\n",
            "Epoch 156 Batch 950 Loss 1.2028 Accuracy 0.7225\n",
            "Epoch 156 Batch 1000 Loss 1.2051 Accuracy 0.7222\n",
            "Epoch 156 Batch 1050 Loss 1.2062 Accuracy 0.7220\n",
            "discarded batch 1059\n",
            "Epoch 156 Batch 1100 Loss 1.2073 Accuracy 0.7216\n",
            "Epoch 156 Loss 1.2080 Accuracy 0.7214\n",
            "Time taken for 1 epoch: 22.392282724380493 secs\n",
            "\n",
            "discarded batch 23\n",
            "Epoch 156 VALIDATION: Loss 2.5722 Accuracy 0.5748\n",
            "\n",
            "Epoch 157 Batch 0 Loss 1.1865 Accuracy 0.7228\n",
            "Epoch 157 Batch 50 Loss 1.1249 Accuracy 0.7364\n",
            "Epoch 157 Batch 100 Loss 1.1399 Accuracy 0.7339\n",
            "Epoch 157 Batch 150 Loss 1.1472 Accuracy 0.7320\n",
            "Epoch 157 Batch 200 Loss 1.1547 Accuracy 0.7309\n",
            "Epoch 157 Batch 250 Loss 1.1589 Accuracy 0.7295\n",
            "Epoch 157 Batch 300 Loss 1.1650 Accuracy 0.7284\n",
            "discarded batch 343\n",
            "Epoch 157 Batch 350 Loss 1.1693 Accuracy 0.7282\n",
            "Epoch 157 Batch 400 Loss 1.1736 Accuracy 0.7274\n",
            "Epoch 157 Batch 450 Loss 1.1757 Accuracy 0.7270\n",
            "Epoch 157 Batch 500 Loss 1.1787 Accuracy 0.7261\n",
            "Epoch 157 Batch 550 Loss 1.1835 Accuracy 0.7253\n",
            "Epoch 157 Batch 600 Loss 1.1865 Accuracy 0.7249\n",
            "Epoch 157 Batch 650 Loss 1.1891 Accuracy 0.7241\n",
            "Epoch 157 Batch 700 Loss 1.1903 Accuracy 0.7238\n",
            "Epoch 157 Batch 750 Loss 1.1926 Accuracy 0.7235\n",
            "Epoch 157 Batch 800 Loss 1.1937 Accuracy 0.7235\n",
            "Epoch 157 Batch 850 Loss 1.1956 Accuracy 0.7232\n",
            "Epoch 157 Batch 900 Loss 1.1978 Accuracy 0.7230\n",
            "Epoch 157 Batch 950 Loss 1.1994 Accuracy 0.7228\n",
            "Epoch 157 Batch 1000 Loss 1.2009 Accuracy 0.7226\n",
            "Epoch 157 Batch 1050 Loss 1.2032 Accuracy 0.7223\n",
            "Epoch 157 Batch 1100 Loss 1.2057 Accuracy 0.7220\n",
            "Epoch 157 Loss 1.2068 Accuracy 0.7219\n",
            "Time taken for 1 epoch: 22.401437759399414 secs\n",
            "\n",
            "Epoch 158 Batch 0 Loss 1.0826 Accuracy 0.7558\n",
            "Epoch 158 Batch 50 Loss 1.1565 Accuracy 0.7329\n",
            "Epoch 158 Batch 100 Loss 1.1693 Accuracy 0.7300\n",
            "Epoch 158 Batch 150 Loss 1.1751 Accuracy 0.7279\n",
            "Epoch 158 Batch 200 Loss 1.1760 Accuracy 0.7274\n",
            "Epoch 158 Batch 250 Loss 1.1765 Accuracy 0.7278\n",
            "Epoch 158 Batch 300 Loss 1.1833 Accuracy 0.7266\n",
            "Epoch 158 Batch 350 Loss 1.1814 Accuracy 0.7264\n",
            "Epoch 158 Batch 400 Loss 1.1814 Accuracy 0.7268\n",
            "Epoch 158 Batch 450 Loss 1.1858 Accuracy 0.7260\n",
            "Epoch 158 Batch 500 Loss 1.1898 Accuracy 0.7251\n",
            "Epoch 158 Batch 550 Loss 1.1916 Accuracy 0.7246\n",
            "Epoch 158 Batch 600 Loss 1.1923 Accuracy 0.7245\n",
            "Epoch 158 Batch 650 Loss 1.1922 Accuracy 0.7247\n",
            "Epoch 158 Batch 700 Loss 1.1940 Accuracy 0.7245\n",
            "Epoch 158 Batch 750 Loss 1.1939 Accuracy 0.7248\n",
            "Epoch 158 Batch 800 Loss 1.1960 Accuracy 0.7245\n",
            "Epoch 158 Batch 850 Loss 1.1974 Accuracy 0.7240\n",
            "Epoch 158 Batch 900 Loss 1.1994 Accuracy 0.7238\n",
            "Epoch 158 Batch 950 Loss 1.1999 Accuracy 0.7235\n",
            "Epoch 158 Batch 1000 Loss 1.2015 Accuracy 0.7232\n",
            "Epoch 158 Batch 1050 Loss 1.2035 Accuracy 0.7228\n",
            "discarded batch 1068\n",
            "Epoch 158 Batch 1100 Loss 1.2053 Accuracy 0.7224\n",
            "Epoch 158 Loss 1.2056 Accuracy 0.7224\n",
            "Time taken for 1 epoch: 22.60106062889099 secs\n",
            "\n",
            "Epoch 159 Batch 0 Loss 1.1800 Accuracy 0.7294\n",
            "Epoch 159 Batch 50 Loss 1.1441 Accuracy 0.7316\n",
            "Epoch 159 Batch 100 Loss 1.1499 Accuracy 0.7318\n",
            "discarded batch 125\n",
            "Epoch 159 Batch 150 Loss 1.1576 Accuracy 0.7310\n",
            "Epoch 159 Batch 200 Loss 1.1571 Accuracy 0.7306\n",
            "Epoch 159 Batch 250 Loss 1.1585 Accuracy 0.7300\n",
            "Epoch 159 Batch 300 Loss 1.1618 Accuracy 0.7292\n",
            "Epoch 159 Batch 350 Loss 1.1674 Accuracy 0.7284\n",
            "Epoch 159 Batch 400 Loss 1.1715 Accuracy 0.7274\n",
            "Epoch 159 Batch 450 Loss 1.1728 Accuracy 0.7271\n",
            "Epoch 159 Batch 500 Loss 1.1735 Accuracy 0.7268\n",
            "Epoch 159 Batch 550 Loss 1.1768 Accuracy 0.7262\n",
            "Epoch 159 Batch 600 Loss 1.1788 Accuracy 0.7256\n",
            "Epoch 159 Batch 650 Loss 1.1810 Accuracy 0.7251\n",
            "Epoch 159 Batch 700 Loss 1.1862 Accuracy 0.7243\n",
            "Epoch 159 Batch 750 Loss 1.1896 Accuracy 0.7239\n",
            "Epoch 159 Batch 800 Loss 1.1924 Accuracy 0.7234\n",
            "Epoch 159 Batch 850 Loss 1.1952 Accuracy 0.7234\n",
            "Epoch 159 Batch 900 Loss 1.1966 Accuracy 0.7233\n",
            "Epoch 159 Batch 950 Loss 1.1993 Accuracy 0.7228\n",
            "Epoch 159 Batch 1000 Loss 1.2020 Accuracy 0.7225\n",
            "Epoch 159 Batch 1050 Loss 1.2037 Accuracy 0.7222\n",
            "Epoch 159 Batch 1100 Loss 1.2054 Accuracy 0.7221\n",
            "Epoch 159 Loss 1.2054 Accuracy 0.7221\n",
            "Time taken for 1 epoch: 22.64218282699585 secs\n",
            "\n",
            "Epoch 160 Batch 0 Loss 1.0702 Accuracy 0.7393\n",
            "Epoch 160 Batch 50 Loss 1.1689 Accuracy 0.7286\n",
            "Epoch 160 Batch 100 Loss 1.1684 Accuracy 0.7284\n",
            "Epoch 160 Batch 150 Loss 1.1709 Accuracy 0.7288\n",
            "Epoch 160 Batch 200 Loss 1.1721 Accuracy 0.7294\n",
            "Epoch 160 Batch 250 Loss 1.1680 Accuracy 0.7301\n",
            "Epoch 160 Batch 300 Loss 1.1733 Accuracy 0.7284\n",
            "Epoch 160 Batch 350 Loss 1.1766 Accuracy 0.7276\n",
            "Epoch 160 Batch 400 Loss 1.1786 Accuracy 0.7270\n",
            "Epoch 160 Batch 450 Loss 1.1775 Accuracy 0.7273\n",
            "Epoch 160 Batch 500 Loss 1.1785 Accuracy 0.7268\n",
            "Epoch 160 Batch 550 Loss 1.1803 Accuracy 0.7261\n",
            "Epoch 160 Batch 600 Loss 1.1821 Accuracy 0.7257\n",
            "discarded batch 637\n",
            "Epoch 160 Batch 650 Loss 1.1848 Accuracy 0.7253\n",
            "Epoch 160 Batch 700 Loss 1.1859 Accuracy 0.7252\n",
            "Epoch 160 Batch 750 Loss 1.1883 Accuracy 0.7249\n",
            "Epoch 160 Batch 800 Loss 1.1893 Accuracy 0.7246\n",
            "Epoch 160 Batch 850 Loss 1.1906 Accuracy 0.7244\n",
            "Epoch 160 Batch 900 Loss 1.1930 Accuracy 0.7239\n",
            "Epoch 160 Batch 950 Loss 1.1938 Accuracy 0.7238\n",
            "Epoch 160 Batch 1000 Loss 1.1943 Accuracy 0.7238\n",
            "Epoch 160 Batch 1050 Loss 1.1973 Accuracy 0.7236\n",
            "Epoch 160 Batch 1100 Loss 1.1997 Accuracy 0.7234\n",
            "Saving checkpoint for epoch 160 at ./checkpoints/train/ckpt-32\n",
            "Epoch 160 Loss 1.2009 Accuracy 0.7233\n",
            "Time taken for 1 epoch: 22.7620747089386 secs\n",
            "\n",
            "Epoch 161 Batch 0 Loss 1.1810 Accuracy 0.7360\n",
            "Epoch 161 Batch 50 Loss 1.1324 Accuracy 0.7346\n",
            "Epoch 161 Batch 100 Loss 1.1402 Accuracy 0.7334\n",
            "Epoch 161 Batch 150 Loss 1.1505 Accuracy 0.7318\n",
            "Epoch 161 Batch 200 Loss 1.1536 Accuracy 0.7303\n",
            "Epoch 161 Batch 250 Loss 1.1568 Accuracy 0.7294\n",
            "Epoch 161 Batch 300 Loss 1.1580 Accuracy 0.7292\n",
            "Epoch 161 Batch 350 Loss 1.1601 Accuracy 0.7291\n",
            "Epoch 161 Batch 400 Loss 1.1634 Accuracy 0.7288\n",
            "Epoch 161 Batch 450 Loss 1.1672 Accuracy 0.7279\n",
            "Epoch 161 Batch 500 Loss 1.1707 Accuracy 0.7271\n",
            "Epoch 161 Batch 550 Loss 1.1726 Accuracy 0.7269\n",
            "Epoch 161 Batch 600 Loss 1.1747 Accuracy 0.7264\n",
            "Epoch 161 Batch 650 Loss 1.1775 Accuracy 0.7259\n",
            "discarded batch 656\n",
            "Epoch 161 Batch 700 Loss 1.1799 Accuracy 0.7255\n",
            "Epoch 161 Batch 750 Loss 1.1818 Accuracy 0.7253\n",
            "Epoch 161 Batch 800 Loss 1.1853 Accuracy 0.7247\n",
            "Epoch 161 Batch 850 Loss 1.1876 Accuracy 0.7243\n",
            "Epoch 161 Batch 900 Loss 1.1914 Accuracy 0.7238\n",
            "Epoch 161 Batch 950 Loss 1.1929 Accuracy 0.7238\n",
            "Epoch 161 Batch 1000 Loss 1.1951 Accuracy 0.7233\n",
            "Epoch 161 Batch 1050 Loss 1.1963 Accuracy 0.7232\n",
            "Epoch 161 Batch 1100 Loss 1.1977 Accuracy 0.7230\n",
            "Epoch 161 Loss 1.1992 Accuracy 0.7229\n",
            "Time taken for 1 epoch: 22.655568838119507 secs\n",
            "\n",
            "discarded batch 23\n",
            "Epoch 161 VALIDATION: Loss 2.6099 Accuracy 0.5701\n",
            "\n",
            "Epoch 162 Batch 0 Loss 1.2470 Accuracy 0.7360\n",
            "Epoch 162 Batch 50 Loss 1.1095 Accuracy 0.7397\n",
            "Epoch 162 Batch 100 Loss 1.1261 Accuracy 0.7397\n",
            "Epoch 162 Batch 150 Loss 1.1369 Accuracy 0.7364\n",
            "Epoch 162 Batch 200 Loss 1.1464 Accuracy 0.7339\n",
            "discarded batch 245\n",
            "Epoch 162 Batch 250 Loss 1.1556 Accuracy 0.7320\n",
            "Epoch 162 Batch 300 Loss 1.1567 Accuracy 0.7311\n",
            "Epoch 162 Batch 350 Loss 1.1606 Accuracy 0.7304\n",
            "Epoch 162 Batch 400 Loss 1.1638 Accuracy 0.7297\n",
            "Epoch 162 Batch 450 Loss 1.1655 Accuracy 0.7290\n",
            "Epoch 162 Batch 500 Loss 1.1698 Accuracy 0.7280\n",
            "Epoch 162 Batch 550 Loss 1.1728 Accuracy 0.7272\n",
            "Epoch 162 Batch 600 Loss 1.1753 Accuracy 0.7268\n",
            "Epoch 162 Batch 650 Loss 1.1793 Accuracy 0.7264\n",
            "Epoch 162 Batch 700 Loss 1.1807 Accuracy 0.7258\n",
            "Epoch 162 Batch 750 Loss 1.1831 Accuracy 0.7253\n",
            "Epoch 162 Batch 800 Loss 1.1838 Accuracy 0.7253\n",
            "Epoch 162 Batch 850 Loss 1.1860 Accuracy 0.7250\n",
            "Epoch 162 Batch 900 Loss 1.1894 Accuracy 0.7242\n",
            "Epoch 162 Batch 950 Loss 1.1923 Accuracy 0.7239\n",
            "Epoch 162 Batch 1000 Loss 1.1952 Accuracy 0.7235\n",
            "Epoch 162 Batch 1050 Loss 1.1967 Accuracy 0.7232\n",
            "Epoch 162 Batch 1100 Loss 1.1980 Accuracy 0.7231\n",
            "Epoch 162 Loss 1.1990 Accuracy 0.7231\n",
            "Time taken for 1 epoch: 22.66016983985901 secs\n",
            "\n",
            "Epoch 163 Batch 0 Loss 1.1204 Accuracy 0.7327\n",
            "Epoch 163 Batch 50 Loss 1.1436 Accuracy 0.7347\n",
            "Epoch 163 Batch 100 Loss 1.1469 Accuracy 0.7311\n",
            "Epoch 163 Batch 150 Loss 1.1520 Accuracy 0.7298\n",
            "Epoch 163 Batch 200 Loss 1.1506 Accuracy 0.7298\n",
            "Epoch 163 Batch 250 Loss 1.1598 Accuracy 0.7291\n",
            "Epoch 163 Batch 300 Loss 1.1637 Accuracy 0.7282\n",
            "Epoch 163 Batch 350 Loss 1.1673 Accuracy 0.7275\n",
            "discarded batch 392\n",
            "Epoch 163 Batch 400 Loss 1.1671 Accuracy 0.7278\n",
            "Epoch 163 Batch 450 Loss 1.1703 Accuracy 0.7274\n",
            "Epoch 163 Batch 500 Loss 1.1716 Accuracy 0.7275\n",
            "Epoch 163 Batch 550 Loss 1.1743 Accuracy 0.7270\n",
            "Epoch 163 Batch 600 Loss 1.1777 Accuracy 0.7265\n",
            "Epoch 163 Batch 650 Loss 1.1807 Accuracy 0.7260\n",
            "Epoch 163 Batch 700 Loss 1.1801 Accuracy 0.7260\n",
            "Epoch 163 Batch 750 Loss 1.1828 Accuracy 0.7255\n",
            "Epoch 163 Batch 800 Loss 1.1853 Accuracy 0.7250\n",
            "Epoch 163 Batch 850 Loss 1.1871 Accuracy 0.7248\n",
            "Epoch 163 Batch 900 Loss 1.1897 Accuracy 0.7245\n",
            "Epoch 163 Batch 950 Loss 1.1917 Accuracy 0.7242\n",
            "Epoch 163 Batch 1000 Loss 1.1935 Accuracy 0.7240\n",
            "Epoch 163 Batch 1050 Loss 1.1944 Accuracy 0.7236\n",
            "Epoch 163 Batch 1100 Loss 1.1966 Accuracy 0.7231\n",
            "Epoch 163 Loss 1.1977 Accuracy 0.7229\n",
            "Time taken for 1 epoch: 22.944245100021362 secs\n",
            "\n",
            "Epoch 164 Batch 0 Loss 1.1402 Accuracy 0.7360\n",
            "Epoch 164 Batch 50 Loss 1.1165 Accuracy 0.7362\n",
            "Epoch 164 Batch 100 Loss 1.1418 Accuracy 0.7323\n",
            "Epoch 164 Batch 150 Loss 1.1454 Accuracy 0.7308\n",
            "Epoch 164 Batch 200 Loss 1.1512 Accuracy 0.7308\n",
            "Epoch 164 Batch 250 Loss 1.1560 Accuracy 0.7298\n",
            "Epoch 164 Batch 300 Loss 1.1597 Accuracy 0.7287\n",
            "Epoch 164 Batch 350 Loss 1.1610 Accuracy 0.7290\n",
            "Epoch 164 Batch 400 Loss 1.1635 Accuracy 0.7286\n",
            "Epoch 164 Batch 450 Loss 1.1660 Accuracy 0.7283\n",
            "Epoch 164 Batch 500 Loss 1.1688 Accuracy 0.7276\n",
            "Epoch 164 Batch 550 Loss 1.1709 Accuracy 0.7275\n",
            "Epoch 164 Batch 600 Loss 1.1741 Accuracy 0.7271\n",
            "Epoch 164 Batch 650 Loss 1.1754 Accuracy 0.7270\n",
            "Epoch 164 Batch 700 Loss 1.1772 Accuracy 0.7266\n",
            "Epoch 164 Batch 750 Loss 1.1795 Accuracy 0.7262\n",
            "Epoch 164 Batch 800 Loss 1.1813 Accuracy 0.7257\n",
            "Epoch 164 Batch 850 Loss 1.1842 Accuracy 0.7255\n",
            "Epoch 164 Batch 900 Loss 1.1873 Accuracy 0.7252\n",
            "Epoch 164 Batch 950 Loss 1.1888 Accuracy 0.7249\n",
            "Epoch 164 Batch 1000 Loss 1.1906 Accuracy 0.7247\n",
            "Epoch 164 Batch 1050 Loss 1.1926 Accuracy 0.7243\n",
            "discarded batch 1074\n",
            "Epoch 164 Batch 1100 Loss 1.1938 Accuracy 0.7241\n",
            "Epoch 164 Loss 1.1948 Accuracy 0.7240\n",
            "Time taken for 1 epoch: 22.4227237701416 secs\n",
            "\n",
            "Epoch 165 Batch 0 Loss 1.1035 Accuracy 0.7558\n",
            "Epoch 165 Batch 50 Loss 1.1370 Accuracy 0.7328\n",
            "Epoch 165 Batch 100 Loss 1.1437 Accuracy 0.7315\n",
            "Epoch 165 Batch 150 Loss 1.1522 Accuracy 0.7287\n",
            "Epoch 165 Batch 200 Loss 1.1607 Accuracy 0.7298\n",
            "Epoch 165 Batch 250 Loss 1.1624 Accuracy 0.7294\n",
            "Epoch 165 Batch 300 Loss 1.1658 Accuracy 0.7288\n",
            "Epoch 165 Batch 350 Loss 1.1655 Accuracy 0.7285\n",
            "Epoch 165 Batch 400 Loss 1.1673 Accuracy 0.7281\n",
            "Epoch 165 Batch 450 Loss 1.1690 Accuracy 0.7277\n",
            "Epoch 165 Batch 500 Loss 1.1718 Accuracy 0.7273\n",
            "Epoch 165 Batch 550 Loss 1.1728 Accuracy 0.7272\n",
            "Epoch 165 Batch 600 Loss 1.1753 Accuracy 0.7267\n",
            "Epoch 165 Batch 650 Loss 1.1775 Accuracy 0.7264\n",
            "Epoch 165 Batch 700 Loss 1.1796 Accuracy 0.7259\n",
            "Epoch 165 Batch 750 Loss 1.1817 Accuracy 0.7256\n",
            "Epoch 165 Batch 800 Loss 1.1823 Accuracy 0.7254\n",
            "Epoch 165 Batch 850 Loss 1.1842 Accuracy 0.7252\n",
            "Epoch 165 Batch 900 Loss 1.1866 Accuracy 0.7248\n",
            "Epoch 165 Batch 950 Loss 1.1880 Accuracy 0.7246\n",
            "Epoch 165 Batch 1000 Loss 1.1891 Accuracy 0.7243\n",
            "discarded batch 1013\n",
            "Epoch 165 Batch 1050 Loss 1.1904 Accuracy 0.7240\n",
            "Epoch 165 Batch 1100 Loss 1.1915 Accuracy 0.7239\n",
            "Saving checkpoint for epoch 165 at ./checkpoints/train/ckpt-33\n",
            "Epoch 165 Loss 1.1922 Accuracy 0.7239\n",
            "Time taken for 1 epoch: 22.40262722969055 secs\n",
            "\n",
            "Epoch 166 Batch 0 Loss 1.1007 Accuracy 0.7492\n",
            "Epoch 166 Batch 50 Loss 1.1335 Accuracy 0.7328\n",
            "Epoch 166 Batch 100 Loss 1.1374 Accuracy 0.7316\n",
            "Epoch 166 Batch 150 Loss 1.1369 Accuracy 0.7328\n",
            "Epoch 166 Batch 200 Loss 1.1384 Accuracy 0.7322\n",
            "Epoch 166 Batch 250 Loss 1.1409 Accuracy 0.7312\n",
            "Epoch 166 Batch 300 Loss 1.1470 Accuracy 0.7304\n",
            "Epoch 166 Batch 350 Loss 1.1488 Accuracy 0.7308\n",
            "Epoch 166 Batch 400 Loss 1.1512 Accuracy 0.7301\n",
            "Epoch 166 Batch 450 Loss 1.1550 Accuracy 0.7297\n",
            "Epoch 166 Batch 500 Loss 1.1578 Accuracy 0.7297\n",
            "Epoch 166 Batch 550 Loss 1.1609 Accuracy 0.7292\n",
            "Epoch 166 Batch 600 Loss 1.1630 Accuracy 0.7291\n",
            "Epoch 166 Batch 650 Loss 1.1671 Accuracy 0.7286\n",
            "Epoch 166 Batch 700 Loss 1.1697 Accuracy 0.7281\n",
            "Epoch 166 Batch 750 Loss 1.1710 Accuracy 0.7279\n",
            "Epoch 166 Batch 800 Loss 1.1733 Accuracy 0.7276\n",
            "Epoch 166 Batch 850 Loss 1.1746 Accuracy 0.7273\n",
            "Epoch 166 Batch 900 Loss 1.1776 Accuracy 0.7268\n",
            "Epoch 166 Batch 950 Loss 1.1804 Accuracy 0.7262\n",
            "Epoch 166 Batch 1000 Loss 1.1826 Accuracy 0.7257\n",
            "discarded batch 1023\n",
            "Epoch 166 Batch 1050 Loss 1.1850 Accuracy 0.7253\n",
            "Epoch 166 Batch 1100 Loss 1.1870 Accuracy 0.7251\n",
            "Epoch 166 Loss 1.1883 Accuracy 0.7249\n",
            "Time taken for 1 epoch: 22.43497920036316 secs\n",
            "\n",
            "discarded batch 23\n",
            "Epoch 166 VALIDATION: Loss 2.6226 Accuracy 0.5738\n",
            "\n",
            "Epoch 167 Batch 0 Loss 0.9854 Accuracy 0.7591\n",
            "Epoch 167 Batch 50 Loss 1.1183 Accuracy 0.7358\n",
            "Epoch 167 Batch 100 Loss 1.1330 Accuracy 0.7325\n",
            "Epoch 167 Batch 150 Loss 1.1371 Accuracy 0.7318\n",
            "Epoch 167 Batch 200 Loss 1.1413 Accuracy 0.7324\n",
            "Epoch 167 Batch 250 Loss 1.1462 Accuracy 0.7309\n",
            "Epoch 167 Batch 300 Loss 1.1485 Accuracy 0.7306\n",
            "Epoch 167 Batch 350 Loss 1.1538 Accuracy 0.7302\n",
            "Epoch 167 Batch 400 Loss 1.1570 Accuracy 0.7302\n",
            "Epoch 167 Batch 450 Loss 1.1582 Accuracy 0.7300\n",
            "Epoch 167 Batch 500 Loss 1.1609 Accuracy 0.7297\n",
            "Epoch 167 Batch 550 Loss 1.1634 Accuracy 0.7295\n",
            "Epoch 167 Batch 600 Loss 1.1643 Accuracy 0.7292\n",
            "Epoch 167 Batch 650 Loss 1.1676 Accuracy 0.7290\n",
            "Epoch 167 Batch 700 Loss 1.1724 Accuracy 0.7279\n",
            "Epoch 167 Batch 750 Loss 1.1741 Accuracy 0.7277\n",
            "Epoch 167 Batch 800 Loss 1.1757 Accuracy 0.7273\n",
            "Epoch 167 Batch 850 Loss 1.1776 Accuracy 0.7271\n",
            "Epoch 167 Batch 900 Loss 1.1797 Accuracy 0.7269\n",
            "Epoch 167 Batch 950 Loss 1.1815 Accuracy 0.7266\n",
            "discarded batch 954\n",
            "Epoch 167 Batch 1000 Loss 1.1831 Accuracy 0.7260\n",
            "Epoch 167 Batch 1050 Loss 1.1851 Accuracy 0.7256\n",
            "Epoch 167 Batch 1100 Loss 1.1869 Accuracy 0.7254\n",
            "Epoch 167 Loss 1.1876 Accuracy 0.7253\n",
            "Time taken for 1 epoch: 22.47606873512268 secs\n",
            "\n",
            "Epoch 168 Batch 0 Loss 1.2138 Accuracy 0.7261\n",
            "Epoch 168 Batch 50 Loss 1.1374 Accuracy 0.7336\n",
            "Epoch 168 Batch 100 Loss 1.1443 Accuracy 0.7310\n",
            "Epoch 168 Batch 150 Loss 1.1462 Accuracy 0.7320\n",
            "Epoch 168 Batch 200 Loss 1.1461 Accuracy 0.7325\n",
            "Epoch 168 Batch 250 Loss 1.1541 Accuracy 0.7307\n",
            "Epoch 168 Batch 300 Loss 1.1575 Accuracy 0.7303\n",
            "Epoch 168 Batch 350 Loss 1.1582 Accuracy 0.7302\n",
            "Epoch 168 Batch 400 Loss 1.1595 Accuracy 0.7293\n",
            "Epoch 168 Batch 450 Loss 1.1625 Accuracy 0.7289\n",
            "Epoch 168 Batch 500 Loss 1.1620 Accuracy 0.7291\n",
            "Epoch 168 Batch 550 Loss 1.1616 Accuracy 0.7287\n",
            "Epoch 168 Batch 600 Loss 1.1657 Accuracy 0.7280\n",
            "Epoch 168 Batch 650 Loss 1.1688 Accuracy 0.7277\n",
            "Epoch 168 Batch 700 Loss 1.1709 Accuracy 0.7271\n",
            "Epoch 168 Batch 750 Loss 1.1731 Accuracy 0.7268\n",
            "Epoch 168 Batch 800 Loss 1.1766 Accuracy 0.7262\n",
            "Epoch 168 Batch 850 Loss 1.1788 Accuracy 0.7260\n",
            "Epoch 168 Batch 900 Loss 1.1800 Accuracy 0.7259\n",
            "Epoch 168 Batch 950 Loss 1.1825 Accuracy 0.7255\n",
            "Epoch 168 Batch 1000 Loss 1.1836 Accuracy 0.7254\n",
            "discarded batch 1023\n",
            "Epoch 168 Batch 1050 Loss 1.1844 Accuracy 0.7254\n",
            "Epoch 168 Batch 1100 Loss 1.1850 Accuracy 0.7253\n",
            "Epoch 168 Loss 1.1858 Accuracy 0.7252\n",
            "Time taken for 1 epoch: 22.366503477096558 secs\n",
            "\n",
            "Epoch 169 Batch 0 Loss 1.0056 Accuracy 0.7624\n",
            "Epoch 169 Batch 50 Loss 1.1307 Accuracy 0.7341\n",
            "discarded batch 71\n",
            "Epoch 169 Batch 100 Loss 1.1413 Accuracy 0.7316\n",
            "Epoch 169 Batch 150 Loss 1.1429 Accuracy 0.7309\n",
            "Epoch 169 Batch 200 Loss 1.1457 Accuracy 0.7307\n",
            "Epoch 169 Batch 250 Loss 1.1526 Accuracy 0.7300\n",
            "Epoch 169 Batch 300 Loss 1.1542 Accuracy 0.7298\n",
            "Epoch 169 Batch 350 Loss 1.1536 Accuracy 0.7299\n",
            "Epoch 169 Batch 400 Loss 1.1565 Accuracy 0.7296\n",
            "Epoch 169 Batch 450 Loss 1.1574 Accuracy 0.7291\n",
            "Epoch 169 Batch 500 Loss 1.1622 Accuracy 0.7282\n",
            "Epoch 169 Batch 550 Loss 1.1634 Accuracy 0.7284\n",
            "Epoch 169 Batch 600 Loss 1.1641 Accuracy 0.7284\n",
            "Epoch 169 Batch 650 Loss 1.1661 Accuracy 0.7278\n",
            "Epoch 169 Batch 700 Loss 1.1690 Accuracy 0.7274\n",
            "Epoch 169 Batch 750 Loss 1.1721 Accuracy 0.7271\n",
            "Epoch 169 Batch 800 Loss 1.1742 Accuracy 0.7266\n",
            "Epoch 169 Batch 850 Loss 1.1754 Accuracy 0.7263\n",
            "Epoch 169 Batch 900 Loss 1.1767 Accuracy 0.7260\n",
            "Epoch 169 Batch 950 Loss 1.1785 Accuracy 0.7258\n",
            "Epoch 169 Batch 1000 Loss 1.1802 Accuracy 0.7256\n",
            "Epoch 169 Batch 1050 Loss 1.1828 Accuracy 0.7251\n",
            "Epoch 169 Batch 1100 Loss 1.1847 Accuracy 0.7248\n",
            "Epoch 169 Loss 1.1857 Accuracy 0.7247\n",
            "Time taken for 1 epoch: 22.211049556732178 secs\n",
            "\n",
            "Epoch 170 Batch 0 Loss 1.2000 Accuracy 0.7195\n",
            "Epoch 170 Batch 50 Loss 1.1556 Accuracy 0.7310\n",
            "discarded batch 77\n",
            "Epoch 170 Batch 100 Loss 1.1452 Accuracy 0.7341\n",
            "Epoch 170 Batch 150 Loss 1.1513 Accuracy 0.7325\n",
            "Epoch 170 Batch 200 Loss 1.1548 Accuracy 0.7315\n",
            "Epoch 170 Batch 250 Loss 1.1567 Accuracy 0.7306\n",
            "Epoch 170 Batch 300 Loss 1.1527 Accuracy 0.7316\n",
            "Epoch 170 Batch 350 Loss 1.1588 Accuracy 0.7305\n",
            "Epoch 170 Batch 400 Loss 1.1588 Accuracy 0.7306\n",
            "Epoch 170 Batch 450 Loss 1.1576 Accuracy 0.7306\n",
            "Epoch 170 Batch 500 Loss 1.1600 Accuracy 0.7300\n",
            "Epoch 170 Batch 550 Loss 1.1632 Accuracy 0.7292\n",
            "Epoch 170 Batch 600 Loss 1.1647 Accuracy 0.7289\n",
            "Epoch 170 Batch 650 Loss 1.1672 Accuracy 0.7285\n",
            "Epoch 170 Batch 700 Loss 1.1705 Accuracy 0.7276\n",
            "Epoch 170 Batch 750 Loss 1.1728 Accuracy 0.7272\n",
            "Epoch 170 Batch 800 Loss 1.1749 Accuracy 0.7266\n",
            "Epoch 170 Batch 850 Loss 1.1753 Accuracy 0.7267\n",
            "Epoch 170 Batch 900 Loss 1.1758 Accuracy 0.7265\n",
            "Epoch 170 Batch 950 Loss 1.1786 Accuracy 0.7260\n",
            "Epoch 170 Batch 1000 Loss 1.1796 Accuracy 0.7258\n",
            "Epoch 170 Batch 1050 Loss 1.1820 Accuracy 0.7253\n",
            "Epoch 170 Batch 1100 Loss 1.1829 Accuracy 0.7251\n",
            "Saving checkpoint for epoch 170 at ./checkpoints/train/ckpt-34\n",
            "Epoch 170 Loss 1.1846 Accuracy 0.7249\n",
            "Time taken for 1 epoch: 22.826204538345337 secs\n",
            "\n",
            "Epoch 171 Batch 0 Loss 1.1915 Accuracy 0.7162\n",
            "Epoch 171 Batch 50 Loss 1.1344 Accuracy 0.7315\n",
            "Epoch 171 Batch 100 Loss 1.1290 Accuracy 0.7342\n",
            "Epoch 171 Batch 150 Loss 1.1379 Accuracy 0.7328\n",
            "Epoch 171 Batch 200 Loss 1.1358 Accuracy 0.7331\n",
            "Epoch 171 Batch 250 Loss 1.1415 Accuracy 0.7326\n",
            "Epoch 171 Batch 300 Loss 1.1514 Accuracy 0.7315\n",
            "Epoch 171 Batch 350 Loss 1.1571 Accuracy 0.7298\n",
            "Epoch 171 Batch 400 Loss 1.1564 Accuracy 0.7300\n",
            "Epoch 171 Batch 450 Loss 1.1585 Accuracy 0.7299\n",
            "discarded batch 460\n",
            "Epoch 171 Batch 500 Loss 1.1602 Accuracy 0.7297\n",
            "Epoch 171 Batch 550 Loss 1.1623 Accuracy 0.7293\n",
            "Epoch 171 Batch 600 Loss 1.1643 Accuracy 0.7287\n",
            "Epoch 171 Batch 650 Loss 1.1665 Accuracy 0.7282\n",
            "Epoch 171 Batch 700 Loss 1.1696 Accuracy 0.7276\n",
            "Epoch 171 Batch 750 Loss 1.1703 Accuracy 0.7276\n",
            "Epoch 171 Batch 800 Loss 1.1727 Accuracy 0.7273\n",
            "Epoch 171 Batch 850 Loss 1.1734 Accuracy 0.7271\n",
            "Epoch 171 Batch 900 Loss 1.1756 Accuracy 0.7265\n",
            "Epoch 171 Batch 950 Loss 1.1775 Accuracy 0.7264\n",
            "Epoch 171 Batch 1000 Loss 1.1796 Accuracy 0.7260\n",
            "Epoch 171 Batch 1050 Loss 1.1801 Accuracy 0.7258\n",
            "Epoch 171 Batch 1100 Loss 1.1807 Accuracy 0.7259\n",
            "Epoch 171 Loss 1.1816 Accuracy 0.7257\n",
            "Time taken for 1 epoch: 22.516698122024536 secs\n",
            "\n",
            "discarded batch 23\n",
            "Epoch 171 VALIDATION: Loss 2.6507 Accuracy 0.5724\n",
            "\n",
            "Epoch 172 Batch 0 Loss 1.1432 Accuracy 0.7162\n",
            "Epoch 172 Batch 50 Loss 1.1394 Accuracy 0.7318\n",
            "Epoch 172 Batch 100 Loss 1.1423 Accuracy 0.7319\n",
            "Epoch 172 Batch 150 Loss 1.1409 Accuracy 0.7321\n",
            "Epoch 172 Batch 200 Loss 1.1411 Accuracy 0.7321\n",
            "Epoch 172 Batch 250 Loss 1.1435 Accuracy 0.7314\n",
            "Epoch 172 Batch 300 Loss 1.1460 Accuracy 0.7314\n",
            "discarded batch 322\n",
            "Epoch 172 Batch 350 Loss 1.1508 Accuracy 0.7307\n",
            "Epoch 172 Batch 400 Loss 1.1528 Accuracy 0.7303\n",
            "Epoch 172 Batch 450 Loss 1.1557 Accuracy 0.7296\n",
            "Epoch 172 Batch 500 Loss 1.1589 Accuracy 0.7289\n",
            "Epoch 172 Batch 550 Loss 1.1624 Accuracy 0.7285\n",
            "Epoch 172 Batch 600 Loss 1.1658 Accuracy 0.7280\n",
            "Epoch 172 Batch 650 Loss 1.1683 Accuracy 0.7276\n",
            "Epoch 172 Batch 700 Loss 1.1692 Accuracy 0.7274\n",
            "Epoch 172 Batch 750 Loss 1.1721 Accuracy 0.7270\n",
            "Epoch 172 Batch 800 Loss 1.1738 Accuracy 0.7267\n",
            "Epoch 172 Batch 850 Loss 1.1754 Accuracy 0.7267\n",
            "Epoch 172 Batch 900 Loss 1.1763 Accuracy 0.7266\n",
            "Epoch 172 Batch 950 Loss 1.1771 Accuracy 0.7263\n",
            "Epoch 172 Batch 1000 Loss 1.1776 Accuracy 0.7263\n",
            "Epoch 172 Batch 1050 Loss 1.1798 Accuracy 0.7260\n",
            "Epoch 172 Batch 1100 Loss 1.1812 Accuracy 0.7258\n",
            "Epoch 172 Loss 1.1823 Accuracy 0.7257\n",
            "Time taken for 1 epoch: 22.358026027679443 secs\n",
            "\n",
            "Epoch 173 Batch 0 Loss 1.1105 Accuracy 0.7393\n",
            "Epoch 173 Batch 50 Loss 1.1314 Accuracy 0.7336\n",
            "Epoch 173 Batch 100 Loss 1.1280 Accuracy 0.7341\n",
            "Epoch 173 Batch 150 Loss 1.1335 Accuracy 0.7344\n",
            "Epoch 173 Batch 200 Loss 1.1333 Accuracy 0.7342\n",
            "discarded batch 232\n",
            "Epoch 173 Batch 250 Loss 1.1376 Accuracy 0.7335\n",
            "Epoch 173 Batch 300 Loss 1.1427 Accuracy 0.7328\n",
            "Epoch 173 Batch 350 Loss 1.1458 Accuracy 0.7325\n",
            "Epoch 173 Batch 400 Loss 1.1494 Accuracy 0.7318\n",
            "Epoch 173 Batch 450 Loss 1.1511 Accuracy 0.7313\n",
            "Epoch 173 Batch 500 Loss 1.1525 Accuracy 0.7309\n",
            "Epoch 173 Batch 550 Loss 1.1525 Accuracy 0.7309\n",
            "Epoch 173 Batch 600 Loss 1.1565 Accuracy 0.7303\n",
            "Epoch 173 Batch 650 Loss 1.1590 Accuracy 0.7298\n",
            "Epoch 173 Batch 700 Loss 1.1613 Accuracy 0.7293\n",
            "Epoch 173 Batch 750 Loss 1.1640 Accuracy 0.7287\n",
            "Epoch 173 Batch 800 Loss 1.1661 Accuracy 0.7284\n",
            "Epoch 173 Batch 850 Loss 1.1690 Accuracy 0.7280\n",
            "Epoch 173 Batch 900 Loss 1.1714 Accuracy 0.7274\n",
            "Epoch 173 Batch 950 Loss 1.1734 Accuracy 0.7270\n",
            "Epoch 173 Batch 1000 Loss 1.1757 Accuracy 0.7269\n",
            "Epoch 173 Batch 1050 Loss 1.1780 Accuracy 0.7265\n",
            "Epoch 173 Batch 1100 Loss 1.1789 Accuracy 0.7264\n",
            "Epoch 173 Loss 1.1793 Accuracy 0.7262\n",
            "Time taken for 1 epoch: 22.24671196937561 secs\n",
            "\n",
            "Epoch 174 Batch 0 Loss 1.2565 Accuracy 0.7327\n",
            "Epoch 174 Batch 50 Loss 1.1492 Accuracy 0.7354\n",
            "Epoch 174 Batch 100 Loss 1.1411 Accuracy 0.7358\n",
            "Epoch 174 Batch 150 Loss 1.1498 Accuracy 0.7345\n",
            "discarded batch 184\n",
            "Epoch 174 Batch 200 Loss 1.1517 Accuracy 0.7329\n",
            "Epoch 174 Batch 250 Loss 1.1498 Accuracy 0.7326\n",
            "Epoch 174 Batch 300 Loss 1.1512 Accuracy 0.7321\n",
            "Epoch 174 Batch 350 Loss 1.1519 Accuracy 0.7318\n",
            "Epoch 174 Batch 400 Loss 1.1529 Accuracy 0.7316\n",
            "Epoch 174 Batch 450 Loss 1.1545 Accuracy 0.7312\n",
            "Epoch 174 Batch 500 Loss 1.1574 Accuracy 0.7299\n",
            "Epoch 174 Batch 550 Loss 1.1584 Accuracy 0.7300\n",
            "Epoch 174 Batch 600 Loss 1.1586 Accuracy 0.7299\n",
            "Epoch 174 Batch 650 Loss 1.1622 Accuracy 0.7289\n",
            "Epoch 174 Batch 700 Loss 1.1639 Accuracy 0.7288\n",
            "Epoch 174 Batch 750 Loss 1.1663 Accuracy 0.7282\n",
            "Epoch 174 Batch 800 Loss 1.1678 Accuracy 0.7279\n",
            "Epoch 174 Batch 850 Loss 1.1687 Accuracy 0.7280\n",
            "Epoch 174 Batch 900 Loss 1.1706 Accuracy 0.7277\n",
            "Epoch 174 Batch 950 Loss 1.1724 Accuracy 0.7275\n",
            "Epoch 174 Batch 1000 Loss 1.1739 Accuracy 0.7271\n",
            "Epoch 174 Batch 1050 Loss 1.1740 Accuracy 0.7272\n",
            "Epoch 174 Batch 1100 Loss 1.1760 Accuracy 0.7267\n",
            "Epoch 174 Loss 1.1773 Accuracy 0.7265\n",
            "Time taken for 1 epoch: 22.505531787872314 secs\n",
            "\n",
            "Epoch 175 Batch 0 Loss 1.1402 Accuracy 0.7063\n",
            "Epoch 175 Batch 50 Loss 1.1235 Accuracy 0.7363\n",
            "Epoch 175 Batch 100 Loss 1.1246 Accuracy 0.7360\n",
            "Epoch 175 Batch 150 Loss 1.1258 Accuracy 0.7357\n",
            "Epoch 175 Batch 200 Loss 1.1315 Accuracy 0.7341\n",
            "Epoch 175 Batch 250 Loss 1.1320 Accuracy 0.7337\n",
            "Epoch 175 Batch 300 Loss 1.1353 Accuracy 0.7328\n",
            "Epoch 175 Batch 350 Loss 1.1378 Accuracy 0.7327\n",
            "discarded batch 372\n",
            "Epoch 175 Batch 400 Loss 1.1406 Accuracy 0.7326\n",
            "Epoch 175 Batch 450 Loss 1.1428 Accuracy 0.7318\n",
            "Epoch 175 Batch 500 Loss 1.1458 Accuracy 0.7315\n",
            "Epoch 175 Batch 550 Loss 1.1492 Accuracy 0.7309\n",
            "Epoch 175 Batch 600 Loss 1.1507 Accuracy 0.7306\n",
            "Epoch 175 Batch 650 Loss 1.1516 Accuracy 0.7306\n",
            "Epoch 175 Batch 700 Loss 1.1542 Accuracy 0.7299\n",
            "Epoch 175 Batch 750 Loss 1.1570 Accuracy 0.7298\n",
            "Epoch 175 Batch 800 Loss 1.1603 Accuracy 0.7292\n",
            "Epoch 175 Batch 850 Loss 1.1631 Accuracy 0.7288\n",
            "Epoch 175 Batch 900 Loss 1.1657 Accuracy 0.7284\n",
            "Epoch 175 Batch 950 Loss 1.1678 Accuracy 0.7280\n",
            "Epoch 175 Batch 1000 Loss 1.1700 Accuracy 0.7277\n",
            "Epoch 175 Batch 1050 Loss 1.1717 Accuracy 0.7273\n",
            "Epoch 175 Batch 1100 Loss 1.1738 Accuracy 0.7270\n",
            "Saving checkpoint for epoch 175 at ./checkpoints/train/ckpt-35\n",
            "Epoch 175 Loss 1.1747 Accuracy 0.7267\n",
            "Time taken for 1 epoch: 22.623807907104492 secs\n",
            "\n",
            "Epoch 176 Batch 0 Loss 1.0606 Accuracy 0.7558\n",
            "Epoch 176 Batch 50 Loss 1.1053 Accuracy 0.7363\n",
            "Epoch 176 Batch 100 Loss 1.1261 Accuracy 0.7324\n",
            "Epoch 176 Batch 150 Loss 1.1387 Accuracy 0.7330\n",
            "Epoch 176 Batch 200 Loss 1.1400 Accuracy 0.7321\n",
            "Epoch 176 Batch 250 Loss 1.1373 Accuracy 0.7321\n",
            "Epoch 176 Batch 300 Loss 1.1378 Accuracy 0.7318\n",
            "Epoch 176 Batch 350 Loss 1.1423 Accuracy 0.7315\n",
            "Epoch 176 Batch 400 Loss 1.1450 Accuracy 0.7314\n",
            "Epoch 176 Batch 450 Loss 1.1472 Accuracy 0.7312\n",
            "Epoch 176 Batch 500 Loss 1.1502 Accuracy 0.7307\n",
            "Epoch 176 Batch 550 Loss 1.1519 Accuracy 0.7304\n",
            "Epoch 176 Batch 600 Loss 1.1547 Accuracy 0.7298\n",
            "Epoch 176 Batch 650 Loss 1.1570 Accuracy 0.7294\n",
            "Epoch 176 Batch 700 Loss 1.1575 Accuracy 0.7291\n",
            "discarded batch 718\n",
            "Epoch 176 Batch 750 Loss 1.1591 Accuracy 0.7289\n",
            "Epoch 176 Batch 800 Loss 1.1627 Accuracy 0.7285\n",
            "Epoch 176 Batch 850 Loss 1.1649 Accuracy 0.7281\n",
            "Epoch 176 Batch 900 Loss 1.1665 Accuracy 0.7278\n",
            "Epoch 176 Batch 950 Loss 1.1678 Accuracy 0.7277\n",
            "Epoch 176 Batch 1000 Loss 1.1689 Accuracy 0.7277\n",
            "Epoch 176 Batch 1050 Loss 1.1707 Accuracy 0.7273\n",
            "Epoch 176 Batch 1100 Loss 1.1727 Accuracy 0.7270\n",
            "Epoch 176 Loss 1.1734 Accuracy 0.7271\n",
            "Time taken for 1 epoch: 22.38076114654541 secs\n",
            "\n",
            "discarded batch 23\n",
            "Epoch 176 VALIDATION: Loss 2.6306 Accuracy 0.5717\n",
            "\n",
            "Epoch 177 Batch 0 Loss 1.0614 Accuracy 0.7426\n",
            "Epoch 177 Batch 50 Loss 1.1202 Accuracy 0.7367\n",
            "Epoch 177 Batch 100 Loss 1.1173 Accuracy 0.7368\n",
            "Epoch 177 Batch 150 Loss 1.1245 Accuracy 0.7359\n",
            "discarded batch 170\n",
            "Epoch 177 Batch 200 Loss 1.1258 Accuracy 0.7358\n",
            "Epoch 177 Batch 250 Loss 1.1296 Accuracy 0.7351\n",
            "Epoch 177 Batch 300 Loss 1.1334 Accuracy 0.7347\n",
            "Epoch 177 Batch 350 Loss 1.1348 Accuracy 0.7343\n",
            "Epoch 177 Batch 400 Loss 1.1399 Accuracy 0.7334\n",
            "Epoch 177 Batch 450 Loss 1.1451 Accuracy 0.7322\n",
            "Epoch 177 Batch 500 Loss 1.1469 Accuracy 0.7318\n",
            "Epoch 177 Batch 550 Loss 1.1496 Accuracy 0.7311\n",
            "Epoch 177 Batch 600 Loss 1.1524 Accuracy 0.7308\n",
            "Epoch 177 Batch 650 Loss 1.1537 Accuracy 0.7306\n",
            "Epoch 177 Batch 700 Loss 1.1546 Accuracy 0.7303\n",
            "Epoch 177 Batch 750 Loss 1.1564 Accuracy 0.7298\n",
            "Epoch 177 Batch 800 Loss 1.1591 Accuracy 0.7294\n",
            "Epoch 177 Batch 850 Loss 1.1615 Accuracy 0.7291\n",
            "Epoch 177 Batch 900 Loss 1.1631 Accuracy 0.7290\n",
            "Epoch 177 Batch 950 Loss 1.1644 Accuracy 0.7290\n",
            "Epoch 177 Batch 1000 Loss 1.1659 Accuracy 0.7286\n",
            "Epoch 177 Batch 1050 Loss 1.1685 Accuracy 0.7281\n",
            "Epoch 177 Batch 1100 Loss 1.1698 Accuracy 0.7280\n",
            "Epoch 177 Loss 1.1715 Accuracy 0.7278\n",
            "Time taken for 1 epoch: 22.749914169311523 secs\n",
            "\n",
            "Epoch 178 Batch 0 Loss 1.1131 Accuracy 0.7294\n",
            "Epoch 178 Batch 50 Loss 1.1179 Accuracy 0.7324\n",
            "Epoch 178 Batch 100 Loss 1.1233 Accuracy 0.7359\n",
            "Epoch 178 Batch 150 Loss 1.1307 Accuracy 0.7346\n",
            "Epoch 178 Batch 200 Loss 1.1315 Accuracy 0.7341\n",
            "Epoch 178 Batch 250 Loss 1.1335 Accuracy 0.7333\n",
            "Epoch 178 Batch 300 Loss 1.1359 Accuracy 0.7331\n",
            "discarded batch 320\n",
            "Epoch 178 Batch 350 Loss 1.1388 Accuracy 0.7325\n",
            "Epoch 178 Batch 400 Loss 1.1408 Accuracy 0.7319\n",
            "Epoch 178 Batch 450 Loss 1.1428 Accuracy 0.7320\n",
            "Epoch 178 Batch 500 Loss 1.1442 Accuracy 0.7319\n",
            "Epoch 178 Batch 550 Loss 1.1458 Accuracy 0.7312\n",
            "Epoch 178 Batch 600 Loss 1.1477 Accuracy 0.7306\n",
            "Epoch 178 Batch 650 Loss 1.1500 Accuracy 0.7304\n",
            "Epoch 178 Batch 700 Loss 1.1518 Accuracy 0.7303\n",
            "Epoch 178 Batch 750 Loss 1.1547 Accuracy 0.7298\n",
            "Epoch 178 Batch 800 Loss 1.1558 Accuracy 0.7298\n",
            "Epoch 178 Batch 850 Loss 1.1561 Accuracy 0.7299\n",
            "Epoch 178 Batch 900 Loss 1.1578 Accuracy 0.7294\n",
            "Epoch 178 Batch 950 Loss 1.1597 Accuracy 0.7291\n",
            "Epoch 178 Batch 1000 Loss 1.1626 Accuracy 0.7286\n",
            "Epoch 178 Batch 1050 Loss 1.1657 Accuracy 0.7282\n",
            "Epoch 178 Batch 1100 Loss 1.1670 Accuracy 0.7281\n",
            "Epoch 178 Loss 1.1682 Accuracy 0.7280\n",
            "Time taken for 1 epoch: 22.421929121017456 secs\n",
            "\n",
            "Epoch 179 Batch 0 Loss 1.1684 Accuracy 0.7294\n",
            "Epoch 179 Batch 50 Loss 1.1250 Accuracy 0.7372\n",
            "Epoch 179 Batch 100 Loss 1.1208 Accuracy 0.7370\n",
            "Epoch 179 Batch 150 Loss 1.1184 Accuracy 0.7376\n",
            "Epoch 179 Batch 200 Loss 1.1258 Accuracy 0.7357\n",
            "Epoch 179 Batch 250 Loss 1.1290 Accuracy 0.7351\n",
            "Epoch 179 Batch 300 Loss 1.1292 Accuracy 0.7349\n",
            "Epoch 179 Batch 350 Loss 1.1331 Accuracy 0.7339\n",
            "Epoch 179 Batch 400 Loss 1.1359 Accuracy 0.7332\n",
            "Epoch 179 Batch 450 Loss 1.1385 Accuracy 0.7330\n",
            "Epoch 179 Batch 500 Loss 1.1414 Accuracy 0.7326\n",
            "Epoch 179 Batch 550 Loss 1.1433 Accuracy 0.7324\n",
            "Epoch 179 Batch 600 Loss 1.1464 Accuracy 0.7315\n",
            "Epoch 179 Batch 650 Loss 1.1495 Accuracy 0.7311\n",
            "Epoch 179 Batch 700 Loss 1.1511 Accuracy 0.7308\n",
            "Epoch 179 Batch 750 Loss 1.1533 Accuracy 0.7303\n",
            "Epoch 179 Batch 800 Loss 1.1564 Accuracy 0.7299\n",
            "Epoch 179 Batch 850 Loss 1.1588 Accuracy 0.7293\n",
            "Epoch 179 Batch 900 Loss 1.1619 Accuracy 0.7286\n",
            "Epoch 179 Batch 950 Loss 1.1629 Accuracy 0.7284\n",
            "Epoch 179 Batch 1000 Loss 1.1637 Accuracy 0.7285\n",
            "discarded batch 1006\n",
            "Epoch 179 Batch 1050 Loss 1.1656 Accuracy 0.7283\n",
            "Epoch 179 Batch 1100 Loss 1.1672 Accuracy 0.7280\n",
            "Epoch 179 Loss 1.1681 Accuracy 0.7279\n",
            "Time taken for 1 epoch: 22.2721004486084 secs\n",
            "\n",
            "Epoch 180 Batch 0 Loss 1.0630 Accuracy 0.7591\n",
            "Epoch 180 Batch 50 Loss 1.0982 Accuracy 0.7417\n",
            "Epoch 180 Batch 100 Loss 1.0961 Accuracy 0.7416\n",
            "Epoch 180 Batch 150 Loss 1.1062 Accuracy 0.7399\n",
            "Epoch 180 Batch 200 Loss 1.1142 Accuracy 0.7377\n",
            "Epoch 180 Batch 250 Loss 1.1211 Accuracy 0.7364\n",
            "Epoch 180 Batch 300 Loss 1.1252 Accuracy 0.7358\n",
            "Epoch 180 Batch 350 Loss 1.1331 Accuracy 0.7342\n",
            "Epoch 180 Batch 400 Loss 1.1342 Accuracy 0.7343\n",
            "Epoch 180 Batch 450 Loss 1.1390 Accuracy 0.7335\n",
            "Epoch 180 Batch 500 Loss 1.1418 Accuracy 0.7327\n",
            "Epoch 180 Batch 550 Loss 1.1442 Accuracy 0.7317\n",
            "Epoch 180 Batch 600 Loss 1.1470 Accuracy 0.7311\n",
            "Epoch 180 Batch 650 Loss 1.1507 Accuracy 0.7305\n",
            "Epoch 180 Batch 700 Loss 1.1532 Accuracy 0.7300\n",
            "Epoch 180 Batch 750 Loss 1.1559 Accuracy 0.7295\n",
            "discarded batch 755\n",
            "Epoch 180 Batch 800 Loss 1.1588 Accuracy 0.7291\n",
            "Epoch 180 Batch 850 Loss 1.1601 Accuracy 0.7289\n",
            "Epoch 180 Batch 900 Loss 1.1613 Accuracy 0.7288\n",
            "Epoch 180 Batch 950 Loss 1.1630 Accuracy 0.7284\n",
            "Epoch 180 Batch 1000 Loss 1.1648 Accuracy 0.7282\n",
            "Epoch 180 Batch 1050 Loss 1.1669 Accuracy 0.7279\n",
            "Epoch 180 Batch 1100 Loss 1.1680 Accuracy 0.7277\n",
            "Saving checkpoint for epoch 180 at ./checkpoints/train/ckpt-36\n",
            "Epoch 180 Loss 1.1687 Accuracy 0.7278\n",
            "Time taken for 1 epoch: 22.569928646087646 secs\n",
            "\n",
            "Epoch 181 Batch 0 Loss 1.2000 Accuracy 0.7327\n",
            "Epoch 181 Batch 50 Loss 1.1114 Accuracy 0.7385\n",
            "Epoch 181 Batch 100 Loss 1.1203 Accuracy 0.7360\n",
            "Epoch 181 Batch 150 Loss 1.1246 Accuracy 0.7351\n",
            "Epoch 181 Batch 200 Loss 1.1262 Accuracy 0.7350\n",
            "Epoch 181 Batch 250 Loss 1.1311 Accuracy 0.7338\n",
            "Epoch 181 Batch 300 Loss 1.1388 Accuracy 0.7317\n",
            "Epoch 181 Batch 350 Loss 1.1405 Accuracy 0.7317\n",
            "Epoch 181 Batch 400 Loss 1.1407 Accuracy 0.7317\n",
            "Epoch 181 Batch 450 Loss 1.1411 Accuracy 0.7318\n",
            "Epoch 181 Batch 500 Loss 1.1424 Accuracy 0.7315\n",
            "discarded batch 525\n",
            "Epoch 181 Batch 550 Loss 1.1437 Accuracy 0.7314\n",
            "Epoch 181 Batch 600 Loss 1.1448 Accuracy 0.7313\n",
            "Epoch 181 Batch 650 Loss 1.1467 Accuracy 0.7309\n",
            "Epoch 181 Batch 700 Loss 1.1493 Accuracy 0.7307\n",
            "Epoch 181 Batch 750 Loss 1.1507 Accuracy 0.7309\n",
            "Epoch 181 Batch 800 Loss 1.1539 Accuracy 0.7303\n",
            "Epoch 181 Batch 850 Loss 1.1553 Accuracy 0.7302\n",
            "Epoch 181 Batch 900 Loss 1.1578 Accuracy 0.7298\n",
            "Epoch 181 Batch 950 Loss 1.1605 Accuracy 0.7293\n",
            "Epoch 181 Batch 1000 Loss 1.1622 Accuracy 0.7291\n",
            "Epoch 181 Batch 1050 Loss 1.1631 Accuracy 0.7291\n",
            "Epoch 181 Batch 1100 Loss 1.1654 Accuracy 0.7287\n",
            "Epoch 181 Loss 1.1663 Accuracy 0.7284\n",
            "Time taken for 1 epoch: 22.344817638397217 secs\n",
            "\n",
            "discarded batch 23\n",
            "Epoch 181 VALIDATION: Loss 2.6794 Accuracy 0.5695\n",
            "\n",
            "Epoch 182 Batch 0 Loss 1.1849 Accuracy 0.7360\n",
            "Epoch 182 Batch 50 Loss 1.1204 Accuracy 0.7358\n",
            "Epoch 182 Batch 100 Loss 1.1266 Accuracy 0.7330\n",
            "Epoch 182 Batch 150 Loss 1.1156 Accuracy 0.7353\n",
            "Epoch 182 Batch 200 Loss 1.1237 Accuracy 0.7339\n",
            "Epoch 182 Batch 250 Loss 1.1238 Accuracy 0.7339\n",
            "Epoch 182 Batch 300 Loss 1.1258 Accuracy 0.7338\n",
            "Epoch 182 Batch 350 Loss 1.1305 Accuracy 0.7331\n",
            "Epoch 182 Batch 400 Loss 1.1341 Accuracy 0.7327\n",
            "Epoch 182 Batch 450 Loss 1.1400 Accuracy 0.7315\n",
            "Epoch 182 Batch 500 Loss 1.1427 Accuracy 0.7309\n",
            "Epoch 182 Batch 550 Loss 1.1429 Accuracy 0.7310\n",
            "Epoch 182 Batch 600 Loss 1.1448 Accuracy 0.7307\n",
            "Epoch 182 Batch 650 Loss 1.1453 Accuracy 0.7311\n",
            "Epoch 182 Batch 700 Loss 1.1466 Accuracy 0.7308\n",
            "discarded batch 739\n",
            "Epoch 182 Batch 750 Loss 1.1485 Accuracy 0.7304\n",
            "Epoch 182 Batch 800 Loss 1.1515 Accuracy 0.7301\n",
            "Epoch 182 Batch 850 Loss 1.1538 Accuracy 0.7298\n",
            "Epoch 182 Batch 900 Loss 1.1551 Accuracy 0.7297\n",
            "Epoch 182 Batch 950 Loss 1.1569 Accuracy 0.7294\n",
            "Epoch 182 Batch 1000 Loss 1.1591 Accuracy 0.7291\n",
            "Epoch 182 Batch 1050 Loss 1.1614 Accuracy 0.7288\n",
            "Epoch 182 Batch 1100 Loss 1.1633 Accuracy 0.7285\n",
            "Epoch 182 Loss 1.1645 Accuracy 0.7283\n",
            "Time taken for 1 epoch: 22.196327209472656 secs\n",
            "\n",
            "Epoch 183 Batch 0 Loss 1.0354 Accuracy 0.7822\n",
            "Epoch 183 Batch 50 Loss 1.1178 Accuracy 0.7381\n",
            "Epoch 183 Batch 100 Loss 1.1288 Accuracy 0.7347\n",
            "Epoch 183 Batch 150 Loss 1.1313 Accuracy 0.7347\n",
            "Epoch 183 Batch 200 Loss 1.1335 Accuracy 0.7338\n",
            "Epoch 183 Batch 250 Loss 1.1344 Accuracy 0.7337\n",
            "Epoch 183 Batch 300 Loss 1.1358 Accuracy 0.7336\n",
            "Epoch 183 Batch 350 Loss 1.1375 Accuracy 0.7325\n",
            "Epoch 183 Batch 400 Loss 1.1383 Accuracy 0.7325\n",
            "Epoch 183 Batch 450 Loss 1.1394 Accuracy 0.7324\n",
            "Epoch 183 Batch 500 Loss 1.1398 Accuracy 0.7320\n",
            "Epoch 183 Batch 550 Loss 1.1410 Accuracy 0.7318\n",
            "Epoch 183 Batch 600 Loss 1.1426 Accuracy 0.7314\n",
            "Epoch 183 Batch 650 Loss 1.1437 Accuracy 0.7316\n",
            "Epoch 183 Batch 700 Loss 1.1486 Accuracy 0.7308\n",
            "Epoch 183 Batch 750 Loss 1.1496 Accuracy 0.7304\n",
            "Epoch 183 Batch 800 Loss 1.1498 Accuracy 0.7303\n",
            "Epoch 183 Batch 850 Loss 1.1517 Accuracy 0.7300\n",
            "Epoch 183 Batch 900 Loss 1.1528 Accuracy 0.7298\n",
            "Epoch 183 Batch 950 Loss 1.1540 Accuracy 0.7296\n",
            "Epoch 183 Batch 1000 Loss 1.1563 Accuracy 0.7293\n",
            "Epoch 183 Batch 1050 Loss 1.1590 Accuracy 0.7288\n",
            "discarded batch 1094\n",
            "Epoch 183 Batch 1100 Loss 1.1610 Accuracy 0.7288\n",
            "Epoch 183 Loss 1.1624 Accuracy 0.7286\n",
            "Time taken for 1 epoch: 22.336502075195312 secs\n",
            "\n",
            "Epoch 184 Batch 0 Loss 1.1331 Accuracy 0.7294\n",
            "Epoch 184 Batch 50 Loss 1.0986 Accuracy 0.7386\n",
            "Epoch 184 Batch 100 Loss 1.1143 Accuracy 0.7376\n",
            "Epoch 184 Batch 150 Loss 1.1171 Accuracy 0.7373\n",
            "Epoch 184 Batch 200 Loss 1.1181 Accuracy 0.7368\n",
            "Epoch 184 Batch 250 Loss 1.1241 Accuracy 0.7354\n",
            "Epoch 184 Batch 300 Loss 1.1273 Accuracy 0.7349\n",
            "Epoch 184 Batch 350 Loss 1.1302 Accuracy 0.7348\n",
            "Epoch 184 Batch 400 Loss 1.1323 Accuracy 0.7347\n",
            "Epoch 184 Batch 450 Loss 1.1359 Accuracy 0.7343\n",
            "Epoch 184 Batch 500 Loss 1.1389 Accuracy 0.7336\n",
            "Epoch 184 Batch 550 Loss 1.1426 Accuracy 0.7331\n",
            "Epoch 184 Batch 600 Loss 1.1447 Accuracy 0.7327\n",
            "Epoch 184 Batch 650 Loss 1.1456 Accuracy 0.7325\n",
            "discarded batch 660\n",
            "Epoch 184 Batch 700 Loss 1.1485 Accuracy 0.7318\n",
            "Epoch 184 Batch 750 Loss 1.1491 Accuracy 0.7316\n",
            "Epoch 184 Batch 800 Loss 1.1495 Accuracy 0.7314\n",
            "Epoch 184 Batch 850 Loss 1.1517 Accuracy 0.7312\n",
            "Epoch 184 Batch 900 Loss 1.1525 Accuracy 0.7311\n",
            "Epoch 184 Batch 950 Loss 1.1554 Accuracy 0.7307\n",
            "Epoch 184 Batch 1000 Loss 1.1576 Accuracy 0.7303\n",
            "Epoch 184 Batch 1050 Loss 1.1599 Accuracy 0.7299\n",
            "Epoch 184 Batch 1100 Loss 1.1611 Accuracy 0.7296\n",
            "Epoch 184 Loss 1.1620 Accuracy 0.7294\n",
            "Time taken for 1 epoch: 22.423553943634033 secs\n",
            "\n",
            "Epoch 185 Batch 0 Loss 1.1300 Accuracy 0.7162\n",
            "Epoch 185 Batch 50 Loss 1.1149 Accuracy 0.7364\n",
            "Epoch 185 Batch 100 Loss 1.1142 Accuracy 0.7378\n",
            "Epoch 185 Batch 150 Loss 1.1114 Accuracy 0.7386\n",
            "Epoch 185 Batch 200 Loss 1.1085 Accuracy 0.7389\n",
            "Epoch 185 Batch 250 Loss 1.1116 Accuracy 0.7391\n",
            "Epoch 185 Batch 300 Loss 1.1185 Accuracy 0.7382\n",
            "Epoch 185 Batch 350 Loss 1.1216 Accuracy 0.7375\n",
            "Epoch 185 Batch 400 Loss 1.1261 Accuracy 0.7367\n",
            "Epoch 185 Batch 450 Loss 1.1285 Accuracy 0.7362\n",
            "discarded batch 461\n",
            "Epoch 185 Batch 500 Loss 1.1309 Accuracy 0.7355\n",
            "Epoch 185 Batch 550 Loss 1.1341 Accuracy 0.7348\n",
            "Epoch 185 Batch 600 Loss 1.1346 Accuracy 0.7345\n",
            "Epoch 185 Batch 650 Loss 1.1370 Accuracy 0.7341\n",
            "Epoch 185 Batch 700 Loss 1.1385 Accuracy 0.7339\n",
            "Epoch 185 Batch 750 Loss 1.1425 Accuracy 0.7332\n",
            "Epoch 185 Batch 800 Loss 1.1429 Accuracy 0.7331\n",
            "Epoch 185 Batch 850 Loss 1.1457 Accuracy 0.7327\n",
            "Epoch 185 Batch 900 Loss 1.1486 Accuracy 0.7320\n",
            "Epoch 185 Batch 950 Loss 1.1501 Accuracy 0.7316\n",
            "Epoch 185 Batch 1000 Loss 1.1527 Accuracy 0.7311\n",
            "Epoch 185 Batch 1050 Loss 1.1549 Accuracy 0.7306\n",
            "Epoch 185 Batch 1100 Loss 1.1563 Accuracy 0.7304\n",
            "Saving checkpoint for epoch 185 at ./checkpoints/train/ckpt-37\n",
            "Epoch 185 Loss 1.1565 Accuracy 0.7304\n",
            "Time taken for 1 epoch: 22.628817796707153 secs\n",
            "\n",
            "Epoch 186 Batch 0 Loss 1.2002 Accuracy 0.7492\n",
            "Epoch 186 Batch 50 Loss 1.1108 Accuracy 0.7395\n",
            "Epoch 186 Batch 100 Loss 1.1272 Accuracy 0.7367\n",
            "discarded batch 108\n",
            "Epoch 186 Batch 150 Loss 1.1297 Accuracy 0.7362\n",
            "Epoch 186 Batch 200 Loss 1.1289 Accuracy 0.7364\n",
            "Epoch 186 Batch 250 Loss 1.1309 Accuracy 0.7359\n",
            "Epoch 186 Batch 300 Loss 1.1306 Accuracy 0.7357\n",
            "Epoch 186 Batch 350 Loss 1.1293 Accuracy 0.7359\n",
            "Epoch 186 Batch 400 Loss 1.1307 Accuracy 0.7353\n",
            "Epoch 186 Batch 450 Loss 1.1352 Accuracy 0.7342\n",
            "Epoch 186 Batch 500 Loss 1.1380 Accuracy 0.7334\n",
            "Epoch 186 Batch 550 Loss 1.1393 Accuracy 0.7333\n",
            "Epoch 186 Batch 600 Loss 1.1411 Accuracy 0.7329\n",
            "Epoch 186 Batch 650 Loss 1.1430 Accuracy 0.7326\n",
            "Epoch 186 Batch 700 Loss 1.1436 Accuracy 0.7324\n",
            "Epoch 186 Batch 750 Loss 1.1463 Accuracy 0.7317\n",
            "Epoch 186 Batch 800 Loss 1.1487 Accuracy 0.7314\n",
            "Epoch 186 Batch 850 Loss 1.1514 Accuracy 0.7310\n",
            "Epoch 186 Batch 900 Loss 1.1520 Accuracy 0.7311\n",
            "Epoch 186 Batch 950 Loss 1.1543 Accuracy 0.7306\n",
            "Epoch 186 Batch 1000 Loss 1.1566 Accuracy 0.7303\n",
            "Epoch 186 Batch 1050 Loss 1.1578 Accuracy 0.7300\n",
            "Epoch 186 Batch 1100 Loss 1.1596 Accuracy 0.7297\n",
            "Epoch 186 Loss 1.1603 Accuracy 0.7297\n",
            "Time taken for 1 epoch: 22.39682960510254 secs\n",
            "\n",
            "discarded batch 23\n",
            "Epoch 186 VALIDATION: Loss 2.6771 Accuracy 0.5682\n",
            "\n",
            "Epoch 187 Batch 0 Loss 1.1820 Accuracy 0.7063\n",
            "Epoch 187 Batch 50 Loss 1.1098 Accuracy 0.7365\n",
            "discarded batch 93\n",
            "Epoch 187 Batch 100 Loss 1.1075 Accuracy 0.7373\n",
            "Epoch 187 Batch 150 Loss 1.1117 Accuracy 0.7368\n",
            "Epoch 187 Batch 200 Loss 1.1151 Accuracy 0.7366\n",
            "Epoch 187 Batch 250 Loss 1.1154 Accuracy 0.7366\n",
            "Epoch 187 Batch 300 Loss 1.1180 Accuracy 0.7360\n",
            "Epoch 187 Batch 350 Loss 1.1203 Accuracy 0.7361\n",
            "Epoch 187 Batch 400 Loss 1.1249 Accuracy 0.7353\n",
            "Epoch 187 Batch 450 Loss 1.1283 Accuracy 0.7344\n",
            "Epoch 187 Batch 500 Loss 1.1307 Accuracy 0.7342\n",
            "Epoch 187 Batch 550 Loss 1.1332 Accuracy 0.7335\n",
            "Epoch 187 Batch 600 Loss 1.1366 Accuracy 0.7328\n",
            "Epoch 187 Batch 650 Loss 1.1394 Accuracy 0.7320\n",
            "Epoch 187 Batch 700 Loss 1.1406 Accuracy 0.7318\n",
            "Epoch 187 Batch 750 Loss 1.1439 Accuracy 0.7315\n",
            "Epoch 187 Batch 800 Loss 1.1473 Accuracy 0.7311\n",
            "Epoch 187 Batch 850 Loss 1.1494 Accuracy 0.7306\n",
            "Epoch 187 Batch 900 Loss 1.1512 Accuracy 0.7303\n",
            "Epoch 187 Batch 950 Loss 1.1525 Accuracy 0.7303\n",
            "Epoch 187 Batch 1000 Loss 1.1542 Accuracy 0.7300\n",
            "Epoch 187 Batch 1050 Loss 1.1557 Accuracy 0.7299\n",
            "Epoch 187 Batch 1100 Loss 1.1571 Accuracy 0.7297\n",
            "Epoch 187 Loss 1.1582 Accuracy 0.7295\n",
            "Time taken for 1 epoch: 22.444329261779785 secs\n",
            "\n",
            "Epoch 188 Batch 0 Loss 1.0754 Accuracy 0.7459\n",
            "Epoch 188 Batch 50 Loss 1.0981 Accuracy 0.7421\n",
            "Epoch 188 Batch 100 Loss 1.1105 Accuracy 0.7393\n",
            "Epoch 188 Batch 150 Loss 1.1065 Accuracy 0.7387\n",
            "Epoch 188 Batch 200 Loss 1.1078 Accuracy 0.7394\n",
            "Epoch 188 Batch 250 Loss 1.1129 Accuracy 0.7379\n",
            "Epoch 188 Batch 300 Loss 1.1138 Accuracy 0.7379\n",
            "Epoch 188 Batch 350 Loss 1.1189 Accuracy 0.7372\n",
            "Epoch 188 Batch 400 Loss 1.1235 Accuracy 0.7360\n",
            "Epoch 188 Batch 450 Loss 1.1257 Accuracy 0.7354\n",
            "Epoch 188 Batch 500 Loss 1.1260 Accuracy 0.7353\n",
            "Epoch 188 Batch 550 Loss 1.1288 Accuracy 0.7344\n",
            "Epoch 188 Batch 600 Loss 1.1307 Accuracy 0.7342\n",
            "Epoch 188 Batch 650 Loss 1.1331 Accuracy 0.7335\n",
            "Epoch 188 Batch 700 Loss 1.1351 Accuracy 0.7331\n",
            "discarded batch 722\n",
            "Epoch 188 Batch 750 Loss 1.1378 Accuracy 0.7326\n",
            "Epoch 188 Batch 800 Loss 1.1401 Accuracy 0.7324\n",
            "Epoch 188 Batch 850 Loss 1.1433 Accuracy 0.7318\n",
            "Epoch 188 Batch 900 Loss 1.1468 Accuracy 0.7314\n",
            "Epoch 188 Batch 950 Loss 1.1484 Accuracy 0.7311\n",
            "Epoch 188 Batch 1000 Loss 1.1504 Accuracy 0.7308\n",
            "Epoch 188 Batch 1050 Loss 1.1529 Accuracy 0.7303\n",
            "Epoch 188 Batch 1100 Loss 1.1545 Accuracy 0.7300\n",
            "Epoch 188 Loss 1.1552 Accuracy 0.7298\n",
            "Time taken for 1 epoch: 22.523151874542236 secs\n",
            "\n",
            "Epoch 189 Batch 0 Loss 0.9537 Accuracy 0.7492\n",
            "Epoch 189 Batch 50 Loss 1.0974 Accuracy 0.7358\n",
            "Epoch 189 Batch 100 Loss 1.0948 Accuracy 0.7385\n",
            "Epoch 189 Batch 150 Loss 1.1031 Accuracy 0.7390\n",
            "Epoch 189 Batch 200 Loss 1.1064 Accuracy 0.7388\n",
            "Epoch 189 Batch 250 Loss 1.1090 Accuracy 0.7379\n",
            "Epoch 189 Batch 300 Loss 1.1118 Accuracy 0.7370\n",
            "Epoch 189 Batch 350 Loss 1.1175 Accuracy 0.7366\n",
            "Epoch 189 Batch 400 Loss 1.1213 Accuracy 0.7360\n",
            "Epoch 189 Batch 450 Loss 1.1233 Accuracy 0.7353\n",
            "Epoch 189 Batch 500 Loss 1.1260 Accuracy 0.7343\n",
            "Epoch 189 Batch 550 Loss 1.1286 Accuracy 0.7344\n",
            "Epoch 189 Batch 600 Loss 1.1305 Accuracy 0.7342\n",
            "Epoch 189 Batch 650 Loss 1.1336 Accuracy 0.7339\n",
            "Epoch 189 Batch 700 Loss 1.1347 Accuracy 0.7338\n",
            "Epoch 189 Batch 750 Loss 1.1361 Accuracy 0.7334\n",
            "Epoch 189 Batch 800 Loss 1.1387 Accuracy 0.7329\n",
            "Epoch 189 Batch 850 Loss 1.1410 Accuracy 0.7326\n",
            "Epoch 189 Batch 900 Loss 1.1427 Accuracy 0.7324\n",
            "Epoch 189 Batch 950 Loss 1.1459 Accuracy 0.7319\n",
            "Epoch 189 Batch 1000 Loss 1.1479 Accuracy 0.7316\n",
            "Epoch 189 Batch 1050 Loss 1.1495 Accuracy 0.7313\n",
            "discarded batch 1051\n",
            "Epoch 189 Batch 1100 Loss 1.1519 Accuracy 0.7309\n",
            "Epoch 189 Loss 1.1530 Accuracy 0.7307\n",
            "Time taken for 1 epoch: 22.453689098358154 secs\n",
            "\n",
            "Epoch 190 Batch 0 Loss 1.0916 Accuracy 0.7426\n",
            "Epoch 190 Batch 50 Loss 1.1134 Accuracy 0.7348\n",
            "Epoch 190 Batch 100 Loss 1.1241 Accuracy 0.7346\n",
            "Epoch 190 Batch 150 Loss 1.1200 Accuracy 0.7342\n",
            "Epoch 190 Batch 200 Loss 1.1133 Accuracy 0.7358\n",
            "Epoch 190 Batch 250 Loss 1.1198 Accuracy 0.7347\n",
            "Epoch 190 Batch 300 Loss 1.1201 Accuracy 0.7347\n",
            "Epoch 190 Batch 350 Loss 1.1199 Accuracy 0.7351\n",
            "Epoch 190 Batch 400 Loss 1.1223 Accuracy 0.7345\n",
            "Epoch 190 Batch 450 Loss 1.1259 Accuracy 0.7340\n",
            "Epoch 190 Batch 500 Loss 1.1280 Accuracy 0.7341\n",
            "discarded batch 508\n",
            "Epoch 190 Batch 550 Loss 1.1297 Accuracy 0.7339\n",
            "Epoch 190 Batch 600 Loss 1.1329 Accuracy 0.7329\n",
            "Epoch 190 Batch 650 Loss 1.1362 Accuracy 0.7321\n",
            "Epoch 190 Batch 700 Loss 1.1378 Accuracy 0.7320\n",
            "Epoch 190 Batch 750 Loss 1.1394 Accuracy 0.7322\n",
            "Epoch 190 Batch 800 Loss 1.1412 Accuracy 0.7321\n",
            "Epoch 190 Batch 850 Loss 1.1452 Accuracy 0.7313\n",
            "Epoch 190 Batch 900 Loss 1.1464 Accuracy 0.7312\n",
            "Epoch 190 Batch 950 Loss 1.1475 Accuracy 0.7311\n",
            "Epoch 190 Batch 1000 Loss 1.1501 Accuracy 0.7306\n",
            "Epoch 190 Batch 1050 Loss 1.1512 Accuracy 0.7303\n",
            "Epoch 190 Batch 1100 Loss 1.1523 Accuracy 0.7300\n",
            "Saving checkpoint for epoch 190 at ./checkpoints/train/ckpt-38\n",
            "Epoch 190 Loss 1.1533 Accuracy 0.7298\n",
            "Time taken for 1 epoch: 22.697656869888306 secs\n",
            "\n",
            "Epoch 191 Batch 0 Loss 1.0227 Accuracy 0.7294\n",
            "Epoch 191 Batch 50 Loss 1.0957 Accuracy 0.7382\n",
            "Epoch 191 Batch 100 Loss 1.0946 Accuracy 0.7410\n",
            "Epoch 191 Batch 150 Loss 1.1010 Accuracy 0.7404\n",
            "discarded batch 164\n",
            "Epoch 191 Batch 200 Loss 1.1048 Accuracy 0.7394\n",
            "Epoch 191 Batch 250 Loss 1.1074 Accuracy 0.7393\n",
            "Epoch 191 Batch 300 Loss 1.1132 Accuracy 0.7382\n",
            "Epoch 191 Batch 350 Loss 1.1142 Accuracy 0.7381\n",
            "Epoch 191 Batch 400 Loss 1.1170 Accuracy 0.7377\n",
            "Epoch 191 Batch 450 Loss 1.1209 Accuracy 0.7370\n",
            "Epoch 191 Batch 500 Loss 1.1226 Accuracy 0.7363\n",
            "Epoch 191 Batch 550 Loss 1.1276 Accuracy 0.7350\n",
            "Epoch 191 Batch 600 Loss 1.1321 Accuracy 0.7342\n",
            "Epoch 191 Batch 650 Loss 1.1340 Accuracy 0.7340\n",
            "Epoch 191 Batch 700 Loss 1.1357 Accuracy 0.7337\n",
            "Epoch 191 Batch 750 Loss 1.1365 Accuracy 0.7337\n",
            "Epoch 191 Batch 800 Loss 1.1380 Accuracy 0.7334\n",
            "Epoch 191 Batch 850 Loss 1.1396 Accuracy 0.7331\n",
            "Epoch 191 Batch 900 Loss 1.1419 Accuracy 0.7329\n",
            "Epoch 191 Batch 950 Loss 1.1433 Accuracy 0.7324\n",
            "Epoch 191 Batch 1000 Loss 1.1451 Accuracy 0.7320\n",
            "Epoch 191 Batch 1050 Loss 1.1460 Accuracy 0.7319\n",
            "Epoch 191 Batch 1100 Loss 1.1471 Accuracy 0.7317\n",
            "Epoch 191 Loss 1.1481 Accuracy 0.7315\n",
            "Time taken for 1 epoch: 22.561334371566772 secs\n",
            "\n",
            "discarded batch 23\n",
            "Epoch 191 VALIDATION: Loss 2.6506 Accuracy 0.5744\n",
            "\n",
            "Epoch 192 Batch 0 Loss 1.0975 Accuracy 0.7294\n",
            "Epoch 192 Batch 50 Loss 1.1029 Accuracy 0.7379\n",
            "Epoch 192 Batch 100 Loss 1.0992 Accuracy 0.7382\n",
            "Epoch 192 Batch 150 Loss 1.1025 Accuracy 0.7381\n",
            "Epoch 192 Batch 200 Loss 1.1106 Accuracy 0.7365\n",
            "Epoch 192 Batch 250 Loss 1.1177 Accuracy 0.7350\n",
            "Epoch 192 Batch 300 Loss 1.1193 Accuracy 0.7348\n",
            "discarded batch 346\n",
            "Epoch 192 Batch 350 Loss 1.1223 Accuracy 0.7347\n",
            "Epoch 192 Batch 400 Loss 1.1243 Accuracy 0.7340\n",
            "Epoch 192 Batch 450 Loss 1.1245 Accuracy 0.7340\n",
            "Epoch 192 Batch 500 Loss 1.1237 Accuracy 0.7345\n",
            "Epoch 192 Batch 550 Loss 1.1250 Accuracy 0.7343\n",
            "Epoch 192 Batch 600 Loss 1.1299 Accuracy 0.7337\n",
            "Epoch 192 Batch 650 Loss 1.1325 Accuracy 0.7336\n",
            "Epoch 192 Batch 700 Loss 1.1329 Accuracy 0.7334\n",
            "Epoch 192 Batch 750 Loss 1.1342 Accuracy 0.7332\n",
            "Epoch 192 Batch 800 Loss 1.1370 Accuracy 0.7328\n",
            "Epoch 192 Batch 850 Loss 1.1381 Accuracy 0.7326\n",
            "Epoch 192 Batch 900 Loss 1.1404 Accuracy 0.7322\n",
            "Epoch 192 Batch 950 Loss 1.1431 Accuracy 0.7318\n",
            "Epoch 192 Batch 1000 Loss 1.1446 Accuracy 0.7317\n",
            "Epoch 192 Batch 1050 Loss 1.1462 Accuracy 0.7315\n",
            "Epoch 192 Batch 1100 Loss 1.1482 Accuracy 0.7311\n",
            "Epoch 192 Loss 1.1493 Accuracy 0.7310\n",
            "Time taken for 1 epoch: 22.400598764419556 secs\n",
            "\n",
            "Epoch 193 Batch 0 Loss 1.0276 Accuracy 0.7657\n",
            "Epoch 193 Batch 50 Loss 1.0888 Accuracy 0.7398\n",
            "Epoch 193 Batch 100 Loss 1.0995 Accuracy 0.7391\n",
            "Epoch 193 Batch 150 Loss 1.1092 Accuracy 0.7374\n",
            "Epoch 193 Batch 200 Loss 1.1140 Accuracy 0.7355\n",
            "Epoch 193 Batch 250 Loss 1.1164 Accuracy 0.7355\n",
            "Epoch 193 Batch 300 Loss 1.1193 Accuracy 0.7349\n",
            "Epoch 193 Batch 350 Loss 1.1197 Accuracy 0.7349\n",
            "Epoch 193 Batch 400 Loss 1.1238 Accuracy 0.7342\n",
            "Epoch 193 Batch 450 Loss 1.1256 Accuracy 0.7343\n",
            "Epoch 193 Batch 500 Loss 1.1259 Accuracy 0.7342\n",
            "Epoch 193 Batch 550 Loss 1.1284 Accuracy 0.7341\n",
            "Epoch 193 Batch 600 Loss 1.1293 Accuracy 0.7338\n",
            "Epoch 193 Batch 650 Loss 1.1317 Accuracy 0.7331\n",
            "discarded batch 691\n",
            "Epoch 193 Batch 700 Loss 1.1322 Accuracy 0.7331\n",
            "Epoch 193 Batch 750 Loss 1.1339 Accuracy 0.7331\n",
            "Epoch 193 Batch 800 Loss 1.1358 Accuracy 0.7327\n",
            "Epoch 193 Batch 850 Loss 1.1378 Accuracy 0.7323\n",
            "Epoch 193 Batch 900 Loss 1.1399 Accuracy 0.7320\n",
            "Epoch 193 Batch 950 Loss 1.1424 Accuracy 0.7319\n",
            "Epoch 193 Batch 1000 Loss 1.1438 Accuracy 0.7319\n",
            "Epoch 193 Batch 1050 Loss 1.1454 Accuracy 0.7316\n",
            "Epoch 193 Batch 1100 Loss 1.1474 Accuracy 0.7313\n",
            "Epoch 193 Loss 1.1478 Accuracy 0.7313\n",
            "Time taken for 1 epoch: 22.287099838256836 secs\n",
            "\n",
            "Epoch 194 Batch 0 Loss 1.2149 Accuracy 0.7096\n",
            "Epoch 194 Batch 50 Loss 1.0937 Accuracy 0.7404\n",
            "Epoch 194 Batch 100 Loss 1.0947 Accuracy 0.7405\n",
            "Epoch 194 Batch 150 Loss 1.1024 Accuracy 0.7393\n",
            "Epoch 194 Batch 200 Loss 1.1040 Accuracy 0.7383\n",
            "Epoch 194 Batch 250 Loss 1.1029 Accuracy 0.7384\n",
            "Epoch 194 Batch 300 Loss 1.1076 Accuracy 0.7374\n",
            "Epoch 194 Batch 350 Loss 1.1139 Accuracy 0.7368\n",
            "Epoch 194 Batch 400 Loss 1.1152 Accuracy 0.7363\n",
            "Epoch 194 Batch 450 Loss 1.1147 Accuracy 0.7362\n",
            "Epoch 194 Batch 500 Loss 1.1180 Accuracy 0.7358\n",
            "discarded batch 510\n",
            "Epoch 194 Batch 550 Loss 1.1201 Accuracy 0.7354\n",
            "Epoch 194 Batch 600 Loss 1.1217 Accuracy 0.7350\n",
            "Epoch 194 Batch 650 Loss 1.1239 Accuracy 0.7346\n",
            "Epoch 194 Batch 700 Loss 1.1272 Accuracy 0.7341\n",
            "Epoch 194 Batch 750 Loss 1.1302 Accuracy 0.7338\n",
            "Epoch 194 Batch 800 Loss 1.1316 Accuracy 0.7335\n",
            "Epoch 194 Batch 850 Loss 1.1349 Accuracy 0.7331\n",
            "Epoch 194 Batch 900 Loss 1.1369 Accuracy 0.7331\n",
            "Epoch 194 Batch 950 Loss 1.1383 Accuracy 0.7327\n",
            "Epoch 194 Batch 1000 Loss 1.1398 Accuracy 0.7325\n",
            "Epoch 194 Batch 1050 Loss 1.1419 Accuracy 0.7323\n",
            "Epoch 194 Batch 1100 Loss 1.1442 Accuracy 0.7318\n",
            "Epoch 194 Loss 1.1448 Accuracy 0.7318\n",
            "Time taken for 1 epoch: 22.262707710266113 secs\n",
            "\n",
            "Epoch 195 Batch 0 Loss 1.1226 Accuracy 0.7261\n",
            "Epoch 195 Batch 50 Loss 1.0935 Accuracy 0.7412\n",
            "discarded batch 86\n",
            "Epoch 195 Batch 100 Loss 1.0967 Accuracy 0.7403\n",
            "Epoch 195 Batch 150 Loss 1.0970 Accuracy 0.7402\n",
            "Epoch 195 Batch 200 Loss 1.1036 Accuracy 0.7385\n",
            "Epoch 195 Batch 250 Loss 1.1045 Accuracy 0.7387\n",
            "Epoch 195 Batch 300 Loss 1.1066 Accuracy 0.7380\n",
            "Epoch 195 Batch 350 Loss 1.1094 Accuracy 0.7375\n",
            "Epoch 195 Batch 400 Loss 1.1110 Accuracy 0.7368\n",
            "Epoch 195 Batch 450 Loss 1.1113 Accuracy 0.7371\n",
            "Epoch 195 Batch 500 Loss 1.1153 Accuracy 0.7367\n",
            "Epoch 195 Batch 550 Loss 1.1186 Accuracy 0.7361\n",
            "Epoch 195 Batch 600 Loss 1.1213 Accuracy 0.7355\n",
            "Epoch 195 Batch 650 Loss 1.1244 Accuracy 0.7348\n",
            "Epoch 195 Batch 700 Loss 1.1267 Accuracy 0.7346\n",
            "Epoch 195 Batch 750 Loss 1.1293 Accuracy 0.7344\n",
            "Epoch 195 Batch 800 Loss 1.1323 Accuracy 0.7339\n",
            "Epoch 195 Batch 850 Loss 1.1335 Accuracy 0.7337\n",
            "Epoch 195 Batch 900 Loss 1.1351 Accuracy 0.7336\n",
            "Epoch 195 Batch 950 Loss 1.1375 Accuracy 0.7332\n",
            "Epoch 195 Batch 1000 Loss 1.1391 Accuracy 0.7328\n",
            "Epoch 195 Batch 1050 Loss 1.1412 Accuracy 0.7323\n",
            "Epoch 195 Batch 1100 Loss 1.1420 Accuracy 0.7322\n",
            "Saving checkpoint for epoch 195 at ./checkpoints/train/ckpt-39\n",
            "Epoch 195 Loss 1.1430 Accuracy 0.7320\n",
            "Time taken for 1 epoch: 22.840068340301514 secs\n",
            "\n",
            "Epoch 196 Batch 0 Loss 1.1264 Accuracy 0.7327\n",
            "discarded batch 17\n",
            "Epoch 196 Batch 50 Loss 1.0691 Accuracy 0.7473\n",
            "Epoch 196 Batch 100 Loss 1.0804 Accuracy 0.7434\n",
            "Epoch 196 Batch 150 Loss 1.0943 Accuracy 0.7411\n",
            "Epoch 196 Batch 200 Loss 1.0979 Accuracy 0.7402\n",
            "Epoch 196 Batch 250 Loss 1.1041 Accuracy 0.7398\n",
            "Epoch 196 Batch 300 Loss 1.1099 Accuracy 0.7388\n",
            "Epoch 196 Batch 350 Loss 1.1103 Accuracy 0.7382\n",
            "Epoch 196 Batch 400 Loss 1.1114 Accuracy 0.7380\n",
            "Epoch 196 Batch 450 Loss 1.1152 Accuracy 0.7373\n",
            "Epoch 196 Batch 500 Loss 1.1191 Accuracy 0.7363\n",
            "Epoch 196 Batch 550 Loss 1.1207 Accuracy 0.7361\n",
            "Epoch 196 Batch 600 Loss 1.1242 Accuracy 0.7353\n",
            "Epoch 196 Batch 650 Loss 1.1258 Accuracy 0.7352\n",
            "Epoch 196 Batch 700 Loss 1.1289 Accuracy 0.7345\n",
            "Epoch 196 Batch 750 Loss 1.1312 Accuracy 0.7338\n",
            "Epoch 196 Batch 800 Loss 1.1329 Accuracy 0.7333\n",
            "Epoch 196 Batch 850 Loss 1.1351 Accuracy 0.7330\n",
            "Epoch 196 Batch 900 Loss 1.1372 Accuracy 0.7328\n",
            "Epoch 196 Batch 950 Loss 1.1390 Accuracy 0.7325\n",
            "Epoch 196 Batch 1000 Loss 1.1405 Accuracy 0.7322\n",
            "Epoch 196 Batch 1050 Loss 1.1428 Accuracy 0.7317\n",
            "Epoch 196 Batch 1100 Loss 1.1447 Accuracy 0.7315\n",
            "Epoch 196 Loss 1.1460 Accuracy 0.7312\n",
            "Time taken for 1 epoch: 22.33499312400818 secs\n",
            "\n",
            "discarded batch 23\n",
            "Epoch 196 VALIDATION: Loss 2.6790 Accuracy 0.5705\n",
            "\n",
            "Epoch 197 Batch 0 Loss 1.0898 Accuracy 0.7195\n",
            "Epoch 197 Batch 50 Loss 1.0857 Accuracy 0.7411\n",
            "Epoch 197 Batch 100 Loss 1.0880 Accuracy 0.7413\n",
            "Epoch 197 Batch 150 Loss 1.0934 Accuracy 0.7407\n",
            "Epoch 197 Batch 200 Loss 1.0980 Accuracy 0.7406\n",
            "Epoch 197 Batch 250 Loss 1.1045 Accuracy 0.7396\n",
            "Epoch 197 Batch 300 Loss 1.1065 Accuracy 0.7389\n",
            "Epoch 197 Batch 350 Loss 1.1112 Accuracy 0.7380\n",
            "Epoch 197 Batch 400 Loss 1.1136 Accuracy 0.7372\n",
            "discarded batch 447\n",
            "Epoch 197 Batch 450 Loss 1.1161 Accuracy 0.7369\n",
            "Epoch 197 Batch 500 Loss 1.1176 Accuracy 0.7363\n",
            "Epoch 197 Batch 550 Loss 1.1181 Accuracy 0.7362\n",
            "Epoch 197 Batch 600 Loss 1.1213 Accuracy 0.7358\n",
            "Epoch 197 Batch 650 Loss 1.1248 Accuracy 0.7354\n",
            "Epoch 197 Batch 700 Loss 1.1267 Accuracy 0.7350\n",
            "Epoch 197 Batch 750 Loss 1.1296 Accuracy 0.7349\n",
            "Epoch 197 Batch 800 Loss 1.1300 Accuracy 0.7349\n",
            "Epoch 197 Batch 850 Loss 1.1305 Accuracy 0.7347\n",
            "Epoch 197 Batch 900 Loss 1.1341 Accuracy 0.7340\n",
            "Epoch 197 Batch 950 Loss 1.1369 Accuracy 0.7335\n",
            "Epoch 197 Batch 1000 Loss 1.1381 Accuracy 0.7334\n",
            "Epoch 197 Batch 1050 Loss 1.1394 Accuracy 0.7332\n",
            "Epoch 197 Batch 1100 Loss 1.1413 Accuracy 0.7329\n",
            "Epoch 197 Loss 1.1425 Accuracy 0.7325\n",
            "Time taken for 1 epoch: 22.24552059173584 secs\n",
            "\n",
            "Epoch 198 Batch 0 Loss 1.0743 Accuracy 0.7591\n",
            "Epoch 198 Batch 50 Loss 1.0829 Accuracy 0.7444\n",
            "Epoch 198 Batch 100 Loss 1.0926 Accuracy 0.7410\n",
            "Epoch 198 Batch 150 Loss 1.0930 Accuracy 0.7399\n",
            "Epoch 198 Batch 200 Loss 1.0981 Accuracy 0.7386\n",
            "Epoch 198 Batch 250 Loss 1.1042 Accuracy 0.7384\n",
            "Epoch 198 Batch 300 Loss 1.1095 Accuracy 0.7379\n",
            "Epoch 198 Batch 350 Loss 1.1093 Accuracy 0.7379\n",
            "Epoch 198 Batch 400 Loss 1.1131 Accuracy 0.7373\n",
            "Epoch 198 Batch 450 Loss 1.1156 Accuracy 0.7369\n",
            "Epoch 198 Batch 500 Loss 1.1169 Accuracy 0.7364\n",
            "Epoch 198 Batch 550 Loss 1.1164 Accuracy 0.7362\n",
            "Epoch 198 Batch 600 Loss 1.1188 Accuracy 0.7359\n",
            "Epoch 198 Batch 650 Loss 1.1193 Accuracy 0.7358\n",
            "Epoch 198 Batch 700 Loss 1.1213 Accuracy 0.7355\n",
            "discarded batch 721\n",
            "Epoch 198 Batch 750 Loss 1.1238 Accuracy 0.7346\n",
            "Epoch 198 Batch 800 Loss 1.1256 Accuracy 0.7342\n",
            "Epoch 198 Batch 850 Loss 1.1282 Accuracy 0.7339\n",
            "Epoch 198 Batch 900 Loss 1.1305 Accuracy 0.7338\n",
            "Epoch 198 Batch 950 Loss 1.1338 Accuracy 0.7333\n",
            "Epoch 198 Batch 1000 Loss 1.1358 Accuracy 0.7330\n",
            "Epoch 198 Batch 1050 Loss 1.1374 Accuracy 0.7327\n",
            "Epoch 198 Batch 1100 Loss 1.1393 Accuracy 0.7323\n",
            "Epoch 198 Loss 1.1398 Accuracy 0.7322\n",
            "Time taken for 1 epoch: 22.207104682922363 secs\n",
            "\n",
            "Epoch 199 Batch 0 Loss 1.0807 Accuracy 0.7558\n",
            "Epoch 199 Batch 50 Loss 1.0837 Accuracy 0.7421\n",
            "Epoch 199 Batch 100 Loss 1.0978 Accuracy 0.7399\n",
            "Epoch 199 Batch 150 Loss 1.0936 Accuracy 0.7403\n",
            "Epoch 199 Batch 200 Loss 1.0951 Accuracy 0.7399\n",
            "Epoch 199 Batch 250 Loss 1.1002 Accuracy 0.7397\n",
            "Epoch 199 Batch 300 Loss 1.1042 Accuracy 0.7390\n",
            "Epoch 199 Batch 350 Loss 1.1062 Accuracy 0.7389\n",
            "Epoch 199 Batch 400 Loss 1.1118 Accuracy 0.7376\n",
            "Epoch 199 Batch 450 Loss 1.1136 Accuracy 0.7378\n",
            "Epoch 199 Batch 500 Loss 1.1152 Accuracy 0.7373\n",
            "discarded batch 541\n",
            "Epoch 199 Batch 550 Loss 1.1178 Accuracy 0.7366\n",
            "Epoch 199 Batch 600 Loss 1.1198 Accuracy 0.7362\n",
            "Epoch 199 Batch 650 Loss 1.1214 Accuracy 0.7359\n",
            "Epoch 199 Batch 700 Loss 1.1243 Accuracy 0.7356\n",
            "Epoch 199 Batch 750 Loss 1.1245 Accuracy 0.7354\n",
            "Epoch 199 Batch 800 Loss 1.1272 Accuracy 0.7350\n",
            "Epoch 199 Batch 850 Loss 1.1295 Accuracy 0.7344\n",
            "Epoch 199 Batch 900 Loss 1.1322 Accuracy 0.7340\n",
            "Epoch 199 Batch 950 Loss 1.1358 Accuracy 0.7336\n",
            "Epoch 199 Batch 1000 Loss 1.1374 Accuracy 0.7335\n",
            "Epoch 199 Batch 1050 Loss 1.1390 Accuracy 0.7331\n",
            "Epoch 199 Batch 1100 Loss 1.1405 Accuracy 0.7330\n",
            "Epoch 199 Loss 1.1405 Accuracy 0.7330\n",
            "Time taken for 1 epoch: 22.218749523162842 secs\n",
            "\n",
            "Epoch 200 Batch 0 Loss 1.2111 Accuracy 0.7228\n",
            "Epoch 200 Batch 50 Loss 1.0963 Accuracy 0.7386\n",
            "Epoch 200 Batch 100 Loss 1.0886 Accuracy 0.7404\n",
            "discarded batch 134\n",
            "Epoch 200 Batch 150 Loss 1.0966 Accuracy 0.7394\n",
            "Epoch 200 Batch 200 Loss 1.1019 Accuracy 0.7391\n",
            "Epoch 200 Batch 250 Loss 1.1083 Accuracy 0.7384\n",
            "Epoch 200 Batch 300 Loss 1.1100 Accuracy 0.7381\n",
            "Epoch 200 Batch 350 Loss 1.1123 Accuracy 0.7372\n",
            "Epoch 200 Batch 400 Loss 1.1103 Accuracy 0.7372\n",
            "Epoch 200 Batch 450 Loss 1.1119 Accuracy 0.7372\n",
            "Epoch 200 Batch 500 Loss 1.1138 Accuracy 0.7368\n",
            "Epoch 200 Batch 550 Loss 1.1150 Accuracy 0.7367\n",
            "Epoch 200 Batch 600 Loss 1.1168 Accuracy 0.7362\n",
            "Epoch 200 Batch 650 Loss 1.1189 Accuracy 0.7359\n",
            "Epoch 200 Batch 700 Loss 1.1221 Accuracy 0.7354\n",
            "Epoch 200 Batch 750 Loss 1.1242 Accuracy 0.7353\n",
            "Epoch 200 Batch 800 Loss 1.1255 Accuracy 0.7351\n",
            "Epoch 200 Batch 850 Loss 1.1273 Accuracy 0.7346\n",
            "Epoch 200 Batch 900 Loss 1.1292 Accuracy 0.7342\n",
            "Epoch 200 Batch 950 Loss 1.1317 Accuracy 0.7339\n",
            "Epoch 200 Batch 1000 Loss 1.1317 Accuracy 0.7340\n",
            "Epoch 200 Batch 1050 Loss 1.1335 Accuracy 0.7337\n",
            "Epoch 200 Batch 1100 Loss 1.1352 Accuracy 0.7334\n",
            "Saving checkpoint for epoch 200 at ./checkpoints/train/ckpt-40\n",
            "Epoch 200 Loss 1.1358 Accuracy 0.7333\n",
            "Time taken for 1 epoch: 22.415147066116333 secs\n",
            "\n",
            "Epoch 201 Batch 0 Loss 1.0492 Accuracy 0.7789\n",
            "Epoch 201 Batch 50 Loss 1.0762 Accuracy 0.7463\n",
            "Epoch 201 Batch 100 Loss 1.0760 Accuracy 0.7459\n",
            "Epoch 201 Batch 150 Loss 1.0832 Accuracy 0.7443\n",
            "Epoch 201 Batch 200 Loss 1.0896 Accuracy 0.7418\n",
            "Epoch 201 Batch 250 Loss 1.0951 Accuracy 0.7406\n",
            "Epoch 201 Batch 300 Loss 1.0992 Accuracy 0.7398\n",
            "Epoch 201 Batch 350 Loss 1.1038 Accuracy 0.7391\n",
            "Epoch 201 Batch 400 Loss 1.1050 Accuracy 0.7389\n",
            "Epoch 201 Batch 450 Loss 1.1088 Accuracy 0.7382\n",
            "Epoch 201 Batch 500 Loss 1.1124 Accuracy 0.7376\n",
            "Epoch 201 Batch 550 Loss 1.1170 Accuracy 0.7369\n",
            "Epoch 201 Batch 600 Loss 1.1185 Accuracy 0.7366\n",
            "Epoch 201 Batch 650 Loss 1.1201 Accuracy 0.7362\n",
            "discarded batch 651\n",
            "Epoch 201 Batch 700 Loss 1.1208 Accuracy 0.7359\n",
            "Epoch 201 Batch 750 Loss 1.1226 Accuracy 0.7357\n",
            "Epoch 201 Batch 800 Loss 1.1255 Accuracy 0.7352\n",
            "Epoch 201 Batch 850 Loss 1.1271 Accuracy 0.7351\n",
            "Epoch 201 Batch 900 Loss 1.1294 Accuracy 0.7346\n",
            "Epoch 201 Batch 950 Loss 1.1313 Accuracy 0.7343\n",
            "Epoch 201 Batch 1000 Loss 1.1321 Accuracy 0.7341\n",
            "Epoch 201 Batch 1050 Loss 1.1347 Accuracy 0.7335\n",
            "Epoch 201 Batch 1100 Loss 1.1359 Accuracy 0.7332\n",
            "Epoch 201 Loss 1.1376 Accuracy 0.7329\n",
            "Time taken for 1 epoch: 22.22265625 secs\n",
            "\n",
            "discarded batch 23\n",
            "Epoch 201 VALIDATION: Loss 2.6929 Accuracy 0.5717\n",
            "\n",
            " sara e rebecca iu  e colei\n",
            "che fu bisava al cantor che per doglia\n",
            "del fallo disse ' miserere mei '\n",
            "                  \n",
            " puoi tu veder cosí di soglia in soglia\n",
            "giú digradar com ' io ch ' a proprio nome\n",
            "vo per la rosa giú di foglia in foglia\n",
            "         \n",
            " e dal settimo grado in giú sí come\n",
            "infino ad esso succedono ebree\n",
            "dirimendo del fior tutte le chiome\n",
            "                   \n",
            " perché secondo lo sguardo che fee\n",
            "la fede in cristo queste sono il muro\n",
            "a che si parton le sacre scalee\n",
            "                  \n",
            "---------------------------\n",
            " \n",
            " grida lui ci a parte si ra\n",
            "retro a grido grida lui che malva\n",
            "ce come a cagio ! a lui ca\n",
            "                      \n",
            "\n",
            " \n",
            " ma che ' l mio maestro vivo sazia\n",
            "tosto è color che ' l mondo fa gelo\n",
            "fa che ' l modo ha di lui da chi chi sazia\n",
            "     \n",
            " ma perché non fa chi fa chi favello\n",
            "non è chi che fa pie vi venni ?\n",
            "e ' l venni ancor chi sie il mondo  ?\n",
            "\n",
            " \n",
            " se le mali avvisi non le calle\n",
            "e non m ' acce e se non vanni vanni\n",
            "e chi l ' uno e l ' un vanno si favelle\n",
            "    \n",
            " non te che fosse a chi ad ogni mani\n",
            "non reduce re ad ogni fidanni\n",
            "mente a voi chi sieme ci restri ?\n",
            "\n",
            "\n",
            " li occhi e e or le e vi ragioni\n",
            "si sta e l ' un grido e l ' altro volta\n",
            "se a lui a tal che entro l ' altro ?\n",
            "  \n",
            " e quel gridò come a cera a forta\n",
            "vidi in giuso in giro e ' l sermove ?\n",
            "gridando il serpensa la sua via scorta\n",
            "          \n",
            " li gridò a me tu che non se ste\n",
            "se ne ne ' tu che di fati ' mar tarni\n",
            "e grida ' l te ne ' o ' l petto le ?\n",
            "\n",
            "\n",
            " o o s ' accise omai che tu ti ?\n",
            "se tu forte in su la giustizia\n",
            "tosto omai che tu ti ti sol ti piani\n",
            "              \n",
            "                                                                          \n",
            " cose ti farò che ti docini\n",
            "non ti dove ma per cara sí dura\n",
            "tosti di segno o te o vi ra\n",
            "\n",
            "\n",
            " non vide a more omai privo ?\n",
            "ché questi vi questi non è nostri il vito\n",
            "se a me l ' alto tuo vi vi vivo\n",
            "             \n",
            "                                                                           \n",
            "\n",
            "li dio forza\n",
            " vele ci repente retro\n",
            "non postre non ne ' piú vage\n",
            "ma con l ' altra ver  la via\n",
            "                       \n",
            "                rente ?\n",
            "non va com ' è ma che se ingegno\n",
            "la gente che ' l mondo ha la gente\n",
            "\n",
            " \n",
            " se le gambe le braccia le cupi\n",
            "le verbo  di lor che non mi rima\n",
            "non duce sí che non ce vi viste\n",
            "                    \n",
            "\n",
            " \n",
            " ché ' l mio maestro e ' l petto dietro\n",
            "a ra tra ' l tace e ' l seganto\n",
            "so a me ! ' lui men men men men casti\n",
            "   \n",
            " e ' l mio frati ce e ' l sesto to\n",
            "non si manifece e chi ' l segno\n",
            "ce chi chi siete e chi ' l segno to noto\n",
            "\n",
            " \n",
            " se le costa mi fa che tu segno\n",
            "non li è dice ché non fu mani\n",
            "lo è do ' io ' l mio dir non si posto degno\n",
            "         \n",
            " dove se tu se ' che tu si reni\n",
            "dove si di se ne ne re aspetta\n",
            "ché non ti ne ' l mondo si reni\n",
            "\n",
            "\n",
            "Epoch 202 Batch 0 Loss 1.0241 Accuracy 0.7756\n",
            "Epoch 202 Batch 50 Loss 1.0877 Accuracy 0.7421\n",
            "Epoch 202 Batch 100 Loss 1.0848 Accuracy 0.7413\n",
            "Epoch 202 Batch 150 Loss 1.0893 Accuracy 0.7402\n",
            "Epoch 202 Batch 200 Loss 1.0930 Accuracy 0.7399\n",
            "Epoch 202 Batch 250 Loss 1.0966 Accuracy 0.7395\n",
            "Epoch 202 Batch 300 Loss 1.0976 Accuracy 0.7394\n",
            "Epoch 202 Batch 350 Loss 1.0999 Accuracy 0.7394\n",
            "Epoch 202 Batch 400 Loss 1.0994 Accuracy 0.7397\n",
            "Epoch 202 Batch 450 Loss 1.1044 Accuracy 0.7389\n",
            "Epoch 202 Batch 500 Loss 1.1063 Accuracy 0.7384\n",
            "Epoch 202 Batch 550 Loss 1.1106 Accuracy 0.7377\n",
            "Epoch 202 Batch 600 Loss 1.1131 Accuracy 0.7371\n",
            "Epoch 202 Batch 650 Loss 1.1163 Accuracy 0.7367\n",
            "Epoch 202 Batch 700 Loss 1.1201 Accuracy 0.7363\n",
            "Epoch 202 Batch 750 Loss 1.1218 Accuracy 0.7360\n",
            "Epoch 202 Batch 800 Loss 1.1235 Accuracy 0.7358\n",
            "Epoch 202 Batch 850 Loss 1.1256 Accuracy 0.7353\n",
            "Epoch 202 Batch 900 Loss 1.1273 Accuracy 0.7350\n",
            "Epoch 202 Batch 950 Loss 1.1289 Accuracy 0.7348\n",
            "Epoch 202 Batch 1000 Loss 1.1306 Accuracy 0.7344\n",
            "Epoch 202 Batch 1050 Loss 1.1310 Accuracy 0.7342\n",
            "discarded batch 1086\n",
            "Epoch 202 Batch 1100 Loss 1.1331 Accuracy 0.7339\n",
            "Epoch 202 Loss 1.1336 Accuracy 0.7337\n",
            "Time taken for 1 epoch: 22.233211517333984 secs\n",
            "\n",
            "Epoch 203 Batch 0 Loss 1.2313 Accuracy 0.7360\n",
            "Epoch 203 Batch 50 Loss 1.0885 Accuracy 0.7420\n",
            "Epoch 203 Batch 100 Loss 1.0979 Accuracy 0.7397\n",
            "Epoch 203 Batch 150 Loss 1.1050 Accuracy 0.7376\n",
            "Epoch 203 Batch 200 Loss 1.1035 Accuracy 0.7386\n",
            "Epoch 203 Batch 250 Loss 1.1042 Accuracy 0.7384\n",
            "Epoch 203 Batch 300 Loss 1.1049 Accuracy 0.7379\n",
            "Epoch 203 Batch 350 Loss 1.1086 Accuracy 0.7373\n",
            "Epoch 203 Batch 400 Loss 1.1102 Accuracy 0.7371\n",
            "Epoch 203 Batch 450 Loss 1.1158 Accuracy 0.7363\n",
            "Epoch 203 Batch 500 Loss 1.1170 Accuracy 0.7361\n",
            "Epoch 203 Batch 550 Loss 1.1169 Accuracy 0.7362\n",
            "Epoch 203 Batch 600 Loss 1.1182 Accuracy 0.7359\n",
            "Epoch 203 Batch 650 Loss 1.1193 Accuracy 0.7354\n",
            "Epoch 203 Batch 700 Loss 1.1211 Accuracy 0.7350\n",
            "Epoch 203 Batch 750 Loss 1.1222 Accuracy 0.7347\n",
            "Epoch 203 Batch 800 Loss 1.1241 Accuracy 0.7345\n",
            "Epoch 203 Batch 850 Loss 1.1260 Accuracy 0.7343\n",
            "discarded batch 872\n",
            "Epoch 203 Batch 900 Loss 1.1280 Accuracy 0.7340\n",
            "Epoch 203 Batch 950 Loss 1.1289 Accuracy 0.7342\n",
            "Epoch 203 Batch 1000 Loss 1.1304 Accuracy 0.7340\n",
            "Epoch 203 Batch 1050 Loss 1.1309 Accuracy 0.7339\n",
            "Epoch 203 Batch 1100 Loss 1.1333 Accuracy 0.7337\n",
            "Epoch 203 Loss 1.1340 Accuracy 0.7336\n",
            "Time taken for 1 epoch: 22.223445415496826 secs\n",
            "\n",
            "Epoch 204 Batch 0 Loss 1.0748 Accuracy 0.7690\n",
            "Epoch 204 Batch 50 Loss 1.0913 Accuracy 0.7406\n",
            "Epoch 204 Batch 100 Loss 1.0897 Accuracy 0.7390\n",
            "Epoch 204 Batch 150 Loss 1.0842 Accuracy 0.7411\n",
            "Epoch 204 Batch 200 Loss 1.0860 Accuracy 0.7405\n",
            "Epoch 204 Batch 250 Loss 1.0905 Accuracy 0.7402\n",
            "Epoch 204 Batch 300 Loss 1.0935 Accuracy 0.7395\n",
            "Epoch 204 Batch 350 Loss 1.0962 Accuracy 0.7392\n",
            "Epoch 204 Batch 400 Loss 1.0965 Accuracy 0.7391\n",
            "Epoch 204 Batch 450 Loss 1.1006 Accuracy 0.7385\n",
            "Epoch 204 Batch 500 Loss 1.1060 Accuracy 0.7377\n",
            "Epoch 204 Batch 550 Loss 1.1077 Accuracy 0.7376\n",
            "Epoch 204 Batch 600 Loss 1.1092 Accuracy 0.7374\n",
            "Epoch 204 Batch 650 Loss 1.1103 Accuracy 0.7372\n",
            "Epoch 204 Batch 700 Loss 1.1125 Accuracy 0.7367\n",
            "Epoch 204 Batch 750 Loss 1.1146 Accuracy 0.7363\n",
            "Epoch 204 Batch 800 Loss 1.1162 Accuracy 0.7361\n",
            "Epoch 204 Batch 850 Loss 1.1192 Accuracy 0.7358\n",
            "Epoch 204 Batch 900 Loss 1.1214 Accuracy 0.7354\n",
            "Epoch 204 Batch 950 Loss 1.1238 Accuracy 0.7351\n",
            "Epoch 204 Batch 1000 Loss 1.1253 Accuracy 0.7348\n",
            "discarded batch 1021\n",
            "Epoch 204 Batch 1050 Loss 1.1263 Accuracy 0.7349\n",
            "Epoch 204 Batch 1100 Loss 1.1286 Accuracy 0.7346\n",
            "Epoch 204 Loss 1.1297 Accuracy 0.7345\n",
            "Time taken for 1 epoch: 22.194135189056396 secs\n",
            "\n",
            "Epoch 205 Batch 0 Loss 1.0267 Accuracy 0.7624\n",
            "Epoch 205 Batch 50 Loss 1.0876 Accuracy 0.7445\n",
            "Epoch 205 Batch 100 Loss 1.0841 Accuracy 0.7441\n",
            "Epoch 205 Batch 150 Loss 1.0924 Accuracy 0.7408\n",
            "Epoch 205 Batch 200 Loss 1.0947 Accuracy 0.7401\n",
            "Epoch 205 Batch 250 Loss 1.0938 Accuracy 0.7395\n",
            "Epoch 205 Batch 300 Loss 1.0964 Accuracy 0.7393\n",
            "Epoch 205 Batch 350 Loss 1.0971 Accuracy 0.7394\n",
            "Epoch 205 Batch 400 Loss 1.0997 Accuracy 0.7392\n",
            "Epoch 205 Batch 450 Loss 1.1025 Accuracy 0.7386\n",
            "Epoch 205 Batch 500 Loss 1.1073 Accuracy 0.7378\n",
            "Epoch 205 Batch 550 Loss 1.1106 Accuracy 0.7369\n",
            "Epoch 205 Batch 600 Loss 1.1137 Accuracy 0.7366\n",
            "Epoch 205 Batch 650 Loss 1.1157 Accuracy 0.7364\n",
            "Epoch 205 Batch 700 Loss 1.1176 Accuracy 0.7360\n",
            "Epoch 205 Batch 750 Loss 1.1187 Accuracy 0.7359\n",
            "Epoch 205 Batch 800 Loss 1.1202 Accuracy 0.7356\n",
            "Epoch 205 Batch 850 Loss 1.1229 Accuracy 0.7351\n",
            "Epoch 205 Batch 900 Loss 1.1245 Accuracy 0.7347\n",
            "Epoch 205 Batch 950 Loss 1.1269 Accuracy 0.7343\n",
            "Epoch 205 Batch 1000 Loss 1.1286 Accuracy 0.7341\n",
            "Epoch 205 Batch 1050 Loss 1.1295 Accuracy 0.7340\n",
            "discarded batch 1073\n",
            "Epoch 205 Batch 1100 Loss 1.1314 Accuracy 0.7337\n",
            "Saving checkpoint for epoch 205 at ./checkpoints/train/ckpt-41\n",
            "Epoch 205 Loss 1.1319 Accuracy 0.7338\n",
            "Time taken for 1 epoch: 22.450785875320435 secs\n",
            "\n",
            "Epoch 206 Batch 0 Loss 1.2437 Accuracy 0.7030\n",
            "Epoch 206 Batch 50 Loss 1.0675 Accuracy 0.7463\n",
            "Epoch 206 Batch 100 Loss 1.0744 Accuracy 0.7436\n",
            "Epoch 206 Batch 150 Loss 1.0761 Accuracy 0.7439\n",
            "Epoch 206 Batch 200 Loss 1.0872 Accuracy 0.7423\n",
            "Epoch 206 Batch 250 Loss 1.0919 Accuracy 0.7409\n",
            "Epoch 206 Batch 300 Loss 1.0951 Accuracy 0.7399\n",
            "Epoch 206 Batch 350 Loss 1.0967 Accuracy 0.7398\n",
            "Epoch 206 Batch 400 Loss 1.1006 Accuracy 0.7393\n",
            "Epoch 206 Batch 450 Loss 1.1028 Accuracy 0.7388\n",
            "Epoch 206 Batch 500 Loss 1.1035 Accuracy 0.7387\n",
            "Epoch 206 Batch 550 Loss 1.1055 Accuracy 0.7383\n",
            "discarded batch 595\n",
            "Epoch 206 Batch 600 Loss 1.1087 Accuracy 0.7377\n",
            "Epoch 206 Batch 650 Loss 1.1121 Accuracy 0.7369\n",
            "Epoch 206 Batch 700 Loss 1.1144 Accuracy 0.7364\n",
            "Epoch 206 Batch 750 Loss 1.1171 Accuracy 0.7362\n",
            "Epoch 206 Batch 800 Loss 1.1200 Accuracy 0.7358\n",
            "Epoch 206 Batch 850 Loss 1.1216 Accuracy 0.7355\n",
            "Epoch 206 Batch 900 Loss 1.1229 Accuracy 0.7353\n",
            "Epoch 206 Batch 950 Loss 1.1254 Accuracy 0.7350\n",
            "Epoch 206 Batch 1000 Loss 1.1273 Accuracy 0.7347\n",
            "Epoch 206 Batch 1050 Loss 1.1288 Accuracy 0.7344\n",
            "Epoch 206 Batch 1100 Loss 1.1305 Accuracy 0.7340\n",
            "Epoch 206 Loss 1.1315 Accuracy 0.7339\n",
            "Time taken for 1 epoch: 22.304769277572632 secs\n",
            "\n",
            "discarded batch 23\n",
            "Epoch 206 VALIDATION: Loss 2.6833 Accuracy 0.5702\n",
            "\n",
            "Epoch 207 Batch 0 Loss 0.9504 Accuracy 0.7789\n",
            "Epoch 207 Batch 50 Loss 1.0637 Accuracy 0.7448\n",
            "Epoch 207 Batch 100 Loss 1.0716 Accuracy 0.7428\n",
            "Epoch 207 Batch 150 Loss 1.0713 Accuracy 0.7430\n",
            "Epoch 207 Batch 200 Loss 1.0784 Accuracy 0.7433\n",
            "Epoch 207 Batch 250 Loss 1.0795 Accuracy 0.7431\n",
            "Epoch 207 Batch 300 Loss 1.0843 Accuracy 0.7426\n",
            "Epoch 207 Batch 350 Loss 1.0870 Accuracy 0.7420\n",
            "Epoch 207 Batch 400 Loss 1.0904 Accuracy 0.7414\n",
            "Epoch 207 Batch 450 Loss 1.0916 Accuracy 0.7407\n",
            "Epoch 207 Batch 500 Loss 1.0945 Accuracy 0.7402\n",
            "discarded batch 532\n",
            "Epoch 207 Batch 550 Loss 1.0979 Accuracy 0.7394\n",
            "Epoch 207 Batch 600 Loss 1.1024 Accuracy 0.7384\n",
            "Epoch 207 Batch 650 Loss 1.1052 Accuracy 0.7382\n",
            "Epoch 207 Batch 700 Loss 1.1078 Accuracy 0.7376\n",
            "Epoch 207 Batch 750 Loss 1.1110 Accuracy 0.7373\n",
            "Epoch 207 Batch 800 Loss 1.1140 Accuracy 0.7369\n",
            "Epoch 207 Batch 850 Loss 1.1180 Accuracy 0.7363\n",
            "Epoch 207 Batch 900 Loss 1.1185 Accuracy 0.7359\n",
            "Epoch 207 Batch 950 Loss 1.1214 Accuracy 0.7354\n",
            "Epoch 207 Batch 1000 Loss 1.1234 Accuracy 0.7352\n",
            "Epoch 207 Batch 1050 Loss 1.1248 Accuracy 0.7351\n",
            "Epoch 207 Batch 1100 Loss 1.1272 Accuracy 0.7348\n",
            "Epoch 207 Loss 1.1277 Accuracy 0.7347\n",
            "Time taken for 1 epoch: 22.230344772338867 secs\n",
            "\n",
            "Epoch 208 Batch 0 Loss 1.1422 Accuracy 0.7360\n",
            "Epoch 208 Batch 50 Loss 1.0777 Accuracy 0.7444\n",
            "Epoch 208 Batch 100 Loss 1.0780 Accuracy 0.7442\n",
            "Epoch 208 Batch 150 Loss 1.0798 Accuracy 0.7437\n",
            "Epoch 208 Batch 200 Loss 1.0854 Accuracy 0.7420\n",
            "Epoch 208 Batch 250 Loss 1.0920 Accuracy 0.7407\n",
            "Epoch 208 Batch 300 Loss 1.0960 Accuracy 0.7401\n",
            "Epoch 208 Batch 350 Loss 1.0939 Accuracy 0.7408\n",
            "Epoch 208 Batch 400 Loss 1.0967 Accuracy 0.7401\n",
            "Epoch 208 Batch 450 Loss 1.1032 Accuracy 0.7392\n",
            "Epoch 208 Batch 500 Loss 1.1050 Accuracy 0.7388\n",
            "Epoch 208 Batch 550 Loss 1.1072 Accuracy 0.7384\n",
            "Epoch 208 Batch 600 Loss 1.1090 Accuracy 0.7383\n",
            "Epoch 208 Batch 650 Loss 1.1106 Accuracy 0.7378\n",
            "Epoch 208 Batch 700 Loss 1.1133 Accuracy 0.7373\n",
            "Epoch 208 Batch 750 Loss 1.1153 Accuracy 0.7371\n",
            "Epoch 208 Batch 800 Loss 1.1162 Accuracy 0.7368\n",
            "Epoch 208 Batch 850 Loss 1.1189 Accuracy 0.7362\n",
            "Epoch 208 Batch 900 Loss 1.1206 Accuracy 0.7359\n",
            "Epoch 208 Batch 950 Loss 1.1222 Accuracy 0.7358\n",
            "Epoch 208 Batch 1000 Loss 1.1238 Accuracy 0.7353\n",
            "Epoch 208 Batch 1050 Loss 1.1256 Accuracy 0.7349\n",
            "discarded batch 1066\n",
            "Epoch 208 Batch 1100 Loss 1.1273 Accuracy 0.7345\n",
            "Epoch 208 Loss 1.1285 Accuracy 0.7343\n",
            "Time taken for 1 epoch: 22.242496013641357 secs\n",
            "\n",
            "Epoch 209 Batch 0 Loss 1.3339 Accuracy 0.7360\n",
            "Epoch 209 Batch 50 Loss 1.0872 Accuracy 0.7445\n",
            "Epoch 209 Batch 100 Loss 1.0809 Accuracy 0.7436\n",
            "Epoch 209 Batch 150 Loss 1.0844 Accuracy 0.7431\n",
            "Epoch 209 Batch 200 Loss 1.0861 Accuracy 0.7423\n",
            "Epoch 209 Batch 250 Loss 1.0893 Accuracy 0.7412\n",
            "Epoch 209 Batch 300 Loss 1.0942 Accuracy 0.7405\n",
            "Epoch 209 Batch 350 Loss 1.0987 Accuracy 0.7401\n",
            "Epoch 209 Batch 400 Loss 1.1013 Accuracy 0.7395\n",
            "Epoch 209 Batch 450 Loss 1.1021 Accuracy 0.7394\n",
            "Epoch 209 Batch 500 Loss 1.1048 Accuracy 0.7388\n",
            "discarded batch 518\n",
            "Epoch 209 Batch 550 Loss 1.1053 Accuracy 0.7386\n",
            "Epoch 209 Batch 600 Loss 1.1064 Accuracy 0.7383\n",
            "Epoch 209 Batch 650 Loss 1.1105 Accuracy 0.7375\n",
            "Epoch 209 Batch 700 Loss 1.1121 Accuracy 0.7374\n",
            "Epoch 209 Batch 750 Loss 1.1132 Accuracy 0.7373\n",
            "Epoch 209 Batch 800 Loss 1.1142 Accuracy 0.7369\n",
            "Epoch 209 Batch 850 Loss 1.1159 Accuracy 0.7366\n",
            "Epoch 209 Batch 900 Loss 1.1170 Accuracy 0.7362\n",
            "Epoch 209 Batch 950 Loss 1.1192 Accuracy 0.7358\n",
            "Epoch 209 Batch 1000 Loss 1.1226 Accuracy 0.7352\n",
            "Epoch 209 Batch 1050 Loss 1.1248 Accuracy 0.7348\n",
            "Epoch 209 Batch 1100 Loss 1.1259 Accuracy 0.7347\n",
            "Epoch 209 Loss 1.1271 Accuracy 0.7347\n",
            "Time taken for 1 epoch: 22.25759983062744 secs\n",
            "\n",
            "Epoch 210 Batch 0 Loss 1.0673 Accuracy 0.7393\n",
            "Epoch 210 Batch 50 Loss 1.0853 Accuracy 0.7424\n",
            "Epoch 210 Batch 100 Loss 1.0838 Accuracy 0.7422\n",
            "Epoch 210 Batch 150 Loss 1.0841 Accuracy 0.7431\n",
            "discarded batch 188\n",
            "Epoch 210 Batch 200 Loss 1.0902 Accuracy 0.7420\n",
            "Epoch 210 Batch 250 Loss 1.0931 Accuracy 0.7422\n",
            "Epoch 210 Batch 300 Loss 1.0958 Accuracy 0.7414\n",
            "Epoch 210 Batch 350 Loss 1.0983 Accuracy 0.7403\n",
            "Epoch 210 Batch 400 Loss 1.0989 Accuracy 0.7400\n",
            "Epoch 210 Batch 450 Loss 1.0998 Accuracy 0.7394\n",
            "Epoch 210 Batch 500 Loss 1.1046 Accuracy 0.7385\n",
            "Epoch 210 Batch 550 Loss 1.1079 Accuracy 0.7378\n",
            "Epoch 210 Batch 600 Loss 1.1096 Accuracy 0.7378\n",
            "Epoch 210 Batch 650 Loss 1.1099 Accuracy 0.7376\n",
            "Epoch 210 Batch 700 Loss 1.1127 Accuracy 0.7372\n",
            "Epoch 210 Batch 750 Loss 1.1138 Accuracy 0.7371\n",
            "Epoch 210 Batch 800 Loss 1.1166 Accuracy 0.7367\n",
            "Epoch 210 Batch 850 Loss 1.1177 Accuracy 0.7366\n",
            "Epoch 210 Batch 900 Loss 1.1196 Accuracy 0.7361\n",
            "Epoch 210 Batch 950 Loss 1.1209 Accuracy 0.7358\n",
            "Epoch 210 Batch 1000 Loss 1.1211 Accuracy 0.7357\n",
            "Epoch 210 Batch 1050 Loss 1.1223 Accuracy 0.7353\n",
            "Epoch 210 Batch 1100 Loss 1.1239 Accuracy 0.7350\n",
            "Saving checkpoint for epoch 210 at ./checkpoints/train/ckpt-42\n",
            "Epoch 210 Loss 1.1244 Accuracy 0.7350\n",
            "Time taken for 1 epoch: 22.573127508163452 secs\n",
            "\n",
            "Epoch 211 Batch 0 Loss 1.1579 Accuracy 0.7459\n",
            "Epoch 211 Batch 50 Loss 1.0835 Accuracy 0.7426\n",
            "Epoch 211 Batch 100 Loss 1.0773 Accuracy 0.7443\n",
            "Epoch 211 Batch 150 Loss 1.0786 Accuracy 0.7436\n",
            "Epoch 211 Batch 200 Loss 1.0808 Accuracy 0.7435\n",
            "Epoch 211 Batch 250 Loss 1.0863 Accuracy 0.7420\n",
            "Epoch 211 Batch 300 Loss 1.0907 Accuracy 0.7416\n",
            "Epoch 211 Batch 350 Loss 1.0936 Accuracy 0.7411\n",
            "Epoch 211 Batch 400 Loss 1.0950 Accuracy 0.7406\n",
            "Epoch 211 Batch 450 Loss 1.0955 Accuracy 0.7402\n",
            "Epoch 211 Batch 500 Loss 1.0980 Accuracy 0.7396\n",
            "Epoch 211 Batch 550 Loss 1.1011 Accuracy 0.7390\n",
            "Epoch 211 Batch 600 Loss 1.1040 Accuracy 0.7383\n",
            "Epoch 211 Batch 650 Loss 1.1050 Accuracy 0.7379\n",
            "Epoch 211 Batch 700 Loss 1.1060 Accuracy 0.7378\n",
            "Epoch 211 Batch 750 Loss 1.1092 Accuracy 0.7372\n",
            "Epoch 211 Batch 800 Loss 1.1111 Accuracy 0.7367\n",
            "Epoch 211 Batch 850 Loss 1.1140 Accuracy 0.7361\n",
            "Epoch 211 Batch 900 Loss 1.1169 Accuracy 0.7355\n",
            "Epoch 211 Batch 950 Loss 1.1173 Accuracy 0.7356\n",
            "discarded batch 999\n",
            "Epoch 211 Batch 1000 Loss 1.1194 Accuracy 0.7352\n",
            "Epoch 211 Batch 1050 Loss 1.1205 Accuracy 0.7351\n",
            "Epoch 211 Batch 1100 Loss 1.1220 Accuracy 0.7349\n",
            "Epoch 211 Loss 1.1226 Accuracy 0.7348\n",
            "Time taken for 1 epoch: 22.55261516571045 secs\n",
            "\n",
            "discarded batch 23\n",
            "Epoch 211 VALIDATION: Loss 2.6796 Accuracy 0.5741\n",
            "\n",
            "Epoch 212 Batch 0 Loss 1.0929 Accuracy 0.7723\n",
            "Epoch 212 Batch 50 Loss 1.0627 Accuracy 0.7474\n",
            "Epoch 212 Batch 100 Loss 1.0716 Accuracy 0.7453\n",
            "Epoch 212 Batch 150 Loss 1.0733 Accuracy 0.7454\n",
            "Epoch 212 Batch 200 Loss 1.0828 Accuracy 0.7424\n",
            "Epoch 212 Batch 250 Loss 1.0832 Accuracy 0.7427\n",
            "Epoch 212 Batch 300 Loss 1.0857 Accuracy 0.7426\n",
            "Epoch 212 Batch 350 Loss 1.0885 Accuracy 0.7419\n",
            "Epoch 212 Batch 400 Loss 1.0931 Accuracy 0.7409\n",
            "Epoch 212 Batch 450 Loss 1.0960 Accuracy 0.7402\n",
            "Epoch 212 Batch 500 Loss 1.1003 Accuracy 0.7394\n",
            "Epoch 212 Batch 550 Loss 1.1023 Accuracy 0.7393\n",
            "Epoch 212 Batch 600 Loss 1.1035 Accuracy 0.7390\n",
            "Epoch 212 Batch 650 Loss 1.1050 Accuracy 0.7382\n",
            "Epoch 212 Batch 700 Loss 1.1064 Accuracy 0.7382\n",
            "Epoch 212 Batch 750 Loss 1.1074 Accuracy 0.7381\n",
            "Epoch 212 Batch 800 Loss 1.1086 Accuracy 0.7380\n",
            "Epoch 212 Batch 850 Loss 1.1115 Accuracy 0.7374\n",
            "Epoch 212 Batch 900 Loss 1.1124 Accuracy 0.7373\n",
            "Epoch 212 Batch 950 Loss 1.1143 Accuracy 0.7368\n",
            "Epoch 212 Batch 1000 Loss 1.1157 Accuracy 0.7365\n",
            "Epoch 212 Batch 1050 Loss 1.1168 Accuracy 0.7364\n",
            "discarded batch 1075\n",
            "Epoch 212 Batch 1100 Loss 1.1187 Accuracy 0.7362\n",
            "Epoch 212 Loss 1.1213 Accuracy 0.7358\n",
            "Time taken for 1 epoch: 22.284124612808228 secs\n",
            "\n",
            "Epoch 213 Batch 0 Loss 1.0488 Accuracy 0.7657\n",
            "Epoch 213 Batch 50 Loss 1.0643 Accuracy 0.7461\n",
            "Epoch 213 Batch 100 Loss 1.0745 Accuracy 0.7435\n",
            "Epoch 213 Batch 150 Loss 1.0785 Accuracy 0.7432\n",
            "Epoch 213 Batch 200 Loss 1.0790 Accuracy 0.7432\n",
            "Epoch 213 Batch 250 Loss 1.0790 Accuracy 0.7424\n",
            "Epoch 213 Batch 300 Loss 1.0794 Accuracy 0.7427\n",
            "Epoch 213 Batch 350 Loss 1.0838 Accuracy 0.7424\n",
            "Epoch 213 Batch 400 Loss 1.0885 Accuracy 0.7419\n",
            "Epoch 213 Batch 450 Loss 1.0929 Accuracy 0.7412\n",
            "Epoch 213 Batch 500 Loss 1.0952 Accuracy 0.7408\n",
            "Epoch 213 Batch 550 Loss 1.0956 Accuracy 0.7408\n",
            "Epoch 213 Batch 600 Loss 1.0984 Accuracy 0.7402\n",
            "Epoch 213 Batch 650 Loss 1.1018 Accuracy 0.7395\n",
            "discarded batch 651\n",
            "Epoch 213 Batch 700 Loss 1.1036 Accuracy 0.7393\n",
            "Epoch 213 Batch 750 Loss 1.1067 Accuracy 0.7387\n",
            "Epoch 213 Batch 800 Loss 1.1084 Accuracy 0.7382\n",
            "Epoch 213 Batch 850 Loss 1.1110 Accuracy 0.7376\n",
            "Epoch 213 Batch 900 Loss 1.1128 Accuracy 0.7372\n",
            "Epoch 213 Batch 950 Loss 1.1149 Accuracy 0.7369\n",
            "Epoch 213 Batch 1000 Loss 1.1166 Accuracy 0.7366\n",
            "Epoch 213 Batch 1050 Loss 1.1191 Accuracy 0.7362\n",
            "Epoch 213 Batch 1100 Loss 1.1214 Accuracy 0.7358\n",
            "Epoch 213 Loss 1.1224 Accuracy 0.7356\n",
            "Time taken for 1 epoch: 22.153098821640015 secs\n",
            "\n",
            "Epoch 214 Batch 0 Loss 0.9947 Accuracy 0.7756\n",
            "Epoch 214 Batch 50 Loss 1.0736 Accuracy 0.7465\n",
            "Epoch 214 Batch 100 Loss 1.0761 Accuracy 0.7466\n",
            "Epoch 214 Batch 150 Loss 1.0801 Accuracy 0.7452\n",
            "Epoch 214 Batch 200 Loss 1.0821 Accuracy 0.7436\n",
            "Epoch 214 Batch 250 Loss 1.0862 Accuracy 0.7428\n",
            "Epoch 214 Batch 300 Loss 1.0901 Accuracy 0.7413\n",
            "Epoch 214 Batch 350 Loss 1.0893 Accuracy 0.7414\n",
            "Epoch 214 Batch 400 Loss 1.0895 Accuracy 0.7412\n",
            "Epoch 214 Batch 450 Loss 1.0900 Accuracy 0.7410\n",
            "Epoch 214 Batch 500 Loss 1.0942 Accuracy 0.7405\n",
            "Epoch 214 Batch 550 Loss 1.0967 Accuracy 0.7403\n",
            "Epoch 214 Batch 600 Loss 1.0999 Accuracy 0.7402\n",
            "Epoch 214 Batch 650 Loss 1.1020 Accuracy 0.7397\n",
            "Epoch 214 Batch 700 Loss 1.1039 Accuracy 0.7391\n",
            "discarded batch 712\n",
            "Epoch 214 Batch 750 Loss 1.1052 Accuracy 0.7385\n",
            "Epoch 214 Batch 800 Loss 1.1072 Accuracy 0.7382\n",
            "Epoch 214 Batch 850 Loss 1.1098 Accuracy 0.7378\n",
            "Epoch 214 Batch 900 Loss 1.1106 Accuracy 0.7374\n",
            "Epoch 214 Batch 950 Loss 1.1120 Accuracy 0.7373\n",
            "Epoch 214 Batch 1000 Loss 1.1148 Accuracy 0.7369\n",
            "Epoch 214 Batch 1050 Loss 1.1159 Accuracy 0.7365\n",
            "Epoch 214 Batch 1100 Loss 1.1175 Accuracy 0.7363\n",
            "Epoch 214 Loss 1.1182 Accuracy 0.7361\n",
            "Time taken for 1 epoch: 22.199267387390137 secs\n",
            "\n",
            "Epoch 215 Batch 0 Loss 0.9701 Accuracy 0.7657\n",
            "Epoch 215 Batch 50 Loss 1.0710 Accuracy 0.7445\n",
            "Epoch 215 Batch 100 Loss 1.0665 Accuracy 0.7446\n",
            "Epoch 215 Batch 150 Loss 1.0779 Accuracy 0.7433\n",
            "Epoch 215 Batch 200 Loss 1.0828 Accuracy 0.7418\n",
            "Epoch 215 Batch 250 Loss 1.0863 Accuracy 0.7410\n",
            "Epoch 215 Batch 300 Loss 1.0872 Accuracy 0.7418\n",
            "Epoch 215 Batch 350 Loss 1.0880 Accuracy 0.7413\n",
            "Epoch 215 Batch 400 Loss 1.0896 Accuracy 0.7407\n",
            "Epoch 215 Batch 450 Loss 1.0933 Accuracy 0.7403\n",
            "Epoch 215 Batch 500 Loss 1.0940 Accuracy 0.7400\n",
            "Epoch 215 Batch 550 Loss 1.0966 Accuracy 0.7397\n",
            "Epoch 215 Batch 600 Loss 1.0973 Accuracy 0.7396\n",
            "Epoch 215 Batch 650 Loss 1.0981 Accuracy 0.7395\n",
            "Epoch 215 Batch 700 Loss 1.1007 Accuracy 0.7390\n",
            "Epoch 215 Batch 750 Loss 1.1036 Accuracy 0.7384\n",
            "Epoch 215 Batch 800 Loss 1.1065 Accuracy 0.7380\n",
            "Epoch 215 Batch 850 Loss 1.1086 Accuracy 0.7377\n",
            "Epoch 215 Batch 900 Loss 1.1105 Accuracy 0.7374\n",
            "discarded batch 948\n",
            "Epoch 215 Batch 950 Loss 1.1124 Accuracy 0.7371\n",
            "Epoch 215 Batch 1000 Loss 1.1138 Accuracy 0.7368\n",
            "Epoch 215 Batch 1050 Loss 1.1158 Accuracy 0.7364\n",
            "Epoch 215 Batch 1100 Loss 1.1167 Accuracy 0.7363\n",
            "Saving checkpoint for epoch 215 at ./checkpoints/train/ckpt-43\n",
            "Epoch 215 Loss 1.1173 Accuracy 0.7362\n",
            "Time taken for 1 epoch: 22.396711826324463 secs\n",
            "\n",
            "Epoch 216 Batch 0 Loss 1.0464 Accuracy 0.7789\n",
            "Epoch 216 Batch 50 Loss 1.0860 Accuracy 0.7412\n",
            "Epoch 216 Batch 100 Loss 1.0776 Accuracy 0.7421\n",
            "Epoch 216 Batch 150 Loss 1.0846 Accuracy 0.7413\n",
            "Epoch 216 Batch 200 Loss 1.0873 Accuracy 0.7412\n",
            "Epoch 216 Batch 250 Loss 1.0876 Accuracy 0.7418\n",
            "Epoch 216 Batch 300 Loss 1.0878 Accuracy 0.7416\n",
            "Epoch 216 Batch 350 Loss 1.0923 Accuracy 0.7406\n",
            "Epoch 216 Batch 400 Loss 1.0913 Accuracy 0.7404\n",
            "Epoch 216 Batch 450 Loss 1.0919 Accuracy 0.7402\n",
            "Epoch 216 Batch 500 Loss 1.0934 Accuracy 0.7399\n",
            "Epoch 216 Batch 550 Loss 1.0950 Accuracy 0.7396\n",
            "Epoch 216 Batch 600 Loss 1.0985 Accuracy 0.7388\n",
            "Epoch 216 Batch 650 Loss 1.1000 Accuracy 0.7385\n",
            "discarded batch 660\n",
            "Epoch 216 Batch 700 Loss 1.1019 Accuracy 0.7381\n",
            "Epoch 216 Batch 750 Loss 1.1045 Accuracy 0.7378\n",
            "Epoch 216 Batch 800 Loss 1.1079 Accuracy 0.7373\n",
            "Epoch 216 Batch 850 Loss 1.1094 Accuracy 0.7372\n",
            "Epoch 216 Batch 900 Loss 1.1112 Accuracy 0.7370\n",
            "Epoch 216 Batch 950 Loss 1.1127 Accuracy 0.7368\n",
            "Epoch 216 Batch 1000 Loss 1.1144 Accuracy 0.7363\n",
            "Epoch 216 Batch 1050 Loss 1.1164 Accuracy 0.7360\n",
            "Epoch 216 Batch 1100 Loss 1.1181 Accuracy 0.7357\n",
            "Epoch 216 Loss 1.1196 Accuracy 0.7354\n",
            "Time taken for 1 epoch: 22.21025848388672 secs\n",
            "\n",
            "discarded batch 23\n",
            "Epoch 216 VALIDATION: Loss 2.6966 Accuracy 0.5711\n",
            "\n",
            "Epoch 217 Batch 0 Loss 1.0380 Accuracy 0.7459\n",
            "discarded batch 35\n",
            "Epoch 217 Batch 50 Loss 1.0619 Accuracy 0.7471\n",
            "Epoch 217 Batch 100 Loss 1.0610 Accuracy 0.7482\n",
            "Epoch 217 Batch 150 Loss 1.0676 Accuracy 0.7453\n",
            "Epoch 217 Batch 200 Loss 1.0730 Accuracy 0.7440\n",
            "Epoch 217 Batch 250 Loss 1.0748 Accuracy 0.7439\n",
            "Epoch 217 Batch 300 Loss 1.0739 Accuracy 0.7439\n",
            "Epoch 217 Batch 350 Loss 1.0784 Accuracy 0.7440\n",
            "Epoch 217 Batch 400 Loss 1.0820 Accuracy 0.7436\n",
            "Epoch 217 Batch 450 Loss 1.0876 Accuracy 0.7425\n",
            "Epoch 217 Batch 500 Loss 1.0905 Accuracy 0.7417\n",
            "Epoch 217 Batch 550 Loss 1.0912 Accuracy 0.7411\n",
            "Epoch 217 Batch 600 Loss 1.0948 Accuracy 0.7408\n",
            "Epoch 217 Batch 650 Loss 1.0979 Accuracy 0.7403\n",
            "Epoch 217 Batch 700 Loss 1.1004 Accuracy 0.7398\n",
            "Epoch 217 Batch 750 Loss 1.1033 Accuracy 0.7391\n",
            "Epoch 217 Batch 800 Loss 1.1058 Accuracy 0.7386\n",
            "Epoch 217 Batch 850 Loss 1.1072 Accuracy 0.7384\n",
            "Epoch 217 Batch 900 Loss 1.1094 Accuracy 0.7378\n",
            "Epoch 217 Batch 950 Loss 1.1112 Accuracy 0.7377\n",
            "Epoch 217 Batch 1000 Loss 1.1122 Accuracy 0.7375\n",
            "Epoch 217 Batch 1050 Loss 1.1139 Accuracy 0.7372\n",
            "Epoch 217 Batch 1100 Loss 1.1160 Accuracy 0.7369\n",
            "Epoch 217 Loss 1.1179 Accuracy 0.7366\n",
            "Time taken for 1 epoch: 22.286826133728027 secs\n",
            "\n",
            "Epoch 218 Batch 0 Loss 0.9919 Accuracy 0.7789\n",
            "Epoch 218 Batch 50 Loss 1.0738 Accuracy 0.7455\n",
            "Epoch 218 Batch 100 Loss 1.0813 Accuracy 0.7413\n",
            "Epoch 218 Batch 150 Loss 1.0819 Accuracy 0.7419\n",
            "Epoch 218 Batch 200 Loss 1.0801 Accuracy 0.7426\n",
            "Epoch 218 Batch 250 Loss 1.0848 Accuracy 0.7413\n",
            "Epoch 218 Batch 300 Loss 1.0897 Accuracy 0.7407\n",
            "Epoch 218 Batch 350 Loss 1.0901 Accuracy 0.7405\n",
            "Epoch 218 Batch 400 Loss 1.0920 Accuracy 0.7402\n",
            "Epoch 218 Batch 450 Loss 1.0944 Accuracy 0.7401\n",
            "Epoch 218 Batch 500 Loss 1.0964 Accuracy 0.7395\n",
            "Epoch 218 Batch 550 Loss 1.0958 Accuracy 0.7393\n",
            "Epoch 218 Batch 600 Loss 1.0978 Accuracy 0.7390\n",
            "Epoch 218 Batch 650 Loss 1.1000 Accuracy 0.7385\n",
            "Epoch 218 Batch 700 Loss 1.1016 Accuracy 0.7382\n",
            "Epoch 218 Batch 750 Loss 1.1044 Accuracy 0.7379\n",
            "Epoch 218 Batch 800 Loss 1.1057 Accuracy 0.7377\n",
            "Epoch 218 Batch 850 Loss 1.1073 Accuracy 0.7374\n",
            "Epoch 218 Batch 900 Loss 1.1077 Accuracy 0.7373\n",
            "Epoch 218 Batch 950 Loss 1.1109 Accuracy 0.7368\n",
            "discarded batch 971\n",
            "Epoch 218 Batch 1000 Loss 1.1125 Accuracy 0.7368\n",
            "Epoch 218 Batch 1050 Loss 1.1132 Accuracy 0.7369\n",
            "Epoch 218 Batch 1100 Loss 1.1146 Accuracy 0.7366\n",
            "Epoch 218 Loss 1.1151 Accuracy 0.7367\n",
            "Time taken for 1 epoch: 22.245700120925903 secs\n",
            "\n",
            "Epoch 219 Batch 0 Loss 0.9462 Accuracy 0.7690\n",
            "Epoch 219 Batch 50 Loss 1.0512 Accuracy 0.7478\n",
            "Epoch 219 Batch 100 Loss 1.0654 Accuracy 0.7458\n",
            "Epoch 219 Batch 150 Loss 1.0705 Accuracy 0.7440\n",
            "Epoch 219 Batch 200 Loss 1.0724 Accuracy 0.7438\n",
            "Epoch 219 Batch 250 Loss 1.0737 Accuracy 0.7434\n",
            "Epoch 219 Batch 300 Loss 1.0761 Accuracy 0.7433\n",
            "Epoch 219 Batch 350 Loss 1.0791 Accuracy 0.7425\n",
            "Epoch 219 Batch 400 Loss 1.0845 Accuracy 0.7411\n",
            "Epoch 219 Batch 450 Loss 1.0895 Accuracy 0.7408\n",
            "Epoch 219 Batch 500 Loss 1.0920 Accuracy 0.7407\n",
            "Epoch 219 Batch 550 Loss 1.0955 Accuracy 0.7401\n",
            "Epoch 219 Batch 600 Loss 1.1005 Accuracy 0.7395\n",
            "Epoch 219 Batch 650 Loss 1.1026 Accuracy 0.7388\n",
            "Epoch 219 Batch 700 Loss 1.1038 Accuracy 0.7387\n",
            "Epoch 219 Batch 750 Loss 1.1039 Accuracy 0.7386\n",
            "Epoch 219 Batch 800 Loss 1.1054 Accuracy 0.7381\n",
            "Epoch 219 Batch 850 Loss 1.1067 Accuracy 0.7379\n",
            "discarded batch 852\n",
            "Epoch 219 Batch 900 Loss 1.1075 Accuracy 0.7377\n",
            "Epoch 219 Batch 950 Loss 1.1094 Accuracy 0.7375\n",
            "Epoch 219 Batch 1000 Loss 1.1115 Accuracy 0.7371\n",
            "Epoch 219 Batch 1050 Loss 1.1134 Accuracy 0.7368\n",
            "Epoch 219 Batch 1100 Loss 1.1140 Accuracy 0.7368\n",
            "Epoch 219 Loss 1.1147 Accuracy 0.7367\n",
            "Time taken for 1 epoch: 22.207033395767212 secs\n",
            "\n",
            "Epoch 220 Batch 0 Loss 1.0942 Accuracy 0.7195\n",
            "Epoch 220 Batch 50 Loss 1.0234 Accuracy 0.7518\n",
            "Epoch 220 Batch 100 Loss 1.0593 Accuracy 0.7466\n",
            "Epoch 220 Batch 150 Loss 1.0735 Accuracy 0.7443\n",
            "Epoch 220 Batch 200 Loss 1.0726 Accuracy 0.7442\n",
            "Epoch 220 Batch 250 Loss 1.0733 Accuracy 0.7443\n",
            "Epoch 220 Batch 300 Loss 1.0773 Accuracy 0.7435\n",
            "Epoch 220 Batch 350 Loss 1.0778 Accuracy 0.7433\n",
            "Epoch 220 Batch 400 Loss 1.0772 Accuracy 0.7432\n",
            "Epoch 220 Batch 450 Loss 1.0793 Accuracy 0.7428\n",
            "Epoch 220 Batch 500 Loss 1.0811 Accuracy 0.7426\n",
            "Epoch 220 Batch 550 Loss 1.0881 Accuracy 0.7412\n",
            "Epoch 220 Batch 600 Loss 1.0900 Accuracy 0.7406\n",
            "Epoch 220 Batch 650 Loss 1.0922 Accuracy 0.7402\n",
            "Epoch 220 Batch 700 Loss 1.0940 Accuracy 0.7397\n",
            "Epoch 220 Batch 750 Loss 1.0956 Accuracy 0.7395\n",
            "Epoch 220 Batch 800 Loss 1.0970 Accuracy 0.7392\n",
            "Epoch 220 Batch 850 Loss 1.0999 Accuracy 0.7388\n",
            "Epoch 220 Batch 900 Loss 1.1035 Accuracy 0.7382\n",
            "Epoch 220 Batch 950 Loss 1.1047 Accuracy 0.7381\n",
            "Epoch 220 Batch 1000 Loss 1.1066 Accuracy 0.7378\n",
            "Epoch 220 Batch 1050 Loss 1.1090 Accuracy 0.7374\n",
            "discarded batch 1078\n",
            "Epoch 220 Batch 1100 Loss 1.1105 Accuracy 0.7373\n",
            "Saving checkpoint for epoch 220 at ./checkpoints/train/ckpt-44\n",
            "Epoch 220 Loss 1.1117 Accuracy 0.7372\n",
            "Time taken for 1 epoch: 22.4510760307312 secs\n",
            "\n",
            "Epoch 221 Batch 0 Loss 0.9802 Accuracy 0.7591\n",
            "Epoch 221 Batch 50 Loss 1.0617 Accuracy 0.7445\n",
            "Epoch 221 Batch 100 Loss 1.0640 Accuracy 0.7454\n",
            "Epoch 221 Batch 150 Loss 1.0690 Accuracy 0.7450\n",
            "Epoch 221 Batch 200 Loss 1.0687 Accuracy 0.7452\n",
            "Epoch 221 Batch 250 Loss 1.0747 Accuracy 0.7438\n",
            "Epoch 221 Batch 300 Loss 1.0782 Accuracy 0.7433\n",
            "Epoch 221 Batch 350 Loss 1.0818 Accuracy 0.7425\n",
            "Epoch 221 Batch 400 Loss 1.0841 Accuracy 0.7420\n",
            "Epoch 221 Batch 450 Loss 1.0867 Accuracy 0.7418\n",
            "Epoch 221 Batch 500 Loss 1.0856 Accuracy 0.7422\n",
            "Epoch 221 Batch 550 Loss 1.0868 Accuracy 0.7418\n",
            "Epoch 221 Batch 600 Loss 1.0886 Accuracy 0.7413\n",
            "Epoch 221 Batch 650 Loss 1.0917 Accuracy 0.7407\n",
            "Epoch 221 Batch 700 Loss 1.0955 Accuracy 0.7400\n",
            "Epoch 221 Batch 750 Loss 1.0982 Accuracy 0.7395\n",
            "Epoch 221 Batch 800 Loss 1.0996 Accuracy 0.7393\n",
            "Epoch 221 Batch 850 Loss 1.1009 Accuracy 0.7389\n",
            "Epoch 221 Batch 900 Loss 1.1027 Accuracy 0.7386\n",
            "Epoch 221 Batch 950 Loss 1.1055 Accuracy 0.7381\n",
            "discarded batch 995\n",
            "Epoch 221 Batch 1000 Loss 1.1082 Accuracy 0.7375\n",
            "Epoch 221 Batch 1050 Loss 1.1096 Accuracy 0.7373\n",
            "Epoch 221 Batch 1100 Loss 1.1110 Accuracy 0.7369\n",
            "Epoch 221 Loss 1.1123 Accuracy 0.7368\n",
            "Time taken for 1 epoch: 22.235434770584106 secs\n",
            "\n",
            "discarded batch 23\n",
            "Epoch 221 VALIDATION: Loss 2.7086 Accuracy 0.5674\n",
            "\n",
            "Epoch 222 Batch 0 Loss 1.0102 Accuracy 0.7723\n",
            "Epoch 222 Batch 50 Loss 1.0626 Accuracy 0.7472\n",
            "Epoch 222 Batch 100 Loss 1.0572 Accuracy 0.7469\n",
            "Epoch 222 Batch 150 Loss 1.0591 Accuracy 0.7450\n",
            "Epoch 222 Batch 200 Loss 1.0605 Accuracy 0.7453\n",
            "Epoch 222 Batch 250 Loss 1.0650 Accuracy 0.7449\n",
            "Epoch 222 Batch 300 Loss 1.0714 Accuracy 0.7433\n",
            "Epoch 222 Batch 350 Loss 1.0751 Accuracy 0.7424\n",
            "Epoch 222 Batch 400 Loss 1.0793 Accuracy 0.7416\n",
            "Epoch 222 Batch 450 Loss 1.0816 Accuracy 0.7412\n",
            "Epoch 222 Batch 500 Loss 1.0849 Accuracy 0.7407\n",
            "discarded batch 542\n",
            "Epoch 222 Batch 550 Loss 1.0865 Accuracy 0.7405\n",
            "Epoch 222 Batch 600 Loss 1.0906 Accuracy 0.7400\n",
            "Epoch 222 Batch 650 Loss 1.0944 Accuracy 0.7395\n",
            "Epoch 222 Batch 700 Loss 1.0972 Accuracy 0.7392\n",
            "Epoch 222 Batch 750 Loss 1.0985 Accuracy 0.7392\n",
            "Epoch 222 Batch 800 Loss 1.1006 Accuracy 0.7390\n",
            "Epoch 222 Batch 850 Loss 1.1023 Accuracy 0.7384\n",
            "Epoch 222 Batch 900 Loss 1.1051 Accuracy 0.7379\n",
            "Epoch 222 Batch 950 Loss 1.1060 Accuracy 0.7377\n",
            "Epoch 222 Batch 1000 Loss 1.1071 Accuracy 0.7375\n",
            "Epoch 222 Batch 1050 Loss 1.1082 Accuracy 0.7374\n",
            "Epoch 222 Batch 1100 Loss 1.1085 Accuracy 0.7373\n",
            "Epoch 222 Loss 1.1097 Accuracy 0.7371\n",
            "Time taken for 1 epoch: 22.188074588775635 secs\n",
            "\n",
            "Epoch 223 Batch 0 Loss 1.0741 Accuracy 0.7492\n",
            "Epoch 223 Batch 50 Loss 1.0440 Accuracy 0.7496\n",
            "Epoch 223 Batch 100 Loss 1.0498 Accuracy 0.7463\n",
            "Epoch 223 Batch 150 Loss 1.0501 Accuracy 0.7447\n",
            "Epoch 223 Batch 200 Loss 1.0570 Accuracy 0.7446\n",
            "Epoch 223 Batch 250 Loss 1.0623 Accuracy 0.7437\n",
            "discarded batch 261\n",
            "Epoch 223 Batch 300 Loss 1.0679 Accuracy 0.7432\n",
            "Epoch 223 Batch 350 Loss 1.0708 Accuracy 0.7428\n",
            "Epoch 223 Batch 400 Loss 1.0745 Accuracy 0.7424\n",
            "Epoch 223 Batch 450 Loss 1.0759 Accuracy 0.7423\n",
            "Epoch 223 Batch 500 Loss 1.0783 Accuracy 0.7423\n",
            "Epoch 223 Batch 550 Loss 1.0805 Accuracy 0.7420\n",
            "Epoch 223 Batch 600 Loss 1.0836 Accuracy 0.7412\n",
            "Epoch 223 Batch 650 Loss 1.0867 Accuracy 0.7407\n",
            "Epoch 223 Batch 700 Loss 1.0876 Accuracy 0.7405\n",
            "Epoch 223 Batch 750 Loss 1.0899 Accuracy 0.7402\n",
            "Epoch 223 Batch 800 Loss 1.0929 Accuracy 0.7397\n",
            "Epoch 223 Batch 850 Loss 1.0954 Accuracy 0.7394\n",
            "Epoch 223 Batch 900 Loss 1.0969 Accuracy 0.7392\n",
            "Epoch 223 Batch 950 Loss 1.0992 Accuracy 0.7389\n",
            "Epoch 223 Batch 1000 Loss 1.1011 Accuracy 0.7385\n",
            "Epoch 223 Batch 1050 Loss 1.1039 Accuracy 0.7381\n",
            "Epoch 223 Batch 1100 Loss 1.1042 Accuracy 0.7382\n",
            "Epoch 223 Loss 1.1059 Accuracy 0.7381\n",
            "Time taken for 1 epoch: 22.21179986000061 secs\n",
            "\n",
            "Epoch 224 Batch 0 Loss 1.1551 Accuracy 0.7327\n",
            "Epoch 224 Batch 50 Loss 1.0633 Accuracy 0.7449\n",
            "Epoch 224 Batch 100 Loss 1.0602 Accuracy 0.7473\n",
            "Epoch 224 Batch 150 Loss 1.0623 Accuracy 0.7478\n",
            "Epoch 224 Batch 200 Loss 1.0591 Accuracy 0.7478\n",
            "Epoch 224 Batch 250 Loss 1.0638 Accuracy 0.7465\n",
            "Epoch 224 Batch 300 Loss 1.0699 Accuracy 0.7448\n",
            "Epoch 224 Batch 350 Loss 1.0743 Accuracy 0.7440\n",
            "Epoch 224 Batch 400 Loss 1.0787 Accuracy 0.7433\n",
            "Epoch 224 Batch 450 Loss 1.0774 Accuracy 0.7438\n",
            "Epoch 224 Batch 500 Loss 1.0808 Accuracy 0.7427\n",
            "Epoch 224 Batch 550 Loss 1.0843 Accuracy 0.7420\n",
            "Epoch 224 Batch 600 Loss 1.0860 Accuracy 0.7419\n",
            "Epoch 224 Batch 650 Loss 1.0879 Accuracy 0.7415\n",
            "Epoch 224 Batch 700 Loss 1.0892 Accuracy 0.7411\n",
            "Epoch 224 Batch 750 Loss 1.0916 Accuracy 0.7405\n",
            "Epoch 224 Batch 800 Loss 1.0929 Accuracy 0.7402\n",
            "Epoch 224 Batch 850 Loss 1.0944 Accuracy 0.7400\n",
            "Epoch 224 Batch 900 Loss 1.0969 Accuracy 0.7397\n",
            "discarded batch 929\n",
            "Epoch 224 Batch 950 Loss 1.0990 Accuracy 0.7394\n",
            "Epoch 224 Batch 1000 Loss 1.1008 Accuracy 0.7392\n",
            "Epoch 224 Batch 1050 Loss 1.1034 Accuracy 0.7388\n",
            "Epoch 224 Batch 1100 Loss 1.1059 Accuracy 0.7383\n",
            "Epoch 224 Loss 1.1064 Accuracy 0.7383\n",
            "Time taken for 1 epoch: 22.3589346408844 secs\n",
            "\n",
            "Epoch 225 Batch 0 Loss 1.0008 Accuracy 0.7657\n",
            "Epoch 225 Batch 50 Loss 1.0667 Accuracy 0.7442\n",
            "Epoch 225 Batch 100 Loss 1.0614 Accuracy 0.7451\n",
            "Epoch 225 Batch 150 Loss 1.0647 Accuracy 0.7446\n",
            "Epoch 225 Batch 200 Loss 1.0639 Accuracy 0.7451\n",
            "Epoch 225 Batch 250 Loss 1.0686 Accuracy 0.7442\n",
            "Epoch 225 Batch 300 Loss 1.0699 Accuracy 0.7439\n",
            "Epoch 225 Batch 350 Loss 1.0722 Accuracy 0.7440\n",
            "Epoch 225 Batch 400 Loss 1.0727 Accuracy 0.7441\n",
            "Epoch 225 Batch 450 Loss 1.0764 Accuracy 0.7436\n",
            "discarded batch 478\n",
            "Epoch 225 Batch 500 Loss 1.0778 Accuracy 0.7433\n",
            "Epoch 225 Batch 550 Loss 1.0837 Accuracy 0.7424\n",
            "Epoch 225 Batch 600 Loss 1.0856 Accuracy 0.7420\n",
            "Epoch 225 Batch 650 Loss 1.0883 Accuracy 0.7417\n",
            "Epoch 225 Batch 700 Loss 1.0892 Accuracy 0.7415\n",
            "Epoch 225 Batch 750 Loss 1.0909 Accuracy 0.7412\n",
            "Epoch 225 Batch 800 Loss 1.0932 Accuracy 0.7410\n",
            "Epoch 225 Batch 850 Loss 1.0960 Accuracy 0.7403\n",
            "Epoch 225 Batch 900 Loss 1.0983 Accuracy 0.7397\n",
            "Epoch 225 Batch 950 Loss 1.0999 Accuracy 0.7394\n",
            "Epoch 225 Batch 1000 Loss 1.1021 Accuracy 0.7389\n",
            "Epoch 225 Batch 1050 Loss 1.1029 Accuracy 0.7388\n",
            "Epoch 225 Batch 1100 Loss 1.1038 Accuracy 0.7385\n",
            "Saving checkpoint for epoch 225 at ./checkpoints/train/ckpt-45\n",
            "Epoch 225 Loss 1.1050 Accuracy 0.7384\n",
            "Time taken for 1 epoch: 22.795310497283936 secs\n",
            "\n",
            "Epoch 226 Batch 0 Loss 1.0068 Accuracy 0.7723\n",
            "Epoch 226 Batch 50 Loss 1.0699 Accuracy 0.7423\n",
            "Epoch 226 Batch 100 Loss 1.0612 Accuracy 0.7462\n",
            "Epoch 226 Batch 150 Loss 1.0708 Accuracy 0.7443\n",
            "Epoch 226 Batch 200 Loss 1.0689 Accuracy 0.7457\n",
            "Epoch 226 Batch 250 Loss 1.0694 Accuracy 0.7454\n",
            "Epoch 226 Batch 300 Loss 1.0672 Accuracy 0.7452\n",
            "Epoch 226 Batch 350 Loss 1.0733 Accuracy 0.7439\n",
            "Epoch 226 Batch 400 Loss 1.0762 Accuracy 0.7435\n",
            "Epoch 226 Batch 450 Loss 1.0784 Accuracy 0.7431\n",
            "Epoch 226 Batch 500 Loss 1.0800 Accuracy 0.7432\n",
            "Epoch 226 Batch 550 Loss 1.0823 Accuracy 0.7428\n",
            "Epoch 226 Batch 600 Loss 1.0840 Accuracy 0.7427\n",
            "Epoch 226 Batch 650 Loss 1.0865 Accuracy 0.7420\n",
            "Epoch 226 Batch 700 Loss 1.0875 Accuracy 0.7417\n",
            "Epoch 226 Batch 750 Loss 1.0909 Accuracy 0.7413\n",
            "Epoch 226 Batch 800 Loss 1.0929 Accuracy 0.7408\n",
            "Epoch 226 Batch 850 Loss 1.0931 Accuracy 0.7405\n",
            "Epoch 226 Batch 900 Loss 1.0957 Accuracy 0.7403\n",
            "Epoch 226 Batch 950 Loss 1.0980 Accuracy 0.7397\n",
            "Epoch 226 Batch 1000 Loss 1.0995 Accuracy 0.7394\n",
            "Epoch 226 Batch 1050 Loss 1.1014 Accuracy 0.7392\n",
            "discarded batch 1082\n",
            "Epoch 226 Batch 1100 Loss 1.1033 Accuracy 0.7388\n",
            "Epoch 226 Loss 1.1038 Accuracy 0.7387\n",
            "Time taken for 1 epoch: 22.233848094940186 secs\n",
            "\n",
            "discarded batch 23\n",
            "Epoch 226 VALIDATION: Loss 2.7311 Accuracy 0.5678\n",
            "\n",
            "Epoch 227 Batch 0 Loss 1.0965 Accuracy 0.7294\n",
            "Epoch 227 Batch 50 Loss 1.0725 Accuracy 0.7432\n",
            "Epoch 227 Batch 100 Loss 1.0625 Accuracy 0.7445\n",
            "Epoch 227 Batch 150 Loss 1.0677 Accuracy 0.7432\n",
            "Epoch 227 Batch 200 Loss 1.0704 Accuracy 0.7440\n",
            "Epoch 227 Batch 250 Loss 1.0704 Accuracy 0.7440\n",
            "Epoch 227 Batch 300 Loss 1.0719 Accuracy 0.7440\n",
            "Epoch 227 Batch 350 Loss 1.0755 Accuracy 0.7433\n",
            "Epoch 227 Batch 400 Loss 1.0742 Accuracy 0.7433\n",
            "discarded batch 402\n",
            "Epoch 227 Batch 450 Loss 1.0767 Accuracy 0.7429\n",
            "Epoch 227 Batch 500 Loss 1.0779 Accuracy 0.7427\n",
            "Epoch 227 Batch 550 Loss 1.0793 Accuracy 0.7421\n",
            "Epoch 227 Batch 600 Loss 1.0820 Accuracy 0.7416\n",
            "Epoch 227 Batch 650 Loss 1.0852 Accuracy 0.7409\n",
            "Epoch 227 Batch 700 Loss 1.0861 Accuracy 0.7410\n",
            "Epoch 227 Batch 750 Loss 1.0868 Accuracy 0.7409\n",
            "Epoch 227 Batch 800 Loss 1.0894 Accuracy 0.7406\n",
            "Epoch 227 Batch 850 Loss 1.0915 Accuracy 0.7402\n",
            "Epoch 227 Batch 900 Loss 1.0943 Accuracy 0.7398\n",
            "Epoch 227 Batch 950 Loss 1.0958 Accuracy 0.7395\n",
            "Epoch 227 Batch 1000 Loss 1.0985 Accuracy 0.7390\n",
            "Epoch 227 Batch 1050 Loss 1.1005 Accuracy 0.7387\n",
            "Epoch 227 Batch 1100 Loss 1.1035 Accuracy 0.7383\n",
            "Epoch 227 Loss 1.1041 Accuracy 0.7383\n",
            "Time taken for 1 epoch: 22.257226705551147 secs\n",
            "\n",
            "Epoch 228 Batch 0 Loss 0.9001 Accuracy 0.7558\n",
            "Epoch 228 Batch 50 Loss 1.0556 Accuracy 0.7456\n",
            "Epoch 228 Batch 100 Loss 1.0529 Accuracy 0.7487\n",
            "Epoch 228 Batch 150 Loss 1.0571 Accuracy 0.7474\n",
            "Epoch 228 Batch 200 Loss 1.0641 Accuracy 0.7461\n",
            "Epoch 228 Batch 250 Loss 1.0666 Accuracy 0.7449\n",
            "Epoch 228 Batch 300 Loss 1.0693 Accuracy 0.7443\n",
            "Epoch 228 Batch 350 Loss 1.0716 Accuracy 0.7444\n",
            "Epoch 228 Batch 400 Loss 1.0741 Accuracy 0.7436\n",
            "discarded batch 412\n",
            "Epoch 228 Batch 450 Loss 1.0763 Accuracy 0.7433\n",
            "Epoch 228 Batch 500 Loss 1.0775 Accuracy 0.7433\n",
            "Epoch 228 Batch 550 Loss 1.0796 Accuracy 0.7430\n",
            "Epoch 228 Batch 600 Loss 1.0810 Accuracy 0.7424\n",
            "Epoch 228 Batch 650 Loss 1.0841 Accuracy 0.7418\n",
            "Epoch 228 Batch 700 Loss 1.0860 Accuracy 0.7415\n",
            "Epoch 228 Batch 750 Loss 1.0874 Accuracy 0.7411\n",
            "Epoch 228 Batch 800 Loss 1.0880 Accuracy 0.7412\n",
            "Epoch 228 Batch 850 Loss 1.0911 Accuracy 0.7406\n",
            "Epoch 228 Batch 900 Loss 1.0932 Accuracy 0.7402\n",
            "Epoch 228 Batch 950 Loss 1.0952 Accuracy 0.7400\n",
            "Epoch 228 Batch 1000 Loss 1.0963 Accuracy 0.7399\n",
            "Epoch 228 Batch 1050 Loss 1.0978 Accuracy 0.7397\n",
            "Epoch 228 Batch 1100 Loss 1.1001 Accuracy 0.7392\n",
            "Epoch 228 Loss 1.1012 Accuracy 0.7390\n",
            "Time taken for 1 epoch: 22.286998748779297 secs\n",
            "\n",
            "Epoch 229 Batch 0 Loss 1.0987 Accuracy 0.7426\n",
            "Epoch 229 Batch 50 Loss 1.0639 Accuracy 0.7500\n",
            "Epoch 229 Batch 100 Loss 1.0559 Accuracy 0.7497\n",
            "Epoch 229 Batch 150 Loss 1.0614 Accuracy 0.7477\n",
            "Epoch 229 Batch 200 Loss 1.0659 Accuracy 0.7459\n",
            "Epoch 229 Batch 250 Loss 1.0664 Accuracy 0.7448\n",
            "Epoch 229 Batch 300 Loss 1.0705 Accuracy 0.7441\n",
            "Epoch 229 Batch 350 Loss 1.0733 Accuracy 0.7434\n",
            "Epoch 229 Batch 400 Loss 1.0772 Accuracy 0.7427\n",
            "Epoch 229 Batch 450 Loss 1.0783 Accuracy 0.7426\n",
            "Epoch 229 Batch 500 Loss 1.0799 Accuracy 0.7424\n",
            "Epoch 229 Batch 550 Loss 1.0822 Accuracy 0.7422\n",
            "discarded batch 559\n",
            "Epoch 229 Batch 600 Loss 1.0832 Accuracy 0.7421\n",
            "Epoch 229 Batch 650 Loss 1.0866 Accuracy 0.7415\n",
            "Epoch 229 Batch 700 Loss 1.0892 Accuracy 0.7411\n",
            "Epoch 229 Batch 750 Loss 1.0914 Accuracy 0.7406\n",
            "Epoch 229 Batch 800 Loss 1.0925 Accuracy 0.7403\n",
            "Epoch 229 Batch 850 Loss 1.0947 Accuracy 0.7399\n",
            "Epoch 229 Batch 900 Loss 1.0948 Accuracy 0.7398\n",
            "Epoch 229 Batch 950 Loss 1.0971 Accuracy 0.7394\n",
            "Epoch 229 Batch 1000 Loss 1.0992 Accuracy 0.7391\n",
            "Epoch 229 Batch 1050 Loss 1.0999 Accuracy 0.7391\n",
            "Epoch 229 Batch 1100 Loss 1.1010 Accuracy 0.7389\n",
            "Epoch 229 Loss 1.1014 Accuracy 0.7388\n",
            "Time taken for 1 epoch: 22.209104776382446 secs\n",
            "\n",
            "Epoch 230 Batch 0 Loss 1.0384 Accuracy 0.7525\n",
            "Epoch 230 Batch 50 Loss 1.0320 Accuracy 0.7552\n",
            "Epoch 230 Batch 100 Loss 1.0330 Accuracy 0.7533\n",
            "Epoch 230 Batch 150 Loss 1.0449 Accuracy 0.7497\n",
            "Epoch 230 Batch 200 Loss 1.0505 Accuracy 0.7477\n",
            "Epoch 230 Batch 250 Loss 1.0594 Accuracy 0.7469\n",
            "Epoch 230 Batch 300 Loss 1.0631 Accuracy 0.7459\n",
            "Epoch 230 Batch 350 Loss 1.0671 Accuracy 0.7453\n",
            "Epoch 230 Batch 400 Loss 1.0710 Accuracy 0.7444\n",
            "Epoch 230 Batch 450 Loss 1.0752 Accuracy 0.7436\n",
            "Epoch 230 Batch 500 Loss 1.0754 Accuracy 0.7437\n",
            "Epoch 230 Batch 550 Loss 1.0794 Accuracy 0.7430\n",
            "Epoch 230 Batch 600 Loss 1.0818 Accuracy 0.7423\n",
            "discarded batch 638\n",
            "Epoch 230 Batch 650 Loss 1.0832 Accuracy 0.7422\n",
            "Epoch 230 Batch 700 Loss 1.0849 Accuracy 0.7420\n",
            "Epoch 230 Batch 750 Loss 1.0867 Accuracy 0.7415\n",
            "Epoch 230 Batch 800 Loss 1.0892 Accuracy 0.7410\n",
            "Epoch 230 Batch 850 Loss 1.0899 Accuracy 0.7410\n",
            "Epoch 230 Batch 900 Loss 1.0917 Accuracy 0.7406\n",
            "Epoch 230 Batch 950 Loss 1.0942 Accuracy 0.7404\n",
            "Epoch 230 Batch 1000 Loss 1.0954 Accuracy 0.7401\n",
            "Epoch 230 Batch 1050 Loss 1.0975 Accuracy 0.7398\n",
            "Epoch 230 Batch 1100 Loss 1.0995 Accuracy 0.7395\n",
            "Saving checkpoint for epoch 230 at ./checkpoints/train/ckpt-46\n",
            "Epoch 230 Loss 1.1003 Accuracy 0.7394\n",
            "Time taken for 1 epoch: 22.416932344436646 secs\n",
            "\n",
            "Epoch 231 Batch 0 Loss 1.0666 Accuracy 0.7228\n",
            "Epoch 231 Batch 50 Loss 1.0623 Accuracy 0.7455\n",
            "Epoch 231 Batch 100 Loss 1.0631 Accuracy 0.7459\n",
            "Epoch 231 Batch 150 Loss 1.0584 Accuracy 0.7476\n",
            "Epoch 231 Batch 200 Loss 1.0587 Accuracy 0.7475\n",
            "discarded batch 207\n",
            "Epoch 231 Batch 250 Loss 1.0643 Accuracy 0.7468\n",
            "Epoch 231 Batch 300 Loss 1.0676 Accuracy 0.7460\n",
            "Epoch 231 Batch 350 Loss 1.0686 Accuracy 0.7454\n",
            "Epoch 231 Batch 400 Loss 1.0695 Accuracy 0.7451\n",
            "Epoch 231 Batch 450 Loss 1.0723 Accuracy 0.7446\n",
            "Epoch 231 Batch 500 Loss 1.0768 Accuracy 0.7435\n",
            "Epoch 231 Batch 550 Loss 1.0791 Accuracy 0.7433\n",
            "Epoch 231 Batch 600 Loss 1.0810 Accuracy 0.7428\n",
            "Epoch 231 Batch 650 Loss 1.0840 Accuracy 0.7421\n",
            "Epoch 231 Batch 700 Loss 1.0855 Accuracy 0.7419\n",
            "Epoch 231 Batch 750 Loss 1.0876 Accuracy 0.7414\n",
            "Epoch 231 Batch 800 Loss 1.0884 Accuracy 0.7413\n",
            "Epoch 231 Batch 850 Loss 1.0910 Accuracy 0.7408\n",
            "Epoch 231 Batch 900 Loss 1.0923 Accuracy 0.7406\n",
            "Epoch 231 Batch 950 Loss 1.0938 Accuracy 0.7403\n",
            "Epoch 231 Batch 1000 Loss 1.0954 Accuracy 0.7399\n",
            "Epoch 231 Batch 1050 Loss 1.0969 Accuracy 0.7397\n",
            "Epoch 231 Batch 1100 Loss 1.0985 Accuracy 0.7396\n",
            "Epoch 231 Loss 1.0992 Accuracy 0.7394\n",
            "Time taken for 1 epoch: 22.238702535629272 secs\n",
            "\n",
            "discarded batch 23\n",
            "Epoch 231 VALIDATION: Loss 2.7124 Accuracy 0.5692\n",
            "\n",
            "Epoch 232 Batch 0 Loss 1.1658 Accuracy 0.7294\n",
            "Epoch 232 Batch 50 Loss 1.0416 Accuracy 0.7490\n",
            "Epoch 232 Batch 100 Loss 1.0519 Accuracy 0.7474\n",
            "Epoch 232 Batch 150 Loss 1.0606 Accuracy 0.7450\n",
            "Epoch 232 Batch 200 Loss 1.0627 Accuracy 0.7444\n",
            "Epoch 232 Batch 250 Loss 1.0636 Accuracy 0.7438\n",
            "Epoch 232 Batch 300 Loss 1.0665 Accuracy 0.7435\n",
            "Epoch 232 Batch 350 Loss 1.0681 Accuracy 0.7432\n",
            "Epoch 232 Batch 400 Loss 1.0732 Accuracy 0.7425\n",
            "Epoch 232 Batch 450 Loss 1.0752 Accuracy 0.7423\n",
            "Epoch 232 Batch 500 Loss 1.0762 Accuracy 0.7423\n",
            "Epoch 232 Batch 550 Loss 1.0784 Accuracy 0.7421\n",
            "Epoch 232 Batch 600 Loss 1.0800 Accuracy 0.7422\n",
            "Epoch 232 Batch 650 Loss 1.0816 Accuracy 0.7418\n",
            "Epoch 232 Batch 700 Loss 1.0833 Accuracy 0.7416\n",
            "Epoch 232 Batch 750 Loss 1.0845 Accuracy 0.7414\n",
            "Epoch 232 Batch 800 Loss 1.0872 Accuracy 0.7409\n",
            "discarded batch 817\n",
            "Epoch 232 Batch 850 Loss 1.0903 Accuracy 0.7404\n",
            "Epoch 232 Batch 900 Loss 1.0914 Accuracy 0.7404\n",
            "Epoch 232 Batch 950 Loss 1.0929 Accuracy 0.7402\n",
            "Epoch 232 Batch 1000 Loss 1.0946 Accuracy 0.7399\n",
            "Epoch 232 Batch 1050 Loss 1.0971 Accuracy 0.7394\n",
            "Epoch 232 Batch 1100 Loss 1.0992 Accuracy 0.7391\n",
            "Epoch 232 Loss 1.1002 Accuracy 0.7391\n",
            "Time taken for 1 epoch: 22.240909814834595 secs\n",
            "\n",
            "Epoch 233 Batch 0 Loss 1.1371 Accuracy 0.7492\n",
            "Epoch 233 Batch 50 Loss 1.0466 Accuracy 0.7486\n",
            "Epoch 233 Batch 100 Loss 1.0643 Accuracy 0.7454\n",
            "Epoch 233 Batch 150 Loss 1.0600 Accuracy 0.7468\n",
            "Epoch 233 Batch 200 Loss 1.0565 Accuracy 0.7474\n",
            "Epoch 233 Batch 250 Loss 1.0585 Accuracy 0.7460\n",
            "Epoch 233 Batch 300 Loss 1.0611 Accuracy 0.7452\n",
            "Epoch 233 Batch 350 Loss 1.0634 Accuracy 0.7447\n",
            "Epoch 233 Batch 400 Loss 1.0675 Accuracy 0.7444\n",
            "Epoch 233 Batch 450 Loss 1.0681 Accuracy 0.7443\n",
            "Epoch 233 Batch 500 Loss 1.0714 Accuracy 0.7437\n",
            "Epoch 233 Batch 550 Loss 1.0745 Accuracy 0.7429\n",
            "Epoch 233 Batch 600 Loss 1.0768 Accuracy 0.7425\n",
            "Epoch 233 Batch 650 Loss 1.0794 Accuracy 0.7422\n",
            "Epoch 233 Batch 700 Loss 1.0832 Accuracy 0.7416\n",
            "Epoch 233 Batch 750 Loss 1.0851 Accuracy 0.7412\n",
            "Epoch 233 Batch 800 Loss 1.0862 Accuracy 0.7411\n",
            "Epoch 233 Batch 850 Loss 1.0877 Accuracy 0.7409\n",
            "discarded batch 893\n",
            "Epoch 233 Batch 900 Loss 1.0897 Accuracy 0.7407\n",
            "Epoch 233 Batch 950 Loss 1.0915 Accuracy 0.7404\n",
            "Epoch 233 Batch 1000 Loss 1.0925 Accuracy 0.7402\n",
            "Epoch 233 Batch 1050 Loss 1.0954 Accuracy 0.7397\n",
            "Epoch 233 Batch 1100 Loss 1.0970 Accuracy 0.7395\n",
            "Epoch 233 Loss 1.0982 Accuracy 0.7392\n",
            "Time taken for 1 epoch: 22.20193386077881 secs\n",
            "\n",
            "Epoch 234 Batch 0 Loss 1.0604 Accuracy 0.7327\n",
            "Epoch 234 Batch 50 Loss 1.0533 Accuracy 0.7481\n",
            "discarded batch 75\n",
            "Epoch 234 Batch 100 Loss 1.0461 Accuracy 0.7496\n",
            "Epoch 234 Batch 150 Loss 1.0520 Accuracy 0.7471\n",
            "Epoch 234 Batch 200 Loss 1.0490 Accuracy 0.7479\n",
            "Epoch 234 Batch 250 Loss 1.0528 Accuracy 0.7479\n",
            "Epoch 234 Batch 300 Loss 1.0588 Accuracy 0.7464\n",
            "Epoch 234 Batch 350 Loss 1.0605 Accuracy 0.7457\n",
            "Epoch 234 Batch 400 Loss 1.0630 Accuracy 0.7452\n",
            "Epoch 234 Batch 450 Loss 1.0669 Accuracy 0.7443\n",
            "Epoch 234 Batch 500 Loss 1.0701 Accuracy 0.7436\n",
            "Epoch 234 Batch 550 Loss 1.0730 Accuracy 0.7437\n",
            "Epoch 234 Batch 600 Loss 1.0757 Accuracy 0.7430\n",
            "Epoch 234 Batch 650 Loss 1.0785 Accuracy 0.7428\n",
            "Epoch 234 Batch 700 Loss 1.0801 Accuracy 0.7427\n",
            "Epoch 234 Batch 750 Loss 1.0824 Accuracy 0.7422\n",
            "Epoch 234 Batch 800 Loss 1.0849 Accuracy 0.7417\n",
            "Epoch 234 Batch 850 Loss 1.0875 Accuracy 0.7412\n",
            "Epoch 234 Batch 900 Loss 1.0901 Accuracy 0.7406\n",
            "Epoch 234 Batch 950 Loss 1.0934 Accuracy 0.7401\n",
            "Epoch 234 Batch 1000 Loss 1.0947 Accuracy 0.7399\n",
            "Epoch 234 Batch 1050 Loss 1.0960 Accuracy 0.7398\n",
            "Epoch 234 Batch 1100 Loss 1.0965 Accuracy 0.7395\n",
            "Epoch 234 Loss 1.0974 Accuracy 0.7394\n",
            "Time taken for 1 epoch: 22.192549467086792 secs\n",
            "\n",
            "Epoch 235 Batch 0 Loss 1.1320 Accuracy 0.7261\n",
            "Epoch 235 Batch 50 Loss 1.0554 Accuracy 0.7475\n",
            "discarded batch 60\n",
            "Epoch 235 Batch 100 Loss 1.0534 Accuracy 0.7467\n",
            "Epoch 235 Batch 150 Loss 1.0572 Accuracy 0.7468\n",
            "Epoch 235 Batch 200 Loss 1.0550 Accuracy 0.7476\n",
            "Epoch 235 Batch 250 Loss 1.0567 Accuracy 0.7467\n",
            "Epoch 235 Batch 300 Loss 1.0608 Accuracy 0.7460\n",
            "Epoch 235 Batch 350 Loss 1.0643 Accuracy 0.7461\n",
            "Epoch 235 Batch 400 Loss 1.0677 Accuracy 0.7457\n",
            "Epoch 235 Batch 450 Loss 1.0722 Accuracy 0.7445\n",
            "Epoch 235 Batch 500 Loss 1.0715 Accuracy 0.7449\n",
            "Epoch 235 Batch 550 Loss 1.0759 Accuracy 0.7436\n",
            "Epoch 235 Batch 600 Loss 1.0778 Accuracy 0.7433\n",
            "Epoch 235 Batch 650 Loss 1.0816 Accuracy 0.7426\n",
            "Epoch 235 Batch 700 Loss 1.0849 Accuracy 0.7422\n",
            "Epoch 235 Batch 750 Loss 1.0873 Accuracy 0.7421\n",
            "Epoch 235 Batch 800 Loss 1.0891 Accuracy 0.7417\n",
            "Epoch 235 Batch 850 Loss 1.0910 Accuracy 0.7414\n",
            "Epoch 235 Batch 900 Loss 1.0921 Accuracy 0.7410\n",
            "Epoch 235 Batch 950 Loss 1.0942 Accuracy 0.7406\n",
            "Epoch 235 Batch 1000 Loss 1.0952 Accuracy 0.7406\n",
            "Epoch 235 Batch 1050 Loss 1.0955 Accuracy 0.7405\n",
            "Epoch 235 Batch 1100 Loss 1.0969 Accuracy 0.7402\n",
            "Saving checkpoint for epoch 235 at ./checkpoints/train/ckpt-47\n",
            "Epoch 235 Loss 1.0976 Accuracy 0.7400\n",
            "Time taken for 1 epoch: 22.508981227874756 secs\n",
            "\n",
            "Epoch 236 Batch 0 Loss 0.9813 Accuracy 0.7591\n",
            "Epoch 236 Batch 50 Loss 1.0600 Accuracy 0.7474\n",
            "Epoch 236 Batch 100 Loss 1.0507 Accuracy 0.7495\n",
            "Epoch 236 Batch 150 Loss 1.0506 Accuracy 0.7484\n",
            "Epoch 236 Batch 200 Loss 1.0488 Accuracy 0.7484\n",
            "Epoch 236 Batch 250 Loss 1.0522 Accuracy 0.7476\n",
            "Epoch 236 Batch 300 Loss 1.0571 Accuracy 0.7470\n",
            "Epoch 236 Batch 350 Loss 1.0579 Accuracy 0.7470\n",
            "Epoch 236 Batch 400 Loss 1.0621 Accuracy 0.7460\n",
            "Epoch 236 Batch 450 Loss 1.0645 Accuracy 0.7457\n",
            "Epoch 236 Batch 500 Loss 1.0688 Accuracy 0.7446\n",
            "Epoch 236 Batch 550 Loss 1.0707 Accuracy 0.7442\n",
            "Epoch 236 Batch 600 Loss 1.0741 Accuracy 0.7437\n",
            "Epoch 236 Batch 650 Loss 1.0762 Accuracy 0.7438\n",
            "Epoch 236 Batch 700 Loss 1.0768 Accuracy 0.7434\n",
            "Epoch 236 Batch 750 Loss 1.0779 Accuracy 0.7433\n",
            "Epoch 236 Batch 800 Loss 1.0788 Accuracy 0.7434\n",
            "Epoch 236 Batch 850 Loss 1.0807 Accuracy 0.7429\n",
            "Epoch 236 Batch 900 Loss 1.0838 Accuracy 0.7421\n",
            "discarded batch 944\n",
            "Epoch 236 Batch 950 Loss 1.0854 Accuracy 0.7416\n",
            "Epoch 236 Batch 1000 Loss 1.0871 Accuracy 0.7413\n",
            "Epoch 236 Batch 1050 Loss 1.0885 Accuracy 0.7410\n",
            "Epoch 236 Batch 1100 Loss 1.0915 Accuracy 0.7403\n",
            "Epoch 236 Loss 1.0925 Accuracy 0.7401\n",
            "Time taken for 1 epoch: 22.16933274269104 secs\n",
            "\n",
            "discarded batch 23\n",
            "Epoch 236 VALIDATION: Loss 2.7192 Accuracy 0.5701\n",
            "\n",
            "Epoch 237 Batch 0 Loss 1.0660 Accuracy 0.7195\n",
            "Epoch 237 Batch 50 Loss 1.0347 Accuracy 0.7487\n",
            "Epoch 237 Batch 100 Loss 1.0450 Accuracy 0.7496\n",
            "discarded batch 123\n",
            "Epoch 237 Batch 150 Loss 1.0484 Accuracy 0.7492\n",
            "Epoch 237 Batch 200 Loss 1.0532 Accuracy 0.7491\n",
            "Epoch 237 Batch 250 Loss 1.0559 Accuracy 0.7489\n",
            "Epoch 237 Batch 300 Loss 1.0567 Accuracy 0.7483\n",
            "Epoch 237 Batch 350 Loss 1.0604 Accuracy 0.7473\n",
            "Epoch 237 Batch 400 Loss 1.0607 Accuracy 0.7469\n",
            "Epoch 237 Batch 450 Loss 1.0654 Accuracy 0.7453\n",
            "Epoch 237 Batch 500 Loss 1.0682 Accuracy 0.7447\n",
            "Epoch 237 Batch 550 Loss 1.0706 Accuracy 0.7445\n",
            "Epoch 237 Batch 600 Loss 1.0723 Accuracy 0.7441\n",
            "Epoch 237 Batch 650 Loss 1.0741 Accuracy 0.7441\n",
            "Epoch 237 Batch 700 Loss 1.0762 Accuracy 0.7435\n",
            "Epoch 237 Batch 750 Loss 1.0788 Accuracy 0.7432\n",
            "Epoch 237 Batch 800 Loss 1.0820 Accuracy 0.7428\n",
            "Epoch 237 Batch 850 Loss 1.0839 Accuracy 0.7422\n",
            "Epoch 237 Batch 900 Loss 1.0865 Accuracy 0.7417\n",
            "Epoch 237 Batch 950 Loss 1.0874 Accuracy 0.7414\n",
            "Epoch 237 Batch 1000 Loss 1.0890 Accuracy 0.7411\n",
            "Epoch 237 Batch 1050 Loss 1.0903 Accuracy 0.7408\n",
            "Epoch 237 Batch 1100 Loss 1.0924 Accuracy 0.7403\n",
            "Epoch 237 Loss 1.0939 Accuracy 0.7401\n",
            "Time taken for 1 epoch: 22.194910526275635 secs\n",
            "\n",
            "Epoch 238 Batch 0 Loss 1.0836 Accuracy 0.7096\n",
            "Epoch 238 Batch 50 Loss 1.0183 Accuracy 0.7537\n",
            "Epoch 238 Batch 100 Loss 1.0317 Accuracy 0.7506\n",
            "Epoch 238 Batch 150 Loss 1.0440 Accuracy 0.7480\n",
            "Epoch 238 Batch 200 Loss 1.0462 Accuracy 0.7492\n",
            "Epoch 238 Batch 250 Loss 1.0478 Accuracy 0.7482\n",
            "Epoch 238 Batch 300 Loss 1.0541 Accuracy 0.7467\n",
            "Epoch 238 Batch 350 Loss 1.0593 Accuracy 0.7457\n",
            "Epoch 238 Batch 400 Loss 1.0633 Accuracy 0.7450\n",
            "Epoch 238 Batch 450 Loss 1.0646 Accuracy 0.7449\n",
            "Epoch 238 Batch 500 Loss 1.0659 Accuracy 0.7446\n",
            "Epoch 238 Batch 550 Loss 1.0669 Accuracy 0.7448\n",
            "Epoch 238 Batch 600 Loss 1.0675 Accuracy 0.7448\n",
            "Epoch 238 Batch 650 Loss 1.0704 Accuracy 0.7445\n",
            "Epoch 238 Batch 700 Loss 1.0719 Accuracy 0.7442\n",
            "Epoch 238 Batch 750 Loss 1.0746 Accuracy 0.7439\n",
            "Epoch 238 Batch 800 Loss 1.0758 Accuracy 0.7436\n",
            "Epoch 238 Batch 850 Loss 1.0782 Accuracy 0.7432\n",
            "Epoch 238 Batch 900 Loss 1.0796 Accuracy 0.7431\n",
            "Epoch 238 Batch 950 Loss 1.0815 Accuracy 0.7428\n",
            "Epoch 238 Batch 1000 Loss 1.0837 Accuracy 0.7425\n",
            "discarded batch 1012\n",
            "Epoch 238 Batch 1050 Loss 1.0866 Accuracy 0.7420\n",
            "Epoch 238 Batch 1100 Loss 1.0881 Accuracy 0.7415\n",
            "Epoch 238 Loss 1.0896 Accuracy 0.7413\n",
            "Time taken for 1 epoch: 22.38284158706665 secs\n",
            "\n",
            "Epoch 239 Batch 0 Loss 0.8861 Accuracy 0.7525\n",
            "Epoch 239 Batch 50 Loss 1.0322 Accuracy 0.7485\n",
            "Epoch 239 Batch 100 Loss 1.0436 Accuracy 0.7472\n",
            "Epoch 239 Batch 150 Loss 1.0461 Accuracy 0.7474\n",
            "Epoch 239 Batch 200 Loss 1.0515 Accuracy 0.7468\n",
            "Epoch 239 Batch 250 Loss 1.0538 Accuracy 0.7460\n",
            "Epoch 239 Batch 300 Loss 1.0533 Accuracy 0.7459\n",
            "Epoch 239 Batch 350 Loss 1.0544 Accuracy 0.7463\n",
            "Epoch 239 Batch 400 Loss 1.0562 Accuracy 0.7465\n",
            "Epoch 239 Batch 450 Loss 1.0606 Accuracy 0.7456\n",
            "Epoch 239 Batch 500 Loss 1.0634 Accuracy 0.7450\n",
            "discarded batch 550\n",
            "Epoch 239 Batch 600 Loss 1.0694 Accuracy 0.7438\n",
            "Epoch 239 Batch 650 Loss 1.0722 Accuracy 0.7434\n",
            "Epoch 239 Batch 700 Loss 1.0741 Accuracy 0.7429\n",
            "Epoch 239 Batch 750 Loss 1.0764 Accuracy 0.7426\n",
            "Epoch 239 Batch 800 Loss 1.0780 Accuracy 0.7422\n",
            "Epoch 239 Batch 850 Loss 1.0807 Accuracy 0.7418\n",
            "Epoch 239 Batch 900 Loss 1.0831 Accuracy 0.7412\n",
            "Epoch 239 Batch 950 Loss 1.0843 Accuracy 0.7411\n",
            "Epoch 239 Batch 1000 Loss 1.0849 Accuracy 0.7412\n",
            "Epoch 239 Batch 1050 Loss 1.0873 Accuracy 0.7409\n",
            "Epoch 239 Batch 1100 Loss 1.0884 Accuracy 0.7409\n",
            "Epoch 239 Loss 1.0900 Accuracy 0.7407\n",
            "Time taken for 1 epoch: 22.544721841812134 secs\n",
            "\n",
            "Epoch 240 Batch 0 Loss 1.0509 Accuracy 0.7624\n",
            "Epoch 240 Batch 50 Loss 1.0393 Accuracy 0.7469\n",
            "Epoch 240 Batch 100 Loss 1.0395 Accuracy 0.7478\n",
            "Epoch 240 Batch 150 Loss 1.0402 Accuracy 0.7472\n",
            "Epoch 240 Batch 200 Loss 1.0401 Accuracy 0.7476\n",
            "Epoch 240 Batch 250 Loss 1.0475 Accuracy 0.7462\n",
            "Epoch 240 Batch 300 Loss 1.0507 Accuracy 0.7459\n",
            "Epoch 240 Batch 350 Loss 1.0515 Accuracy 0.7458\n",
            "Epoch 240 Batch 400 Loss 1.0545 Accuracy 0.7451\n",
            "Epoch 240 Batch 450 Loss 1.0578 Accuracy 0.7446\n",
            "Epoch 240 Batch 500 Loss 1.0619 Accuracy 0.7440\n",
            "Epoch 240 Batch 550 Loss 1.0646 Accuracy 0.7439\n",
            "Epoch 240 Batch 600 Loss 1.0681 Accuracy 0.7437\n",
            "Epoch 240 Batch 650 Loss 1.0713 Accuracy 0.7434\n",
            "Epoch 240 Batch 700 Loss 1.0745 Accuracy 0.7430\n",
            "Epoch 240 Batch 750 Loss 1.0760 Accuracy 0.7428\n",
            "discarded batch 787\n",
            "Epoch 240 Batch 800 Loss 1.0772 Accuracy 0.7425\n",
            "Epoch 240 Batch 850 Loss 1.0788 Accuracy 0.7422\n",
            "Epoch 240 Batch 900 Loss 1.0801 Accuracy 0.7421\n",
            "Epoch 240 Batch 950 Loss 1.0822 Accuracy 0.7418\n",
            "Epoch 240 Batch 1000 Loss 1.0832 Accuracy 0.7418\n",
            "Epoch 240 Batch 1050 Loss 1.0857 Accuracy 0.7415\n",
            "Epoch 240 Batch 1100 Loss 1.0882 Accuracy 0.7410\n",
            "Saving checkpoint for epoch 240 at ./checkpoints/train/ckpt-48\n",
            "Epoch 240 Loss 1.0901 Accuracy 0.7408\n",
            "Time taken for 1 epoch: 22.385891437530518 secs\n",
            "\n",
            "Epoch 241 Batch 0 Loss 1.7077 Accuracy 0.7261\n",
            "Epoch 241 Batch 50 Loss 1.0449 Accuracy 0.7544\n",
            "Epoch 241 Batch 100 Loss 1.0465 Accuracy 0.7506\n",
            "Epoch 241 Batch 150 Loss 1.0480 Accuracy 0.7484\n",
            "Epoch 241 Batch 200 Loss 1.0525 Accuracy 0.7481\n",
            "Epoch 241 Batch 250 Loss 1.0517 Accuracy 0.7478\n",
            "Epoch 241 Batch 300 Loss 1.0548 Accuracy 0.7469\n",
            "Epoch 241 Batch 350 Loss 1.0561 Accuracy 0.7461\n",
            "Epoch 241 Batch 400 Loss 1.0573 Accuracy 0.7462\n",
            "Epoch 241 Batch 450 Loss 1.0611 Accuracy 0.7459\n",
            "Epoch 241 Batch 500 Loss 1.0642 Accuracy 0.7454\n",
            "Epoch 241 Batch 550 Loss 1.0648 Accuracy 0.7456\n",
            "Epoch 241 Batch 600 Loss 1.0668 Accuracy 0.7452\n",
            "Epoch 241 Batch 650 Loss 1.0698 Accuracy 0.7443\n",
            "Epoch 241 Batch 700 Loss 1.0711 Accuracy 0.7440\n",
            "discarded batch 748\n",
            "Epoch 241 Batch 750 Loss 1.0728 Accuracy 0.7439\n",
            "Epoch 241 Batch 800 Loss 1.0762 Accuracy 0.7433\n",
            "Epoch 241 Batch 850 Loss 1.0778 Accuracy 0.7433\n",
            "Epoch 241 Batch 900 Loss 1.0806 Accuracy 0.7428\n",
            "Epoch 241 Batch 950 Loss 1.0824 Accuracy 0.7425\n",
            "Epoch 241 Batch 1000 Loss 1.0851 Accuracy 0.7420\n",
            "Epoch 241 Batch 1050 Loss 1.0873 Accuracy 0.7416\n",
            "Epoch 241 Batch 1100 Loss 1.0887 Accuracy 0.7414\n",
            "Epoch 241 Loss 1.0898 Accuracy 0.7410\n",
            "Time taken for 1 epoch: 22.26258373260498 secs\n",
            "\n",
            "discarded batch 23\n",
            "Epoch 241 VALIDATION: Loss 2.6686 Accuracy 0.5755\n",
            "\n",
            "Epoch 242 Batch 0 Loss 1.1387 Accuracy 0.6931\n",
            "Epoch 242 Batch 50 Loss 1.0300 Accuracy 0.7503\n",
            "Epoch 242 Batch 100 Loss 1.0306 Accuracy 0.7513\n",
            "Epoch 242 Batch 150 Loss 1.0448 Accuracy 0.7492\n",
            "Epoch 242 Batch 200 Loss 1.0528 Accuracy 0.7478\n",
            "Epoch 242 Batch 250 Loss 1.0537 Accuracy 0.7470\n",
            "Epoch 242 Batch 300 Loss 1.0560 Accuracy 0.7467\n",
            "Epoch 242 Batch 350 Loss 1.0596 Accuracy 0.7463\n",
            "Epoch 242 Batch 400 Loss 1.0625 Accuracy 0.7458\n",
            "Epoch 242 Batch 450 Loss 1.0643 Accuracy 0.7453\n",
            "Epoch 242 Batch 500 Loss 1.0669 Accuracy 0.7449\n",
            "Epoch 242 Batch 550 Loss 1.0668 Accuracy 0.7449\n",
            "Epoch 242 Batch 600 Loss 1.0685 Accuracy 0.7445\n",
            "Epoch 242 Batch 650 Loss 1.0703 Accuracy 0.7442\n",
            "Epoch 242 Batch 700 Loss 1.0720 Accuracy 0.7437\n",
            "Epoch 242 Batch 750 Loss 1.0736 Accuracy 0.7436\n",
            "Epoch 242 Batch 800 Loss 1.0767 Accuracy 0.7431\n",
            "Epoch 242 Batch 850 Loss 1.0781 Accuracy 0.7428\n",
            "Epoch 242 Batch 900 Loss 1.0798 Accuracy 0.7423\n",
            "Epoch 242 Batch 950 Loss 1.0825 Accuracy 0.7418\n",
            "discarded batch 971\n",
            "Epoch 242 Batch 1000 Loss 1.0834 Accuracy 0.7416\n",
            "Epoch 242 Batch 1050 Loss 1.0862 Accuracy 0.7411\n",
            "Epoch 242 Batch 1100 Loss 1.0883 Accuracy 0.7407\n",
            "Epoch 242 Loss 1.0895 Accuracy 0.7407\n",
            "Time taken for 1 epoch: 22.337876558303833 secs\n",
            "\n",
            "Epoch 243 Batch 0 Loss 1.0293 Accuracy 0.7360\n",
            "discarded batch 37\n",
            "Epoch 243 Batch 50 Loss 1.0342 Accuracy 0.7518\n",
            "Epoch 243 Batch 100 Loss 1.0455 Accuracy 0.7502\n",
            "Epoch 243 Batch 150 Loss 1.0466 Accuracy 0.7501\n",
            "Epoch 243 Batch 200 Loss 1.0466 Accuracy 0.7493\n",
            "Epoch 243 Batch 250 Loss 1.0489 Accuracy 0.7489\n",
            "Epoch 243 Batch 300 Loss 1.0526 Accuracy 0.7487\n",
            "Epoch 243 Batch 350 Loss 1.0551 Accuracy 0.7476\n",
            "Epoch 243 Batch 400 Loss 1.0562 Accuracy 0.7470\n",
            "Epoch 243 Batch 450 Loss 1.0587 Accuracy 0.7467\n",
            "Epoch 243 Batch 500 Loss 1.0617 Accuracy 0.7459\n",
            "Epoch 243 Batch 550 Loss 1.0641 Accuracy 0.7455\n",
            "Epoch 243 Batch 600 Loss 1.0655 Accuracy 0.7454\n",
            "Epoch 243 Batch 650 Loss 1.0669 Accuracy 0.7452\n",
            "Epoch 243 Batch 700 Loss 1.0699 Accuracy 0.7447\n",
            "Epoch 243 Batch 750 Loss 1.0716 Accuracy 0.7443\n",
            "Epoch 243 Batch 800 Loss 1.0749 Accuracy 0.7440\n",
            "Epoch 243 Batch 850 Loss 1.0778 Accuracy 0.7436\n",
            "Epoch 243 Batch 900 Loss 1.0803 Accuracy 0.7432\n",
            "Epoch 243 Batch 950 Loss 1.0815 Accuracy 0.7430\n",
            "Epoch 243 Batch 1000 Loss 1.0829 Accuracy 0.7428\n",
            "Epoch 243 Batch 1050 Loss 1.0843 Accuracy 0.7425\n",
            "Epoch 243 Batch 1100 Loss 1.0865 Accuracy 0.7422\n",
            "Epoch 243 Loss 1.0868 Accuracy 0.7423\n",
            "Time taken for 1 epoch: 22.23344612121582 secs\n",
            "\n",
            "Epoch 244 Batch 0 Loss 1.0156 Accuracy 0.7492\n",
            "Epoch 244 Batch 50 Loss 1.0422 Accuracy 0.7486\n",
            "Epoch 244 Batch 100 Loss 1.0443 Accuracy 0.7491\n",
            "discarded batch 136\n",
            "Epoch 244 Batch 150 Loss 1.0446 Accuracy 0.7482\n",
            "Epoch 244 Batch 200 Loss 1.0498 Accuracy 0.7475\n",
            "Epoch 244 Batch 250 Loss 1.0461 Accuracy 0.7487\n",
            "Epoch 244 Batch 300 Loss 1.0495 Accuracy 0.7480\n",
            "Epoch 244 Batch 350 Loss 1.0535 Accuracy 0.7472\n",
            "Epoch 244 Batch 400 Loss 1.0567 Accuracy 0.7467\n",
            "Epoch 244 Batch 450 Loss 1.0596 Accuracy 0.7458\n",
            "Epoch 244 Batch 500 Loss 1.0598 Accuracy 0.7457\n",
            "Epoch 244 Batch 550 Loss 1.0638 Accuracy 0.7450\n",
            "Epoch 244 Batch 600 Loss 1.0655 Accuracy 0.7448\n",
            "Epoch 244 Batch 650 Loss 1.0671 Accuracy 0.7444\n",
            "Epoch 244 Batch 700 Loss 1.0688 Accuracy 0.7441\n",
            "Epoch 244 Batch 750 Loss 1.0697 Accuracy 0.7441\n",
            "Epoch 244 Batch 800 Loss 1.0716 Accuracy 0.7438\n",
            "Epoch 244 Batch 850 Loss 1.0738 Accuracy 0.7436\n",
            "Epoch 244 Batch 900 Loss 1.0760 Accuracy 0.7432\n",
            "Epoch 244 Batch 950 Loss 1.0779 Accuracy 0.7428\n",
            "Epoch 244 Batch 1000 Loss 1.0799 Accuracy 0.7426\n",
            "Epoch 244 Batch 1050 Loss 1.0823 Accuracy 0.7423\n",
            "Epoch 244 Batch 1100 Loss 1.0841 Accuracy 0.7418\n",
            "Epoch 244 Loss 1.0855 Accuracy 0.7416\n",
            "Time taken for 1 epoch: 22.25625777244568 secs\n",
            "\n",
            "Epoch 245 Batch 0 Loss 1.0275 Accuracy 0.7690\n",
            "Epoch 245 Batch 50 Loss 1.0643 Accuracy 0.7475\n",
            "Epoch 245 Batch 100 Loss 1.0459 Accuracy 0.7487\n",
            "Epoch 245 Batch 150 Loss 1.0488 Accuracy 0.7475\n",
            "discarded batch 195\n",
            "Epoch 245 Batch 200 Loss 1.0457 Accuracy 0.7487\n",
            "Epoch 245 Batch 250 Loss 1.0434 Accuracy 0.7486\n",
            "Epoch 245 Batch 300 Loss 1.0496 Accuracy 0.7476\n",
            "Epoch 245 Batch 350 Loss 1.0526 Accuracy 0.7471\n",
            "Epoch 245 Batch 400 Loss 1.0528 Accuracy 0.7469\n",
            "Epoch 245 Batch 450 Loss 1.0584 Accuracy 0.7466\n",
            "Epoch 245 Batch 500 Loss 1.0612 Accuracy 0.7461\n",
            "Epoch 245 Batch 550 Loss 1.0644 Accuracy 0.7453\n",
            "Epoch 245 Batch 600 Loss 1.0656 Accuracy 0.7450\n",
            "Epoch 245 Batch 650 Loss 1.0690 Accuracy 0.7444\n",
            "Epoch 245 Batch 700 Loss 1.0706 Accuracy 0.7443\n",
            "Epoch 245 Batch 750 Loss 1.0724 Accuracy 0.7438\n",
            "Epoch 245 Batch 800 Loss 1.0738 Accuracy 0.7435\n",
            "Epoch 245 Batch 850 Loss 1.0748 Accuracy 0.7434\n",
            "Epoch 245 Batch 900 Loss 1.0763 Accuracy 0.7431\n",
            "Epoch 245 Batch 950 Loss 1.0772 Accuracy 0.7432\n",
            "Epoch 245 Batch 1000 Loss 1.0780 Accuracy 0.7431\n",
            "Epoch 245 Batch 1050 Loss 1.0788 Accuracy 0.7430\n",
            "Epoch 245 Batch 1100 Loss 1.0811 Accuracy 0.7425\n",
            "Saving checkpoint for epoch 245 at ./checkpoints/train/ckpt-49\n",
            "Epoch 245 Loss 1.0820 Accuracy 0.7424\n",
            "Time taken for 1 epoch: 22.47273087501526 secs\n",
            "\n",
            "Epoch 246 Batch 0 Loss 1.2190 Accuracy 0.7063\n",
            "Epoch 246 Batch 50 Loss 1.0454 Accuracy 0.7457\n",
            "Epoch 246 Batch 100 Loss 1.0410 Accuracy 0.7491\n",
            "Epoch 246 Batch 150 Loss 1.0437 Accuracy 0.7479\n",
            "Epoch 246 Batch 200 Loss 1.0428 Accuracy 0.7482\n",
            "Epoch 246 Batch 250 Loss 1.0426 Accuracy 0.7478\n",
            "Epoch 246 Batch 300 Loss 1.0494 Accuracy 0.7467\n",
            "Epoch 246 Batch 350 Loss 1.0531 Accuracy 0.7465\n",
            "Epoch 246 Batch 400 Loss 1.0560 Accuracy 0.7460\n",
            "Epoch 246 Batch 450 Loss 1.0611 Accuracy 0.7456\n",
            "Epoch 246 Batch 500 Loss 1.0659 Accuracy 0.7449\n",
            "Epoch 246 Batch 550 Loss 1.0677 Accuracy 0.7445\n",
            "Epoch 246 Batch 600 Loss 1.0688 Accuracy 0.7439\n",
            "Epoch 246 Batch 650 Loss 1.0688 Accuracy 0.7440\n",
            "Epoch 246 Batch 700 Loss 1.0699 Accuracy 0.7440\n",
            "Epoch 246 Batch 750 Loss 1.0720 Accuracy 0.7437\n",
            "Epoch 246 Batch 800 Loss 1.0735 Accuracy 0.7436\n",
            "discarded batch 843\n",
            "Epoch 246 Batch 850 Loss 1.0735 Accuracy 0.7438\n",
            "Epoch 246 Batch 900 Loss 1.0745 Accuracy 0.7436\n",
            "Epoch 246 Batch 950 Loss 1.0759 Accuracy 0.7433\n",
            "Epoch 246 Batch 1000 Loss 1.0785 Accuracy 0.7430\n",
            "Epoch 246 Batch 1050 Loss 1.0813 Accuracy 0.7426\n",
            "Epoch 246 Batch 1100 Loss 1.0837 Accuracy 0.7422\n",
            "Epoch 246 Loss 1.0846 Accuracy 0.7421\n",
            "Time taken for 1 epoch: 22.29915499687195 secs\n",
            "\n",
            "discarded batch 23\n",
            "Epoch 246 VALIDATION: Loss 2.7138 Accuracy 0.5721\n",
            "\n",
            "Epoch 247 Batch 0 Loss 1.0134 Accuracy 0.7657\n",
            "discarded batch 42\n",
            "Epoch 247 Batch 50 Loss 1.0415 Accuracy 0.7477\n",
            "Epoch 247 Batch 100 Loss 1.0339 Accuracy 0.7507\n",
            "Epoch 247 Batch 150 Loss 1.0391 Accuracy 0.7496\n",
            "Epoch 247 Batch 200 Loss 1.0399 Accuracy 0.7486\n",
            "Epoch 247 Batch 250 Loss 1.0417 Accuracy 0.7491\n",
            "Epoch 247 Batch 300 Loss 1.0475 Accuracy 0.7477\n",
            "Epoch 247 Batch 350 Loss 1.0534 Accuracy 0.7471\n",
            "Epoch 247 Batch 400 Loss 1.0540 Accuracy 0.7470\n",
            "Epoch 247 Batch 450 Loss 1.0575 Accuracy 0.7468\n",
            "Epoch 247 Batch 500 Loss 1.0580 Accuracy 0.7465\n",
            "Epoch 247 Batch 550 Loss 1.0598 Accuracy 0.7463\n",
            "Epoch 247 Batch 600 Loss 1.0611 Accuracy 0.7461\n",
            "Epoch 247 Batch 650 Loss 1.0629 Accuracy 0.7457\n",
            "Epoch 247 Batch 700 Loss 1.0639 Accuracy 0.7454\n",
            "Epoch 247 Batch 750 Loss 1.0665 Accuracy 0.7449\n",
            "Epoch 247 Batch 800 Loss 1.0670 Accuracy 0.7449\n",
            "Epoch 247 Batch 850 Loss 1.0711 Accuracy 0.7440\n",
            "Epoch 247 Batch 900 Loss 1.0709 Accuracy 0.7441\n",
            "Epoch 247 Batch 950 Loss 1.0741 Accuracy 0.7435\n",
            "Epoch 247 Batch 1000 Loss 1.0767 Accuracy 0.7431\n",
            "Epoch 247 Batch 1050 Loss 1.0774 Accuracy 0.7430\n",
            "Epoch 247 Batch 1100 Loss 1.0802 Accuracy 0.7426\n",
            "Epoch 247 Loss 1.0813 Accuracy 0.7424\n",
            "Time taken for 1 epoch: 22.22989583015442 secs\n",
            "\n",
            "Epoch 248 Batch 0 Loss 1.0656 Accuracy 0.7624\n",
            "Epoch 248 Batch 50 Loss 1.0128 Accuracy 0.7544\n",
            "Epoch 248 Batch 100 Loss 1.0266 Accuracy 0.7496\n",
            "Epoch 248 Batch 150 Loss 1.0391 Accuracy 0.7479\n",
            "Epoch 248 Batch 200 Loss 1.0376 Accuracy 0.7483\n",
            "Epoch 248 Batch 250 Loss 1.0404 Accuracy 0.7480\n",
            "Epoch 248 Batch 300 Loss 1.0438 Accuracy 0.7477\n",
            "Epoch 248 Batch 350 Loss 1.0482 Accuracy 0.7468\n",
            "Epoch 248 Batch 400 Loss 1.0504 Accuracy 0.7464\n",
            "Epoch 248 Batch 450 Loss 1.0510 Accuracy 0.7462\n",
            "Epoch 248 Batch 500 Loss 1.0545 Accuracy 0.7456\n",
            "Epoch 248 Batch 550 Loss 1.0586 Accuracy 0.7452\n",
            "discarded batch 579\n",
            "Epoch 248 Batch 600 Loss 1.0590 Accuracy 0.7456\n",
            "Epoch 248 Batch 650 Loss 1.0615 Accuracy 0.7453\n",
            "Epoch 248 Batch 700 Loss 1.0628 Accuracy 0.7449\n",
            "Epoch 248 Batch 750 Loss 1.0654 Accuracy 0.7444\n",
            "Epoch 248 Batch 800 Loss 1.0678 Accuracy 0.7441\n",
            "Epoch 248 Batch 850 Loss 1.0716 Accuracy 0.7434\n",
            "Epoch 248 Batch 900 Loss 1.0731 Accuracy 0.7433\n",
            "Epoch 248 Batch 950 Loss 1.0757 Accuracy 0.7431\n",
            "Epoch 248 Batch 1000 Loss 1.0776 Accuracy 0.7428\n",
            "Epoch 248 Batch 1050 Loss 1.0790 Accuracy 0.7425\n",
            "Epoch 248 Batch 1100 Loss 1.0812 Accuracy 0.7422\n",
            "Epoch 248 Loss 1.0821 Accuracy 0.7421\n",
            "Time taken for 1 epoch: 22.295022010803223 secs\n",
            "\n",
            "Epoch 249 Batch 0 Loss 1.1219 Accuracy 0.7096\n",
            "Epoch 249 Batch 50 Loss 1.0229 Accuracy 0.7514\n",
            "Epoch 249 Batch 100 Loss 1.0291 Accuracy 0.7499\n",
            "Epoch 249 Batch 150 Loss 1.0315 Accuracy 0.7510\n",
            "Epoch 249 Batch 200 Loss 1.0399 Accuracy 0.7489\n",
            "Epoch 249 Batch 250 Loss 1.0424 Accuracy 0.7491\n",
            "Epoch 249 Batch 300 Loss 1.0455 Accuracy 0.7485\n",
            "Epoch 249 Batch 350 Loss 1.0460 Accuracy 0.7483\n",
            "Epoch 249 Batch 400 Loss 1.0508 Accuracy 0.7477\n",
            "Epoch 249 Batch 450 Loss 1.0508 Accuracy 0.7475\n",
            "Epoch 249 Batch 500 Loss 1.0551 Accuracy 0.7466\n",
            "Epoch 249 Batch 550 Loss 1.0572 Accuracy 0.7463\n",
            "Epoch 249 Batch 600 Loss 1.0597 Accuracy 0.7459\n",
            "Epoch 249 Batch 650 Loss 1.0634 Accuracy 0.7453\n",
            "Epoch 249 Batch 700 Loss 1.0655 Accuracy 0.7449\n",
            "Epoch 249 Batch 750 Loss 1.0666 Accuracy 0.7445\n",
            "Epoch 249 Batch 800 Loss 1.0698 Accuracy 0.7443\n",
            "Epoch 249 Batch 850 Loss 1.0708 Accuracy 0.7441\n",
            "Epoch 249 Batch 900 Loss 1.0718 Accuracy 0.7439\n",
            "discarded batch 941\n",
            "Epoch 249 Batch 950 Loss 1.0733 Accuracy 0.7435\n",
            "Epoch 249 Batch 1000 Loss 1.0753 Accuracy 0.7431\n",
            "Epoch 249 Batch 1050 Loss 1.0760 Accuracy 0.7431\n",
            "Epoch 249 Batch 1100 Loss 1.0780 Accuracy 0.7428\n",
            "Epoch 249 Loss 1.0785 Accuracy 0.7427\n",
            "Time taken for 1 epoch: 22.332411289215088 secs\n",
            "\n",
            "Epoch 250 Batch 0 Loss 1.0256 Accuracy 0.7393\n",
            "Epoch 250 Batch 50 Loss 1.0193 Accuracy 0.7531\n",
            "Epoch 250 Batch 100 Loss 1.0375 Accuracy 0.7490\n",
            "Epoch 250 Batch 150 Loss 1.0424 Accuracy 0.7492\n",
            "Epoch 250 Batch 200 Loss 1.0450 Accuracy 0.7485\n",
            "Epoch 250 Batch 250 Loss 1.0416 Accuracy 0.7488\n",
            "Epoch 250 Batch 300 Loss 1.0412 Accuracy 0.7486\n",
            "Epoch 250 Batch 350 Loss 1.0446 Accuracy 0.7482\n",
            "Epoch 250 Batch 400 Loss 1.0475 Accuracy 0.7477\n",
            "discarded batch 435\n",
            "Epoch 250 Batch 450 Loss 1.0479 Accuracy 0.7474\n",
            "Epoch 250 Batch 500 Loss 1.0509 Accuracy 0.7465\n",
            "Epoch 250 Batch 550 Loss 1.0521 Accuracy 0.7465\n",
            "Epoch 250 Batch 600 Loss 1.0537 Accuracy 0.7463\n",
            "Epoch 250 Batch 650 Loss 1.0560 Accuracy 0.7460\n",
            "Epoch 250 Batch 700 Loss 1.0585 Accuracy 0.7456\n",
            "Epoch 250 Batch 750 Loss 1.0606 Accuracy 0.7454\n",
            "Epoch 250 Batch 800 Loss 1.0625 Accuracy 0.7450\n",
            "Epoch 250 Batch 850 Loss 1.0641 Accuracy 0.7447\n",
            "Epoch 250 Batch 900 Loss 1.0672 Accuracy 0.7442\n",
            "Epoch 250 Batch 950 Loss 1.0689 Accuracy 0.7440\n",
            "Epoch 250 Batch 1000 Loss 1.0706 Accuracy 0.7436\n",
            "Epoch 250 Batch 1050 Loss 1.0725 Accuracy 0.7431\n",
            "Epoch 250 Batch 1100 Loss 1.0749 Accuracy 0.7427\n",
            "Saving checkpoint for epoch 250 at ./checkpoints/train/ckpt-50\n",
            "Epoch 250 Loss 1.0754 Accuracy 0.7427\n",
            "Time taken for 1 epoch: 22.508808374404907 secs\n",
            "\n",
            "Epoch 251 Batch 0 Loss 0.9977 Accuracy 0.7756\n",
            "Epoch 251 Batch 50 Loss 1.0309 Accuracy 0.7516\n",
            "Epoch 251 Batch 100 Loss 1.0246 Accuracy 0.7513\n",
            "Epoch 251 Batch 150 Loss 1.0368 Accuracy 0.7494\n",
            "Epoch 251 Batch 200 Loss 1.0394 Accuracy 0.7491\n",
            "Epoch 251 Batch 250 Loss 1.0384 Accuracy 0.7487\n",
            "Epoch 251 Batch 300 Loss 1.0458 Accuracy 0.7476\n",
            "Epoch 251 Batch 350 Loss 1.0481 Accuracy 0.7471\n",
            "discarded batch 376\n",
            "Epoch 251 Batch 400 Loss 1.0483 Accuracy 0.7470\n",
            "Epoch 251 Batch 450 Loss 1.0516 Accuracy 0.7463\n",
            "Epoch 251 Batch 500 Loss 1.0548 Accuracy 0.7454\n",
            "Epoch 251 Batch 550 Loss 1.0557 Accuracy 0.7453\n",
            "Epoch 251 Batch 600 Loss 1.0579 Accuracy 0.7453\n",
            "Epoch 251 Batch 650 Loss 1.0584 Accuracy 0.7453\n",
            "Epoch 251 Batch 700 Loss 1.0602 Accuracy 0.7450\n",
            "Epoch 251 Batch 750 Loss 1.0623 Accuracy 0.7447\n",
            "Epoch 251 Batch 800 Loss 1.0661 Accuracy 0.7441\n",
            "Epoch 251 Batch 850 Loss 1.0686 Accuracy 0.7438\n",
            "Epoch 251 Batch 900 Loss 1.0709 Accuracy 0.7435\n",
            "Epoch 251 Batch 950 Loss 1.0728 Accuracy 0.7433\n",
            "Epoch 251 Batch 1000 Loss 1.0737 Accuracy 0.7432\n",
            "Epoch 251 Batch 1050 Loss 1.0746 Accuracy 0.7429\n",
            "Epoch 251 Batch 1100 Loss 1.0756 Accuracy 0.7427\n",
            "Epoch 251 Loss 1.0767 Accuracy 0.7425\n",
            "Time taken for 1 epoch: 22.26609516143799 secs\n",
            "\n",
            "discarded batch 23\n",
            "Epoch 251 VALIDATION: Loss 2.7646 Accuracy 0.5651\n",
            "\n",
            "Epoch 252 Batch 0 Loss 0.9635 Accuracy 0.7723\n",
            "Epoch 252 Batch 50 Loss 1.0252 Accuracy 0.7512\n",
            "Epoch 252 Batch 100 Loss 1.0269 Accuracy 0.7508\n",
            "Epoch 252 Batch 150 Loss 1.0322 Accuracy 0.7504\n",
            "Epoch 252 Batch 200 Loss 1.0407 Accuracy 0.7485\n",
            "Epoch 252 Batch 250 Loss 1.0421 Accuracy 0.7477\n",
            "Epoch 252 Batch 300 Loss 1.0446 Accuracy 0.7473\n",
            "Epoch 252 Batch 350 Loss 1.0454 Accuracy 0.7472\n",
            "Epoch 252 Batch 400 Loss 1.0478 Accuracy 0.7470\n",
            "discarded batch 447\n",
            "Epoch 252 Batch 450 Loss 1.0478 Accuracy 0.7469\n",
            "Epoch 252 Batch 500 Loss 1.0511 Accuracy 0.7465\n",
            "Epoch 252 Batch 550 Loss 1.0524 Accuracy 0.7464\n",
            "Epoch 252 Batch 600 Loss 1.0557 Accuracy 0.7461\n",
            "Epoch 252 Batch 650 Loss 1.0593 Accuracy 0.7457\n",
            "Epoch 252 Batch 700 Loss 1.0615 Accuracy 0.7452\n",
            "Epoch 252 Batch 750 Loss 1.0632 Accuracy 0.7451\n",
            "Epoch 252 Batch 800 Loss 1.0651 Accuracy 0.7448\n",
            "Epoch 252 Batch 850 Loss 1.0678 Accuracy 0.7444\n",
            "Epoch 252 Batch 900 Loss 1.0706 Accuracy 0.7440\n",
            "Epoch 252 Batch 950 Loss 1.0723 Accuracy 0.7435\n",
            "Epoch 252 Batch 1000 Loss 1.0740 Accuracy 0.7433\n",
            "Epoch 252 Batch 1050 Loss 1.0760 Accuracy 0.7429\n",
            "Epoch 252 Batch 1100 Loss 1.0772 Accuracy 0.7428\n",
            "Epoch 252 Loss 1.0782 Accuracy 0.7427\n",
            "Time taken for 1 epoch: 22.403808116912842 secs\n",
            "\n",
            "Epoch 253 Batch 0 Loss 1.0102 Accuracy 0.7558\n",
            "Epoch 253 Batch 50 Loss 1.0070 Accuracy 0.7607\n",
            "Epoch 253 Batch 100 Loss 1.0143 Accuracy 0.7565\n",
            "Epoch 253 Batch 150 Loss 1.0200 Accuracy 0.7560\n",
            "Epoch 253 Batch 200 Loss 1.0249 Accuracy 0.7544\n",
            "Epoch 253 Batch 250 Loss 1.0302 Accuracy 0.7532\n",
            "Epoch 253 Batch 300 Loss 1.0325 Accuracy 0.7524\n",
            "Epoch 253 Batch 350 Loss 1.0362 Accuracy 0.7518\n",
            "Epoch 253 Batch 400 Loss 1.0401 Accuracy 0.7507\n",
            "Epoch 253 Batch 450 Loss 1.0429 Accuracy 0.7500\n",
            "Epoch 253 Batch 500 Loss 1.0451 Accuracy 0.7493\n",
            "discarded batch 525\n",
            "Epoch 253 Batch 550 Loss 1.0485 Accuracy 0.7486\n",
            "Epoch 253 Batch 600 Loss 1.0511 Accuracy 0.7482\n",
            "Epoch 253 Batch 650 Loss 1.0534 Accuracy 0.7476\n",
            "Epoch 253 Batch 700 Loss 1.0561 Accuracy 0.7471\n",
            "Epoch 253 Batch 750 Loss 1.0575 Accuracy 0.7466\n",
            "Epoch 253 Batch 800 Loss 1.0599 Accuracy 0.7461\n",
            "Epoch 253 Batch 850 Loss 1.0615 Accuracy 0.7459\n",
            "Epoch 253 Batch 900 Loss 1.0649 Accuracy 0.7451\n",
            "Epoch 253 Batch 950 Loss 1.0668 Accuracy 0.7447\n",
            "Epoch 253 Batch 1000 Loss 1.0695 Accuracy 0.7443\n",
            "Epoch 253 Batch 1050 Loss 1.0708 Accuracy 0.7441\n",
            "Epoch 253 Batch 1100 Loss 1.0734 Accuracy 0.7437\n",
            "Epoch 253 Loss 1.0745 Accuracy 0.7435\n",
            "Time taken for 1 epoch: 22.490482330322266 secs\n",
            "\n",
            "Epoch 254 Batch 0 Loss 1.2146 Accuracy 0.7063\n",
            "Epoch 254 Batch 50 Loss 1.0135 Accuracy 0.7541\n",
            "Epoch 254 Batch 100 Loss 1.0301 Accuracy 0.7511\n",
            "Epoch 254 Batch 150 Loss 1.0382 Accuracy 0.7497\n",
            "Epoch 254 Batch 200 Loss 1.0415 Accuracy 0.7493\n",
            "Epoch 254 Batch 250 Loss 1.0431 Accuracy 0.7487\n",
            "Epoch 254 Batch 300 Loss 1.0452 Accuracy 0.7479\n",
            "Epoch 254 Batch 350 Loss 1.0484 Accuracy 0.7476\n",
            "Epoch 254 Batch 400 Loss 1.0519 Accuracy 0.7471\n",
            "Epoch 254 Batch 450 Loss 1.0548 Accuracy 0.7465\n",
            "discarded batch 498\n",
            "Epoch 254 Batch 500 Loss 1.0564 Accuracy 0.7463\n",
            "Epoch 254 Batch 550 Loss 1.0573 Accuracy 0.7459\n",
            "Epoch 254 Batch 600 Loss 1.0607 Accuracy 0.7455\n",
            "Epoch 254 Batch 650 Loss 1.0620 Accuracy 0.7453\n",
            "Epoch 254 Batch 700 Loss 1.0625 Accuracy 0.7454\n",
            "Epoch 254 Batch 750 Loss 1.0649 Accuracy 0.7450\n",
            "Epoch 254 Batch 800 Loss 1.0658 Accuracy 0.7446\n",
            "Epoch 254 Batch 850 Loss 1.0670 Accuracy 0.7441\n",
            "Epoch 254 Batch 900 Loss 1.0684 Accuracy 0.7441\n",
            "Epoch 254 Batch 950 Loss 1.0694 Accuracy 0.7437\n",
            "Epoch 254 Batch 1000 Loss 1.0710 Accuracy 0.7433\n",
            "Epoch 254 Batch 1050 Loss 1.0728 Accuracy 0.7431\n",
            "Epoch 254 Batch 1100 Loss 1.0741 Accuracy 0.7429\n",
            "Epoch 254 Loss 1.0753 Accuracy 0.7426\n",
            "Time taken for 1 epoch: 22.174371242523193 secs\n",
            "\n",
            "Epoch 255 Batch 0 Loss 1.0056 Accuracy 0.7723\n",
            "Epoch 255 Batch 50 Loss 1.0266 Accuracy 0.7531\n",
            "Epoch 255 Batch 100 Loss 1.0277 Accuracy 0.7521\n",
            "Epoch 255 Batch 150 Loss 1.0271 Accuracy 0.7544\n",
            "Epoch 255 Batch 200 Loss 1.0347 Accuracy 0.7523\n",
            "Epoch 255 Batch 250 Loss 1.0374 Accuracy 0.7514\n",
            "Epoch 255 Batch 300 Loss 1.0411 Accuracy 0.7503\n",
            "Epoch 255 Batch 350 Loss 1.0450 Accuracy 0.7495\n",
            "Epoch 255 Batch 400 Loss 1.0468 Accuracy 0.7487\n",
            "Epoch 255 Batch 450 Loss 1.0492 Accuracy 0.7480\n",
            "Epoch 255 Batch 500 Loss 1.0500 Accuracy 0.7479\n",
            "Epoch 255 Batch 550 Loss 1.0517 Accuracy 0.7476\n",
            "Epoch 255 Batch 600 Loss 1.0519 Accuracy 0.7474\n",
            "Epoch 255 Batch 650 Loss 1.0554 Accuracy 0.7466\n",
            "discarded batch 682\n",
            "Epoch 255 Batch 700 Loss 1.0579 Accuracy 0.7461\n",
            "Epoch 255 Batch 750 Loss 1.0606 Accuracy 0.7456\n",
            "Epoch 255 Batch 800 Loss 1.0632 Accuracy 0.7452\n",
            "Epoch 255 Batch 850 Loss 1.0637 Accuracy 0.7452\n",
            "Epoch 255 Batch 900 Loss 1.0663 Accuracy 0.7449\n",
            "Epoch 255 Batch 950 Loss 1.0678 Accuracy 0.7447\n",
            "Epoch 255 Batch 1000 Loss 1.0696 Accuracy 0.7443\n",
            "Epoch 255 Batch 1050 Loss 1.0703 Accuracy 0.7443\n",
            "Epoch 255 Batch 1100 Loss 1.0720 Accuracy 0.7441\n",
            "Saving checkpoint for epoch 255 at ./checkpoints/train/ckpt-51\n",
            "Epoch 255 Loss 1.0725 Accuracy 0.7440\n",
            "Time taken for 1 epoch: 22.431771278381348 secs\n",
            "\n",
            "Epoch 256 Batch 0 Loss 0.9884 Accuracy 0.7459\n",
            "Epoch 256 Batch 50 Loss 1.0319 Accuracy 0.7531\n",
            "Epoch 256 Batch 100 Loss 1.0300 Accuracy 0.7511\n",
            "Epoch 256 Batch 150 Loss 1.0316 Accuracy 0.7504\n",
            "Epoch 256 Batch 200 Loss 1.0248 Accuracy 0.7530\n",
            "Epoch 256 Batch 250 Loss 1.0300 Accuracy 0.7512\n",
            "Epoch 256 Batch 300 Loss 1.0343 Accuracy 0.7502\n",
            "Epoch 256 Batch 350 Loss 1.0395 Accuracy 0.7489\n",
            "Epoch 256 Batch 400 Loss 1.0410 Accuracy 0.7485\n",
            "Epoch 256 Batch 450 Loss 1.0434 Accuracy 0.7479\n",
            "Epoch 256 Batch 500 Loss 1.0458 Accuracy 0.7476\n",
            "Epoch 256 Batch 550 Loss 1.0484 Accuracy 0.7473\n",
            "Epoch 256 Batch 600 Loss 1.0499 Accuracy 0.7470\n",
            "Epoch 256 Batch 650 Loss 1.0510 Accuracy 0.7468\n",
            "Epoch 256 Batch 700 Loss 1.0538 Accuracy 0.7465\n",
            "discarded batch 739\n",
            "Epoch 256 Batch 750 Loss 1.0558 Accuracy 0.7462\n",
            "Epoch 256 Batch 800 Loss 1.0581 Accuracy 0.7459\n",
            "Epoch 256 Batch 850 Loss 1.0608 Accuracy 0.7456\n",
            "Epoch 256 Batch 900 Loss 1.0611 Accuracy 0.7457\n",
            "Epoch 256 Batch 950 Loss 1.0621 Accuracy 0.7455\n",
            "Epoch 256 Batch 1000 Loss 1.0642 Accuracy 0.7452\n",
            "Epoch 256 Batch 1050 Loss 1.0673 Accuracy 0.7449\n",
            "Epoch 256 Batch 1100 Loss 1.0698 Accuracy 0.7445\n",
            "Epoch 256 Loss 1.0708 Accuracy 0.7443\n",
            "Time taken for 1 epoch: 22.266319751739502 secs\n",
            "\n",
            "discarded batch 23\n",
            "Epoch 256 VALIDATION: Loss 2.7396 Accuracy 0.5692\n",
            "\n",
            "Epoch 257 Batch 0 Loss 1.1396 Accuracy 0.7426\n",
            "Epoch 257 Batch 50 Loss 1.0282 Accuracy 0.7535\n",
            "Epoch 257 Batch 100 Loss 1.0230 Accuracy 0.7539\n",
            "Epoch 257 Batch 150 Loss 1.0242 Accuracy 0.7525\n",
            "Epoch 257 Batch 200 Loss 1.0268 Accuracy 0.7520\n",
            "discarded batch 233\n",
            "Epoch 257 Batch 250 Loss 1.0336 Accuracy 0.7502\n",
            "Epoch 257 Batch 300 Loss 1.0366 Accuracy 0.7499\n",
            "Epoch 257 Batch 350 Loss 1.0432 Accuracy 0.7490\n",
            "Epoch 257 Batch 400 Loss 1.0450 Accuracy 0.7486\n",
            "Epoch 257 Batch 450 Loss 1.0445 Accuracy 0.7483\n",
            "Epoch 257 Batch 500 Loss 1.0454 Accuracy 0.7483\n",
            "Epoch 257 Batch 550 Loss 1.0482 Accuracy 0.7480\n",
            "Epoch 257 Batch 600 Loss 1.0509 Accuracy 0.7473\n",
            "Epoch 257 Batch 650 Loss 1.0539 Accuracy 0.7465\n",
            "Epoch 257 Batch 700 Loss 1.0569 Accuracy 0.7458\n",
            "Epoch 257 Batch 750 Loss 1.0583 Accuracy 0.7456\n",
            "Epoch 257 Batch 800 Loss 1.0606 Accuracy 0.7454\n",
            "Epoch 257 Batch 850 Loss 1.0625 Accuracy 0.7450\n",
            "Epoch 257 Batch 900 Loss 1.0630 Accuracy 0.7450\n",
            "Epoch 257 Batch 950 Loss 1.0650 Accuracy 0.7448\n",
            "Epoch 257 Batch 1000 Loss 1.0668 Accuracy 0.7446\n",
            "Epoch 257 Batch 1050 Loss 1.0687 Accuracy 0.7441\n",
            "Epoch 257 Batch 1100 Loss 1.0704 Accuracy 0.7437\n",
            "Epoch 257 Loss 1.0714 Accuracy 0.7436\n",
            "Time taken for 1 epoch: 22.262969255447388 secs\n",
            "\n",
            "Epoch 258 Batch 0 Loss 0.9164 Accuracy 0.7855\n",
            "Epoch 258 Batch 50 Loss 1.0279 Accuracy 0.7512\n",
            "Epoch 258 Batch 100 Loss 1.0222 Accuracy 0.7535\n",
            "Epoch 258 Batch 150 Loss 1.0270 Accuracy 0.7530\n",
            "Epoch 258 Batch 200 Loss 1.0382 Accuracy 0.7511\n",
            "Epoch 258 Batch 250 Loss 1.0362 Accuracy 0.7517\n",
            "Epoch 258 Batch 300 Loss 1.0363 Accuracy 0.7518\n",
            "Epoch 258 Batch 350 Loss 1.0396 Accuracy 0.7509\n",
            "Epoch 258 Batch 400 Loss 1.0414 Accuracy 0.7509\n",
            "Epoch 258 Batch 450 Loss 1.0418 Accuracy 0.7506\n",
            "Epoch 258 Batch 500 Loss 1.0445 Accuracy 0.7500\n",
            "Epoch 258 Batch 550 Loss 1.0470 Accuracy 0.7491\n",
            "Epoch 258 Batch 600 Loss 1.0505 Accuracy 0.7485\n",
            "Epoch 258 Batch 650 Loss 1.0520 Accuracy 0.7483\n",
            "Epoch 258 Batch 700 Loss 1.0537 Accuracy 0.7480\n",
            "Epoch 258 Batch 750 Loss 1.0552 Accuracy 0.7474\n",
            "Epoch 258 Batch 800 Loss 1.0564 Accuracy 0.7474\n",
            "Epoch 258 Batch 850 Loss 1.0586 Accuracy 0.7470\n",
            "discarded batch 854\n",
            "Epoch 258 Batch 900 Loss 1.0605 Accuracy 0.7466\n",
            "Epoch 258 Batch 950 Loss 1.0628 Accuracy 0.7462\n",
            "Epoch 258 Batch 1000 Loss 1.0640 Accuracy 0.7459\n",
            "Epoch 258 Batch 1050 Loss 1.0655 Accuracy 0.7458\n",
            "Epoch 258 Batch 1100 Loss 1.0674 Accuracy 0.7453\n",
            "Epoch 258 Loss 1.0680 Accuracy 0.7452\n",
            "Time taken for 1 epoch: 22.221460342407227 secs\n",
            "\n",
            "Epoch 259 Batch 0 Loss 1.1637 Accuracy 0.7129\n",
            "Epoch 259 Batch 50 Loss 1.0078 Accuracy 0.7583\n",
            "Epoch 259 Batch 100 Loss 1.0125 Accuracy 0.7544\n",
            "Epoch 259 Batch 150 Loss 1.0164 Accuracy 0.7542\n",
            "Epoch 259 Batch 200 Loss 1.0250 Accuracy 0.7517\n",
            "Epoch 259 Batch 250 Loss 1.0262 Accuracy 0.7518\n",
            "Epoch 259 Batch 300 Loss 1.0302 Accuracy 0.7502\n",
            "Epoch 259 Batch 350 Loss 1.0346 Accuracy 0.7495\n",
            "Epoch 259 Batch 400 Loss 1.0368 Accuracy 0.7486\n",
            "discarded batch 419\n",
            "Epoch 259 Batch 450 Loss 1.0382 Accuracy 0.7487\n",
            "Epoch 259 Batch 500 Loss 1.0406 Accuracy 0.7480\n",
            "Epoch 259 Batch 550 Loss 1.0452 Accuracy 0.7473\n",
            "Epoch 259 Batch 600 Loss 1.0490 Accuracy 0.7466\n",
            "Epoch 259 Batch 650 Loss 1.0515 Accuracy 0.7462\n",
            "Epoch 259 Batch 700 Loss 1.0537 Accuracy 0.7459\n",
            "Epoch 259 Batch 750 Loss 1.0558 Accuracy 0.7456\n",
            "Epoch 259 Batch 800 Loss 1.0584 Accuracy 0.7453\n",
            "Epoch 259 Batch 850 Loss 1.0601 Accuracy 0.7448\n",
            "Epoch 259 Batch 900 Loss 1.0621 Accuracy 0.7445\n",
            "Epoch 259 Batch 950 Loss 1.0646 Accuracy 0.7441\n",
            "Epoch 259 Batch 1000 Loss 1.0660 Accuracy 0.7440\n",
            "Epoch 259 Batch 1050 Loss 1.0672 Accuracy 0.7438\n",
            "Epoch 259 Batch 1100 Loss 1.0684 Accuracy 0.7435\n",
            "Epoch 259 Loss 1.0701 Accuracy 0.7433\n",
            "Time taken for 1 epoch: 22.18670392036438 secs\n",
            "\n",
            "Epoch 260 Batch 0 Loss 0.9307 Accuracy 0.7888\n",
            "discarded batch 14\n",
            "Epoch 260 Batch 50 Loss 1.0118 Accuracy 0.7545\n",
            "Epoch 260 Batch 100 Loss 1.0144 Accuracy 0.7561\n",
            "Epoch 260 Batch 150 Loss 1.0212 Accuracy 0.7551\n",
            "Epoch 260 Batch 200 Loss 1.0197 Accuracy 0.7548\n",
            "Epoch 260 Batch 250 Loss 1.0276 Accuracy 0.7525\n",
            "Epoch 260 Batch 300 Loss 1.0315 Accuracy 0.7520\n",
            "Epoch 260 Batch 350 Loss 1.0346 Accuracy 0.7514\n",
            "Epoch 260 Batch 400 Loss 1.0377 Accuracy 0.7507\n",
            "Epoch 260 Batch 450 Loss 1.0395 Accuracy 0.7496\n",
            "Epoch 260 Batch 500 Loss 1.0431 Accuracy 0.7492\n",
            "Epoch 260 Batch 550 Loss 1.0439 Accuracy 0.7491\n",
            "Epoch 260 Batch 600 Loss 1.0457 Accuracy 0.7486\n",
            "Epoch 260 Batch 650 Loss 1.0480 Accuracy 0.7484\n",
            "Epoch 260 Batch 700 Loss 1.0512 Accuracy 0.7478\n",
            "Epoch 260 Batch 750 Loss 1.0521 Accuracy 0.7474\n",
            "Epoch 260 Batch 800 Loss 1.0531 Accuracy 0.7472\n",
            "Epoch 260 Batch 850 Loss 1.0539 Accuracy 0.7469\n",
            "Epoch 260 Batch 900 Loss 1.0567 Accuracy 0.7463\n",
            "Epoch 260 Batch 950 Loss 1.0608 Accuracy 0.7458\n",
            "Epoch 260 Batch 1000 Loss 1.0630 Accuracy 0.7454\n",
            "Epoch 260 Batch 1050 Loss 1.0642 Accuracy 0.7452\n",
            "Epoch 260 Batch 1100 Loss 1.0664 Accuracy 0.7448\n",
            "Saving checkpoint for epoch 260 at ./checkpoints/train/ckpt-52\n",
            "Epoch 260 Loss 1.0675 Accuracy 0.7446\n",
            "Time taken for 1 epoch: 22.49490451812744 secs\n",
            "\n",
            "Epoch 261 Batch 0 Loss 1.0852 Accuracy 0.7426\n",
            "Epoch 261 Batch 50 Loss 1.0170 Accuracy 0.7572\n",
            "Epoch 261 Batch 100 Loss 1.0237 Accuracy 0.7525\n",
            "Epoch 261 Batch 150 Loss 1.0262 Accuracy 0.7519\n",
            "Epoch 261 Batch 200 Loss 1.0237 Accuracy 0.7524\n",
            "Epoch 261 Batch 250 Loss 1.0257 Accuracy 0.7516\n",
            "Epoch 261 Batch 300 Loss 1.0270 Accuracy 0.7511\n",
            "Epoch 261 Batch 350 Loss 1.0313 Accuracy 0.7502\n",
            "Epoch 261 Batch 400 Loss 1.0341 Accuracy 0.7493\n",
            "discarded batch 438\n",
            "Epoch 261 Batch 450 Loss 1.0354 Accuracy 0.7495\n",
            "Epoch 261 Batch 500 Loss 1.0387 Accuracy 0.7487\n",
            "Epoch 261 Batch 550 Loss 1.0420 Accuracy 0.7482\n",
            "Epoch 261 Batch 600 Loss 1.0433 Accuracy 0.7477\n",
            "Epoch 261 Batch 650 Loss 1.0444 Accuracy 0.7478\n",
            "Epoch 261 Batch 700 Loss 1.0472 Accuracy 0.7475\n",
            "Epoch 261 Batch 750 Loss 1.0493 Accuracy 0.7469\n",
            "Epoch 261 Batch 800 Loss 1.0521 Accuracy 0.7467\n",
            "Epoch 261 Batch 850 Loss 1.0548 Accuracy 0.7464\n",
            "Epoch 261 Batch 900 Loss 1.0563 Accuracy 0.7461\n",
            "Epoch 261 Batch 950 Loss 1.0591 Accuracy 0.7457\n",
            "Epoch 261 Batch 1000 Loss 1.0617 Accuracy 0.7453\n",
            "Epoch 261 Batch 1050 Loss 1.0641 Accuracy 0.7448\n",
            "Epoch 261 Batch 1100 Loss 1.0656 Accuracy 0.7447\n",
            "Epoch 261 Loss 1.0665 Accuracy 0.7446\n",
            "Time taken for 1 epoch: 22.193726301193237 secs\n",
            "\n",
            "discarded batch 23\n",
            "Epoch 261 VALIDATION: Loss 2.7620 Accuracy 0.5678\n",
            "\n",
            "Epoch 262 Batch 0 Loss 1.0283 Accuracy 0.7459\n",
            "Epoch 262 Batch 50 Loss 1.0198 Accuracy 0.7513\n",
            "Epoch 262 Batch 100 Loss 1.0205 Accuracy 0.7508\n",
            "Epoch 262 Batch 150 Loss 1.0267 Accuracy 0.7501\n",
            "Epoch 262 Batch 200 Loss 1.0243 Accuracy 0.7508\n",
            "Epoch 262 Batch 250 Loss 1.0263 Accuracy 0.7500\n",
            "Epoch 262 Batch 300 Loss 1.0305 Accuracy 0.7498\n",
            "Epoch 262 Batch 350 Loss 1.0313 Accuracy 0.7496\n",
            "Epoch 262 Batch 400 Loss 1.0364 Accuracy 0.7489\n",
            "Epoch 262 Batch 450 Loss 1.0376 Accuracy 0.7487\n",
            "Epoch 262 Batch 500 Loss 1.0400 Accuracy 0.7486\n",
            "Epoch 262 Batch 550 Loss 1.0424 Accuracy 0.7485\n",
            "Epoch 262 Batch 600 Loss 1.0448 Accuracy 0.7481\n",
            "Epoch 262 Batch 650 Loss 1.0466 Accuracy 0.7481\n",
            "Epoch 262 Batch 700 Loss 1.0485 Accuracy 0.7476\n",
            "Epoch 262 Batch 750 Loss 1.0504 Accuracy 0.7473\n",
            "Epoch 262 Batch 800 Loss 1.0515 Accuracy 0.7471\n",
            "discarded batch 821\n",
            "Epoch 262 Batch 850 Loss 1.0550 Accuracy 0.7465\n",
            "Epoch 262 Batch 900 Loss 1.0556 Accuracy 0.7462\n",
            "Epoch 262 Batch 950 Loss 1.0580 Accuracy 0.7459\n",
            "Epoch 262 Batch 1000 Loss 1.0608 Accuracy 0.7454\n",
            "Epoch 262 Batch 1050 Loss 1.0638 Accuracy 0.7448\n",
            "Epoch 262 Batch 1100 Loss 1.0659 Accuracy 0.7445\n",
            "Epoch 262 Loss 1.0675 Accuracy 0.7443\n",
            "Time taken for 1 epoch: 22.13499879837036 secs\n",
            "\n",
            "Epoch 263 Batch 0 Loss 1.0839 Accuracy 0.7360\n",
            "Epoch 263 Batch 50 Loss 1.0232 Accuracy 0.7525\n",
            "Epoch 263 Batch 100 Loss 1.0214 Accuracy 0.7515\n",
            "Epoch 263 Batch 150 Loss 1.0272 Accuracy 0.7500\n",
            "Epoch 263 Batch 200 Loss 1.0300 Accuracy 0.7500\n",
            "Epoch 263 Batch 250 Loss 1.0309 Accuracy 0.7493\n",
            "Epoch 263 Batch 300 Loss 1.0367 Accuracy 0.7484\n",
            "Epoch 263 Batch 350 Loss 1.0398 Accuracy 0.7483\n",
            "Epoch 263 Batch 400 Loss 1.0406 Accuracy 0.7481\n",
            "discarded batch 420\n",
            "Epoch 263 Batch 450 Loss 1.0436 Accuracy 0.7476\n",
            "Epoch 263 Batch 500 Loss 1.0475 Accuracy 0.7468\n",
            "Epoch 263 Batch 550 Loss 1.0495 Accuracy 0.7466\n",
            "Epoch 263 Batch 600 Loss 1.0530 Accuracy 0.7460\n",
            "Epoch 263 Batch 650 Loss 1.0552 Accuracy 0.7458\n",
            "Epoch 263 Batch 700 Loss 1.0564 Accuracy 0.7455\n",
            "Epoch 263 Batch 750 Loss 1.0580 Accuracy 0.7453\n",
            "Epoch 263 Batch 800 Loss 1.0589 Accuracy 0.7453\n",
            "Epoch 263 Batch 850 Loss 1.0597 Accuracy 0.7451\n",
            "Epoch 263 Batch 900 Loss 1.0613 Accuracy 0.7450\n",
            "Epoch 263 Batch 950 Loss 1.0625 Accuracy 0.7450\n",
            "Epoch 263 Batch 1000 Loss 1.0643 Accuracy 0.7446\n",
            "Epoch 263 Batch 1050 Loss 1.0657 Accuracy 0.7444\n",
            "Epoch 263 Batch 1100 Loss 1.0671 Accuracy 0.7442\n",
            "Epoch 263 Loss 1.0672 Accuracy 0.7442\n",
            "Time taken for 1 epoch: 22.2069890499115 secs\n",
            "\n",
            "Epoch 264 Batch 0 Loss 0.9665 Accuracy 0.7558\n",
            "Epoch 264 Batch 50 Loss 1.0023 Accuracy 0.7543\n",
            "Epoch 264 Batch 100 Loss 1.0065 Accuracy 0.7542\n",
            "Epoch 264 Batch 150 Loss 1.0109 Accuracy 0.7545\n",
            "Epoch 264 Batch 200 Loss 1.0193 Accuracy 0.7533\n",
            "Epoch 264 Batch 250 Loss 1.0169 Accuracy 0.7541\n",
            "discarded batch 273\n",
            "Epoch 264 Batch 300 Loss 1.0271 Accuracy 0.7523\n",
            "Epoch 264 Batch 350 Loss 1.0316 Accuracy 0.7515\n",
            "Epoch 264 Batch 400 Loss 1.0337 Accuracy 0.7513\n",
            "Epoch 264 Batch 450 Loss 1.0383 Accuracy 0.7505\n",
            "Epoch 264 Batch 500 Loss 1.0410 Accuracy 0.7499\n",
            "Epoch 264 Batch 550 Loss 1.0447 Accuracy 0.7491\n",
            "Epoch 264 Batch 600 Loss 1.0479 Accuracy 0.7482\n",
            "Epoch 264 Batch 650 Loss 1.0485 Accuracy 0.7480\n",
            "Epoch 264 Batch 700 Loss 1.0500 Accuracy 0.7479\n",
            "Epoch 264 Batch 750 Loss 1.0525 Accuracy 0.7476\n",
            "Epoch 264 Batch 800 Loss 1.0545 Accuracy 0.7473\n",
            "Epoch 264 Batch 850 Loss 1.0562 Accuracy 0.7467\n",
            "Epoch 264 Batch 900 Loss 1.0584 Accuracy 0.7463\n",
            "Epoch 264 Batch 950 Loss 1.0607 Accuracy 0.7459\n",
            "Epoch 264 Batch 1000 Loss 1.0619 Accuracy 0.7459\n",
            "Epoch 264 Batch 1050 Loss 1.0639 Accuracy 0.7456\n",
            "Epoch 264 Batch 1100 Loss 1.0656 Accuracy 0.7454\n",
            "Epoch 264 Loss 1.0661 Accuracy 0.7453\n",
            "Time taken for 1 epoch: 22.219193696975708 secs\n",
            "\n",
            "Epoch 265 Batch 0 Loss 0.9467 Accuracy 0.7921\n",
            "Epoch 265 Batch 50 Loss 1.0309 Accuracy 0.7520\n",
            "Epoch 265 Batch 100 Loss 1.0236 Accuracy 0.7511\n",
            "Epoch 265 Batch 150 Loss 1.0209 Accuracy 0.7525\n",
            "Epoch 265 Batch 200 Loss 1.0228 Accuracy 0.7520\n",
            "Epoch 265 Batch 250 Loss 1.0284 Accuracy 0.7511\n",
            "Epoch 265 Batch 300 Loss 1.0307 Accuracy 0.7504\n",
            "discarded batch 330\n",
            "Epoch 265 Batch 350 Loss 1.0338 Accuracy 0.7500\n",
            "Epoch 265 Batch 400 Loss 1.0361 Accuracy 0.7492\n",
            "Epoch 265 Batch 450 Loss 1.0377 Accuracy 0.7490\n",
            "Epoch 265 Batch 500 Loss 1.0409 Accuracy 0.7487\n",
            "Epoch 265 Batch 550 Loss 1.0434 Accuracy 0.7480\n",
            "Epoch 265 Batch 600 Loss 1.0446 Accuracy 0.7482\n",
            "Epoch 265 Batch 650 Loss 1.0463 Accuracy 0.7480\n",
            "Epoch 265 Batch 700 Loss 1.0492 Accuracy 0.7479\n",
            "Epoch 265 Batch 750 Loss 1.0515 Accuracy 0.7476\n",
            "Epoch 265 Batch 800 Loss 1.0521 Accuracy 0.7477\n",
            "Epoch 265 Batch 850 Loss 1.0536 Accuracy 0.7473\n",
            "Epoch 265 Batch 900 Loss 1.0564 Accuracy 0.7468\n",
            "Epoch 265 Batch 950 Loss 1.0579 Accuracy 0.7466\n",
            "Epoch 265 Batch 1000 Loss 1.0599 Accuracy 0.7464\n",
            "Epoch 265 Batch 1050 Loss 1.0609 Accuracy 0.7463\n",
            "Epoch 265 Batch 1100 Loss 1.0617 Accuracy 0.7464\n",
            "Saving checkpoint for epoch 265 at ./checkpoints/train/ckpt-53\n",
            "Epoch 265 Loss 1.0632 Accuracy 0.7461\n",
            "Time taken for 1 epoch: 22.455742120742798 secs\n",
            "\n",
            "Epoch 266 Batch 0 Loss 1.0077 Accuracy 0.7525\n",
            "Epoch 266 Batch 50 Loss 1.0104 Accuracy 0.7558\n",
            "Epoch 266 Batch 100 Loss 1.0181 Accuracy 0.7551\n",
            "Epoch 266 Batch 150 Loss 1.0210 Accuracy 0.7541\n",
            "Epoch 266 Batch 200 Loss 1.0250 Accuracy 0.7526\n",
            "Epoch 266 Batch 250 Loss 1.0350 Accuracy 0.7514\n",
            "Epoch 266 Batch 300 Loss 1.0350 Accuracy 0.7506\n",
            "Epoch 266 Batch 350 Loss 1.0322 Accuracy 0.7509\n",
            "Epoch 266 Batch 400 Loss 1.0341 Accuracy 0.7505\n",
            "Epoch 266 Batch 450 Loss 1.0354 Accuracy 0.7505\n",
            "discarded batch 480\n",
            "Epoch 266 Batch 500 Loss 1.0408 Accuracy 0.7497\n",
            "Epoch 266 Batch 550 Loss 1.0416 Accuracy 0.7495\n",
            "Epoch 266 Batch 600 Loss 1.0448 Accuracy 0.7491\n",
            "Epoch 266 Batch 650 Loss 1.0457 Accuracy 0.7488\n",
            "Epoch 266 Batch 700 Loss 1.0471 Accuracy 0.7489\n",
            "Epoch 266 Batch 750 Loss 1.0482 Accuracy 0.7487\n",
            "Epoch 266 Batch 800 Loss 1.0498 Accuracy 0.7485\n",
            "Epoch 266 Batch 850 Loss 1.0523 Accuracy 0.7482\n",
            "Epoch 266 Batch 900 Loss 1.0536 Accuracy 0.7478\n",
            "Epoch 266 Batch 950 Loss 1.0554 Accuracy 0.7474\n",
            "Epoch 266 Batch 1000 Loss 1.0570 Accuracy 0.7473\n",
            "Epoch 266 Batch 1050 Loss 1.0583 Accuracy 0.7470\n",
            "Epoch 266 Batch 1100 Loss 1.0596 Accuracy 0.7467\n",
            "Epoch 266 Loss 1.0604 Accuracy 0.7465\n",
            "Time taken for 1 epoch: 22.133415699005127 secs\n",
            "\n",
            "discarded batch 23\n",
            "Epoch 266 VALIDATION: Loss 2.7481 Accuracy 0.5679\n",
            "\n",
            "Epoch 267 Batch 0 Loss 0.9064 Accuracy 0.7789\n",
            "Epoch 267 Batch 50 Loss 1.0232 Accuracy 0.7498\n",
            "Epoch 267 Batch 100 Loss 1.0210 Accuracy 0.7518\n",
            "Epoch 267 Batch 150 Loss 1.0182 Accuracy 0.7524\n",
            "Epoch 267 Batch 200 Loss 1.0220 Accuracy 0.7515\n",
            "Epoch 267 Batch 250 Loss 1.0256 Accuracy 0.7512\n",
            "Epoch 267 Batch 300 Loss 1.0266 Accuracy 0.7508\n",
            "Epoch 267 Batch 350 Loss 1.0285 Accuracy 0.7504\n",
            "Epoch 267 Batch 400 Loss 1.0283 Accuracy 0.7502\n",
            "Epoch 267 Batch 450 Loss 1.0332 Accuracy 0.7500\n",
            "discarded batch 451\n",
            "Epoch 267 Batch 500 Loss 1.0349 Accuracy 0.7499\n",
            "Epoch 267 Batch 550 Loss 1.0393 Accuracy 0.7492\n",
            "Epoch 267 Batch 600 Loss 1.0420 Accuracy 0.7488\n",
            "Epoch 267 Batch 650 Loss 1.0438 Accuracy 0.7486\n",
            "Epoch 267 Batch 700 Loss 1.0465 Accuracy 0.7485\n",
            "Epoch 267 Batch 750 Loss 1.0499 Accuracy 0.7480\n",
            "Epoch 267 Batch 800 Loss 1.0511 Accuracy 0.7477\n",
            "Epoch 267 Batch 850 Loss 1.0535 Accuracy 0.7474\n",
            "Epoch 267 Batch 900 Loss 1.0546 Accuracy 0.7471\n",
            "Epoch 267 Batch 950 Loss 1.0564 Accuracy 0.7468\n",
            "Epoch 267 Batch 1000 Loss 1.0583 Accuracy 0.7464\n",
            "Epoch 267 Batch 1050 Loss 1.0598 Accuracy 0.7463\n",
            "Epoch 267 Batch 1100 Loss 1.0608 Accuracy 0.7462\n",
            "Epoch 267 Loss 1.0623 Accuracy 0.7460\n",
            "Time taken for 1 epoch: 22.446719646453857 secs\n",
            "\n",
            "Epoch 268 Batch 0 Loss 0.9638 Accuracy 0.7459\n",
            "Epoch 268 Batch 50 Loss 1.0230 Accuracy 0.7526\n",
            "Epoch 268 Batch 100 Loss 1.0277 Accuracy 0.7503\n",
            "Epoch 268 Batch 150 Loss 1.0254 Accuracy 0.7511\n",
            "Epoch 268 Batch 200 Loss 1.0293 Accuracy 0.7508\n",
            "Epoch 268 Batch 250 Loss 1.0340 Accuracy 0.7499\n",
            "Epoch 268 Batch 300 Loss 1.0336 Accuracy 0.7506\n",
            "Epoch 268 Batch 350 Loss 1.0345 Accuracy 0.7502\n",
            "Epoch 268 Batch 400 Loss 1.0384 Accuracy 0.7493\n",
            "Epoch 268 Batch 450 Loss 1.0413 Accuracy 0.7490\n",
            "Epoch 268 Batch 500 Loss 1.0425 Accuracy 0.7489\n",
            "Epoch 268 Batch 550 Loss 1.0426 Accuracy 0.7485\n",
            "Epoch 268 Batch 600 Loss 1.0450 Accuracy 0.7479\n",
            "Epoch 268 Batch 650 Loss 1.0458 Accuracy 0.7480\n",
            "Epoch 268 Batch 700 Loss 1.0468 Accuracy 0.7480\n",
            "discarded batch 729\n",
            "Epoch 268 Batch 750 Loss 1.0493 Accuracy 0.7477\n",
            "Epoch 268 Batch 800 Loss 1.0508 Accuracy 0.7474\n",
            "Epoch 268 Batch 850 Loss 1.0541 Accuracy 0.7471\n",
            "Epoch 268 Batch 900 Loss 1.0553 Accuracy 0.7468\n",
            "Epoch 268 Batch 950 Loss 1.0560 Accuracy 0.7467\n",
            "Epoch 268 Batch 1000 Loss 1.0575 Accuracy 0.7464\n",
            "Epoch 268 Batch 1050 Loss 1.0587 Accuracy 0.7463\n",
            "Epoch 268 Batch 1100 Loss 1.0596 Accuracy 0.7462\n",
            "Epoch 268 Loss 1.0605 Accuracy 0.7459\n",
            "Time taken for 1 epoch: 22.19872260093689 secs\n",
            "\n",
            "Epoch 269 Batch 0 Loss 1.0922 Accuracy 0.7261\n",
            "Epoch 269 Batch 50 Loss 1.0160 Accuracy 0.7554\n",
            "Epoch 269 Batch 100 Loss 1.0172 Accuracy 0.7543\n",
            "Epoch 269 Batch 150 Loss 1.0211 Accuracy 0.7535\n",
            "Epoch 269 Batch 200 Loss 1.0215 Accuracy 0.7535\n",
            "Epoch 269 Batch 250 Loss 1.0276 Accuracy 0.7517\n",
            "Epoch 269 Batch 300 Loss 1.0269 Accuracy 0.7516\n",
            "Epoch 269 Batch 350 Loss 1.0299 Accuracy 0.7511\n",
            "Epoch 269 Batch 400 Loss 1.0310 Accuracy 0.7511\n",
            "Epoch 269 Batch 450 Loss 1.0353 Accuracy 0.7505\n",
            "Epoch 269 Batch 500 Loss 1.0362 Accuracy 0.7502\n",
            "Epoch 269 Batch 550 Loss 1.0394 Accuracy 0.7502\n",
            "Epoch 269 Batch 600 Loss 1.0418 Accuracy 0.7496\n",
            "Epoch 269 Batch 650 Loss 1.0446 Accuracy 0.7492\n",
            "Epoch 269 Batch 700 Loss 1.0478 Accuracy 0.7489\n",
            "Epoch 269 Batch 750 Loss 1.0499 Accuracy 0.7484\n",
            "Epoch 269 Batch 800 Loss 1.0513 Accuracy 0.7481\n",
            "Epoch 269 Batch 850 Loss 1.0536 Accuracy 0.7477\n",
            "Epoch 269 Batch 900 Loss 1.0554 Accuracy 0.7472\n",
            "discarded batch 909\n",
            "Epoch 269 Batch 950 Loss 1.0570 Accuracy 0.7470\n",
            "Epoch 269 Batch 1000 Loss 1.0574 Accuracy 0.7468\n",
            "Epoch 269 Batch 1050 Loss 1.0597 Accuracy 0.7464\n",
            "Epoch 269 Batch 1100 Loss 1.0624 Accuracy 0.7459\n",
            "Epoch 269 Loss 1.0630 Accuracy 0.7459\n",
            "Time taken for 1 epoch: 22.164448976516724 secs\n",
            "\n",
            "Epoch 270 Batch 0 Loss 1.0690 Accuracy 0.7426\n",
            "Epoch 270 Batch 50 Loss 1.0227 Accuracy 0.7529\n",
            "Epoch 270 Batch 100 Loss 1.0238 Accuracy 0.7531\n",
            "Epoch 270 Batch 150 Loss 1.0198 Accuracy 0.7546\n",
            "Epoch 270 Batch 200 Loss 1.0231 Accuracy 0.7524\n",
            "Epoch 270 Batch 250 Loss 1.0222 Accuracy 0.7521\n",
            "discarded batch 282\n",
            "Epoch 270 Batch 300 Loss 1.0256 Accuracy 0.7517\n",
            "Epoch 270 Batch 350 Loss 1.0283 Accuracy 0.7514\n",
            "Epoch 270 Batch 400 Loss 1.0313 Accuracy 0.7505\n",
            "Epoch 270 Batch 450 Loss 1.0335 Accuracy 0.7497\n",
            "Epoch 270 Batch 500 Loss 1.0371 Accuracy 0.7489\n",
            "Epoch 270 Batch 550 Loss 1.0402 Accuracy 0.7486\n",
            "Epoch 270 Batch 600 Loss 1.0426 Accuracy 0.7485\n",
            "Epoch 270 Batch 650 Loss 1.0445 Accuracy 0.7483\n",
            "Epoch 270 Batch 700 Loss 1.0468 Accuracy 0.7479\n",
            "Epoch 270 Batch 750 Loss 1.0482 Accuracy 0.7475\n",
            "Epoch 270 Batch 800 Loss 1.0507 Accuracy 0.7472\n",
            "Epoch 270 Batch 850 Loss 1.0535 Accuracy 0.7467\n",
            "Epoch 270 Batch 900 Loss 1.0535 Accuracy 0.7467\n",
            "Epoch 270 Batch 950 Loss 1.0548 Accuracy 0.7465\n",
            "Epoch 270 Batch 1000 Loss 1.0570 Accuracy 0.7461\n",
            "Epoch 270 Batch 1050 Loss 1.0600 Accuracy 0.7457\n",
            "Epoch 270 Batch 1100 Loss 1.0608 Accuracy 0.7457\n",
            "Saving checkpoint for epoch 270 at ./checkpoints/train/ckpt-54\n",
            "Epoch 270 Loss 1.0605 Accuracy 0.7458\n",
            "Time taken for 1 epoch: 22.416801691055298 secs\n",
            "\n",
            "Epoch 271 Batch 0 Loss 1.0354 Accuracy 0.7228\n",
            "Epoch 271 Batch 50 Loss 1.0055 Accuracy 0.7573\n",
            "Epoch 271 Batch 100 Loss 1.0180 Accuracy 0.7547\n",
            "Epoch 271 Batch 150 Loss 1.0160 Accuracy 0.7556\n",
            "Epoch 271 Batch 200 Loss 1.0187 Accuracy 0.7555\n",
            "Epoch 271 Batch 250 Loss 1.0195 Accuracy 0.7541\n",
            "Epoch 271 Batch 300 Loss 1.0213 Accuracy 0.7531\n",
            "Epoch 271 Batch 350 Loss 1.0214 Accuracy 0.7531\n",
            "Epoch 271 Batch 400 Loss 1.0257 Accuracy 0.7522\n",
            "Epoch 271 Batch 450 Loss 1.0301 Accuracy 0.7515\n",
            "Epoch 271 Batch 500 Loss 1.0300 Accuracy 0.7512\n",
            "Epoch 271 Batch 550 Loss 1.0309 Accuracy 0.7507\n",
            "Epoch 271 Batch 600 Loss 1.0341 Accuracy 0.7499\n",
            "Epoch 271 Batch 650 Loss 1.0361 Accuracy 0.7494\n",
            "Epoch 271 Batch 700 Loss 1.0385 Accuracy 0.7493\n",
            "Epoch 271 Batch 750 Loss 1.0417 Accuracy 0.7488\n",
            "Epoch 271 Batch 800 Loss 1.0431 Accuracy 0.7484\n",
            "Epoch 271 Batch 850 Loss 1.0461 Accuracy 0.7480\n",
            "Epoch 271 Batch 900 Loss 1.0489 Accuracy 0.7477\n",
            "Epoch 271 Batch 950 Loss 1.0515 Accuracy 0.7472\n",
            "Epoch 271 Batch 1000 Loss 1.0531 Accuracy 0.7469\n",
            "Epoch 271 Batch 1050 Loss 1.0550 Accuracy 0.7468\n",
            "discarded batch 1066\n",
            "Epoch 271 Batch 1100 Loss 1.0572 Accuracy 0.7464\n",
            "Epoch 271 Loss 1.0583 Accuracy 0.7462\n",
            "Time taken for 1 epoch: 22.24789547920227 secs\n",
            "\n",
            "discarded batch 23\n",
            "Epoch 271 VALIDATION: Loss 2.7792 Accuracy 0.5665\n",
            "\n",
            "Epoch 272 Batch 0 Loss 0.9859 Accuracy 0.7525\n",
            "Epoch 272 Batch 50 Loss 1.0150 Accuracy 0.7510\n",
            "Epoch 272 Batch 100 Loss 1.0105 Accuracy 0.7529\n",
            "Epoch 272 Batch 150 Loss 1.0166 Accuracy 0.7527\n",
            "Epoch 272 Batch 200 Loss 1.0179 Accuracy 0.7528\n",
            "discarded batch 208\n",
            "Epoch 272 Batch 250 Loss 1.0183 Accuracy 0.7527\n",
            "Epoch 272 Batch 300 Loss 1.0197 Accuracy 0.7520\n",
            "Epoch 272 Batch 350 Loss 1.0262 Accuracy 0.7505\n",
            "Epoch 272 Batch 400 Loss 1.0286 Accuracy 0.7500\n",
            "Epoch 272 Batch 450 Loss 1.0279 Accuracy 0.7503\n",
            "Epoch 272 Batch 500 Loss 1.0309 Accuracy 0.7499\n",
            "Epoch 272 Batch 550 Loss 1.0350 Accuracy 0.7497\n",
            "Epoch 272 Batch 600 Loss 1.0367 Accuracy 0.7493\n",
            "Epoch 272 Batch 650 Loss 1.0381 Accuracy 0.7491\n",
            "Epoch 272 Batch 700 Loss 1.0405 Accuracy 0.7487\n",
            "Epoch 272 Batch 750 Loss 1.0433 Accuracy 0.7483\n",
            "Epoch 272 Batch 800 Loss 1.0444 Accuracy 0.7483\n",
            "Epoch 272 Batch 850 Loss 1.0466 Accuracy 0.7480\n",
            "Epoch 272 Batch 900 Loss 1.0469 Accuracy 0.7479\n",
            "Epoch 272 Batch 950 Loss 1.0494 Accuracy 0.7476\n",
            "Epoch 272 Batch 1000 Loss 1.0508 Accuracy 0.7474\n",
            "Epoch 272 Batch 1050 Loss 1.0525 Accuracy 0.7470\n",
            "Epoch 272 Batch 1100 Loss 1.0547 Accuracy 0.7468\n",
            "Epoch 272 Loss 1.0556 Accuracy 0.7466\n",
            "Time taken for 1 epoch: 22.103506326675415 secs\n",
            "\n",
            "Epoch 273 Batch 0 Loss 1.0328 Accuracy 0.7459\n",
            "Epoch 273 Batch 50 Loss 1.0052 Accuracy 0.7533\n",
            "Epoch 273 Batch 100 Loss 1.0179 Accuracy 0.7521\n",
            "Epoch 273 Batch 150 Loss 1.0226 Accuracy 0.7523\n",
            "Epoch 273 Batch 200 Loss 1.0212 Accuracy 0.7526\n",
            "Epoch 273 Batch 250 Loss 1.0242 Accuracy 0.7518\n",
            "Epoch 273 Batch 300 Loss 1.0248 Accuracy 0.7519\n",
            "Epoch 273 Batch 350 Loss 1.0270 Accuracy 0.7514\n",
            "Epoch 273 Batch 400 Loss 1.0277 Accuracy 0.7507\n",
            "Epoch 273 Batch 450 Loss 1.0294 Accuracy 0.7507\n",
            "Epoch 273 Batch 500 Loss 1.0333 Accuracy 0.7502\n",
            "Epoch 273 Batch 550 Loss 1.0346 Accuracy 0.7499\n",
            "Epoch 273 Batch 600 Loss 1.0369 Accuracy 0.7495\n",
            "Epoch 273 Batch 650 Loss 1.0373 Accuracy 0.7493\n",
            "Epoch 273 Batch 700 Loss 1.0398 Accuracy 0.7490\n",
            "Epoch 273 Batch 750 Loss 1.0407 Accuracy 0.7489\n",
            "Epoch 273 Batch 800 Loss 1.0434 Accuracy 0.7485\n",
            "Epoch 273 Batch 850 Loss 1.0461 Accuracy 0.7479\n",
            "Epoch 273 Batch 900 Loss 1.0493 Accuracy 0.7474\n",
            "Epoch 273 Batch 950 Loss 1.0488 Accuracy 0.7475\n",
            "discarded batch 993\n",
            "Epoch 273 Batch 1000 Loss 1.0499 Accuracy 0.7472\n",
            "Epoch 273 Batch 1050 Loss 1.0527 Accuracy 0.7468\n",
            "Epoch 273 Batch 1100 Loss 1.0543 Accuracy 0.7466\n",
            "Epoch 273 Loss 1.0553 Accuracy 0.7465\n",
            "Time taken for 1 epoch: 22.098583936691284 secs\n",
            "\n",
            "Epoch 274 Batch 0 Loss 0.9560 Accuracy 0.7657\n",
            "Epoch 274 Batch 50 Loss 0.9863 Accuracy 0.7613\n",
            "Epoch 274 Batch 100 Loss 0.9949 Accuracy 0.7585\n",
            "Epoch 274 Batch 150 Loss 1.0003 Accuracy 0.7563\n",
            "Epoch 274 Batch 200 Loss 1.0073 Accuracy 0.7543\n",
            "Epoch 274 Batch 250 Loss 1.0087 Accuracy 0.7541\n",
            "Epoch 274 Batch 300 Loss 1.0165 Accuracy 0.7528\n",
            "Epoch 274 Batch 350 Loss 1.0195 Accuracy 0.7519\n",
            "Epoch 274 Batch 400 Loss 1.0224 Accuracy 0.7513\n",
            "Epoch 274 Batch 450 Loss 1.0255 Accuracy 0.7506\n",
            "Epoch 274 Batch 500 Loss 1.0290 Accuracy 0.7500\n",
            "Epoch 274 Batch 550 Loss 1.0293 Accuracy 0.7497\n",
            "Epoch 274 Batch 600 Loss 1.0323 Accuracy 0.7493\n",
            "discarded batch 614\n",
            "Epoch 274 Batch 650 Loss 1.0340 Accuracy 0.7491\n",
            "Epoch 274 Batch 700 Loss 1.0355 Accuracy 0.7489\n",
            "Epoch 274 Batch 750 Loss 1.0381 Accuracy 0.7487\n",
            "Epoch 274 Batch 800 Loss 1.0399 Accuracy 0.7482\n",
            "Epoch 274 Batch 850 Loss 1.0428 Accuracy 0.7477\n",
            "Epoch 274 Batch 900 Loss 1.0456 Accuracy 0.7474\n",
            "Epoch 274 Batch 950 Loss 1.0469 Accuracy 0.7473\n",
            "Epoch 274 Batch 1000 Loss 1.0491 Accuracy 0.7471\n",
            "Epoch 274 Batch 1050 Loss 1.0516 Accuracy 0.7468\n",
            "Epoch 274 Batch 1100 Loss 1.0541 Accuracy 0.7463\n",
            "Epoch 274 Loss 1.0544 Accuracy 0.7463\n",
            "Time taken for 1 epoch: 22.078413009643555 secs\n",
            "\n",
            "Epoch 275 Batch 0 Loss 0.9787 Accuracy 0.7822\n",
            "Epoch 275 Batch 50 Loss 0.9977 Accuracy 0.7553\n",
            "Epoch 275 Batch 100 Loss 1.0043 Accuracy 0.7543\n",
            "Epoch 275 Batch 150 Loss 1.0068 Accuracy 0.7549\n",
            "Epoch 275 Batch 200 Loss 1.0162 Accuracy 0.7535\n",
            "Epoch 275 Batch 250 Loss 1.0167 Accuracy 0.7536\n",
            "Epoch 275 Batch 300 Loss 1.0206 Accuracy 0.7527\n",
            "Epoch 275 Batch 350 Loss 1.0189 Accuracy 0.7530\n",
            "Epoch 275 Batch 400 Loss 1.0228 Accuracy 0.7520\n",
            "Epoch 275 Batch 450 Loss 1.0250 Accuracy 0.7511\n",
            "discarded batch 472\n",
            "Epoch 275 Batch 500 Loss 1.0283 Accuracy 0.7509\n",
            "Epoch 275 Batch 550 Loss 1.0310 Accuracy 0.7503\n",
            "Epoch 275 Batch 600 Loss 1.0325 Accuracy 0.7502\n",
            "Epoch 275 Batch 650 Loss 1.0340 Accuracy 0.7496\n",
            "Epoch 275 Batch 700 Loss 1.0365 Accuracy 0.7492\n",
            "Epoch 275 Batch 750 Loss 1.0384 Accuracy 0.7488\n",
            "Epoch 275 Batch 800 Loss 1.0413 Accuracy 0.7483\n",
            "Epoch 275 Batch 850 Loss 1.0437 Accuracy 0.7479\n",
            "Epoch 275 Batch 900 Loss 1.0456 Accuracy 0.7476\n",
            "Epoch 275 Batch 950 Loss 1.0472 Accuracy 0.7473\n",
            "Epoch 275 Batch 1000 Loss 1.0482 Accuracy 0.7471\n",
            "Epoch 275 Batch 1050 Loss 1.0501 Accuracy 0.7469\n",
            "Epoch 275 Batch 1100 Loss 1.0521 Accuracy 0.7467\n",
            "Saving checkpoint for epoch 275 at ./checkpoints/train/ckpt-55\n",
            "Epoch 275 Loss 1.0535 Accuracy 0.7465\n",
            "Time taken for 1 epoch: 22.358827114105225 secs\n",
            "\n",
            "Epoch 276 Batch 0 Loss 0.9563 Accuracy 0.7624\n",
            "Epoch 276 Batch 50 Loss 1.0015 Accuracy 0.7608\n",
            "Epoch 276 Batch 100 Loss 1.0091 Accuracy 0.7573\n",
            "Epoch 276 Batch 150 Loss 1.0158 Accuracy 0.7552\n",
            "Epoch 276 Batch 200 Loss 1.0200 Accuracy 0.7534\n",
            "Epoch 276 Batch 250 Loss 1.0201 Accuracy 0.7538\n",
            "Epoch 276 Batch 300 Loss 1.0217 Accuracy 0.7533\n",
            "Epoch 276 Batch 350 Loss 1.0219 Accuracy 0.7534\n",
            "Epoch 276 Batch 400 Loss 1.0241 Accuracy 0.7529\n",
            "Epoch 276 Batch 450 Loss 1.0270 Accuracy 0.7523\n",
            "Epoch 276 Batch 500 Loss 1.0305 Accuracy 0.7517\n",
            "Epoch 276 Batch 550 Loss 1.0308 Accuracy 0.7512\n",
            "Epoch 276 Batch 600 Loss 1.0338 Accuracy 0.7506\n",
            "Epoch 276 Batch 650 Loss 1.0361 Accuracy 0.7501\n",
            "Epoch 276 Batch 700 Loss 1.0368 Accuracy 0.7498\n",
            "Epoch 276 Batch 750 Loss 1.0396 Accuracy 0.7495\n",
            "Epoch 276 Batch 800 Loss 1.0408 Accuracy 0.7494\n",
            "Epoch 276 Batch 850 Loss 1.0440 Accuracy 0.7489\n",
            "discarded batch 890\n",
            "Epoch 276 Batch 900 Loss 1.0461 Accuracy 0.7487\n",
            "Epoch 276 Batch 950 Loss 1.0482 Accuracy 0.7481\n",
            "Epoch 276 Batch 1000 Loss 1.0493 Accuracy 0.7480\n",
            "Epoch 276 Batch 1050 Loss 1.0519 Accuracy 0.7475\n",
            "Epoch 276 Batch 1100 Loss 1.0529 Accuracy 0.7475\n",
            "Epoch 276 Loss 1.0539 Accuracy 0.7472\n",
            "Time taken for 1 epoch: 22.19545030593872 secs\n",
            "\n",
            "discarded batch 23\n",
            "Epoch 276 VALIDATION: Loss 2.7693 Accuracy 0.5720\n",
            "\n",
            "Epoch 277 Batch 0 Loss 1.0110 Accuracy 0.7492\n",
            "Epoch 277 Batch 50 Loss 0.9751 Accuracy 0.7610\n",
            "Epoch 277 Batch 100 Loss 0.9943 Accuracy 0.7571\n",
            "Epoch 277 Batch 150 Loss 0.9996 Accuracy 0.7557\n",
            "Epoch 277 Batch 200 Loss 1.0039 Accuracy 0.7549\n",
            "Epoch 277 Batch 250 Loss 1.0087 Accuracy 0.7544\n",
            "Epoch 277 Batch 300 Loss 1.0122 Accuracy 0.7547\n",
            "Epoch 277 Batch 350 Loss 1.0170 Accuracy 0.7541\n",
            "Epoch 277 Batch 400 Loss 1.0196 Accuracy 0.7532\n",
            "Epoch 277 Batch 450 Loss 1.0218 Accuracy 0.7527\n",
            "Epoch 277 Batch 500 Loss 1.0250 Accuracy 0.7520\n",
            "Epoch 277 Batch 550 Loss 1.0268 Accuracy 0.7517\n",
            "discarded batch 576\n",
            "Epoch 277 Batch 600 Loss 1.0273 Accuracy 0.7514\n",
            "Epoch 277 Batch 650 Loss 1.0291 Accuracy 0.7513\n",
            "Epoch 277 Batch 700 Loss 1.0306 Accuracy 0.7511\n",
            "Epoch 277 Batch 750 Loss 1.0340 Accuracy 0.7505\n",
            "Epoch 277 Batch 800 Loss 1.0355 Accuracy 0.7503\n",
            "Epoch 277 Batch 850 Loss 1.0383 Accuracy 0.7498\n",
            "Epoch 277 Batch 900 Loss 1.0407 Accuracy 0.7493\n",
            "Epoch 277 Batch 950 Loss 1.0420 Accuracy 0.7490\n",
            "Epoch 277 Batch 1000 Loss 1.0441 Accuracy 0.7488\n",
            "Epoch 277 Batch 1050 Loss 1.0462 Accuracy 0.7485\n",
            "Epoch 277 Batch 1100 Loss 1.0489 Accuracy 0.7481\n",
            "Epoch 277 Loss 1.0502 Accuracy 0.7481\n",
            "Time taken for 1 epoch: 22.170787572860718 secs\n",
            "\n",
            "Epoch 278 Batch 0 Loss 0.9266 Accuracy 0.7954\n",
            "Epoch 278 Batch 50 Loss 1.0114 Accuracy 0.7528\n",
            "Epoch 278 Batch 100 Loss 1.0083 Accuracy 0.7539\n",
            "Epoch 278 Batch 150 Loss 1.0126 Accuracy 0.7536\n",
            "Epoch 278 Batch 200 Loss 1.0125 Accuracy 0.7544\n",
            "Epoch 278 Batch 250 Loss 1.0131 Accuracy 0.7542\n",
            "Epoch 278 Batch 300 Loss 1.0137 Accuracy 0.7540\n",
            "Epoch 278 Batch 350 Loss 1.0171 Accuracy 0.7532\n",
            "Epoch 278 Batch 400 Loss 1.0198 Accuracy 0.7523\n",
            "Epoch 278 Batch 450 Loss 1.0221 Accuracy 0.7518\n",
            "Epoch 278 Batch 500 Loss 1.0243 Accuracy 0.7515\n",
            "Epoch 278 Batch 550 Loss 1.0269 Accuracy 0.7511\n",
            "Epoch 278 Batch 600 Loss 1.0309 Accuracy 0.7508\n",
            "Epoch 278 Batch 650 Loss 1.0334 Accuracy 0.7506\n",
            "Epoch 278 Batch 700 Loss 1.0355 Accuracy 0.7504\n",
            "Epoch 278 Batch 750 Loss 1.0369 Accuracy 0.7501\n",
            "Epoch 278 Batch 800 Loss 1.0396 Accuracy 0.7496\n",
            "discarded batch 848\n",
            "Epoch 278 Batch 850 Loss 1.0414 Accuracy 0.7492\n",
            "Epoch 278 Batch 900 Loss 1.0428 Accuracy 0.7490\n",
            "Epoch 278 Batch 950 Loss 1.0447 Accuracy 0.7487\n",
            "Epoch 278 Batch 1000 Loss 1.0473 Accuracy 0.7482\n",
            "Epoch 278 Batch 1050 Loss 1.0487 Accuracy 0.7478\n",
            "Epoch 278 Batch 1100 Loss 1.0501 Accuracy 0.7476\n",
            "Epoch 278 Loss 1.0510 Accuracy 0.7473\n",
            "Time taken for 1 epoch: 22.17980980873108 secs\n",
            "\n",
            "Epoch 279 Batch 0 Loss 1.0665 Accuracy 0.7690\n",
            "Epoch 279 Batch 50 Loss 0.9988 Accuracy 0.7578\n",
            "Epoch 279 Batch 100 Loss 0.9961 Accuracy 0.7575\n",
            "Epoch 279 Batch 150 Loss 1.0063 Accuracy 0.7560\n",
            "Epoch 279 Batch 200 Loss 1.0093 Accuracy 0.7559\n",
            "Epoch 279 Batch 250 Loss 1.0131 Accuracy 0.7552\n",
            "Epoch 279 Batch 300 Loss 1.0186 Accuracy 0.7546\n",
            "Epoch 279 Batch 350 Loss 1.0264 Accuracy 0.7526\n",
            "Epoch 279 Batch 400 Loss 1.0276 Accuracy 0.7526\n",
            "discarded batch 428\n",
            "Epoch 279 Batch 450 Loss 1.0278 Accuracy 0.7523\n",
            "Epoch 279 Batch 500 Loss 1.0296 Accuracy 0.7518\n",
            "Epoch 279 Batch 550 Loss 1.0305 Accuracy 0.7516\n",
            "Epoch 279 Batch 600 Loss 1.0325 Accuracy 0.7512\n",
            "Epoch 279 Batch 650 Loss 1.0338 Accuracy 0.7507\n",
            "Epoch 279 Batch 700 Loss 1.0360 Accuracy 0.7503\n",
            "Epoch 279 Batch 750 Loss 1.0389 Accuracy 0.7495\n",
            "Epoch 279 Batch 800 Loss 1.0408 Accuracy 0.7490\n",
            "Epoch 279 Batch 850 Loss 1.0418 Accuracy 0.7488\n",
            "Epoch 279 Batch 900 Loss 1.0432 Accuracy 0.7485\n",
            "Epoch 279 Batch 950 Loss 1.0441 Accuracy 0.7485\n",
            "Epoch 279 Batch 1000 Loss 1.0463 Accuracy 0.7482\n",
            "Epoch 279 Batch 1050 Loss 1.0476 Accuracy 0.7478\n",
            "Epoch 279 Batch 1100 Loss 1.0491 Accuracy 0.7475\n",
            "Epoch 279 Loss 1.0501 Accuracy 0.7474\n",
            "Time taken for 1 epoch: 22.24302077293396 secs\n",
            "\n",
            "Epoch 280 Batch 0 Loss 1.0806 Accuracy 0.7360\n",
            "Epoch 280 Batch 50 Loss 1.0151 Accuracy 0.7534\n",
            "Epoch 280 Batch 100 Loss 1.0119 Accuracy 0.7535\n",
            "Epoch 280 Batch 150 Loss 1.0155 Accuracy 0.7531\n",
            "Epoch 280 Batch 200 Loss 1.0204 Accuracy 0.7521\n",
            "Epoch 280 Batch 250 Loss 1.0187 Accuracy 0.7526\n",
            "Epoch 280 Batch 300 Loss 1.0238 Accuracy 0.7519\n",
            "Epoch 280 Batch 350 Loss 1.0232 Accuracy 0.7514\n",
            "Epoch 280 Batch 400 Loss 1.0223 Accuracy 0.7515\n",
            "Epoch 280 Batch 450 Loss 1.0270 Accuracy 0.7508\n",
            "Epoch 280 Batch 500 Loss 1.0299 Accuracy 0.7501\n",
            "Epoch 280 Batch 550 Loss 1.0316 Accuracy 0.7498\n",
            "discarded batch 591\n",
            "Epoch 280 Batch 600 Loss 1.0325 Accuracy 0.7496\n",
            "Epoch 280 Batch 650 Loss 1.0345 Accuracy 0.7496\n",
            "Epoch 280 Batch 700 Loss 1.0368 Accuracy 0.7490\n",
            "Epoch 280 Batch 750 Loss 1.0398 Accuracy 0.7486\n",
            "Epoch 280 Batch 800 Loss 1.0420 Accuracy 0.7482\n",
            "Epoch 280 Batch 850 Loss 1.0446 Accuracy 0.7479\n",
            "Epoch 280 Batch 900 Loss 1.0457 Accuracy 0.7479\n",
            "Epoch 280 Batch 950 Loss 1.0468 Accuracy 0.7477\n",
            "Epoch 280 Batch 1000 Loss 1.0477 Accuracy 0.7476\n",
            "Epoch 280 Batch 1050 Loss 1.0505 Accuracy 0.7473\n",
            "Epoch 280 Batch 1100 Loss 1.0521 Accuracy 0.7471\n",
            "Saving checkpoint for epoch 280 at ./checkpoints/train/ckpt-56\n",
            "Epoch 280 Loss 1.0530 Accuracy 0.7470\n",
            "Time taken for 1 epoch: 22.33090305328369 secs\n",
            "\n",
            "Epoch 281 Batch 0 Loss 0.9899 Accuracy 0.7657\n",
            "Epoch 281 Batch 50 Loss 1.0064 Accuracy 0.7560\n",
            "Epoch 281 Batch 100 Loss 1.0008 Accuracy 0.7570\n",
            "Epoch 281 Batch 150 Loss 1.0093 Accuracy 0.7551\n",
            "Epoch 281 Batch 200 Loss 1.0127 Accuracy 0.7550\n",
            "Epoch 281 Batch 250 Loss 1.0173 Accuracy 0.7537\n",
            "Epoch 281 Batch 300 Loss 1.0220 Accuracy 0.7529\n",
            "Epoch 281 Batch 350 Loss 1.0217 Accuracy 0.7528\n",
            "Epoch 281 Batch 400 Loss 1.0238 Accuracy 0.7523\n",
            "discarded batch 404\n",
            "Epoch 281 Batch 450 Loss 1.0258 Accuracy 0.7520\n",
            "Epoch 281 Batch 500 Loss 1.0291 Accuracy 0.7513\n",
            "Epoch 281 Batch 550 Loss 1.0300 Accuracy 0.7512\n",
            "Epoch 281 Batch 600 Loss 1.0335 Accuracy 0.7507\n",
            "Epoch 281 Batch 650 Loss 1.0347 Accuracy 0.7506\n",
            "Epoch 281 Batch 700 Loss 1.0372 Accuracy 0.7502\n",
            "Epoch 281 Batch 750 Loss 1.0384 Accuracy 0.7499\n",
            "Epoch 281 Batch 800 Loss 1.0396 Accuracy 0.7496\n",
            "Epoch 281 Batch 850 Loss 1.0401 Accuracy 0.7494\n",
            "Epoch 281 Batch 900 Loss 1.0419 Accuracy 0.7491\n",
            "Epoch 281 Batch 950 Loss 1.0431 Accuracy 0.7487\n",
            "Epoch 281 Batch 1000 Loss 1.0460 Accuracy 0.7482\n",
            "Epoch 281 Batch 1050 Loss 1.0478 Accuracy 0.7479\n",
            "Epoch 281 Batch 1100 Loss 1.0495 Accuracy 0.7478\n",
            "Epoch 281 Loss 1.0501 Accuracy 0.7478\n",
            "Time taken for 1 epoch: 22.43509602546692 secs\n",
            "\n",
            "discarded batch 23\n",
            "Epoch 281 VALIDATION: Loss 2.7606 Accuracy 0.5708\n",
            "\n",
            "Epoch 282 Batch 0 Loss 1.0127 Accuracy 0.7789\n",
            "Epoch 282 Batch 50 Loss 1.0003 Accuracy 0.7578\n",
            "Epoch 282 Batch 100 Loss 1.0001 Accuracy 0.7563\n",
            "Epoch 282 Batch 150 Loss 1.0027 Accuracy 0.7563\n",
            "Epoch 282 Batch 200 Loss 1.0066 Accuracy 0.7550\n",
            "Epoch 282 Batch 250 Loss 1.0104 Accuracy 0.7545\n",
            "Epoch 282 Batch 300 Loss 1.0139 Accuracy 0.7539\n",
            "Epoch 282 Batch 350 Loss 1.0157 Accuracy 0.7541\n",
            "Epoch 282 Batch 400 Loss 1.0214 Accuracy 0.7533\n",
            "Epoch 282 Batch 450 Loss 1.0231 Accuracy 0.7532\n",
            "Epoch 282 Batch 500 Loss 1.0245 Accuracy 0.7527\n",
            "Epoch 282 Batch 550 Loss 1.0282 Accuracy 0.7518\n",
            "discarded batch 554\n",
            "Epoch 282 Batch 600 Loss 1.0308 Accuracy 0.7515\n",
            "Epoch 282 Batch 650 Loss 1.0315 Accuracy 0.7513\n",
            "Epoch 282 Batch 700 Loss 1.0328 Accuracy 0.7511\n",
            "Epoch 282 Batch 750 Loss 1.0356 Accuracy 0.7507\n",
            "Epoch 282 Batch 800 Loss 1.0374 Accuracy 0.7503\n",
            "Epoch 282 Batch 850 Loss 1.0384 Accuracy 0.7501\n",
            "Epoch 282 Batch 900 Loss 1.0403 Accuracy 0.7498\n",
            "Epoch 282 Batch 950 Loss 1.0419 Accuracy 0.7494\n",
            "Epoch 282 Batch 1000 Loss 1.0437 Accuracy 0.7492\n",
            "Epoch 282 Batch 1050 Loss 1.0449 Accuracy 0.7490\n",
            "Epoch 282 Batch 1100 Loss 1.0459 Accuracy 0.7489\n",
            "Epoch 282 Loss 1.0470 Accuracy 0.7488\n",
            "Time taken for 1 epoch: 22.206281661987305 secs\n",
            "\n",
            "Epoch 283 Batch 0 Loss 0.9319 Accuracy 0.7690\n",
            "Epoch 283 Batch 50 Loss 1.0040 Accuracy 0.7584\n",
            "Epoch 283 Batch 100 Loss 1.0022 Accuracy 0.7567\n",
            "Epoch 283 Batch 150 Loss 1.0050 Accuracy 0.7569\n",
            "Epoch 283 Batch 200 Loss 1.0090 Accuracy 0.7561\n",
            "Epoch 283 Batch 250 Loss 1.0134 Accuracy 0.7556\n",
            "Epoch 283 Batch 300 Loss 1.0156 Accuracy 0.7551\n",
            "Epoch 283 Batch 350 Loss 1.0186 Accuracy 0.7536\n",
            "Epoch 283 Batch 400 Loss 1.0223 Accuracy 0.7525\n",
            "Epoch 283 Batch 450 Loss 1.0244 Accuracy 0.7521\n",
            "Epoch 283 Batch 500 Loss 1.0277 Accuracy 0.7516\n",
            "Epoch 283 Batch 550 Loss 1.0287 Accuracy 0.7515\n",
            "Epoch 283 Batch 600 Loss 1.0308 Accuracy 0.7511\n",
            "Epoch 283 Batch 650 Loss 1.0330 Accuracy 0.7509\n",
            "Epoch 283 Batch 700 Loss 1.0347 Accuracy 0.7503\n",
            "Epoch 283 Batch 750 Loss 1.0364 Accuracy 0.7500\n",
            "Epoch 283 Batch 800 Loss 1.0372 Accuracy 0.7502\n",
            "discarded batch 830\n",
            "Epoch 283 Batch 850 Loss 1.0393 Accuracy 0.7497\n",
            "Epoch 283 Batch 900 Loss 1.0401 Accuracy 0.7496\n",
            "Epoch 283 Batch 950 Loss 1.0418 Accuracy 0.7492\n",
            "Epoch 283 Batch 1000 Loss 1.0431 Accuracy 0.7489\n",
            "Epoch 283 Batch 1050 Loss 1.0462 Accuracy 0.7484\n",
            "Epoch 283 Batch 1100 Loss 1.0470 Accuracy 0.7482\n",
            "Epoch 283 Loss 1.0481 Accuracy 0.7481\n",
            "Time taken for 1 epoch: 22.11905002593994 secs\n",
            "\n",
            "Epoch 284 Batch 0 Loss 1.1621 Accuracy 0.7261\n",
            "Epoch 284 Batch 50 Loss 0.9870 Accuracy 0.7574\n",
            "Epoch 284 Batch 100 Loss 0.9987 Accuracy 0.7551\n",
            "Epoch 284 Batch 150 Loss 1.0024 Accuracy 0.7559\n",
            "Epoch 284 Batch 200 Loss 1.0028 Accuracy 0.7556\n",
            "Epoch 284 Batch 250 Loss 1.0054 Accuracy 0.7554\n",
            "Epoch 284 Batch 300 Loss 1.0083 Accuracy 0.7546\n",
            "Epoch 284 Batch 350 Loss 1.0122 Accuracy 0.7543\n",
            "Epoch 284 Batch 400 Loss 1.0176 Accuracy 0.7533\n",
            "Epoch 284 Batch 450 Loss 1.0207 Accuracy 0.7529\n",
            "Epoch 284 Batch 500 Loss 1.0249 Accuracy 0.7520\n",
            "Epoch 284 Batch 550 Loss 1.0263 Accuracy 0.7518\n",
            "Epoch 284 Batch 600 Loss 1.0284 Accuracy 0.7513\n",
            "discarded batch 650\n",
            "Epoch 284 Batch 700 Loss 1.0318 Accuracy 0.7508\n",
            "Epoch 284 Batch 750 Loss 1.0330 Accuracy 0.7506\n",
            "Epoch 284 Batch 800 Loss 1.0340 Accuracy 0.7502\n",
            "Epoch 284 Batch 850 Loss 1.0385 Accuracy 0.7494\n",
            "Epoch 284 Batch 900 Loss 1.0404 Accuracy 0.7491\n",
            "Epoch 284 Batch 950 Loss 1.0428 Accuracy 0.7485\n",
            "Epoch 284 Batch 1000 Loss 1.0439 Accuracy 0.7482\n",
            "Epoch 284 Batch 1050 Loss 1.0452 Accuracy 0.7482\n",
            "Epoch 284 Batch 1100 Loss 1.0473 Accuracy 0.7478\n",
            "Epoch 284 Loss 1.0481 Accuracy 0.7477\n",
            "Time taken for 1 epoch: 22.148167371749878 secs\n",
            "\n",
            "Epoch 285 Batch 0 Loss 1.0681 Accuracy 0.7756\n",
            "Epoch 285 Batch 50 Loss 0.9866 Accuracy 0.7604\n",
            "Epoch 285 Batch 100 Loss 0.9829 Accuracy 0.7601\n",
            "Epoch 285 Batch 150 Loss 0.9906 Accuracy 0.7587\n",
            "Epoch 285 Batch 200 Loss 0.9979 Accuracy 0.7573\n",
            "Epoch 285 Batch 250 Loss 1.0054 Accuracy 0.7561\n",
            "Epoch 285 Batch 300 Loss 1.0075 Accuracy 0.7557\n",
            "Epoch 285 Batch 350 Loss 1.0114 Accuracy 0.7546\n",
            "Epoch 285 Batch 400 Loss 1.0156 Accuracy 0.7540\n",
            "Epoch 285 Batch 450 Loss 1.0188 Accuracy 0.7537\n",
            "Epoch 285 Batch 500 Loss 1.0227 Accuracy 0.7530\n",
            "Epoch 285 Batch 550 Loss 1.0245 Accuracy 0.7529\n",
            "Epoch 285 Batch 600 Loss 1.0276 Accuracy 0.7521\n",
            "Epoch 285 Batch 650 Loss 1.0290 Accuracy 0.7516\n",
            "Epoch 285 Batch 700 Loss 1.0307 Accuracy 0.7513\n",
            "Epoch 285 Batch 750 Loss 1.0333 Accuracy 0.7506\n",
            "Epoch 285 Batch 800 Loss 1.0351 Accuracy 0.7502\n",
            "discarded batch 820\n",
            "Epoch 285 Batch 850 Loss 1.0377 Accuracy 0.7497\n",
            "Epoch 285 Batch 900 Loss 1.0396 Accuracy 0.7494\n",
            "Epoch 285 Batch 950 Loss 1.0412 Accuracy 0.7491\n",
            "Epoch 285 Batch 1000 Loss 1.0425 Accuracy 0.7490\n",
            "Epoch 285 Batch 1050 Loss 1.0445 Accuracy 0.7487\n",
            "Epoch 285 Batch 1100 Loss 1.0461 Accuracy 0.7485\n",
            "Saving checkpoint for epoch 285 at ./checkpoints/train/ckpt-57\n",
            "Epoch 285 Loss 1.0467 Accuracy 0.7483\n",
            "Time taken for 1 epoch: 22.398956537246704 secs\n",
            "\n",
            "Epoch 286 Batch 0 Loss 1.0544 Accuracy 0.7360\n",
            "Epoch 286 Batch 50 Loss 0.9965 Accuracy 0.7553\n",
            "Epoch 286 Batch 100 Loss 1.0022 Accuracy 0.7538\n",
            "Epoch 286 Batch 150 Loss 1.0004 Accuracy 0.7552\n",
            "Epoch 286 Batch 200 Loss 1.0058 Accuracy 0.7547\n",
            "Epoch 286 Batch 250 Loss 1.0158 Accuracy 0.7529\n",
            "Epoch 286 Batch 300 Loss 1.0164 Accuracy 0.7529\n",
            "Epoch 286 Batch 350 Loss 1.0201 Accuracy 0.7527\n",
            "Epoch 286 Batch 400 Loss 1.0230 Accuracy 0.7518\n",
            "Epoch 286 Batch 450 Loss 1.0230 Accuracy 0.7518\n",
            "Epoch 286 Batch 500 Loss 1.0275 Accuracy 0.7514\n",
            "discarded batch 549\n",
            "Epoch 286 Batch 550 Loss 1.0289 Accuracy 0.7507\n",
            "Epoch 286 Batch 600 Loss 1.0299 Accuracy 0.7506\n",
            "Epoch 286 Batch 650 Loss 1.0325 Accuracy 0.7500\n",
            "Epoch 286 Batch 700 Loss 1.0333 Accuracy 0.7501\n",
            "Epoch 286 Batch 750 Loss 1.0344 Accuracy 0.7500\n",
            "Epoch 286 Batch 800 Loss 1.0352 Accuracy 0.7500\n",
            "Epoch 286 Batch 850 Loss 1.0376 Accuracy 0.7496\n",
            "Epoch 286 Batch 900 Loss 1.0388 Accuracy 0.7495\n",
            "Epoch 286 Batch 950 Loss 1.0412 Accuracy 0.7492\n",
            "Epoch 286 Batch 1000 Loss 1.0426 Accuracy 0.7491\n",
            "Epoch 286 Batch 1050 Loss 1.0447 Accuracy 0.7487\n",
            "Epoch 286 Batch 1100 Loss 1.0467 Accuracy 0.7482\n",
            "Epoch 286 Loss 1.0469 Accuracy 0.7481\n",
            "Time taken for 1 epoch: 22.15407657623291 secs\n",
            "\n",
            "discarded batch 23\n",
            "Epoch 286 VALIDATION: Loss 2.7587 Accuracy 0.5738\n",
            "\n",
            "Epoch 287 Batch 0 Loss 0.8882 Accuracy 0.8086\n",
            "Epoch 287 Batch 50 Loss 0.9920 Accuracy 0.7569\n",
            "Epoch 287 Batch 100 Loss 0.9918 Accuracy 0.7560\n",
            "Epoch 287 Batch 150 Loss 0.9946 Accuracy 0.7566\n",
            "Epoch 287 Batch 200 Loss 0.9954 Accuracy 0.7573\n",
            "Epoch 287 Batch 250 Loss 1.0005 Accuracy 0.7565\n",
            "Epoch 287 Batch 300 Loss 1.0068 Accuracy 0.7550\n",
            "Epoch 287 Batch 350 Loss 1.0091 Accuracy 0.7549\n",
            "Epoch 287 Batch 400 Loss 1.0118 Accuracy 0.7543\n",
            "Epoch 287 Batch 450 Loss 1.0150 Accuracy 0.7534\n",
            "discarded batch 464\n",
            "Epoch 287 Batch 500 Loss 1.0180 Accuracy 0.7530\n",
            "Epoch 287 Batch 550 Loss 1.0198 Accuracy 0.7524\n",
            "Epoch 287 Batch 600 Loss 1.0234 Accuracy 0.7518\n",
            "Epoch 287 Batch 650 Loss 1.0248 Accuracy 0.7515\n",
            "Epoch 287 Batch 700 Loss 1.0272 Accuracy 0.7512\n",
            "Epoch 287 Batch 750 Loss 1.0301 Accuracy 0.7508\n",
            "Epoch 287 Batch 800 Loss 1.0316 Accuracy 0.7505\n",
            "Epoch 287 Batch 850 Loss 1.0335 Accuracy 0.7503\n",
            "Epoch 287 Batch 900 Loss 1.0370 Accuracy 0.7498\n",
            "Epoch 287 Batch 950 Loss 1.0377 Accuracy 0.7497\n",
            "Epoch 287 Batch 1000 Loss 1.0397 Accuracy 0.7495\n",
            "Epoch 287 Batch 1050 Loss 1.0415 Accuracy 0.7491\n",
            "Epoch 287 Batch 1100 Loss 1.0441 Accuracy 0.7485\n",
            "Epoch 287 Loss 1.0453 Accuracy 0.7483\n",
            "Time taken for 1 epoch: 22.065317392349243 secs\n",
            "\n",
            "Epoch 288 Batch 0 Loss 1.0390 Accuracy 0.7426\n",
            "Epoch 288 Batch 50 Loss 1.0078 Accuracy 0.7555\n",
            "Epoch 288 Batch 100 Loss 1.0151 Accuracy 0.7548\n",
            "Epoch 288 Batch 150 Loss 1.0162 Accuracy 0.7546\n",
            "Epoch 288 Batch 200 Loss 1.0157 Accuracy 0.7542\n",
            "Epoch 288 Batch 250 Loss 1.0159 Accuracy 0.7544\n",
            "Epoch 288 Batch 300 Loss 1.0142 Accuracy 0.7547\n",
            "Epoch 288 Batch 350 Loss 1.0147 Accuracy 0.7545\n",
            "Epoch 288 Batch 400 Loss 1.0158 Accuracy 0.7542\n",
            "Epoch 288 Batch 450 Loss 1.0172 Accuracy 0.7540\n",
            "Epoch 288 Batch 500 Loss 1.0205 Accuracy 0.7531\n",
            "Epoch 288 Batch 550 Loss 1.0229 Accuracy 0.7525\n",
            "Epoch 288 Batch 600 Loss 1.0257 Accuracy 0.7514\n",
            "Epoch 288 Batch 650 Loss 1.0277 Accuracy 0.7512\n",
            "Epoch 288 Batch 700 Loss 1.0293 Accuracy 0.7510\n",
            "Epoch 288 Batch 750 Loss 1.0321 Accuracy 0.7505\n",
            "Epoch 288 Batch 800 Loss 1.0344 Accuracy 0.7500\n",
            "Epoch 288 Batch 850 Loss 1.0358 Accuracy 0.7496\n",
            "Epoch 288 Batch 900 Loss 1.0381 Accuracy 0.7492\n",
            "discarded batch 938\n",
            "Epoch 288 Batch 950 Loss 1.0383 Accuracy 0.7492\n",
            "Epoch 288 Batch 1000 Loss 1.0396 Accuracy 0.7491\n",
            "Epoch 288 Batch 1050 Loss 1.0402 Accuracy 0.7491\n",
            "Epoch 288 Batch 1100 Loss 1.0415 Accuracy 0.7490\n",
            "Epoch 288 Loss 1.0421 Accuracy 0.7489\n",
            "Time taken for 1 epoch: 22.09667134284973 secs\n",
            "\n",
            "Epoch 289 Batch 0 Loss 0.9348 Accuracy 0.7624\n",
            "Epoch 289 Batch 50 Loss 0.9819 Accuracy 0.7593\n",
            "Epoch 289 Batch 100 Loss 0.9958 Accuracy 0.7573\n",
            "discarded batch 132\n",
            "Epoch 289 Batch 150 Loss 0.9973 Accuracy 0.7562\n",
            "Epoch 289 Batch 200 Loss 1.0001 Accuracy 0.7557\n",
            "Epoch 289 Batch 250 Loss 1.0054 Accuracy 0.7543\n",
            "Epoch 289 Batch 300 Loss 1.0102 Accuracy 0.7539\n",
            "Epoch 289 Batch 350 Loss 1.0122 Accuracy 0.7541\n",
            "Epoch 289 Batch 400 Loss 1.0151 Accuracy 0.7538\n",
            "Epoch 289 Batch 450 Loss 1.0186 Accuracy 0.7531\n",
            "Epoch 289 Batch 500 Loss 1.0216 Accuracy 0.7527\n",
            "Epoch 289 Batch 550 Loss 1.0240 Accuracy 0.7523\n",
            "Epoch 289 Batch 600 Loss 1.0262 Accuracy 0.7519\n",
            "Epoch 289 Batch 650 Loss 1.0287 Accuracy 0.7512\n",
            "Epoch 289 Batch 700 Loss 1.0313 Accuracy 0.7510\n",
            "Epoch 289 Batch 750 Loss 1.0325 Accuracy 0.7507\n",
            "Epoch 289 Batch 800 Loss 1.0345 Accuracy 0.7501\n",
            "Epoch 289 Batch 850 Loss 1.0364 Accuracy 0.7496\n",
            "Epoch 289 Batch 900 Loss 1.0400 Accuracy 0.7490\n",
            "Epoch 289 Batch 950 Loss 1.0406 Accuracy 0.7488\n",
            "Epoch 289 Batch 1000 Loss 1.0427 Accuracy 0.7485\n",
            "Epoch 289 Batch 1050 Loss 1.0435 Accuracy 0.7484\n",
            "Epoch 289 Batch 1100 Loss 1.0447 Accuracy 0.7483\n",
            "Epoch 289 Loss 1.0457 Accuracy 0.7481\n",
            "Time taken for 1 epoch: 22.184834241867065 secs\n",
            "\n",
            "Epoch 290 Batch 0 Loss 0.8104 Accuracy 0.7888\n",
            "Epoch 290 Batch 50 Loss 0.9700 Accuracy 0.7629\n",
            "Epoch 290 Batch 100 Loss 0.9844 Accuracy 0.7599\n",
            "Epoch 290 Batch 150 Loss 0.9935 Accuracy 0.7582\n",
            "Epoch 290 Batch 200 Loss 0.9952 Accuracy 0.7574\n",
            "Epoch 290 Batch 250 Loss 0.9986 Accuracy 0.7568\n",
            "Epoch 290 Batch 300 Loss 1.0051 Accuracy 0.7553\n",
            "Epoch 290 Batch 350 Loss 1.0084 Accuracy 0.7546\n",
            "Epoch 290 Batch 400 Loss 1.0115 Accuracy 0.7537\n",
            "Epoch 290 Batch 450 Loss 1.0139 Accuracy 0.7531\n",
            "Epoch 290 Batch 500 Loss 1.0159 Accuracy 0.7527\n",
            "Epoch 290 Batch 550 Loss 1.0193 Accuracy 0.7520\n",
            "Epoch 290 Batch 600 Loss 1.0207 Accuracy 0.7518\n",
            "Epoch 290 Batch 650 Loss 1.0230 Accuracy 0.7515\n",
            "Epoch 290 Batch 700 Loss 1.0268 Accuracy 0.7505\n",
            "Epoch 290 Batch 750 Loss 1.0277 Accuracy 0.7506\n",
            "Epoch 290 Batch 800 Loss 1.0287 Accuracy 0.7504\n",
            "discarded batch 818\n",
            "Epoch 290 Batch 850 Loss 1.0307 Accuracy 0.7502\n",
            "Epoch 290 Batch 900 Loss 1.0318 Accuracy 0.7501\n",
            "Epoch 290 Batch 950 Loss 1.0332 Accuracy 0.7500\n",
            "Epoch 290 Batch 1000 Loss 1.0352 Accuracy 0.7497\n",
            "Epoch 290 Batch 1050 Loss 1.0369 Accuracy 0.7496\n",
            "Epoch 290 Batch 1100 Loss 1.0389 Accuracy 0.7493\n",
            "Saving checkpoint for epoch 290 at ./checkpoints/train/ckpt-58\n",
            "Epoch 290 Loss 1.0397 Accuracy 0.7492\n",
            "Time taken for 1 epoch: 22.246922254562378 secs\n",
            "\n",
            "Epoch 291 Batch 0 Loss 1.1475 Accuracy 0.7492\n",
            "Epoch 291 Batch 50 Loss 0.9947 Accuracy 0.7580\n",
            "Epoch 291 Batch 100 Loss 0.9928 Accuracy 0.7572\n",
            "Epoch 291 Batch 150 Loss 0.9994 Accuracy 0.7571\n",
            "Epoch 291 Batch 200 Loss 1.0034 Accuracy 0.7562\n",
            "Epoch 291 Batch 250 Loss 1.0078 Accuracy 0.7562\n",
            "Epoch 291 Batch 300 Loss 1.0129 Accuracy 0.7552\n",
            "Epoch 291 Batch 350 Loss 1.0142 Accuracy 0.7550\n",
            "Epoch 291 Batch 400 Loss 1.0156 Accuracy 0.7545\n",
            "Epoch 291 Batch 450 Loss 1.0160 Accuracy 0.7548\n",
            "Epoch 291 Batch 500 Loss 1.0185 Accuracy 0.7542\n",
            "Epoch 291 Batch 550 Loss 1.0205 Accuracy 0.7535\n",
            "Epoch 291 Batch 600 Loss 1.0214 Accuracy 0.7535\n",
            "Epoch 291 Batch 650 Loss 1.0243 Accuracy 0.7530\n",
            "discarded batch 667\n",
            "Epoch 291 Batch 700 Loss 1.0255 Accuracy 0.7525\n",
            "Epoch 291 Batch 750 Loss 1.0269 Accuracy 0.7523\n",
            "Epoch 291 Batch 800 Loss 1.0293 Accuracy 0.7518\n",
            "Epoch 291 Batch 850 Loss 1.0302 Accuracy 0.7517\n",
            "Epoch 291 Batch 900 Loss 1.0310 Accuracy 0.7517\n",
            "Epoch 291 Batch 950 Loss 1.0323 Accuracy 0.7513\n",
            "Epoch 291 Batch 1000 Loss 1.0341 Accuracy 0.7509\n",
            "Epoch 291 Batch 1050 Loss 1.0352 Accuracy 0.7506\n",
            "Epoch 291 Batch 1100 Loss 1.0367 Accuracy 0.7503\n",
            "Epoch 291 Loss 1.0377 Accuracy 0.7502\n",
            "Time taken for 1 epoch: 22.075817584991455 secs\n",
            "\n",
            "discarded batch 23\n",
            "Epoch 291 VALIDATION: Loss 2.7693 Accuracy 0.5735\n",
            "\n",
            "Epoch 292 Batch 0 Loss 1.0020 Accuracy 0.7393\n",
            "Epoch 292 Batch 50 Loss 0.9893 Accuracy 0.7586\n",
            "discarded batch 68\n",
            "Epoch 292 Batch 100 Loss 1.0003 Accuracy 0.7563\n",
            "Epoch 292 Batch 150 Loss 1.0013 Accuracy 0.7553\n",
            "Epoch 292 Batch 200 Loss 1.0012 Accuracy 0.7557\n",
            "Epoch 292 Batch 250 Loss 0.9997 Accuracy 0.7561\n",
            "Epoch 292 Batch 300 Loss 1.0022 Accuracy 0.7556\n",
            "Epoch 292 Batch 350 Loss 1.0061 Accuracy 0.7552\n",
            "Epoch 292 Batch 400 Loss 1.0110 Accuracy 0.7543\n",
            "Epoch 292 Batch 450 Loss 1.0137 Accuracy 0.7541\n",
            "Epoch 292 Batch 500 Loss 1.0144 Accuracy 0.7537\n",
            "Epoch 292 Batch 550 Loss 1.0180 Accuracy 0.7532\n",
            "Epoch 292 Batch 600 Loss 1.0209 Accuracy 0.7528\n",
            "Epoch 292 Batch 650 Loss 1.0227 Accuracy 0.7524\n",
            "Epoch 292 Batch 700 Loss 1.0249 Accuracy 0.7518\n",
            "Epoch 292 Batch 750 Loss 1.0264 Accuracy 0.7516\n",
            "Epoch 292 Batch 800 Loss 1.0295 Accuracy 0.7510\n",
            "Epoch 292 Batch 850 Loss 1.0317 Accuracy 0.7508\n",
            "Epoch 292 Batch 900 Loss 1.0328 Accuracy 0.7505\n",
            "Epoch 292 Batch 950 Loss 1.0355 Accuracy 0.7504\n",
            "Epoch 292 Batch 1000 Loss 1.0357 Accuracy 0.7502\n",
            "Epoch 292 Batch 1050 Loss 1.0366 Accuracy 0.7502\n",
            "Epoch 292 Batch 1100 Loss 1.0393 Accuracy 0.7497\n",
            "Epoch 292 Loss 1.0401 Accuracy 0.7495\n",
            "Time taken for 1 epoch: 22.090121746063232 secs\n",
            "\n",
            "Epoch 293 Batch 0 Loss 0.9086 Accuracy 0.7624\n",
            "Epoch 293 Batch 50 Loss 0.9809 Accuracy 0.7555\n",
            "Epoch 293 Batch 100 Loss 0.9888 Accuracy 0.7554\n",
            "Epoch 293 Batch 150 Loss 0.9917 Accuracy 0.7567\n",
            "Epoch 293 Batch 200 Loss 0.9948 Accuracy 0.7563\n",
            "Epoch 293 Batch 250 Loss 1.0008 Accuracy 0.7546\n",
            "Epoch 293 Batch 300 Loss 1.0073 Accuracy 0.7539\n",
            "Epoch 293 Batch 350 Loss 1.0071 Accuracy 0.7544\n",
            "Epoch 293 Batch 400 Loss 1.0125 Accuracy 0.7529\n",
            "Epoch 293 Batch 450 Loss 1.0169 Accuracy 0.7522\n",
            "discarded batch 473\n",
            "Epoch 293 Batch 500 Loss 1.0199 Accuracy 0.7516\n",
            "Epoch 293 Batch 550 Loss 1.0219 Accuracy 0.7515\n",
            "Epoch 293 Batch 600 Loss 1.0222 Accuracy 0.7515\n",
            "Epoch 293 Batch 650 Loss 1.0230 Accuracy 0.7513\n",
            "Epoch 293 Batch 700 Loss 1.0257 Accuracy 0.7509\n",
            "Epoch 293 Batch 750 Loss 1.0294 Accuracy 0.7504\n",
            "Epoch 293 Batch 800 Loss 1.0305 Accuracy 0.7502\n",
            "Epoch 293 Batch 850 Loss 1.0317 Accuracy 0.7503\n",
            "Epoch 293 Batch 900 Loss 1.0337 Accuracy 0.7500\n",
            "Epoch 293 Batch 950 Loss 1.0340 Accuracy 0.7502\n",
            "Epoch 293 Batch 1000 Loss 1.0357 Accuracy 0.7499\n",
            "Epoch 293 Batch 1050 Loss 1.0365 Accuracy 0.7497\n",
            "Epoch 293 Batch 1100 Loss 1.0377 Accuracy 0.7496\n",
            "Epoch 293 Loss 1.0387 Accuracy 0.7493\n",
            "Time taken for 1 epoch: 22.21549677848816 secs\n",
            "\n",
            "Epoch 294 Batch 0 Loss 1.0625 Accuracy 0.7426\n",
            "Epoch 294 Batch 50 Loss 0.9847 Accuracy 0.7604\n",
            "Epoch 294 Batch 100 Loss 0.9933 Accuracy 0.7566\n",
            "Epoch 294 Batch 150 Loss 1.0002 Accuracy 0.7565\n",
            "Epoch 294 Batch 200 Loss 1.0046 Accuracy 0.7546\n",
            "Epoch 294 Batch 250 Loss 1.0062 Accuracy 0.7540\n",
            "Epoch 294 Batch 300 Loss 1.0069 Accuracy 0.7543\n",
            "Epoch 294 Batch 350 Loss 1.0097 Accuracy 0.7543\n",
            "Epoch 294 Batch 400 Loss 1.0125 Accuracy 0.7540\n",
            "Epoch 294 Batch 450 Loss 1.0101 Accuracy 0.7547\n",
            "Epoch 294 Batch 500 Loss 1.0118 Accuracy 0.7545\n",
            "Epoch 294 Batch 550 Loss 1.0152 Accuracy 0.7534\n",
            "Epoch 294 Batch 600 Loss 1.0171 Accuracy 0.7532\n",
            "Epoch 294 Batch 650 Loss 1.0188 Accuracy 0.7528\n",
            "Epoch 294 Batch 700 Loss 1.0203 Accuracy 0.7524\n",
            "Epoch 294 Batch 750 Loss 1.0224 Accuracy 0.7520\n",
            "discarded batch 760\n",
            "Epoch 294 Batch 800 Loss 1.0245 Accuracy 0.7517\n",
            "Epoch 294 Batch 850 Loss 1.0268 Accuracy 0.7514\n",
            "Epoch 294 Batch 900 Loss 1.0290 Accuracy 0.7513\n",
            "Epoch 294 Batch 950 Loss 1.0317 Accuracy 0.7509\n",
            "Epoch 294 Batch 1000 Loss 1.0340 Accuracy 0.7506\n",
            "Epoch 294 Batch 1050 Loss 1.0344 Accuracy 0.7505\n",
            "Epoch 294 Batch 1100 Loss 1.0364 Accuracy 0.7501\n",
            "Epoch 294 Loss 1.0376 Accuracy 0.7499\n",
            "Time taken for 1 epoch: 22.100176095962524 secs\n",
            "\n",
            "Epoch 295 Batch 0 Loss 0.9323 Accuracy 0.7789\n",
            "Epoch 295 Batch 50 Loss 0.9761 Accuracy 0.7580\n",
            "Epoch 295 Batch 100 Loss 0.9752 Accuracy 0.7578\n",
            "Epoch 295 Batch 150 Loss 0.9798 Accuracy 0.7575\n",
            "Epoch 295 Batch 200 Loss 0.9916 Accuracy 0.7561\n",
            "Epoch 295 Batch 250 Loss 0.9959 Accuracy 0.7555\n",
            "Epoch 295 Batch 300 Loss 1.0013 Accuracy 0.7547\n",
            "Epoch 295 Batch 350 Loss 1.0049 Accuracy 0.7542\n",
            "Epoch 295 Batch 400 Loss 1.0087 Accuracy 0.7535\n",
            "Epoch 295 Batch 450 Loss 1.0101 Accuracy 0.7530\n",
            "Epoch 295 Batch 500 Loss 1.0127 Accuracy 0.7528\n",
            "Epoch 295 Batch 550 Loss 1.0157 Accuracy 0.7526\n",
            "Epoch 295 Batch 600 Loss 1.0173 Accuracy 0.7526\n",
            "Epoch 295 Batch 650 Loss 1.0203 Accuracy 0.7522\n",
            "Epoch 295 Batch 700 Loss 1.0230 Accuracy 0.7519\n",
            "Epoch 295 Batch 750 Loss 1.0239 Accuracy 0.7517\n",
            "Epoch 295 Batch 800 Loss 1.0256 Accuracy 0.7511\n",
            "discarded batch 836\n",
            "Epoch 295 Batch 850 Loss 1.0277 Accuracy 0.7507\n",
            "Epoch 295 Batch 900 Loss 1.0301 Accuracy 0.7507\n",
            "Epoch 295 Batch 950 Loss 1.0315 Accuracy 0.7504\n",
            "Epoch 295 Batch 1000 Loss 1.0339 Accuracy 0.7498\n",
            "Epoch 295 Batch 1050 Loss 1.0350 Accuracy 0.7495\n",
            "Epoch 295 Batch 1100 Loss 1.0363 Accuracy 0.7494\n",
            "Saving checkpoint for epoch 295 at ./checkpoints/train/ckpt-59\n",
            "Epoch 295 Loss 1.0381 Accuracy 0.7491\n",
            "Time taken for 1 epoch: 22.56596326828003 secs\n",
            "\n",
            "Epoch 296 Batch 0 Loss 0.8589 Accuracy 0.8152\n",
            "Epoch 296 Batch 50 Loss 0.9690 Accuracy 0.7639\n",
            "Epoch 296 Batch 100 Loss 0.9807 Accuracy 0.7620\n",
            "Epoch 296 Batch 150 Loss 0.9831 Accuracy 0.7599\n",
            "Epoch 296 Batch 200 Loss 0.9873 Accuracy 0.7588\n",
            "Epoch 296 Batch 250 Loss 0.9948 Accuracy 0.7570\n",
            "Epoch 296 Batch 300 Loss 0.9974 Accuracy 0.7568\n",
            "Epoch 296 Batch 350 Loss 1.0014 Accuracy 0.7560\n",
            "Epoch 296 Batch 400 Loss 1.0048 Accuracy 0.7557\n",
            "Epoch 296 Batch 450 Loss 1.0097 Accuracy 0.7547\n",
            "Epoch 296 Batch 500 Loss 1.0119 Accuracy 0.7539\n",
            "Epoch 296 Batch 550 Loss 1.0132 Accuracy 0.7534\n",
            "Epoch 296 Batch 600 Loss 1.0151 Accuracy 0.7531\n",
            "Epoch 296 Batch 650 Loss 1.0175 Accuracy 0.7526\n",
            "Epoch 296 Batch 700 Loss 1.0216 Accuracy 0.7520\n",
            "Epoch 296 Batch 750 Loss 1.0247 Accuracy 0.7514\n",
            "Epoch 296 Batch 800 Loss 1.0269 Accuracy 0.7513\n",
            "Epoch 296 Batch 850 Loss 1.0279 Accuracy 0.7511\n",
            "discarded batch 892\n",
            "Epoch 296 Batch 900 Loss 1.0288 Accuracy 0.7508\n",
            "Epoch 296 Batch 950 Loss 1.0304 Accuracy 0.7506\n",
            "Epoch 296 Batch 1000 Loss 1.0315 Accuracy 0.7502\n",
            "Epoch 296 Batch 1050 Loss 1.0329 Accuracy 0.7500\n",
            "Epoch 296 Batch 1100 Loss 1.0336 Accuracy 0.7499\n",
            "Epoch 296 Loss 1.0350 Accuracy 0.7497\n",
            "Time taken for 1 epoch: 22.220844268798828 secs\n",
            "\n",
            "discarded batch 23\n",
            "Epoch 296 VALIDATION: Loss 2.7717 Accuracy 0.5728\n",
            "\n",
            "Epoch 297 Batch 0 Loss 0.8669 Accuracy 0.7723\n",
            "Epoch 297 Batch 50 Loss 0.9979 Accuracy 0.7562\n",
            "Epoch 297 Batch 100 Loss 0.9897 Accuracy 0.7587\n",
            "Epoch 297 Batch 150 Loss 0.9924 Accuracy 0.7589\n",
            "Epoch 297 Batch 200 Loss 0.9935 Accuracy 0.7582\n",
            "Epoch 297 Batch 250 Loss 0.9967 Accuracy 0.7571\n",
            "Epoch 297 Batch 300 Loss 0.9989 Accuracy 0.7560\n",
            "Epoch 297 Batch 350 Loss 1.0035 Accuracy 0.7545\n",
            "Epoch 297 Batch 400 Loss 1.0079 Accuracy 0.7541\n",
            "Epoch 297 Batch 450 Loss 1.0124 Accuracy 0.7533\n",
            "Epoch 297 Batch 500 Loss 1.0124 Accuracy 0.7531\n",
            "Epoch 297 Batch 550 Loss 1.0142 Accuracy 0.7529\n",
            "Epoch 297 Batch 600 Loss 1.0157 Accuracy 0.7528\n",
            "Epoch 297 Batch 650 Loss 1.0175 Accuracy 0.7524\n",
            "Epoch 297 Batch 700 Loss 1.0191 Accuracy 0.7523\n",
            "Epoch 297 Batch 750 Loss 1.0201 Accuracy 0.7521\n",
            "Epoch 297 Batch 800 Loss 1.0230 Accuracy 0.7514\n",
            "Epoch 297 Batch 850 Loss 1.0255 Accuracy 0.7511\n",
            "Epoch 297 Batch 900 Loss 1.0277 Accuracy 0.7506\n",
            "discarded batch 919\n",
            "Epoch 297 Batch 950 Loss 1.0297 Accuracy 0.7503\n",
            "Epoch 297 Batch 1000 Loss 1.0308 Accuracy 0.7502\n",
            "Epoch 297 Batch 1050 Loss 1.0325 Accuracy 0.7501\n",
            "Epoch 297 Batch 1100 Loss 1.0354 Accuracy 0.7497\n",
            "Epoch 297 Loss 1.0368 Accuracy 0.7494\n",
            "Time taken for 1 epoch: 22.15583848953247 secs\n",
            "\n",
            "Epoch 298 Batch 0 Loss 1.0573 Accuracy 0.7162\n",
            "Epoch 298 Batch 50 Loss 1.0010 Accuracy 0.7571\n",
            "Epoch 298 Batch 100 Loss 0.9930 Accuracy 0.7591\n",
            "Epoch 298 Batch 150 Loss 0.9984 Accuracy 0.7576\n",
            "Epoch 298 Batch 200 Loss 1.0061 Accuracy 0.7565\n",
            "Epoch 298 Batch 250 Loss 1.0055 Accuracy 0.7558\n",
            "Epoch 298 Batch 300 Loss 1.0086 Accuracy 0.7552\n",
            "Epoch 298 Batch 350 Loss 1.0102 Accuracy 0.7551\n",
            "Epoch 298 Batch 400 Loss 1.0118 Accuracy 0.7545\n",
            "Epoch 298 Batch 450 Loss 1.0144 Accuracy 0.7538\n",
            "discarded batch 493\n",
            "Epoch 298 Batch 500 Loss 1.0152 Accuracy 0.7535\n",
            "Epoch 298 Batch 550 Loss 1.0153 Accuracy 0.7536\n",
            "Epoch 298 Batch 600 Loss 1.0162 Accuracy 0.7531\n",
            "Epoch 298 Batch 650 Loss 1.0191 Accuracy 0.7525\n",
            "Epoch 298 Batch 700 Loss 1.0199 Accuracy 0.7524\n",
            "Epoch 298 Batch 750 Loss 1.0214 Accuracy 0.7522\n",
            "Epoch 298 Batch 800 Loss 1.0243 Accuracy 0.7517\n",
            "Epoch 298 Batch 850 Loss 1.0251 Accuracy 0.7513\n",
            "Epoch 298 Batch 900 Loss 1.0263 Accuracy 0.7512\n",
            "Epoch 298 Batch 950 Loss 1.0279 Accuracy 0.7509\n",
            "Epoch 298 Batch 1000 Loss 1.0303 Accuracy 0.7504\n",
            "Epoch 298 Batch 1050 Loss 1.0312 Accuracy 0.7502\n",
            "Epoch 298 Batch 1100 Loss 1.0323 Accuracy 0.7502\n",
            "Epoch 298 Loss 1.0328 Accuracy 0.7503\n",
            "Time taken for 1 epoch: 22.148832082748413 secs\n",
            "\n",
            "Epoch 299 Batch 0 Loss 0.9490 Accuracy 0.7690\n",
            "Epoch 299 Batch 50 Loss 0.9814 Accuracy 0.7589\n",
            "Epoch 299 Batch 100 Loss 0.9887 Accuracy 0.7567\n",
            "discarded batch 146\n",
            "Epoch 299 Batch 150 Loss 0.9959 Accuracy 0.7565\n",
            "Epoch 299 Batch 200 Loss 0.9955 Accuracy 0.7573\n",
            "Epoch 299 Batch 250 Loss 1.0003 Accuracy 0.7564\n",
            "Epoch 299 Batch 300 Loss 1.0022 Accuracy 0.7562\n",
            "Epoch 299 Batch 350 Loss 1.0078 Accuracy 0.7553\n",
            "Epoch 299 Batch 400 Loss 1.0111 Accuracy 0.7546\n",
            "Epoch 299 Batch 450 Loss 1.0125 Accuracy 0.7544\n",
            "Epoch 299 Batch 500 Loss 1.0131 Accuracy 0.7542\n",
            "Epoch 299 Batch 550 Loss 1.0148 Accuracy 0.7534\n",
            "Epoch 299 Batch 600 Loss 1.0173 Accuracy 0.7533\n",
            "Epoch 299 Batch 650 Loss 1.0190 Accuracy 0.7530\n",
            "Epoch 299 Batch 700 Loss 1.0210 Accuracy 0.7526\n",
            "Epoch 299 Batch 750 Loss 1.0225 Accuracy 0.7524\n",
            "Epoch 299 Batch 800 Loss 1.0240 Accuracy 0.7518\n",
            "Epoch 299 Batch 850 Loss 1.0256 Accuracy 0.7516\n",
            "Epoch 299 Batch 900 Loss 1.0268 Accuracy 0.7514\n",
            "Epoch 299 Batch 950 Loss 1.0288 Accuracy 0.7510\n",
            "Epoch 299 Batch 1000 Loss 1.0307 Accuracy 0.7507\n",
            "Epoch 299 Batch 1050 Loss 1.0320 Accuracy 0.7505\n",
            "Epoch 299 Batch 1100 Loss 1.0320 Accuracy 0.7506\n",
            "Epoch 299 Loss 1.0329 Accuracy 0.7506\n",
            "Time taken for 1 epoch: 22.127457857131958 secs\n",
            "\n",
            "Epoch 300 Batch 0 Loss 1.0243 Accuracy 0.7624\n",
            "Epoch 300 Batch 50 Loss 0.9852 Accuracy 0.7575\n",
            "Epoch 300 Batch 100 Loss 0.9858 Accuracy 0.7588\n",
            "Epoch 300 Batch 150 Loss 0.9903 Accuracy 0.7570\n",
            "Epoch 300 Batch 200 Loss 0.9968 Accuracy 0.7560\n",
            "Epoch 300 Batch 250 Loss 0.9967 Accuracy 0.7562\n",
            "Epoch 300 Batch 300 Loss 0.9997 Accuracy 0.7559\n",
            "Epoch 300 Batch 350 Loss 1.0024 Accuracy 0.7553\n",
            "discarded batch 400\n",
            "Epoch 300 Batch 450 Loss 1.0061 Accuracy 0.7554\n",
            "Epoch 300 Batch 500 Loss 1.0096 Accuracy 0.7549\n",
            "Epoch 300 Batch 550 Loss 1.0113 Accuracy 0.7542\n",
            "Epoch 300 Batch 600 Loss 1.0114 Accuracy 0.7544\n",
            "Epoch 300 Batch 650 Loss 1.0133 Accuracy 0.7540\n",
            "Epoch 300 Batch 700 Loss 1.0152 Accuracy 0.7535\n",
            "Epoch 300 Batch 750 Loss 1.0176 Accuracy 0.7533\n",
            "Epoch 300 Batch 800 Loss 1.0195 Accuracy 0.7532\n",
            "Epoch 300 Batch 850 Loss 1.0236 Accuracy 0.7527\n",
            "Epoch 300 Batch 900 Loss 1.0262 Accuracy 0.7525\n",
            "Epoch 300 Batch 950 Loss 1.0277 Accuracy 0.7523\n",
            "Epoch 300 Batch 1000 Loss 1.0291 Accuracy 0.7520\n",
            "Epoch 300 Batch 1050 Loss 1.0296 Accuracy 0.7522\n",
            "Epoch 300 Batch 1100 Loss 1.0313 Accuracy 0.7519\n",
            "Saving checkpoint for epoch 300 at ./checkpoints/train/ckpt-60\n",
            "Epoch 300 Loss 1.0330 Accuracy 0.7517\n",
            "Time taken for 1 epoch: 22.386560678482056 secs\n",
            "\n",
            " sara e rebecca iu  e colei\n",
            "che fu bisava al cantor che per doglia\n",
            "del fallo disse ' miserere mei '\n",
            "                  \n",
            " puoi tu veder cosí di soglia in soglia\n",
            "giú digradar com ' io ch ' a proprio nome\n",
            "vo per la rosa giú di foglia in foglia\n",
            "         \n",
            " e dal settimo grado in giú sí come\n",
            "infino ad esso succedono ebree\n",
            "dirimendo del fior tutte le chiome\n",
            "                   \n",
            " perché secondo lo sguardo che fee\n",
            "la fede in cristo queste sono il muro\n",
            "a che si parton le sacre scalee\n",
            "                  \n",
            "---------------------------\n",
            "\n",
            " grido quando al figlio al figlio guaro\n",
            "verbo le ! al corda ! a quel grido\n",
            "ver come ! a lui caro ! a lui\n",
            "              \n",
            " \n",
            "\n",
            " che tutto le grime non li accese !\n",
            "gridavan sé le ! e di sé coi cera\n",
            "gridò ' l duca ! e ' l senso e ' l passo '   \n",
            " ferma che tutti amoria e li gira\n",
            "ver lo ' l mae colui che non vide\n",
            "tutto gridò ! ' e ' l grido lui era ' tura\n",
            "     \n",
            " non  li orecchi omai non fa sí ?\n",
            "ma ' l viso ! a me sí gridan no\n",
            "che ' l segno ? e non farò non ne vole\n",
            "       \n",
            "\n",
            "\n",
            "\n",
            " li occi diss ' io se tu per testino\n",
            "ti farò che siete l ' archi aspetto\n",
            "ma fa giusti anda  chi re ancino\n",
            "               \n",
            "                         ver la mia voglia è duto\n",
            "per che mostra voglia si reno adorno\n",
            "\n",
            " \n",
            " do vita si le e la stra te !\n",
            "grida la penbo e di bisolore\n",
            "se a guida  sola mia rivolte\n",
            "                     \n",
            "                                                                          \n",
            " lo gridare a beatrice gio\n",
            "che si fe ' reve la muta fera\n",
            "sola sua milestra li e remora\n",
            "\n",
            "\n",
            " come la mia mesegno non vo duto\n",
            "perché que servi con troni e trono\n",
            "danni a lui vivi consi to e voto\n",
            "                  \n",
            " ?\n",
            "              ter vo !\n",
            "con la mana e con la voglia amora\n",
            "e con l ' acqua non vogno e vo\n",
            "\n",
            "\n",
            " ché si fece nanzi al cielo\n",
            "e questa pado reante la  fera\n",
            "pian va le pianta e sí mano\n",
            "                         \n",
            "\n",
            " se le dove mi mi rimara\n",
            "vota mi vo vo la mia donna\n",
            "come ti pre mi mi prese dono !\n",
            "                       \n",
            "\n",
            " se ra a me ra mi fa ra\n",
            "qual dove la mia matera e io\n",
            "tu qui dove e dove dove si pigrina\n",
            "\n",
            "\n",
            " ?\n",
            "                                                                         \n",
            " li trova nome l ' alta mia ra\n",
            "come si rivolse in su la tua vita\n",
            "colto e non è tramai l ' amo la mia mia ria\n",
            "       \n",
            " come si l ' anima tua tua tata\n",
            "per to danna il to tosto gno\n",
            "per esser to l ' amore e ditata\n",
            "\n",
            "\n",
            " li ' tu che co ' l ser fer neta\n",
            "grifa ra e ' l more e ' l mondo\n",
            "bene ' le e ' piedi e ' e ' non non si mente \n",
            " li le tre magnate e ' del mondo\n",
            "sí che sono e ' l mondo e ' l segno\n",
            "forte e di lui non è anco chi chi grido\n",
            "      \n",
            " or è fatti e per le fate re\n",
            "per fidanna al giovi e rimosse\n",
            "e farò per li '  suoi e di lor vigno\n",
            "                \n",
            "\n",
            "\n",
            "\n",
            " ché rea me le sue seme non  se\n",
            "atti letti rade la voce semo\n",
            "tutti li e lacrimanti  sese tese\n",
            "                    \n",
            "                         cande a sé a sé non saranni\n",
            "dove si faran ferse bello\n",
            "\n",
            "? e come si gno le a star d ' un scente\n",
            "grida  verso ' l duto passi bas duto\n",
            "e fece a ' l grido e sol non vo !\n",
            "    \n",
            " e io disse ' l viso mi river ce !\n",
            "creduto mi fece a chi i passi\n",
            "e come va ! e lui qui non vivivi\n",
            "            \n",
            " non   farò che per me si mossi\n",
            "ma per mentre che non stimo va\n",
            "ma per la tua ne ne uom da ' passi\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjLbfLOU_pRw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transformer.save_weights(\"./optimus_rhyme\")\n",
        "#transformer.load_weights(\"./optimus_rhyme\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}