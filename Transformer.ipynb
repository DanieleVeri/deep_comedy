{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNGf88Aa1KYBF6wE1OMup76"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHRo2NsK-Rgd",
        "colab_type": "code",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "7ee2c004-8729-49a0-daa4-b25922a32032"
      },
      "source": [
        "#@title Import & seed\n",
        "import time\n",
        "import re\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import requests\n",
        "import collections\n",
        "import pickle\n",
        "import copy, random\n",
        "import nltk as nl\n",
        "from itertools import zip_longest\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Reshape, BatchNormalization, Dense, Dropout, concatenate,\n",
        "    Embedding, LSTM, Dense, GRU, Bidirectional, Add\n",
        ")\n",
        "from tensorflow.keras.activations import elu, relu, softmax, sigmoid\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "print(tf.__version__)\n",
        "nl.download('punkt')\n",
        "np.random.seed(1234)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onTHkWW_YOHp",
        "colab_type": "code",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b68dc137-9996-481e-daa5-0262022f4262"
      },
      "source": [
        "#@title Model\n",
        "\n",
        "vocab_size = 1797\n",
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "input_vocab_size = vocab_size\n",
        "target_vocab_size = vocab_size\n",
        "dropout_rate = 0.1\n",
        "batch_len = 304\n",
        "EPOCHS = 100\n",
        "\n",
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "    return pos * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                            np.arange(d_model)[np.newaxis, :],\n",
        "                            d_model)\n",
        "    # apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    # apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "def create_padding_mask(seq):   \n",
        "    seq = tf.cast(tf.math.equal(seq, pad), tf.float32)\n",
        "    # add extra dimensions to add the padding\n",
        "    # to the attention logits.\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
        "\n",
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask  # (seq_len, seq_len)\n",
        "\n",
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    \"\"\"Calculate the attention weights.\n",
        "    q, k, v must have matching leading dimensions.\n",
        "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "    The mask has different shapes depending on its type(padding or look ahead) \n",
        "    but it must be broadcastable for addition.\n",
        "    \n",
        "    Args:\n",
        "        q: query shape == (..., seq_len_q, depth)\n",
        "        k: key shape == (..., seq_len_k, depth)\n",
        "        v: value shape == (..., seq_len_v, depth_v)\n",
        "        mask: Float tensor with shape broadcastable \n",
        "            to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "        \n",
        "    Returns:\n",
        "        output, attention_weights\n",
        "    \"\"\"\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "    # scale matmul_qk\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "    # add the mask to the scaled tensor.\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)  \n",
        "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "    # add up to 1.\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "    return output, attention_weights\n",
        "\n",
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "    ])\n",
        "\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        assert d_model % self.num_heads == 0\n",
        "        self.depth = d_model // self.num_heads\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "            \n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"Split the last dimension into (num_heads, depth).\n",
        "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "        \n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "        \n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "        \n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "        \n",
        "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "        concat_attention = tf.reshape(scaled_attention, \n",
        "                                    (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "        return output, attention_weights\n",
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "    def __call__(self, x, training, mask):\n",
        "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "        \n",
        "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "        \n",
        "        return out2\n",
        "\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "    \n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "        \n",
        "    def __call__(self, x, enc_output, training, \n",
        "            look_ahead_mask, padding_mask):\n",
        "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "        \n",
        "        attn2, attn_weights_block2 = self.mha2(\n",
        "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "        \n",
        "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
        "        \n",
        "        return out3, attn_weights_block1, attn_weights_block2\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "                maximum_position_encoding, rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)   ########## rm ??????\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
        "                                                self.d_model)\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
        "                        for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "            \n",
        "    def __call__(self, x, training, mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        # adding embedding and position encoding.\n",
        "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "        x = self.dropout(x, training=training)\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "        return x  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "                maximum_position_encoding, rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
        "                        for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "    def __call__(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "        x = self.dropout(x, training=training)\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                                look_ahead_mask, padding_mask)\n",
        "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "        \n",
        "        # x.shape == (batch_size, target_seq_len, d_model)\n",
        "        return x, attention_weights\n",
        "\n",
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
        "                target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
        "                            input_vocab_size, pe_input, rate)\n",
        "\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
        "                            target_vocab_size, pe_target, rate)\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "        \n",
        "    def __call__(self, inp, tar, training, enc_padding_mask, \n",
        "            look_ahead_mask, dec_padding_mask):\n",
        "\n",
        "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
        "        \n",
        "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "        dec_output, attention_weights = self.decoder(\n",
        "            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "        \n",
        "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "        \n",
        "        return final_output, attention_weights\n",
        "\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "        \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "        \n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "learning_rate = CustomSchedule(d_model)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
        "                                     epsilon=1e-9)\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "    name='train_accuracy')\n",
        "\n",
        "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
        "                          input_vocab_size, target_vocab_size, \n",
        "                          pe_input=input_vocab_size, \n",
        "                          pe_target=target_vocab_size,\n",
        "                          rate=dropout_rate)\n",
        "def create_masks(inp, tar):\n",
        "    # Encoder padding mask\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "    \n",
        "    # Used in the 2nd attention block in the decoder.\n",
        "    # This padding mask is used to mask the encoder outputs.\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "    \n",
        "    # Used in the 1st attention block in the decoder.\n",
        "    # It is used to pad and mask future tokens in the input received by \n",
        "    # the decoder.\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "    \n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask\n",
        "\n",
        "checkpoint_path = \"./checkpoints/train\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "# if a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print ('Latest checkpoint restored!!')\n",
        "\n",
        "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
        "# execution. The function specializes to the precise shape of the argument\n",
        "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
        "# batch sizes (the last batch is smaller), use input_signature to specify\n",
        "# more generic shapes.\n",
        "\n",
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, batch_len), dtype=tf.int64),\n",
        "    tf.TensorSpec(shape=(None, batch_len), dtype=tf.int64),\n",
        "]\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _ = transformer(inp, tar_inp, \n",
        "                                    True, \n",
        "                                    enc_padding_mask, \n",
        "                                    combined_mask, \n",
        "                                    dec_padding_mask)\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "    \n",
        "    train_loss(loss)\n",
        "    train_accuracy(tar_real, predictions)\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Latest checkpoint restored!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GI14h2PZX70w",
        "colab_type": "code",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "outputId": "4af75b68-aff9-4cb4-ac04-e37b41d48777"
      },
      "source": [
        "#@title Preprocessing\n",
        "\n",
        "def get_hyp_lm_tercets(tercets):\n",
        "    new_tercets = []\n",
        "    for tercet in tercets:\n",
        "        new_tercets.append([])\n",
        "        for verse in tercet:\n",
        "            new_tercets[-1].append([])\n",
        "            for hyp_w in verse:\n",
        "                new_tercets[-1][-1].extend(hyp_w)\n",
        "                new_tercets[-1][-1].append('<SEP>')\n",
        "            new_tercets[-1][-1] = new_tercets[-1][-1][:-1]\n",
        "\n",
        "    return new_tercets\n",
        "\n",
        "def is_vowel(c):\n",
        "    return c in 'aeiouAEIOUàìíèéùúüòï'\n",
        "\n",
        "def unsplittable_cons():\n",
        "    u_cons = []\n",
        "    for c1 in ('b', 'c', 'd', 'f', 'g', 'p', 't', 'v'):\n",
        "        for c2 in ('l', 'r'):\n",
        "            u_cons.append(c1 + c2)\n",
        "\n",
        "    others = ['gn', 'gh', 'ch']\n",
        "    u_cons.extend(others)\n",
        "    return u_cons\n",
        "\n",
        "\n",
        "def are_cons_to_split(c1, c2):\n",
        "    to_split = ('cq', 'cn', 'lm', 'rc', 'bd', 'mb', 'mn', 'ld', 'ng', 'nd', 'tm', 'nv', 'nc', 'ft', 'nf', 'gm', 'fm', 'rv', 'fp')\n",
        "    return (c1 + c2) in to_split or (not is_vowel(c1) and (c1 == c2)) or ((c1 + c2) not in unsplittable_cons()) and (\n",
        "        (not is_vowel(c1)) and (not is_vowel(c2)) and c1 != 's')\n",
        "\n",
        "\n",
        "def is_diphthong(c1, c2):\n",
        "    return (c1 + c2) in ('ia', 'ie', 'io', 'iu', 'ua', 'ue', 'uo', 'ui', 'ai', 'ei', 'oi', 'ui', 'au', 'eu', 'ïe', 'iú', 'iù')\n",
        "\n",
        "\n",
        "def is_triphthong(c1, c2, c3):\n",
        "    return (c1 + c2 + c3) in ('iai', 'iei', 'uoi', 'uai', 'uei', 'iuo')\n",
        "\n",
        "\n",
        "def is_toned_vowel(c):\n",
        "    return c in 'àìèéùòï'\n",
        "\n",
        "def has_vowels(sy):\n",
        "    for c in sy:\n",
        "        if is_vowel(c):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def hyphenation(word):\n",
        "    \"\"\"\n",
        "    Split word in syllables\n",
        "    :param word: input string\n",
        "    :return: a list containing syllables of the word\n",
        "    \"\"\"\n",
        "    if not word or word == '':\n",
        "        return []\n",
        "    # elif len(word) == 3 and (is_vowel(word[1]) and is_vowel(word[2]) and not is_toned_vowel(word[2]) and (\n",
        "    #     not is_diphthong(word[1], word[2]) or (word[1] == 'i'))):\n",
        "    elif len(word) == 3 and (is_vowel(word[1]) and is_vowel(word[2]) and not is_toned_vowel(word[2]) and (\n",
        "        not is_diphthong(word[1], word[2]))):\n",
        "        return [word[:2]] + [word[2]]\n",
        "    elif len(word) == 3 and is_vowel(word[0]) and not is_vowel(word[1]) and is_vowel(word[2]):\n",
        "        return [word[:2]] + [word[2]]\n",
        "    elif len(word) == 3:\n",
        "        return [word]\n",
        "\n",
        "    syllables = []\n",
        "    is_done = False\n",
        "    count = 0\n",
        "    while not is_done and count <= len(word) - 1:\n",
        "        syllables.append('')\n",
        "        c = word[count]\n",
        "        while not is_vowel(c) and count < len(word) - 1:\n",
        "            syllables[-1] = syllables[-1] + c\n",
        "            count += 1\n",
        "            c = word[count]\n",
        "\n",
        "        syllables[-1] = syllables[-1] + word[count]\n",
        "\n",
        "        if count == len(word) - 1:\n",
        "            is_done = True\n",
        "        else:\n",
        "            count += 1\n",
        "\n",
        "            if count < len(word) and not is_vowel(word[count]):\n",
        "                if count == len(word) - 1:\n",
        "                    syllables[-1] += word[count]\n",
        "                    count += 1\n",
        "                elif count + 1 < len(word) and are_cons_to_split(word[count], word[count + 1]):\n",
        "                    syllables[-1] += word[count]\n",
        "                    count += 1\n",
        "                elif count + 2 < len(word) and not is_vowel(word[count + 1]) and not is_vowel(word[count + 2]) and word[\n",
        "                    count] != 's':\n",
        "                    syllables[-1] += word[count]\n",
        "                    count += 1\n",
        "            elif count < len(word):\n",
        "                if count + 1 < len(word) and is_triphthong(word[count - 1], word[count], word[count + 1]):\n",
        "                    syllables[-1] += word[count] + word[count + 1]\n",
        "                    count += 2\n",
        "                elif is_diphthong(word[count - 1], word[count]):\n",
        "                    syllables[-1] += word[count]\n",
        "                    count += 1\n",
        "\n",
        "                if count + 1 < len(word) and are_cons_to_split(word[count], word[count + 1]):\n",
        "                    syllables[-1] += word[count]\n",
        "                    count += 1\n",
        "\n",
        "            else:\n",
        "                is_done = True\n",
        "\n",
        "    if not has_vowels(syllables[-1]) and len(syllables) > 1:\n",
        "        syllables[-2] = syllables[-2] + syllables[-1]\n",
        "        syllables = syllables[:-1]\n",
        "\n",
        "    return syllables\n",
        "\n",
        "\n",
        "\n",
        "def get_dc_hyphenation(canti):\n",
        "    hyp_canti, hyp_tokens = [], []\n",
        "    for canto in canti:\n",
        "        hyp_canti.append([])\n",
        "        for verso in canto:\n",
        "            syllables = seq_hyphentation(verso)\n",
        "            hyp_canti[-1].append(syllables)\n",
        "            for syllable in syllables:\n",
        "                hyp_tokens.extend(syllable)\n",
        "\n",
        "    return hyp_canti, hyp_tokens\n",
        "\n",
        "\n",
        "def seq_hyphentation(words):\n",
        "    \"\"\"\n",
        "    Converts words in a list of strings into lists of syllables\n",
        "    :param words: a list of words (strings)\n",
        "    :return: a list of lists containing word syllables\n",
        "    \"\"\"\n",
        "    return [hyphenation(w) for w in words]\n",
        "\n",
        "\n",
        "def get_dc_cantos(filename, encoding=None):\n",
        "    # raw_data = read_words(filename=filename)\n",
        "    cantos, words, raw = [], [], []\n",
        "    with open(filename, \"r\", encoding=encoding) as f:\n",
        "        for line in f:\n",
        "            sentence = line.strip()\n",
        "            sentence = str.replace(sentence, \"\\.\", \" \\. \")\n",
        "            sentence = str.replace(sentence, \"[\", '')\n",
        "            sentence = str.replace(sentence, \"]\", '')\n",
        "            sentence = str.replace(sentence, \"-\", '')\n",
        "            sentence = str.replace(sentence, \";\", \" ; \")\n",
        "            sentence = str.replace(sentence, \",\", \" , \")\n",
        "            # sentence = str.replace(sentence, \" \\'\", '')\n",
        "            sentence = str.replace(sentence, \"\\'\", ' \\' ')\n",
        "            if len(sentence) > 1:\n",
        "                # sentence = sentence.translate(string.punctuation)\n",
        "                tokenized_sentence = nl.word_tokenize(sentence)\n",
        "                # tokenized_sentence = sentence.split()\n",
        "                tokenized_sentence = [w.lower() for w in tokenized_sentence if len(w) > 0]\n",
        "                tokenized_sentence = [w for w in tokenized_sentence if \",\" not in w]\n",
        "                tokenized_sentence = [w for w in tokenized_sentence if \".\" not in w]\n",
        "                tokenized_sentence = [w for w in tokenized_sentence if \":\" not in w]\n",
        "                tokenized_sentence = [w for w in tokenized_sentence if \";\" not in w]\n",
        "                tokenized_sentence = [w for w in tokenized_sentence if \"«\" not in w]\n",
        "                tokenized_sentence = [w for w in tokenized_sentence if \"»\" not in w]\n",
        "                # ts = []\n",
        "                ts = tokenized_sentence\n",
        "                # [ts.extend(re.split(\"(\\')\", e)) for e in tokenized_sentence]\n",
        "                tokenized_sentence = [w for w in ts if len(w) > 0]\n",
        "\n",
        "                if len(tokenized_sentence) == 2:\n",
        "                    cantos.append([])\n",
        "                    raw.append([])\n",
        "                elif len(tokenized_sentence) > 2:\n",
        "                    raw[-1].append(sentence)\n",
        "                    cantos[-1].append(tokenized_sentence)\n",
        "                    words.extend(tokenized_sentence)\n",
        "\n",
        "    return cantos, words, raw\n",
        "\n",
        "\n",
        "def create_tercets(cantos):\n",
        "    tercets = []\n",
        "    for i,canto in enumerate(cantos):\n",
        "        for v,verse in enumerate(canto):\n",
        "            if v%3 == 0:\n",
        "                tercets.append([])\n",
        "\n",
        "            tercets[-1].append(verse)\n",
        "        tercets = tercets[:-1]  # removes the last malformed tercets (only 2 verses)\n",
        "\n",
        "    return tercets\n",
        "\n",
        "def pad_list(l, pad_token, max_l_size, keep_lasts=False, pad_right=True):\n",
        "    \"\"\"\n",
        "    Adds a padding token to a list\n",
        "    inputs:\n",
        "    :param l: input list to pad.\n",
        "    :param pad_token: value to add as padding.\n",
        "    :param max_l_size: length of the new padded list to return,\n",
        "    it truncates lists longer that 'max_l_size' without adding\n",
        "    padding values.\n",
        "    :param keep_lasts: If True, preserves the max_l_size last elements\n",
        "    of a sequence (by keeping the same order).  E.g.:\n",
        "    if keep_lasts is True and max_l_size=3 [1,2,3,4] becomes [2,3,4].\n",
        "\n",
        "\n",
        "    :return: the list padded or truncated.\n",
        "    \"\"\"\n",
        "    to_pad = []\n",
        "    max_l = min(max_l_size, len(l))  # maximum len\n",
        "    l_init = len(l) - max_l if len(l) > max_l and keep_lasts else 0  # initial position where to sample from the list\n",
        "    l_end = len(l) if len(l) > max_l and keep_lasts else max_l\n",
        "    for i in range(l_init, l_end):\n",
        "        to_pad.append(l[i])\n",
        "\n",
        "    # for j in range(len(l), max_l_size):\n",
        "    #     to_pad.append(pad_token)\n",
        "    pad_tokens = [pad_token] * (max_l_size-len(l))\n",
        "    padded_l = to_pad + pad_tokens if pad_right else pad_tokens + to_pad\n",
        "\n",
        "    return padded_l\n",
        "\n",
        "\n",
        "def save_data(data, file):\n",
        "    with open(file, 'wb') as output:\n",
        "        pickle.dump(data, output, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def load_data(file):\n",
        "    with open(file, 'rb') as obj:\n",
        "        return pickle.load(obj)\n",
        "\n",
        "def print_and_write(file, s):\n",
        "    print(s)\n",
        "    file.write(s)\n",
        "\n",
        "\n",
        "class Vocabulary(object):\n",
        "    def __init__(self, vocab_size=None):\n",
        "        self.dictionary = dict()\n",
        "        self.rev_dictionary = dict()\n",
        "        self.count = []\n",
        "        self.special_tokens = []\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    def build_vocabulary_from_counts(self, count, special_tokens=[]):\n",
        "        \"\"\"\n",
        "        Sets all the attributes of the Vocabulary object.\n",
        "        :param count: a list of lists as follows: [['token', number_of_occurrences],...]\n",
        "        :param special_tokens: a list of strings. E.g. ['<EOS>', '<PAD>',...]\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "\n",
        "        dictionary = dict()\n",
        "        for word, _ in count:\n",
        "            dictionary[word] = len(dictionary)\n",
        "\n",
        "        # adding eventual special tokens to the dictionary (e.g. <EOS>,<PAD> etc..)\n",
        "        d = len(dictionary)\n",
        "        for i, token in enumerate(special_tokens):\n",
        "            dictionary[token] = d + i\n",
        "\n",
        "        self.count = count\n",
        "        self.dictionary = dictionary\n",
        "        self.rev_dictionary = dict(zip(self.dictionary.values(), self.dictionary.keys()))\n",
        "        self.special_tokens = special_tokens\n",
        "        self.vocab_size = len(dictionary)\n",
        "\n",
        "    def build_vocabulary_from_tokens(self, tokens, vocabulary_size=None, special_tokens=[]):\n",
        "        \"\"\"\n",
        "        Given a list of tokens, it sets the Vocabulary object attributes by constructing\n",
        "        a dictionary mapping each token to a unique id.\n",
        "        :param tokens: a list of strings.\n",
        "         E.g. [\"the\", \"cat\", \"is\", ... \".\", \"the\", \"house\" ,\"is\" ...].\n",
        "         NB: Here you should put all your token instances of the corpus.\n",
        "        :param vocabulary_size: The number of elements of your vocabulary. If there are more\n",
        "        than 'vocabulary_size' elements on tokens, it considers only the 'vocabulary_size'\n",
        "        most frequent ones.\n",
        "        :param special_tokens: Optional. A list of strings. Useful to add special tokens in vocabulary.\n",
        "        If you don't have any, keep it empty.\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "\n",
        "        vocabulary_size = vocabulary_size if vocabulary_size is not None else self.vocab_size\n",
        "        vocabulary_size = vocabulary_size - (len(special_tokens) + 1) if vocabulary_size else None\n",
        "        # counts occurrences of each token\n",
        "        count = [['<UNK>', -1]]\n",
        "        count.extend(collections.Counter(tokens).most_common(vocabulary_size))  # takes only the most frequent ones, if size is None takes them all\n",
        "        self.build_vocabulary_from_counts(count, special_tokens)  # actually build the vocabulary\n",
        "        self._set_unk_count(tokens)  # set the number of OOV instances\n",
        "\n",
        "    @staticmethod\n",
        "    def merge_vocabulary(vocab0, vocab1, vocabulary_size=-1):\n",
        "        \"\"\"\n",
        "        Merge two Vocabulary objects into a new one.\n",
        "        :param vocab0: first Vocabulary object\n",
        "        :param vocab1: second Vocabulary object\n",
        "        :param vocabulary_size: parameter to decide the merged vocabulary size.\n",
        "        With default value -1, all the words of both vocabularies are preserved.\n",
        "        When set to 0, the size of the vocabulary is set to the size of vocab0,\n",
        "        when set to 1 it is kept the size of vocab1.\n",
        "        :return: a new vocabulary\n",
        "        \"\"\"\n",
        "        # get size of the new vocabulary\n",
        "        vocab_size = vocab0.vocab_size + vocab1.vocab_size if vocabulary_size == -1 else vocabulary_size\n",
        "        merged_special_tokens = list(set(vocab0.special_tokens) | set(vocab1.special_tokens))\n",
        "\n",
        "        # merge the counts from the two vocabularies and then selects the most_common tokens\n",
        "        merged_counts = collections.Counter(dict(vocab0.count)) + collections.Counter(dict(vocab1.count))\n",
        "        merged_counts = merged_counts.most_common(vocab_size)\n",
        "        count = [['<UNK>', -1]]\n",
        "        count.extend(merged_counts)\n",
        "\n",
        "        # create the new vocabulary\n",
        "        merged_vocab = Vocabulary(vocab_size)\n",
        "        merged_vocab.build_vocabulary_from_counts(count, merged_special_tokens)\n",
        "        return merged_vocab\n",
        "\n",
        "    @staticmethod\n",
        "    def merge_vocabularies(vocab_list, vocab_size=None):\n",
        "        \"\"\"\n",
        "        Join a list of vocabularies into a new one.\n",
        "        :param vocab_list: a list of Vocabulary objects\n",
        "        :param vocab_size: the maximum size of the merged vocabulary.\n",
        "        :return: a vocabulary merging them all.\n",
        "        \"\"\"\n",
        "        vocab_size = vocab_size if vocab_size else sum([v.vocab_size for v in vocab_list])\n",
        "        merged_vocab = Vocabulary(vocab_size)\n",
        "        for voc in vocab_list:\n",
        "            merged_vocab = Vocabulary.merge_vocabulary(merged_vocab, voc, vocab_size)\n",
        "        return merged_vocab\n",
        "\n",
        "    def string2id(self, dataset):\n",
        "        \"\"\"\n",
        "        Converts a dataset of strings into a dataset of ids according to the object dictionary.\n",
        "        :param dataset: any string-based dataset with any nested lists.\n",
        "        :return: a new dataset, with the same shape of dataset, where each string is mapped into its\n",
        "        corresponding id associated in the dictionary (0 for unknown tokens).\n",
        "        \"\"\"\n",
        "\n",
        "        def _recursive_call(items):\n",
        "            new_items = []\n",
        "            for item in items:\n",
        "                if isinstance(item, str) or isinstance(item, int) or isinstance(item, float):\n",
        "                    new_items.append(self.word2id(item))\n",
        "                else:\n",
        "                    new_items.append(_recursive_call(item))\n",
        "            return new_items\n",
        "\n",
        "        return _recursive_call(dataset)\n",
        "\n",
        "    def id2string(self, dataset):\n",
        "        \"\"\"\n",
        "        Converts a dataset of integer ids into a dataset of string according to the reverse dictionary.\n",
        "        :param dataset: any int-based dataset with any nested lists. Allowed types are int, np.int32, np.int64.\n",
        "        :return: a new dataset, with the same shape of dataset, where each token is mapped into its\n",
        "        corresponding string associated in the reverse dictionary.\n",
        "        \"\"\"\n",
        "        def _recursive_call(items):\n",
        "            new_items = []\n",
        "            for item in items:\n",
        "                if isinstance(item, int) or isinstance(item, np.int) or isinstance(item, np.int32) or isinstance(item, np.int64):\n",
        "                    new_items.append(self.id2word(item))\n",
        "                else:\n",
        "                    new_items.append(_recursive_call(item))\n",
        "            return new_items\n",
        "\n",
        "        return _recursive_call(dataset)\n",
        "\n",
        "    def word2id(self, item):\n",
        "        \"\"\"\n",
        "        Maps a string token to its corresponding id.\n",
        "        :param item: a string.\n",
        "        :return: If the token belongs to the vocabulary, it returns an integer id > 0, otherwise\n",
        "        it returns the value associated to the unknown symbol, that is typically 0.\n",
        "        \"\"\"\n",
        "        return self.dictionary[item] if item in self.dictionary else self.dictionary['<UNK>']\n",
        "\n",
        "    def id2word(self, token_id):\n",
        "        \"\"\"\n",
        "        Maps an integer token to its corresponding string.\n",
        "        :param token_id: an integer.\n",
        "        :return: If the id belongs to the vocabulary, it returns the string\n",
        "        associated to it, otherwise it returns the string associated\n",
        "        to the unknown symbol, that is '<UNK>'.\n",
        "        \"\"\"\n",
        "\n",
        "        return self.rev_dictionary[token_id] if token_id in self.rev_dictionary else self.rev_dictionary[self.dictionary['<UNK>']]\n",
        "\n",
        "    def get_unk_count(self):\n",
        "        return self.count[0][1]\n",
        "\n",
        "    def _set_unk_count(self, tokens):\n",
        "        \"\"\"\n",
        "        Sets the number of OOV instances in the tokens provided\n",
        "        :param tokens: a list of tokens\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "        data = list()\n",
        "        unk_count = 0\n",
        "        for word in tokens:\n",
        "            if word in self.dictionary:\n",
        "                index = self.dictionary[word]\n",
        "            else:\n",
        "                index = 0  # dictionary['<UNK>']\n",
        "                unk_count += 1\n",
        "            data.append(index)\n",
        "        self.count[0][1] = unk_count\n",
        "\n",
        "    def add_element(self, name, is_special_token=False):\n",
        "        if name not in self.dictionary:\n",
        "            self.vocab_size += 1\n",
        "            self.dictionary[name] = self.vocab_size\n",
        "            self.rev_dictionary[self.vocab_size] = name\n",
        "\n",
        "            if is_special_token:\n",
        "                self.special_tokens = list(self.special_tokens)\n",
        "                self.special_tokens.append(name)\n",
        "\n",
        "            self.count.append([name, 1])\n",
        "\n",
        "    def set_vocabulary(self, dictionary, rev_dictionary, special_tokens, vocab_size):\n",
        "        self.dictionary = dictionary,\n",
        "        self.rev_dictionary = rev_dictionary\n",
        "        self.special_tokens = special_tokens\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    @staticmethod\n",
        "    def load_vocabulary(filename):\n",
        "        return load_data(filename)\n",
        "\n",
        "    def save_vocabulary(self, filename):\n",
        "        save_data(self, filename)\n",
        "\n",
        "class SyLMDataset(object):\n",
        "    def __init__(self, config, sy_vocab=None):\n",
        "        self.config = config\n",
        "        self.vocabulary = sy_vocab\n",
        "\n",
        "        self.raw_train_x = []\n",
        "        self.raw_val_x = []\n",
        "        self.raw_test_x = []\n",
        "        self.raw_x = []\n",
        "\n",
        "        self.train_x, self.train_y = [], []\n",
        "        self.val_x, self.val_y = [], []\n",
        "        self.test_x, self.test_y = [], []\n",
        "        self.x, self.y = [], []\n",
        "\n",
        "    def initialize(self, sess):\n",
        "        pass\n",
        "\n",
        "    def load(self, sources):\n",
        "        \"\"\"\n",
        "        Extract raw texts form sources and gather them all together.\n",
        "        :param sources: a string or an iterable of strings containing the file(s)\n",
        "        to process in order to build the dataset.\n",
        "        :return: a list of raw strings.\n",
        "        \"\"\"\n",
        "        return NotImplementedError\n",
        "\n",
        "    def build(self, sources, split_size=0.8):\n",
        "        \"\"\"\n",
        "        :param sources: a string or an iterable of strings containing the file(s)\n",
        "        to process in order to build the dataset.\n",
        "        :param split_size: the size to split the dataset, set >=1.0 to not split.\n",
        "        \"\"\"\n",
        "\n",
        "        raw_x = self.load(sources)\n",
        "        # raw_x = self.tokenize([self.preprocess(ex) for ex in raw_x])  # fixme\n",
        "        # splitting data\n",
        "        self.raw_x = raw_x\n",
        "        if split_size < 1.0:\n",
        "            self.raw_train_x, self.raw_test_x = self.split(self.raw_x, train_size=split_size)\n",
        "            self.raw_train_x, self.raw_val_x = self.split(self.raw_train_x, train_size=split_size)\n",
        "        else:\n",
        "            self.raw_train_x = self.raw_x\n",
        "\n",
        "        if self.vocabulary is None:\n",
        "            # creates vocabulary\n",
        "            tokens = [item for sublist in self.raw_train_x for item in sublist]  # get tokens\n",
        "            special_tokens = (\"<GO>\", \"<PAD>\", \"<SEP>\", \"<EOS>\", \"<EOV>\")\n",
        "            self._create_vocab(tokens, special_tokens=special_tokens)\n",
        "\n",
        "        # creates x,y for train\n",
        "        self.train_x = self._build_dataset(self.raw_train_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "        self.train_y = self._build_dataset(self.raw_train_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "\n",
        "        # creates x,y for validation\n",
        "        self.val_x = self._build_dataset(self.raw_val_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "        self.val_y = self._build_dataset(self.raw_val_x, insert_go=False, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "\n",
        "        # creates x,y for validation\n",
        "        self.test_x = self._build_dataset(self.raw_test_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "        self.test_y = self._build_dataset(self.raw_test_x, insert_go=False, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "\n",
        "    def _create_vocab(self, tokens, special_tokens=(\"<PAD>\", \"<GO>\", \"<SEP>\", \"<EOV>\", \"<EOS>\")):\n",
        "        \"\"\"\n",
        "        Create the vocabulary. Special tokens can be added to the tokens obtained from\n",
        "        the corpus.\n",
        "        :param tokens: a list of all the tokens in the corpus. Each token is a string.\n",
        "        :param special_tokens: a list of strings.\n",
        "        \"\"\"\n",
        "        vocab = Vocabulary(vocab_size=self.config.input_vocab_size)\n",
        "        vocab.build_vocabulary_from_tokens(tokens, special_tokens=special_tokens)\n",
        "        self.vocabulary = vocab\n",
        "\n",
        "    @staticmethod\n",
        "    def split(raw_data, train_size=0.8):\n",
        "        size = math.floor(len(raw_data)*train_size)\n",
        "        return raw_data[:size], raw_data[size:]\n",
        "\n",
        "    @staticmethod\n",
        "    def preprocess(txt):\n",
        "        return txt\n",
        "\n",
        "    @staticmethod\n",
        "    def shuffle(x):\n",
        "        return random.sample(x, len(x))\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenize(txt):\n",
        "        return txt\n",
        "\n",
        "    def _build_dataset(self, raw_data, max_len=100, insert_go=True, keep_lasts=False, pad_right=True, shuffle=True):\n",
        "        \"\"\"\n",
        "        Converts all the tokens in e1_raw_data by mapping each token with its corresponding\n",
        "        value in the dictionary. In case of token not in the dictionary, they are assigned to\n",
        "        a specific id. Each sequence is padded up to the seq_max_len setup in the config.\n",
        "\n",
        "        :param raw_data: list of sequences, each sequence is a list of tokens (strings).\n",
        "        :param max_len: max length of a sequence, crop longer and pad smaller ones.\n",
        "        :param insert_go: True to insert <GO>, False otherwise.\n",
        "        :param keep_lasts: True to truncate initial elements of a sequence.\n",
        "        :param pad_right: pad to the right (default value True), otherwise pads to left.\n",
        "        :param shuffle: Optional. If True data are shuffled.\n",
        "        :return: A list of sequences where each token in each sequence is an int id.\n",
        "        \"\"\"\n",
        "        dataset = []\n",
        "        for sentence in raw_data:\n",
        "            sentence_ids = [self.vocabulary.word2id(\"<GO>\")] if insert_go else []\n",
        "            sentence_ids.extend([self.vocabulary.word2id(w) for w in sentence])\n",
        "            sentence_ids.append(self.vocabulary.word2id(\"<EOS>\"))\n",
        "            sentence_ids = pad_list(sentence_ids, self.vocabulary.word2id(\"<PAD>\"), max_len, keep_lasts=keep_lasts, pad_right=pad_right)\n",
        "\n",
        "            dataset.append(sentence_ids)\n",
        "\n",
        "        if shuffle:\n",
        "            return random.sample(dataset, len(dataset))\n",
        "        else:\n",
        "            return dataset\n",
        "\n",
        "    def get_batches(self, batch_size=32):\n",
        "        \"\"\"\n",
        "        Iterator over the training set. Useful method to run experiments.\n",
        "        :param batch_size: size of the mini_batch\n",
        "        :return: input and target.\n",
        "        \"\"\"\n",
        "        x, y = self.train_x, self.train_y\n",
        "        \n",
        "        i = 0#random.randint(0, batch_size)\n",
        "        batches = []\n",
        "        eov = self.vocabulary.word2id(\"<EOV>\")\n",
        "        go = self.vocabulary.word2id(\"<GO>\")\n",
        "        # prepare batches\n",
        "        while i < len(x):\n",
        "            j = 0\n",
        "            batch_x, batch_y = [], []\n",
        "            while j < batch_size and i+j<len(x):\n",
        "                for c in x[i+j]:\n",
        "                  batch_x.append(c)\n",
        "                batch_x.append(eov)\n",
        "                for c in y[i+j]:\n",
        "                  batch_y.append(c)\n",
        "                batch_y.append(eov)\n",
        "                j += 1\n",
        "            i += batch_size\n",
        "            batches.append((batch_x, batch_y))\n",
        "\n",
        "        # supply\n",
        "        i = 0\n",
        "        while i < len(batches):\n",
        "            yield batches[i][0], batches[i][1]\n",
        "            i += 1\n",
        "\n",
        "class DanteSyLMDataset(SyLMDataset):\n",
        "    def __init__(self, config, sy_vocab=None):\n",
        "        \"\"\"\n",
        "        Class to create a dataset from Dante Alighieri's Divine Comedy.\n",
        "        :param config: a Config object\n",
        "        :param sy_vocab: (optional) a Vocabulary object where tokens of the dictionary\n",
        "        are syllables. If None, the vocabulary is create automatically from the source.\n",
        "        \"\"\"\n",
        "        super().__init__(config, sy_vocab)\n",
        "\n",
        "    def load(self, sources):\n",
        "        \"\"\"\n",
        "        Load examples from dataset\n",
        "        :param sources: data filepath.\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        canti, _, raw = get_dc_cantos(filename=sources)  # get raw data from file\n",
        "        canti, tokens = get_dc_hyphenation(canti)  # converts each\n",
        "\n",
        "        tercets = create_tercets(canti)\n",
        "        tercets = get_hyp_lm_tercets(tercets)\n",
        "\n",
        "        x = []\n",
        "        for tercet in tercets:\n",
        "            x.append([])\n",
        "            for verse in tercet:\n",
        "                x[-1].extend(verse)\n",
        "                x[-1].append(\"<EOV>\")\n",
        "\n",
        "        #x = self.shuffle(x)\n",
        "        return x\n",
        "\n",
        "def seq2str(seq):\n",
        "    def output2string(batch, rev_vocabulary, special_tokens, end_of_tokens):\n",
        "        to_print = ''\n",
        "        for token in batch:\n",
        "            if token in special_tokens:\n",
        "                to_print += ' '\n",
        "            elif end_of_tokens and token in end_of_tokens:\n",
        "                to_print += '\\n'\n",
        "            elif token in rev_vocabulary:\n",
        "                to_print += rev_vocabulary[token]\n",
        "            else:\n",
        "                to_print += '<UNK>'\n",
        "        return to_print\n",
        "\n",
        "    return output2string(seq, poetry_sy_lm_dataset.vocabulary.rev_dictionary,\n",
        "      special_tokens=[poetry_sy_lm_dataset.vocabulary.word2id(\"<PAD>\"), 0, poetry_sy_lm_dataset.vocabulary.word2id(\"<SEP>\"),\n",
        "                      poetry_sy_lm_dataset.vocabulary.word2id(\"<GO>\"), poetry_sy_lm_dataset.vocabulary.word2id(\"<EOS>\")],\n",
        "      end_of_tokens=[poetry_sy_lm_dataset.vocabulary.word2id(\"<EOV>\")])\n",
        "\n",
        "class cnfg:\n",
        "  vocab_size = 1884\n",
        "  input_vocab_size = 1884\n",
        "  emb_size = 300\n",
        "  sentence_max_len = 75\n",
        "\n",
        "config = cnfg()\n",
        "poetry_sy_lm_dataset = DanteSyLMDataset(config, sy_vocab=None)\n",
        "url = \"https://gitlab.com/zugo91/nlgpoetry/-/raw/release/data/la_divina_commedia.txt\"\n",
        "response = requests.get(url)\n",
        "response.encoding = 'ISO-8859-1'\n",
        "fi = open(\"divcom.txt\",\"w\")\n",
        "fi.write(response.text)\n",
        "fi.close()\n",
        "data_path = os.path.join(os.getcwd(), \"divcom.txt\")  # dataset location, here just the name of the source file\n",
        "poetry_sy_lm_dataset.build(data_path, split_size=0.9)  # actual creation of  vocabulary (if not provided) and dataset\n",
        "print(\"Train size: \" + str(len(poetry_sy_lm_dataset.train_y)))\n",
        "print(\"Val size: \" + str(len(poetry_sy_lm_dataset.val_y)))\n",
        "print(\"Test size: \" + str(len(poetry_sy_lm_dataset.test_y)))\n",
        "\n",
        "eov = poetry_sy_lm_dataset.vocabulary.word2id(\"<EOV>\")\n",
        "pad = poetry_sy_lm_dataset.vocabulary.word2id(\"<PAD>\")\n",
        "go = poetry_sy_lm_dataset.vocabulary.word2id(\"<GO>\")\n",
        "eos = poetry_sy_lm_dataset.vocabulary.word2id(\"<EOS>\")\n",
        "\n",
        "batches = [b for b in poetry_sy_lm_dataset.get_batches(4)]\n",
        "print(batches[0][0])\n",
        "print(batches[0][1])\n",
        "print(len(batches[0][0]))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train size: 3815\n",
            "Val size: 424\n",
            "Test size: 472\n",
            "[1792, 84, 1794, 339, 198, 1794, 50, 1794, 284, 188, 1794, 6, 1794, 24, 136, 1794, 37, 14, 1796, 27, 1794, 32, 61, 509, 1794, 18, 1794, 52, 5, 1794, 404, 47, 1794, 40, 276, 30, 1796, 89, 1794, 8, 1794, 6, 683, 14, 1794, 264, 1794, 92, 5, 1794, 655, 32, 14, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 585, 1794, 78, 9, 1794, 5, 1794, 238, 1794, 162, 1794, 92, 5, 1794, 57, 1794, 12, 45, 1794, 116, 30, 1796, 4, 63, 1794, 404, 47, 1794, 404, 748, 148, 1794, 4, 1794, 5, 1203, 1794, 4, 1794, 120, 10, 1796, 7, 1794, 84, 1794, 166, 536, 1794, 32, 24, 47, 1794, 8, 1794, 455, 30, 1794, 153, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 792, 1794, 3, 1794, 57, 1794, 5, 22, 30, 1794, 7, 1794, 49, 12, 1794, 57, 1794, 64, 1794, 144, 10, 1796, 22, 1794, 18, 1794, 416, 202, 1794, 50, 1794, 154, 1794, 46, 1794, 3, 1794, 41, 1794, 37, 1794, 61, 509, 1796, 6, 150, 1794, 193, 1794, 3, 1794, 15, 124, 1794, 12, 11, 1794, 46, 1794, 3, 1794, 53, 1794, 3, 1794, 405, 1794, 3, 1794, 456, 1794, 336, 10, 1796, 1796, 1792, 41, 1794, 31, 1794, 23, 1794, 154, 1794, 32, 238, 1794, 131, 1794, 3, 1794, 41, 1794, 405, 1794, 3, 1794, 257, 716, 1796, 792, 1794, 3, 1794, 92, 5, 1794, 163, 24, 1794, 6, 1794, 138, 24, 1794, 5, 1794, 60, 1794, 293, 9, 1796, 7, 1794, 8, 1794, 25, 30, 43, 1794, 264, 1794, 337, 539, 17, 702, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796]\n",
            "[1792, 84, 1794, 339, 198, 1794, 50, 1794, 284, 188, 1794, 6, 1794, 24, 136, 1794, 37, 14, 1796, 27, 1794, 32, 61, 509, 1794, 18, 1794, 52, 5, 1794, 404, 47, 1794, 40, 276, 30, 1796, 89, 1794, 8, 1794, 6, 683, 14, 1794, 264, 1794, 92, 5, 1794, 655, 32, 14, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 585, 1794, 78, 9, 1794, 5, 1794, 238, 1794, 162, 1794, 92, 5, 1794, 57, 1794, 12, 45, 1794, 116, 30, 1796, 4, 63, 1794, 404, 47, 1794, 404, 748, 148, 1794, 4, 1794, 5, 1203, 1794, 4, 1794, 120, 10, 1796, 7, 1794, 84, 1794, 166, 536, 1794, 32, 24, 47, 1794, 8, 1794, 455, 30, 1794, 153, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796, 1792, 792, 1794, 3, 1794, 57, 1794, 5, 22, 30, 1794, 7, 1794, 49, 12, 1794, 57, 1794, 64, 1794, 144, 10, 1796, 22, 1794, 18, 1794, 416, 202, 1794, 50, 1794, 154, 1794, 46, 1794, 3, 1794, 41, 1794, 37, 1794, 61, 509, 1796, 6, 150, 1794, 193, 1794, 3, 1794, 15, 124, 1794, 12, 11, 1794, 46, 1794, 3, 1794, 53, 1794, 3, 1794, 405, 1794, 3, 1794, 456, 1794, 336, 10, 1796, 1796, 1792, 41, 1794, 31, 1794, 23, 1794, 154, 1794, 32, 238, 1794, 131, 1794, 3, 1794, 41, 1794, 405, 1794, 3, 1794, 257, 716, 1796, 792, 1794, 3, 1794, 92, 5, 1794, 163, 24, 1794, 6, 1794, 138, 24, 1794, 5, 1794, 60, 1794, 293, 9, 1796, 7, 1794, 8, 1794, 25, 30, 43, 1794, 264, 1794, 337, 539, 17, 702, 1796, 1795, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1793, 1796]\n",
            "304\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwdCuj3HsFux",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "94b06b5a-2c76-4db3-a23f-ad7a3d2674e1"
      },
      "source": [
        " #@title Train loop\n",
        "\n",
        " for epoch in range(EPOCHS):\n",
        "    random.shuffle(batches)\n",
        "    start = time.time()\n",
        "    \n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    \n",
        "    for (batch, (inp, tar)) in enumerate(batches):\n",
        "        if (len(inp) != batch_len or len(tar) != batch_len):\n",
        "            print(\"discarded batch\", batch)\n",
        "            continue\n",
        "        train_step(np.expand_dims(inp, axis=0), np.expand_dims(tar, axis=0))\n",
        "        \n",
        "        if batch % 50 == 0:\n",
        "            print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
        "                epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
        "        \n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        ckpt_save_path = ckpt_manager.save()\n",
        "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
        "                                                            ckpt_save_path))\n",
        "    \n",
        "    print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
        "                                                    train_loss.result(), \n",
        "                                                    train_accuracy.result()))\n",
        "\n",
        "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 1.5246 Accuracy 0.6766\n",
            "Epoch 1 Batch 50 Loss 1.7262 Accuracy 0.6292\n",
            "Epoch 1 Batch 100 Loss 1.7329 Accuracy 0.6292\n",
            "Epoch 1 Batch 150 Loss 1.7546 Accuracy 0.6271\n",
            "Epoch 1 Batch 200 Loss 1.7827 Accuracy 0.6235\n",
            "Epoch 1 Batch 250 Loss 1.7988 Accuracy 0.6218\n",
            "Epoch 1 Batch 300 Loss 1.8092 Accuracy 0.6205\n",
            "Epoch 1 Batch 350 Loss 1.8144 Accuracy 0.6196\n",
            "Epoch 1 Batch 400 Loss 1.8249 Accuracy 0.6182\n",
            "Epoch 1 Batch 450 Loss 1.8329 Accuracy 0.6171\n",
            "Epoch 1 Batch 500 Loss 1.8393 Accuracy 0.6163\n",
            "Epoch 1 Batch 550 Loss 1.8478 Accuracy 0.6155\n",
            "discarded batch 585\n",
            "Epoch 1 Batch 600 Loss 1.8531 Accuracy 0.6146\n",
            "Epoch 1 Batch 650 Loss 1.8570 Accuracy 0.6140\n",
            "Epoch 1 Batch 700 Loss 1.8624 Accuracy 0.6136\n",
            "Epoch 1 Batch 750 Loss 1.8666 Accuracy 0.6132\n",
            "Epoch 1 Batch 800 Loss 1.8731 Accuracy 0.6125\n",
            "Epoch 1 Batch 850 Loss 1.8781 Accuracy 0.6120\n",
            "Epoch 1 Batch 900 Loss 1.8833 Accuracy 0.6113\n",
            "Epoch 1 Batch 950 Loss 1.8868 Accuracy 0.6108\n",
            "Epoch 1 Loss 1.8869 Accuracy 0.6108\n",
            "Time taken for 1 epoch: 33.44931721687317 secs\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.8050 Accuracy 0.6238\n",
            "Epoch 2 Batch 50 Loss 1.7963 Accuracy 0.6201\n",
            "Epoch 2 Batch 100 Loss 1.8133 Accuracy 0.6190\n",
            "Epoch 2 Batch 150 Loss 1.8141 Accuracy 0.6183\n",
            "Epoch 2 Batch 200 Loss 1.8255 Accuracy 0.6173\n",
            "Epoch 2 Batch 250 Loss 1.8264 Accuracy 0.6181\n",
            "Epoch 2 Batch 300 Loss 1.8317 Accuracy 0.6172\n",
            "discarded batch 326\n",
            "Epoch 2 Batch 350 Loss 1.8389 Accuracy 0.6165\n",
            "Epoch 2 Batch 400 Loss 1.8439 Accuracy 0.6154\n",
            "Epoch 2 Batch 450 Loss 1.8458 Accuracy 0.6150\n",
            "Epoch 2 Batch 500 Loss 1.8503 Accuracy 0.6141\n",
            "Epoch 2 Batch 550 Loss 1.8521 Accuracy 0.6140\n",
            "Epoch 2 Batch 600 Loss 1.8537 Accuracy 0.6143\n",
            "Epoch 2 Batch 650 Loss 1.8588 Accuracy 0.6140\n",
            "Epoch 2 Batch 700 Loss 1.8639 Accuracy 0.6135\n",
            "Epoch 2 Batch 750 Loss 1.8659 Accuracy 0.6136\n",
            "Epoch 2 Batch 800 Loss 1.8693 Accuracy 0.6132\n",
            "Epoch 2 Batch 850 Loss 1.8721 Accuracy 0.6128\n",
            "Epoch 2 Batch 900 Loss 1.8757 Accuracy 0.6126\n",
            "Epoch 2 Batch 950 Loss 1.8767 Accuracy 0.6125\n",
            "Epoch 2 Loss 1.8774 Accuracy 0.6124\n",
            "Time taken for 1 epoch: 33.87730360031128 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.6963 Accuracy 0.6304\n",
            "Epoch 3 Batch 50 Loss 1.7516 Accuracy 0.6259\n",
            "Epoch 3 Batch 100 Loss 1.7580 Accuracy 0.6264\n",
            "Epoch 3 Batch 150 Loss 1.7683 Accuracy 0.6244\n",
            "Epoch 3 Batch 200 Loss 1.7777 Accuracy 0.6232\n",
            "Epoch 3 Batch 250 Loss 1.7865 Accuracy 0.6218\n",
            "Epoch 3 Batch 300 Loss 1.7955 Accuracy 0.6204\n",
            "Epoch 3 Batch 350 Loss 1.8022 Accuracy 0.6200\n",
            "discarded batch 363\n",
            "Epoch 3 Batch 400 Loss 1.8060 Accuracy 0.6198\n",
            "Epoch 3 Batch 450 Loss 1.8058 Accuracy 0.6200\n",
            "Epoch 3 Batch 500 Loss 1.8102 Accuracy 0.6200\n",
            "Epoch 3 Batch 550 Loss 1.8155 Accuracy 0.6193\n",
            "Epoch 3 Batch 600 Loss 1.8190 Accuracy 0.6187\n",
            "Epoch 3 Batch 650 Loss 1.8221 Accuracy 0.6186\n",
            "Epoch 3 Batch 700 Loss 1.8237 Accuracy 0.6185\n",
            "Epoch 3 Batch 750 Loss 1.8254 Accuracy 0.6185\n",
            "Epoch 3 Batch 800 Loss 1.8296 Accuracy 0.6182\n",
            "Epoch 3 Batch 850 Loss 1.8327 Accuracy 0.6182\n",
            "Epoch 3 Batch 900 Loss 1.8359 Accuracy 0.6179\n",
            "Epoch 3 Batch 950 Loss 1.8397 Accuracy 0.6175\n",
            "Epoch 3 Loss 1.8397 Accuracy 0.6174\n",
            "Time taken for 1 epoch: 33.10540246963501 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.6930 Accuracy 0.6469\n",
            "Epoch 4 Batch 50 Loss 1.7284 Accuracy 0.6306\n",
            "Epoch 4 Batch 100 Loss 1.7366 Accuracy 0.6300\n",
            "Epoch 4 Batch 150 Loss 1.7354 Accuracy 0.6310\n",
            "Epoch 4 Batch 200 Loss 1.7386 Accuracy 0.6301\n",
            "Epoch 4 Batch 250 Loss 1.7448 Accuracy 0.6286\n",
            "discarded batch 291\n",
            "Epoch 4 Batch 300 Loss 1.7532 Accuracy 0.6279\n",
            "Epoch 4 Batch 350 Loss 1.7569 Accuracy 0.6276\n",
            "Epoch 4 Batch 400 Loss 1.7620 Accuracy 0.6273\n",
            "Epoch 4 Batch 450 Loss 1.7670 Accuracy 0.6266\n",
            "Epoch 4 Batch 500 Loss 1.7738 Accuracy 0.6261\n",
            "Epoch 4 Batch 550 Loss 1.7785 Accuracy 0.6254\n",
            "Epoch 4 Batch 600 Loss 1.7813 Accuracy 0.6254\n",
            "Epoch 4 Batch 650 Loss 1.7845 Accuracy 0.6251\n",
            "Epoch 4 Batch 700 Loss 1.7888 Accuracy 0.6246\n",
            "Epoch 4 Batch 750 Loss 1.7931 Accuracy 0.6242\n",
            "Epoch 4 Batch 800 Loss 1.7961 Accuracy 0.6238\n",
            "Epoch 4 Batch 850 Loss 1.7985 Accuracy 0.6238\n",
            "Epoch 4 Batch 900 Loss 1.7999 Accuracy 0.6236\n",
            "Epoch 4 Batch 950 Loss 1.8008 Accuracy 0.6236\n",
            "Epoch 4 Loss 1.8010 Accuracy 0.6236\n",
            "Time taken for 1 epoch: 34.11290526390076 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.5160 Accuracy 0.6766\n",
            "Epoch 5 Batch 50 Loss 1.6920 Accuracy 0.6337\n",
            "Epoch 5 Batch 100 Loss 1.7085 Accuracy 0.6331\n",
            "Epoch 5 Batch 150 Loss 1.7077 Accuracy 0.6336\n",
            "Epoch 5 Batch 200 Loss 1.7088 Accuracy 0.6342\n",
            "Epoch 5 Batch 250 Loss 1.7150 Accuracy 0.6338\n",
            "Epoch 5 Batch 300 Loss 1.7249 Accuracy 0.6320\n",
            "Epoch 5 Batch 350 Loss 1.7293 Accuracy 0.6310\n",
            "discarded batch 360\n",
            "Epoch 5 Batch 400 Loss 1.7332 Accuracy 0.6308\n",
            "Epoch 5 Batch 450 Loss 1.7372 Accuracy 0.6306\n",
            "Epoch 5 Batch 500 Loss 1.7393 Accuracy 0.6303\n",
            "Epoch 5 Batch 550 Loss 1.7405 Accuracy 0.6300\n",
            "Epoch 5 Batch 600 Loss 1.7434 Accuracy 0.6298\n",
            "Epoch 5 Batch 650 Loss 1.7470 Accuracy 0.6293\n",
            "Epoch 5 Batch 700 Loss 1.7499 Accuracy 0.6293\n",
            "Epoch 5 Batch 750 Loss 1.7530 Accuracy 0.6289\n",
            "Epoch 5 Batch 800 Loss 1.7561 Accuracy 0.6290\n",
            "Epoch 5 Batch 850 Loss 1.7598 Accuracy 0.6285\n",
            "Epoch 5 Batch 900 Loss 1.7629 Accuracy 0.6282\n",
            "Epoch 5 Batch 950 Loss 1.7671 Accuracy 0.6277\n",
            "Saving checkpoint for epoch 5 at ./checkpoints/train/ckpt-1\n",
            "Epoch 5 Loss 1.7670 Accuracy 0.6277\n",
            "Time taken for 1 epoch: 36.2147479057312 secs\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.6737 Accuracy 0.6370\n",
            "Epoch 6 Batch 50 Loss 1.6785 Accuracy 0.6384\n",
            "Epoch 6 Batch 100 Loss 1.6768 Accuracy 0.6395\n",
            "Epoch 6 Batch 150 Loss 1.6721 Accuracy 0.6398\n",
            "Epoch 6 Batch 200 Loss 1.6823 Accuracy 0.6400\n",
            "discarded batch 249\n",
            "Epoch 6 Batch 250 Loss 1.6800 Accuracy 0.6393\n",
            "Epoch 6 Batch 300 Loss 1.6814 Accuracy 0.6389\n",
            "Epoch 6 Batch 350 Loss 1.6875 Accuracy 0.6378\n",
            "Epoch 6 Batch 400 Loss 1.6970 Accuracy 0.6368\n",
            "Epoch 6 Batch 450 Loss 1.7003 Accuracy 0.6366\n",
            "Epoch 6 Batch 500 Loss 1.7038 Accuracy 0.6361\n",
            "Epoch 6 Batch 550 Loss 1.7078 Accuracy 0.6357\n",
            "Epoch 6 Batch 600 Loss 1.7110 Accuracy 0.6353\n",
            "Epoch 6 Batch 650 Loss 1.7137 Accuracy 0.6348\n",
            "Epoch 6 Batch 700 Loss 1.7173 Accuracy 0.6344\n",
            "Epoch 6 Batch 750 Loss 1.7181 Accuracy 0.6344\n",
            "Epoch 6 Batch 800 Loss 1.7208 Accuracy 0.6342\n",
            "Epoch 6 Batch 850 Loss 1.7255 Accuracy 0.6335\n",
            "Epoch 6 Batch 900 Loss 1.7276 Accuracy 0.6334\n",
            "Epoch 6 Batch 950 Loss 1.7312 Accuracy 0.6329\n",
            "Epoch 6 Loss 1.7314 Accuracy 0.6329\n",
            "Time taken for 1 epoch: 34.89782094955444 secs\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.3984 Accuracy 0.6997\n",
            "Epoch 7 Batch 50 Loss 1.6413 Accuracy 0.6416\n",
            "Epoch 7 Batch 100 Loss 1.6285 Accuracy 0.6466\n",
            "discarded batch 144\n",
            "Epoch 7 Batch 150 Loss 1.6396 Accuracy 0.6440\n",
            "Epoch 7 Batch 200 Loss 1.6489 Accuracy 0.6433\n",
            "Epoch 7 Batch 250 Loss 1.6533 Accuracy 0.6429\n",
            "Epoch 7 Batch 300 Loss 1.6563 Accuracy 0.6437\n",
            "Epoch 7 Batch 350 Loss 1.6619 Accuracy 0.6427\n",
            "Epoch 7 Batch 400 Loss 1.6653 Accuracy 0.6420\n",
            "Epoch 7 Batch 450 Loss 1.6696 Accuracy 0.6417\n",
            "Epoch 7 Batch 500 Loss 1.6741 Accuracy 0.6411\n",
            "Epoch 7 Batch 550 Loss 1.6789 Accuracy 0.6405\n",
            "Epoch 7 Batch 600 Loss 1.6828 Accuracy 0.6400\n",
            "Epoch 7 Batch 650 Loss 1.6849 Accuracy 0.6399\n",
            "Epoch 7 Batch 700 Loss 1.6866 Accuracy 0.6398\n",
            "Epoch 7 Batch 750 Loss 1.6897 Accuracy 0.6394\n",
            "Epoch 7 Batch 800 Loss 1.6947 Accuracy 0.6386\n",
            "Epoch 7 Batch 850 Loss 1.6961 Accuracy 0.6384\n",
            "Epoch 7 Batch 900 Loss 1.6988 Accuracy 0.6381\n",
            "Epoch 7 Batch 950 Loss 1.7021 Accuracy 0.6376\n",
            "Epoch 7 Loss 1.7019 Accuracy 0.6376\n",
            "Time taken for 1 epoch: 35.30156183242798 secs\n",
            "\n",
            "Epoch 8 Batch 0 Loss 1.5972 Accuracy 0.6568\n",
            "Epoch 8 Batch 50 Loss 1.5969 Accuracy 0.6515\n",
            "Epoch 8 Batch 100 Loss 1.5931 Accuracy 0.6513\n",
            "Epoch 8 Batch 150 Loss 1.6010 Accuracy 0.6509\n",
            "Epoch 8 Batch 200 Loss 1.6097 Accuracy 0.6495\n",
            "discarded batch 242\n",
            "Epoch 8 Batch 250 Loss 1.6116 Accuracy 0.6496\n",
            "Epoch 8 Batch 300 Loss 1.6199 Accuracy 0.6486\n",
            "Epoch 8 Batch 350 Loss 1.6287 Accuracy 0.6474\n",
            "Epoch 8 Batch 400 Loss 1.6348 Accuracy 0.6467\n",
            "Epoch 8 Batch 450 Loss 1.6393 Accuracy 0.6461\n",
            "Epoch 8 Batch 500 Loss 1.6418 Accuracy 0.6455\n",
            "Epoch 8 Batch 550 Loss 1.6462 Accuracy 0.6447\n",
            "Epoch 8 Batch 600 Loss 1.6525 Accuracy 0.6439\n",
            "Epoch 8 Batch 650 Loss 1.6551 Accuracy 0.6436\n",
            "Epoch 8 Batch 700 Loss 1.6599 Accuracy 0.6429\n",
            "Epoch 8 Batch 750 Loss 1.6645 Accuracy 0.6425\n",
            "Epoch 8 Batch 800 Loss 1.6678 Accuracy 0.6422\n",
            "Epoch 8 Batch 850 Loss 1.6717 Accuracy 0.6419\n",
            "Epoch 8 Batch 900 Loss 1.6735 Accuracy 0.6417\n",
            "Epoch 8 Batch 950 Loss 1.6774 Accuracy 0.6412\n",
            "Epoch 8 Loss 1.6777 Accuracy 0.6411\n",
            "Time taken for 1 epoch: 36.572394371032715 secs\n",
            "\n",
            "Epoch 9 Batch 0 Loss 1.5874 Accuracy 0.6601\n",
            "Epoch 9 Batch 50 Loss 1.5786 Accuracy 0.6537\n",
            "Epoch 9 Batch 100 Loss 1.5852 Accuracy 0.6542\n",
            "discarded batch 102\n",
            "Epoch 9 Batch 150 Loss 1.5884 Accuracy 0.6544\n",
            "Epoch 9 Batch 200 Loss 1.5967 Accuracy 0.6530\n",
            "Epoch 9 Batch 250 Loss 1.5973 Accuracy 0.6528\n",
            "Epoch 9 Batch 300 Loss 1.6010 Accuracy 0.6525\n",
            "Epoch 9 Batch 350 Loss 1.6089 Accuracy 0.6509\n",
            "Epoch 9 Batch 400 Loss 1.6148 Accuracy 0.6504\n",
            "Epoch 9 Batch 450 Loss 1.6193 Accuracy 0.6495\n",
            "Epoch 9 Batch 500 Loss 1.6264 Accuracy 0.6486\n",
            "Epoch 9 Batch 550 Loss 1.6285 Accuracy 0.6483\n",
            "Epoch 9 Batch 600 Loss 1.6310 Accuracy 0.6481\n",
            "Epoch 9 Batch 650 Loss 1.6342 Accuracy 0.6477\n",
            "Epoch 9 Batch 700 Loss 1.6364 Accuracy 0.6471\n",
            "Epoch 9 Batch 750 Loss 1.6395 Accuracy 0.6467\n",
            "Epoch 9 Batch 800 Loss 1.6419 Accuracy 0.6466\n",
            "Epoch 9 Batch 850 Loss 1.6463 Accuracy 0.6461\n",
            "Epoch 9 Batch 900 Loss 1.6485 Accuracy 0.6458\n",
            "Epoch 9 Batch 950 Loss 1.6515 Accuracy 0.6454\n",
            "Epoch 9 Loss 1.6518 Accuracy 0.6454\n",
            "Time taken for 1 epoch: 35.8984739780426 secs\n",
            "\n",
            "Epoch 10 Batch 0 Loss 1.5591 Accuracy 0.6733\n",
            "Epoch 10 Batch 50 Loss 1.5554 Accuracy 0.6588\n",
            "Epoch 10 Batch 100 Loss 1.5623 Accuracy 0.6583\n",
            "Epoch 10 Batch 150 Loss 1.5634 Accuracy 0.6579\n",
            "Epoch 10 Batch 200 Loss 1.5683 Accuracy 0.6572\n",
            "discarded batch 208\n",
            "Epoch 10 Batch 250 Loss 1.5759 Accuracy 0.6563\n",
            "Epoch 10 Batch 300 Loss 1.5847 Accuracy 0.6546\n",
            "Epoch 10 Batch 350 Loss 1.5890 Accuracy 0.6540\n",
            "Epoch 10 Batch 400 Loss 1.5939 Accuracy 0.6530\n",
            "Epoch 10 Batch 450 Loss 1.5972 Accuracy 0.6527\n",
            "Epoch 10 Batch 500 Loss 1.5991 Accuracy 0.6524\n",
            "Epoch 10 Batch 550 Loss 1.6050 Accuracy 0.6512\n",
            "Epoch 10 Batch 600 Loss 1.6078 Accuracy 0.6510\n",
            "Epoch 10 Batch 650 Loss 1.6097 Accuracy 0.6508\n",
            "Epoch 10 Batch 700 Loss 1.6124 Accuracy 0.6503\n",
            "Epoch 10 Batch 750 Loss 1.6160 Accuracy 0.6500\n",
            "Epoch 10 Batch 800 Loss 1.6200 Accuracy 0.6497\n",
            "Epoch 10 Batch 850 Loss 1.6231 Accuracy 0.6493\n",
            "Epoch 10 Batch 900 Loss 1.6260 Accuracy 0.6488\n",
            "Epoch 10 Batch 950 Loss 1.6288 Accuracy 0.6483\n",
            "Saving checkpoint for epoch 10 at ./checkpoints/train/ckpt-2\n",
            "Epoch 10 Loss 1.6292 Accuracy 0.6483\n",
            "Time taken for 1 epoch: 36.795143365859985 secs\n",
            "\n",
            "Epoch 11 Batch 0 Loss 1.5280 Accuracy 0.6964\n",
            "Epoch 11 Batch 50 Loss 1.5228 Accuracy 0.6649\n",
            "Epoch 11 Batch 100 Loss 1.5432 Accuracy 0.6613\n",
            "Epoch 11 Batch 150 Loss 1.5524 Accuracy 0.6590\n",
            "Epoch 11 Batch 200 Loss 1.5562 Accuracy 0.6576\n",
            "Epoch 11 Batch 250 Loss 1.5604 Accuracy 0.6576\n",
            "Epoch 11 Batch 300 Loss 1.5622 Accuracy 0.6572\n",
            "Epoch 11 Batch 350 Loss 1.5680 Accuracy 0.6569\n",
            "Epoch 11 Batch 400 Loss 1.5711 Accuracy 0.6565\n",
            "Epoch 11 Batch 450 Loss 1.5757 Accuracy 0.6559\n",
            "Epoch 11 Batch 500 Loss 1.5821 Accuracy 0.6552\n",
            "Epoch 11 Batch 550 Loss 1.5850 Accuracy 0.6549\n",
            "Epoch 11 Batch 600 Loss 1.5887 Accuracy 0.6543\n",
            "discarded batch 622\n",
            "Epoch 11 Batch 650 Loss 1.5909 Accuracy 0.6542\n",
            "Epoch 11 Batch 700 Loss 1.5934 Accuracy 0.6537\n",
            "Epoch 11 Batch 750 Loss 1.5973 Accuracy 0.6532\n",
            "Epoch 11 Batch 800 Loss 1.6010 Accuracy 0.6528\n",
            "Epoch 11 Batch 850 Loss 1.6043 Accuracy 0.6526\n",
            "Epoch 11 Batch 900 Loss 1.6066 Accuracy 0.6522\n",
            "Epoch 11 Batch 950 Loss 1.6086 Accuracy 0.6522\n",
            "Epoch 11 Loss 1.6090 Accuracy 0.6521\n",
            "Time taken for 1 epoch: 35.27927613258362 secs\n",
            "\n",
            "Epoch 12 Batch 0 Loss 1.5521 Accuracy 0.6469\n",
            "Epoch 12 Batch 50 Loss 1.4889 Accuracy 0.6658\n",
            "discarded batch 85\n",
            "Epoch 12 Batch 100 Loss 1.5119 Accuracy 0.6636\n",
            "Epoch 12 Batch 150 Loss 1.5231 Accuracy 0.6605\n",
            "Epoch 12 Batch 200 Loss 1.5284 Accuracy 0.6610\n",
            "Epoch 12 Batch 250 Loss 1.5371 Accuracy 0.6606\n",
            "Epoch 12 Batch 300 Loss 1.5442 Accuracy 0.6592\n",
            "Epoch 12 Batch 350 Loss 1.5523 Accuracy 0.6584\n",
            "Epoch 12 Batch 400 Loss 1.5566 Accuracy 0.6580\n",
            "Epoch 12 Batch 450 Loss 1.5602 Accuracy 0.6575\n",
            "Epoch 12 Batch 500 Loss 1.5611 Accuracy 0.6574\n",
            "Epoch 12 Batch 550 Loss 1.5634 Accuracy 0.6574\n",
            "Epoch 12 Batch 600 Loss 1.5680 Accuracy 0.6568\n",
            "Epoch 12 Batch 650 Loss 1.5704 Accuracy 0.6566\n",
            "Epoch 12 Batch 700 Loss 1.5726 Accuracy 0.6565\n",
            "Epoch 12 Batch 750 Loss 1.5743 Accuracy 0.6564\n",
            "Epoch 12 Batch 800 Loss 1.5785 Accuracy 0.6559\n",
            "Epoch 12 Batch 850 Loss 1.5824 Accuracy 0.6556\n",
            "Epoch 12 Batch 900 Loss 1.5847 Accuracy 0.6552\n",
            "Epoch 12 Batch 950 Loss 1.5881 Accuracy 0.6547\n",
            "Epoch 12 Loss 1.5883 Accuracy 0.6547\n",
            "Time taken for 1 epoch: 33.821988344192505 secs\n",
            "\n",
            "Epoch 13 Batch 0 Loss 1.3527 Accuracy 0.7195\n",
            "discarded batch 37\n",
            "Epoch 13 Batch 50 Loss 1.4775 Accuracy 0.6704\n",
            "Epoch 13 Batch 100 Loss 1.5024 Accuracy 0.6682\n",
            "Epoch 13 Batch 150 Loss 1.5102 Accuracy 0.6671\n",
            "Epoch 13 Batch 200 Loss 1.5164 Accuracy 0.6657\n",
            "Epoch 13 Batch 250 Loss 1.5179 Accuracy 0.6655\n",
            "Epoch 13 Batch 300 Loss 1.5230 Accuracy 0.6649\n",
            "Epoch 13 Batch 350 Loss 1.5281 Accuracy 0.6643\n",
            "Epoch 13 Batch 400 Loss 1.5327 Accuracy 0.6638\n",
            "Epoch 13 Batch 450 Loss 1.5346 Accuracy 0.6634\n",
            "Epoch 13 Batch 500 Loss 1.5392 Accuracy 0.6626\n",
            "Epoch 13 Batch 550 Loss 1.5403 Accuracy 0.6622\n",
            "Epoch 13 Batch 600 Loss 1.5468 Accuracy 0.6608\n",
            "Epoch 13 Batch 650 Loss 1.5494 Accuracy 0.6605\n",
            "Epoch 13 Batch 700 Loss 1.5532 Accuracy 0.6599\n",
            "Epoch 13 Batch 750 Loss 1.5572 Accuracy 0.6594\n",
            "Epoch 13 Batch 800 Loss 1.5595 Accuracy 0.6590\n",
            "Epoch 13 Batch 850 Loss 1.5620 Accuracy 0.6587\n",
            "Epoch 13 Batch 900 Loss 1.5643 Accuracy 0.6584\n",
            "Epoch 13 Batch 950 Loss 1.5673 Accuracy 0.6580\n",
            "Epoch 13 Loss 1.5677 Accuracy 0.6579\n",
            "Time taken for 1 epoch: 33.780391454696655 secs\n",
            "\n",
            "Epoch 14 Batch 0 Loss 1.4063 Accuracy 0.6634\n",
            "Epoch 14 Batch 50 Loss 1.4710 Accuracy 0.6715\n",
            "Epoch 14 Batch 100 Loss 1.4893 Accuracy 0.6682\n",
            "Epoch 14 Batch 150 Loss 1.4887 Accuracy 0.6692\n",
            "Epoch 14 Batch 200 Loss 1.4911 Accuracy 0.6686\n",
            "Epoch 14 Batch 250 Loss 1.4952 Accuracy 0.6682\n",
            "Epoch 14 Batch 300 Loss 1.5045 Accuracy 0.6668\n",
            "Epoch 14 Batch 350 Loss 1.5096 Accuracy 0.6660\n",
            "Epoch 14 Batch 400 Loss 1.5119 Accuracy 0.6658\n",
            "Epoch 14 Batch 450 Loss 1.5165 Accuracy 0.6651\n",
            "Epoch 14 Batch 500 Loss 1.5201 Accuracy 0.6645\n",
            "Epoch 14 Batch 550 Loss 1.5243 Accuracy 0.6641\n",
            "Epoch 14 Batch 600 Loss 1.5269 Accuracy 0.6639\n",
            "Epoch 14 Batch 650 Loss 1.5308 Accuracy 0.6630\n",
            "Epoch 14 Batch 700 Loss 1.5341 Accuracy 0.6625\n",
            "Epoch 14 Batch 750 Loss 1.5382 Accuracy 0.6622\n",
            "Epoch 14 Batch 800 Loss 1.5407 Accuracy 0.6617\n",
            "Epoch 14 Batch 850 Loss 1.5447 Accuracy 0.6613\n",
            "Epoch 14 Batch 900 Loss 1.5467 Accuracy 0.6610\n",
            "discarded batch 943\n",
            "Epoch 14 Batch 950 Loss 1.5491 Accuracy 0.6607\n",
            "Epoch 14 Loss 1.5495 Accuracy 0.6606\n",
            "Time taken for 1 epoch: 33.564489126205444 secs\n",
            "\n",
            "Epoch 15 Batch 0 Loss 1.6334 Accuracy 0.6337\n",
            "discarded batch 27\n",
            "Epoch 15 Batch 50 Loss 1.4714 Accuracy 0.6711\n",
            "Epoch 15 Batch 100 Loss 1.4625 Accuracy 0.6756\n",
            "Epoch 15 Batch 150 Loss 1.4682 Accuracy 0.6735\n",
            "Epoch 15 Batch 200 Loss 1.4746 Accuracy 0.6726\n",
            "Epoch 15 Batch 250 Loss 1.4757 Accuracy 0.6721\n",
            "Epoch 15 Batch 300 Loss 1.4804 Accuracy 0.6712\n",
            "Epoch 15 Batch 350 Loss 1.4860 Accuracy 0.6705\n",
            "Epoch 15 Batch 400 Loss 1.4935 Accuracy 0.6693\n",
            "Epoch 15 Batch 450 Loss 1.4961 Accuracy 0.6692\n",
            "Epoch 15 Batch 500 Loss 1.4999 Accuracy 0.6685\n",
            "Epoch 15 Batch 550 Loss 1.5036 Accuracy 0.6677\n",
            "Epoch 15 Batch 600 Loss 1.5071 Accuracy 0.6671\n",
            "Epoch 15 Batch 650 Loss 1.5126 Accuracy 0.6664\n",
            "Epoch 15 Batch 700 Loss 1.5173 Accuracy 0.6658\n",
            "Epoch 15 Batch 750 Loss 1.5211 Accuracy 0.6654\n",
            "Epoch 15 Batch 800 Loss 1.5243 Accuracy 0.6649\n",
            "Epoch 15 Batch 850 Loss 1.5280 Accuracy 0.6644\n",
            "Epoch 15 Batch 900 Loss 1.5315 Accuracy 0.6639\n",
            "Epoch 15 Batch 950 Loss 1.5350 Accuracy 0.6634\n",
            "Saving checkpoint for epoch 15 at ./checkpoints/train/ckpt-3\n",
            "Epoch 15 Loss 1.5353 Accuracy 0.6634\n",
            "Time taken for 1 epoch: 33.9581458568573 secs\n",
            "\n",
            "Epoch 16 Batch 0 Loss 1.2613 Accuracy 0.7261\n",
            "Epoch 16 Batch 50 Loss 1.4415 Accuracy 0.6794\n",
            "Epoch 16 Batch 100 Loss 1.4478 Accuracy 0.6764\n",
            "Epoch 16 Batch 150 Loss 1.4515 Accuracy 0.6756\n",
            "Epoch 16 Batch 200 Loss 1.4629 Accuracy 0.6733\n",
            "Epoch 16 Batch 250 Loss 1.4656 Accuracy 0.6731\n",
            "Epoch 16 Batch 300 Loss 1.4701 Accuracy 0.6726\n",
            "discarded batch 339\n",
            "Epoch 16 Batch 350 Loss 1.4694 Accuracy 0.6729\n",
            "Epoch 16 Batch 400 Loss 1.4760 Accuracy 0.6718\n",
            "Epoch 16 Batch 450 Loss 1.4800 Accuracy 0.6715\n",
            "Epoch 16 Batch 500 Loss 1.4821 Accuracy 0.6711\n",
            "Epoch 16 Batch 550 Loss 1.4876 Accuracy 0.6706\n",
            "Epoch 16 Batch 600 Loss 1.4905 Accuracy 0.6700\n",
            "Epoch 16 Batch 650 Loss 1.4949 Accuracy 0.6695\n",
            "Epoch 16 Batch 700 Loss 1.4975 Accuracy 0.6690\n",
            "Epoch 16 Batch 750 Loss 1.5012 Accuracy 0.6684\n",
            "Epoch 16 Batch 800 Loss 1.5051 Accuracy 0.6681\n",
            "Epoch 16 Batch 850 Loss 1.5084 Accuracy 0.6675\n",
            "Epoch 16 Batch 900 Loss 1.5123 Accuracy 0.6671\n",
            "Epoch 16 Batch 950 Loss 1.5161 Accuracy 0.6665\n",
            "Epoch 16 Loss 1.5163 Accuracy 0.6665\n",
            "Time taken for 1 epoch: 33.63599610328674 secs\n",
            "\n",
            "Epoch 17 Batch 0 Loss 1.3761 Accuracy 0.7030\n",
            "Epoch 17 Batch 50 Loss 1.4299 Accuracy 0.6814\n",
            "Epoch 17 Batch 100 Loss 1.4348 Accuracy 0.6800\n",
            "Epoch 17 Batch 150 Loss 1.4412 Accuracy 0.6786\n",
            "Epoch 17 Batch 200 Loss 1.4485 Accuracy 0.6770\n",
            "Epoch 17 Batch 250 Loss 1.4542 Accuracy 0.6757\n",
            "Epoch 17 Batch 300 Loss 1.4586 Accuracy 0.6749\n",
            "Epoch 17 Batch 350 Loss 1.4628 Accuracy 0.6745\n",
            "Epoch 17 Batch 400 Loss 1.4670 Accuracy 0.6741\n",
            "discarded batch 406\n",
            "Epoch 17 Batch 450 Loss 1.4683 Accuracy 0.6741\n",
            "Epoch 17 Batch 500 Loss 1.4736 Accuracy 0.6732\n",
            "Epoch 17 Batch 550 Loss 1.4770 Accuracy 0.6723\n",
            "Epoch 17 Batch 600 Loss 1.4804 Accuracy 0.6718\n",
            "Epoch 17 Batch 650 Loss 1.4833 Accuracy 0.6711\n",
            "Epoch 17 Batch 700 Loss 1.4874 Accuracy 0.6706\n",
            "Epoch 17 Batch 750 Loss 1.4903 Accuracy 0.6705\n",
            "Epoch 17 Batch 800 Loss 1.4935 Accuracy 0.6702\n",
            "Epoch 17 Batch 850 Loss 1.4974 Accuracy 0.6697\n",
            "Epoch 17 Batch 900 Loss 1.4983 Accuracy 0.6696\n",
            "Epoch 17 Batch 950 Loss 1.5014 Accuracy 0.6691\n",
            "Epoch 17 Loss 1.5014 Accuracy 0.6691\n",
            "Time taken for 1 epoch: 33.820441484451294 secs\n",
            "\n",
            "Epoch 18 Batch 0 Loss 1.5068 Accuracy 0.6502\n",
            "Epoch 18 Batch 50 Loss 1.4121 Accuracy 0.6811\n",
            "Epoch 18 Batch 100 Loss 1.4253 Accuracy 0.6786\n",
            "Epoch 18 Batch 150 Loss 1.4301 Accuracy 0.6783\n",
            "Epoch 18 Batch 200 Loss 1.4345 Accuracy 0.6779\n",
            "Epoch 18 Batch 250 Loss 1.4445 Accuracy 0.6762\n",
            "Epoch 18 Batch 300 Loss 1.4510 Accuracy 0.6752\n",
            "Epoch 18 Batch 350 Loss 1.4533 Accuracy 0.6749\n",
            "Epoch 18 Batch 400 Loss 1.4607 Accuracy 0.6741\n",
            "Epoch 18 Batch 450 Loss 1.4622 Accuracy 0.6738\n",
            "Epoch 18 Batch 500 Loss 1.4656 Accuracy 0.6734\n",
            "Epoch 18 Batch 550 Loss 1.4679 Accuracy 0.6733\n",
            "Epoch 18 Batch 600 Loss 1.4699 Accuracy 0.6731\n",
            "Epoch 18 Batch 650 Loss 1.4706 Accuracy 0.6734\n",
            "discarded batch 666\n",
            "Epoch 18 Batch 700 Loss 1.4748 Accuracy 0.6726\n",
            "Epoch 18 Batch 750 Loss 1.4767 Accuracy 0.6725\n",
            "Epoch 18 Batch 800 Loss 1.4787 Accuracy 0.6721\n",
            "Epoch 18 Batch 850 Loss 1.4819 Accuracy 0.6716\n",
            "Epoch 18 Batch 900 Loss 1.4843 Accuracy 0.6712\n",
            "Epoch 18 Batch 950 Loss 1.4873 Accuracy 0.6707\n",
            "Epoch 18 Loss 1.4874 Accuracy 0.6707\n",
            "Time taken for 1 epoch: 34.16314339637756 secs\n",
            "\n",
            "Epoch 19 Batch 0 Loss 1.4077 Accuracy 0.6865\n",
            "Epoch 19 Batch 50 Loss 1.3886 Accuracy 0.6847\n",
            "Epoch 19 Batch 100 Loss 1.4073 Accuracy 0.6821\n",
            "Epoch 19 Batch 150 Loss 1.4095 Accuracy 0.6810\n",
            "Epoch 19 Batch 200 Loss 1.4099 Accuracy 0.6818\n",
            "Epoch 19 Batch 250 Loss 1.4174 Accuracy 0.6804\n",
            "Epoch 19 Batch 300 Loss 1.4228 Accuracy 0.6801\n",
            "Epoch 19 Batch 350 Loss 1.4265 Accuracy 0.6793\n",
            "Epoch 19 Batch 400 Loss 1.4324 Accuracy 0.6785\n",
            "discarded batch 438\n",
            "Epoch 19 Batch 450 Loss 1.4366 Accuracy 0.6780\n",
            "Epoch 19 Batch 500 Loss 1.4408 Accuracy 0.6777\n",
            "Epoch 19 Batch 550 Loss 1.4455 Accuracy 0.6775\n",
            "Epoch 19 Batch 600 Loss 1.4501 Accuracy 0.6768\n",
            "Epoch 19 Batch 650 Loss 1.4530 Accuracy 0.6763\n",
            "Epoch 19 Batch 700 Loss 1.4577 Accuracy 0.6758\n",
            "Epoch 19 Batch 750 Loss 1.4593 Accuracy 0.6756\n",
            "Epoch 19 Batch 800 Loss 1.4628 Accuracy 0.6751\n",
            "Epoch 19 Batch 850 Loss 1.4667 Accuracy 0.6746\n",
            "Epoch 19 Batch 900 Loss 1.4697 Accuracy 0.6740\n",
            "Epoch 19 Batch 950 Loss 1.4738 Accuracy 0.6733\n",
            "Epoch 19 Loss 1.4741 Accuracy 0.6732\n",
            "Time taken for 1 epoch: 33.72943067550659 secs\n",
            "\n",
            "Epoch 20 Batch 0 Loss 1.3694 Accuracy 0.6898\n",
            "Epoch 20 Batch 50 Loss 1.3849 Accuracy 0.6889\n",
            "Epoch 20 Batch 100 Loss 1.3939 Accuracy 0.6865\n",
            "Epoch 20 Batch 150 Loss 1.3993 Accuracy 0.6854\n",
            "Epoch 20 Batch 200 Loss 1.4075 Accuracy 0.6837\n",
            "Epoch 20 Batch 250 Loss 1.4113 Accuracy 0.6814\n",
            "discarded batch 287\n",
            "Epoch 20 Batch 300 Loss 1.4150 Accuracy 0.6810\n",
            "Epoch 20 Batch 350 Loss 1.4175 Accuracy 0.6815\n",
            "Epoch 20 Batch 400 Loss 1.4230 Accuracy 0.6803\n",
            "Epoch 20 Batch 450 Loss 1.4279 Accuracy 0.6799\n",
            "Epoch 20 Batch 500 Loss 1.4319 Accuracy 0.6793\n",
            "Epoch 20 Batch 550 Loss 1.4351 Accuracy 0.6790\n",
            "Epoch 20 Batch 600 Loss 1.4388 Accuracy 0.6784\n",
            "Epoch 20 Batch 650 Loss 1.4434 Accuracy 0.6779\n",
            "Epoch 20 Batch 700 Loss 1.4457 Accuracy 0.6777\n",
            "Epoch 20 Batch 750 Loss 1.4487 Accuracy 0.6771\n",
            "Epoch 20 Batch 800 Loss 1.4509 Accuracy 0.6768\n",
            "Epoch 20 Batch 850 Loss 1.4543 Accuracy 0.6763\n",
            "Epoch 20 Batch 900 Loss 1.4586 Accuracy 0.6757\n",
            "Epoch 20 Batch 950 Loss 1.4613 Accuracy 0.6755\n",
            "Saving checkpoint for epoch 20 at ./checkpoints/train/ckpt-4\n",
            "Epoch 20 Loss 1.4614 Accuracy 0.6754\n",
            "Time taken for 1 epoch: 34.75275540351868 secs\n",
            "\n",
            "Epoch 21 Batch 0 Loss 1.3899 Accuracy 0.6832\n",
            "Epoch 21 Batch 50 Loss 1.3663 Accuracy 0.6880\n",
            "Epoch 21 Batch 100 Loss 1.3688 Accuracy 0.6881\n",
            "Epoch 21 Batch 150 Loss 1.3812 Accuracy 0.6867\n",
            "Epoch 21 Batch 200 Loss 1.3926 Accuracy 0.6847\n",
            "Epoch 21 Batch 250 Loss 1.3961 Accuracy 0.6844\n",
            "Epoch 21 Batch 300 Loss 1.3991 Accuracy 0.6842\n",
            "Epoch 21 Batch 350 Loss 1.4033 Accuracy 0.6831\n",
            "Epoch 21 Batch 400 Loss 1.4102 Accuracy 0.6819\n",
            "Epoch 21 Batch 450 Loss 1.4145 Accuracy 0.6812\n",
            "Epoch 21 Batch 500 Loss 1.4183 Accuracy 0.6806\n",
            "Epoch 21 Batch 550 Loss 1.4217 Accuracy 0.6805\n",
            "Epoch 21 Batch 600 Loss 1.4258 Accuracy 0.6798\n",
            "Epoch 21 Batch 650 Loss 1.4278 Accuracy 0.6796\n",
            "discarded batch 690\n",
            "Epoch 21 Batch 700 Loss 1.4324 Accuracy 0.6792\n",
            "Epoch 21 Batch 750 Loss 1.4355 Accuracy 0.6786\n",
            "Epoch 21 Batch 800 Loss 1.4388 Accuracy 0.6781\n",
            "Epoch 21 Batch 850 Loss 1.4422 Accuracy 0.6776\n",
            "Epoch 21 Batch 900 Loss 1.4450 Accuracy 0.6775\n",
            "Epoch 21 Batch 950 Loss 1.4475 Accuracy 0.6771\n",
            "Epoch 21 Loss 1.4478 Accuracy 0.6770\n",
            "Time taken for 1 epoch: 33.629353046417236 secs\n",
            "\n",
            "Epoch 22 Batch 0 Loss 1.2058 Accuracy 0.7129\n",
            "Epoch 22 Batch 50 Loss 1.3531 Accuracy 0.6874\n",
            "Epoch 22 Batch 100 Loss 1.3612 Accuracy 0.6875\n",
            "Epoch 22 Batch 150 Loss 1.3831 Accuracy 0.6850\n",
            "Epoch 22 Batch 200 Loss 1.3831 Accuracy 0.6849\n",
            "Epoch 22 Batch 250 Loss 1.3848 Accuracy 0.6852\n",
            "Epoch 22 Batch 300 Loss 1.3910 Accuracy 0.6840\n",
            "Epoch 22 Batch 350 Loss 1.3953 Accuracy 0.6838\n",
            "Epoch 22 Batch 400 Loss 1.3982 Accuracy 0.6839\n",
            "Epoch 22 Batch 450 Loss 1.4021 Accuracy 0.6832\n",
            "Epoch 22 Batch 500 Loss 1.4066 Accuracy 0.6828\n",
            "Epoch 22 Batch 550 Loss 1.4104 Accuracy 0.6824\n",
            "discarded batch 593\n",
            "Epoch 22 Batch 600 Loss 1.4142 Accuracy 0.6819\n",
            "Epoch 22 Batch 650 Loss 1.4176 Accuracy 0.6814\n",
            "Epoch 22 Batch 700 Loss 1.4207 Accuracy 0.6809\n",
            "Epoch 22 Batch 750 Loss 1.4244 Accuracy 0.6805\n",
            "Epoch 22 Batch 800 Loss 1.4266 Accuracy 0.6802\n",
            "Epoch 22 Batch 850 Loss 1.4297 Accuracy 0.6798\n",
            "Epoch 22 Batch 900 Loss 1.4330 Accuracy 0.6794\n",
            "Epoch 22 Batch 950 Loss 1.4359 Accuracy 0.6790\n",
            "Epoch 22 Loss 1.4358 Accuracy 0.6791\n",
            "Time taken for 1 epoch: 33.86146306991577 secs\n",
            "\n",
            "Epoch 23 Batch 0 Loss 1.2775 Accuracy 0.6766\n",
            "Epoch 23 Batch 50 Loss 1.3470 Accuracy 0.6904\n",
            "Epoch 23 Batch 100 Loss 1.3649 Accuracy 0.6895\n",
            "Epoch 23 Batch 150 Loss 1.3640 Accuracy 0.6896\n",
            "Epoch 23 Batch 200 Loss 1.3652 Accuracy 0.6906\n",
            "Epoch 23 Batch 250 Loss 1.3702 Accuracy 0.6892\n",
            "Epoch 23 Batch 300 Loss 1.3743 Accuracy 0.6882\n",
            "discarded batch 302\n",
            "Epoch 23 Batch 350 Loss 1.3791 Accuracy 0.6874\n",
            "Epoch 23 Batch 400 Loss 1.3855 Accuracy 0.6867\n",
            "Epoch 23 Batch 450 Loss 1.3913 Accuracy 0.6856\n",
            "Epoch 23 Batch 500 Loss 1.3978 Accuracy 0.6849\n",
            "Epoch 23 Batch 550 Loss 1.4014 Accuracy 0.6843\n",
            "Epoch 23 Batch 600 Loss 1.4044 Accuracy 0.6837\n",
            "Epoch 23 Batch 650 Loss 1.4067 Accuracy 0.6833\n",
            "Epoch 23 Batch 700 Loss 1.4101 Accuracy 0.6827\n",
            "Epoch 23 Batch 750 Loss 1.4131 Accuracy 0.6821\n",
            "Epoch 23 Batch 800 Loss 1.4167 Accuracy 0.6815\n",
            "Epoch 23 Batch 850 Loss 1.4176 Accuracy 0.6816\n",
            "Epoch 23 Batch 900 Loss 1.4200 Accuracy 0.6812\n",
            "Epoch 23 Batch 950 Loss 1.4238 Accuracy 0.6806\n",
            "Epoch 23 Loss 1.4239 Accuracy 0.6806\n",
            "Time taken for 1 epoch: 33.743207693099976 secs\n",
            "\n",
            "Epoch 24 Batch 0 Loss 1.3710 Accuracy 0.6832\n",
            "Epoch 24 Batch 50 Loss 1.3106 Accuracy 0.7013\n",
            "Epoch 24 Batch 100 Loss 1.3332 Accuracy 0.6963\n",
            "Epoch 24 Batch 150 Loss 1.3456 Accuracy 0.6934\n",
            "Epoch 24 Batch 200 Loss 1.3513 Accuracy 0.6916\n",
            "Epoch 24 Batch 250 Loss 1.3598 Accuracy 0.6904\n",
            "Epoch 24 Batch 300 Loss 1.3635 Accuracy 0.6897\n",
            "Epoch 24 Batch 350 Loss 1.3672 Accuracy 0.6889\n",
            "Epoch 24 Batch 400 Loss 1.3745 Accuracy 0.6879\n",
            "Epoch 24 Batch 450 Loss 1.3778 Accuracy 0.6876\n",
            "Epoch 24 Batch 500 Loss 1.3813 Accuracy 0.6873\n",
            "discarded batch 501\n",
            "Epoch 24 Batch 550 Loss 1.3869 Accuracy 0.6865\n",
            "Epoch 24 Batch 600 Loss 1.3904 Accuracy 0.6862\n",
            "Epoch 24 Batch 650 Loss 1.3943 Accuracy 0.6854\n",
            "Epoch 24 Batch 700 Loss 1.3974 Accuracy 0.6848\n",
            "Epoch 24 Batch 750 Loss 1.3985 Accuracy 0.6845\n",
            "Epoch 24 Batch 800 Loss 1.4017 Accuracy 0.6840\n",
            "Epoch 24 Batch 850 Loss 1.4071 Accuracy 0.6833\n",
            "Epoch 24 Batch 900 Loss 1.4102 Accuracy 0.6830\n",
            "Epoch 24 Batch 950 Loss 1.4133 Accuracy 0.6825\n",
            "Epoch 24 Loss 1.4136 Accuracy 0.6825\n",
            "Time taken for 1 epoch: 33.80956792831421 secs\n",
            "\n",
            "Epoch 25 Batch 0 Loss 1.2157 Accuracy 0.7294\n",
            "Epoch 25 Batch 50 Loss 1.3138 Accuracy 0.6994\n",
            "Epoch 25 Batch 100 Loss 1.3271 Accuracy 0.6954\n",
            "Epoch 25 Batch 150 Loss 1.3319 Accuracy 0.6956\n",
            "Epoch 25 Batch 200 Loss 1.3445 Accuracy 0.6925\n",
            "Epoch 25 Batch 250 Loss 1.3497 Accuracy 0.6918\n",
            "Epoch 25 Batch 300 Loss 1.3523 Accuracy 0.6913\n",
            "Epoch 25 Batch 350 Loss 1.3568 Accuracy 0.6910\n",
            "Epoch 25 Batch 400 Loss 1.3607 Accuracy 0.6900\n",
            "Epoch 25 Batch 450 Loss 1.3659 Accuracy 0.6887\n",
            "Epoch 25 Batch 500 Loss 1.3726 Accuracy 0.6876\n",
            "Epoch 25 Batch 550 Loss 1.3772 Accuracy 0.6870\n",
            "Epoch 25 Batch 600 Loss 1.3791 Accuracy 0.6868\n",
            "Epoch 25 Batch 650 Loss 1.3820 Accuracy 0.6865\n",
            "Epoch 25 Batch 700 Loss 1.3858 Accuracy 0.6861\n",
            "Epoch 25 Batch 750 Loss 1.3890 Accuracy 0.6860\n",
            "Epoch 25 Batch 800 Loss 1.3927 Accuracy 0.6856\n",
            "Epoch 25 Batch 850 Loss 1.3962 Accuracy 0.6850\n",
            "Epoch 25 Batch 900 Loss 1.3988 Accuracy 0.6845\n",
            "discarded batch 931\n",
            "Epoch 25 Batch 950 Loss 1.4007 Accuracy 0.6844\n",
            "Saving checkpoint for epoch 25 at ./checkpoints/train/ckpt-5\n",
            "Epoch 25 Loss 1.4009 Accuracy 0.6844\n",
            "Time taken for 1 epoch: 33.97464060783386 secs\n",
            "\n",
            "Epoch 26 Batch 0 Loss 1.3591 Accuracy 0.6700\n",
            "Epoch 26 Batch 50 Loss 1.3031 Accuracy 0.7007\n",
            "discarded batch 51\n",
            "Epoch 26 Batch 100 Loss 1.3189 Accuracy 0.6965\n",
            "Epoch 26 Batch 150 Loss 1.3233 Accuracy 0.6964\n",
            "Epoch 26 Batch 200 Loss 1.3333 Accuracy 0.6948\n",
            "Epoch 26 Batch 250 Loss 1.3399 Accuracy 0.6941\n",
            "Epoch 26 Batch 300 Loss 1.3458 Accuracy 0.6936\n",
            "Epoch 26 Batch 350 Loss 1.3497 Accuracy 0.6927\n",
            "Epoch 26 Batch 400 Loss 1.3517 Accuracy 0.6924\n",
            "Epoch 26 Batch 450 Loss 1.3572 Accuracy 0.6916\n",
            "Epoch 26 Batch 500 Loss 1.3609 Accuracy 0.6914\n",
            "Epoch 26 Batch 550 Loss 1.3640 Accuracy 0.6908\n",
            "Epoch 26 Batch 600 Loss 1.3691 Accuracy 0.6902\n",
            "Epoch 26 Batch 650 Loss 1.3736 Accuracy 0.6892\n",
            "Epoch 26 Batch 700 Loss 1.3758 Accuracy 0.6890\n",
            "Epoch 26 Batch 750 Loss 1.3791 Accuracy 0.6884\n",
            "Epoch 26 Batch 800 Loss 1.3824 Accuracy 0.6877\n",
            "Epoch 26 Batch 850 Loss 1.3856 Accuracy 0.6868\n",
            "Epoch 26 Batch 900 Loss 1.3881 Accuracy 0.6865\n",
            "Epoch 26 Batch 950 Loss 1.3910 Accuracy 0.6861\n",
            "Epoch 26 Loss 1.3912 Accuracy 0.6861\n",
            "Time taken for 1 epoch: 33.626349449157715 secs\n",
            "\n",
            "Epoch 27 Batch 0 Loss 1.3035 Accuracy 0.6832\n",
            "Epoch 27 Batch 50 Loss 1.3371 Accuracy 0.6935\n",
            "Epoch 27 Batch 100 Loss 1.3290 Accuracy 0.6961\n",
            "Epoch 27 Batch 150 Loss 1.3345 Accuracy 0.6948\n",
            "Epoch 27 Batch 200 Loss 1.3307 Accuracy 0.6955\n",
            "Epoch 27 Batch 250 Loss 1.3370 Accuracy 0.6937\n",
            "Epoch 27 Batch 300 Loss 1.3412 Accuracy 0.6931\n",
            "discarded batch 302\n",
            "Epoch 27 Batch 350 Loss 1.3453 Accuracy 0.6929\n",
            "Epoch 27 Batch 400 Loss 1.3475 Accuracy 0.6925\n",
            "Epoch 27 Batch 450 Loss 1.3493 Accuracy 0.6920\n",
            "Epoch 27 Batch 500 Loss 1.3558 Accuracy 0.6907\n",
            "Epoch 27 Batch 550 Loss 1.3598 Accuracy 0.6902\n",
            "Epoch 27 Batch 600 Loss 1.3648 Accuracy 0.6896\n",
            "Epoch 27 Batch 650 Loss 1.3682 Accuracy 0.6892\n",
            "Epoch 27 Batch 700 Loss 1.3737 Accuracy 0.6883\n",
            "Epoch 27 Batch 750 Loss 1.3759 Accuracy 0.6881\n",
            "Epoch 27 Batch 800 Loss 1.3768 Accuracy 0.6882\n",
            "Epoch 27 Batch 850 Loss 1.3783 Accuracy 0.6881\n",
            "Epoch 27 Batch 900 Loss 1.3819 Accuracy 0.6876\n",
            "Epoch 27 Batch 950 Loss 1.3838 Accuracy 0.6876\n",
            "Epoch 27 Loss 1.3842 Accuracy 0.6875\n",
            "Time taken for 1 epoch: 34.16229701042175 secs\n",
            "\n",
            "Epoch 28 Batch 0 Loss 1.2483 Accuracy 0.6997\n",
            "Epoch 28 Batch 50 Loss 1.3017 Accuracy 0.6992\n",
            "Epoch 28 Batch 100 Loss 1.3088 Accuracy 0.6977\n",
            "Epoch 28 Batch 150 Loss 1.3226 Accuracy 0.6954\n",
            "Epoch 28 Batch 200 Loss 1.3265 Accuracy 0.6954\n",
            "Epoch 28 Batch 250 Loss 1.3294 Accuracy 0.6949\n",
            "Epoch 28 Batch 300 Loss 1.3309 Accuracy 0.6943\n",
            "Epoch 28 Batch 350 Loss 1.3348 Accuracy 0.6938\n",
            "Epoch 28 Batch 400 Loss 1.3376 Accuracy 0.6934\n",
            "Epoch 28 Batch 450 Loss 1.3416 Accuracy 0.6932\n",
            "Epoch 28 Batch 500 Loss 1.3452 Accuracy 0.6928\n",
            "Epoch 28 Batch 550 Loss 1.3495 Accuracy 0.6920\n",
            "Epoch 28 Batch 600 Loss 1.3537 Accuracy 0.6914\n",
            "Epoch 28 Batch 650 Loss 1.3552 Accuracy 0.6913\n",
            "Epoch 28 Batch 700 Loss 1.3578 Accuracy 0.6908\n",
            "Epoch 28 Batch 750 Loss 1.3623 Accuracy 0.6904\n",
            "Epoch 28 Batch 800 Loss 1.3650 Accuracy 0.6900\n",
            "Epoch 28 Batch 850 Loss 1.3691 Accuracy 0.6892\n",
            "discarded batch 862\n",
            "Epoch 28 Batch 900 Loss 1.3719 Accuracy 0.6888\n",
            "Epoch 28 Batch 950 Loss 1.3738 Accuracy 0.6884\n",
            "Epoch 28 Loss 1.3741 Accuracy 0.6884\n",
            "Time taken for 1 epoch: 33.62106990814209 secs\n",
            "\n",
            "Epoch 29 Batch 0 Loss 0.9858 Accuracy 0.7690\n",
            "Epoch 29 Batch 50 Loss 1.2975 Accuracy 0.7015\n",
            "Epoch 29 Batch 100 Loss 1.2859 Accuracy 0.7027\n",
            "Epoch 29 Batch 150 Loss 1.2895 Accuracy 0.7027\n",
            "Epoch 29 Batch 200 Loss 1.2979 Accuracy 0.7008\n",
            "Epoch 29 Batch 250 Loss 1.3056 Accuracy 0.7001\n",
            "Epoch 29 Batch 300 Loss 1.3070 Accuracy 0.7002\n",
            "Epoch 29 Batch 350 Loss 1.3143 Accuracy 0.6991\n",
            "Epoch 29 Batch 400 Loss 1.3196 Accuracy 0.6986\n",
            "Epoch 29 Batch 450 Loss 1.3255 Accuracy 0.6974\n",
            "Epoch 29 Batch 500 Loss 1.3302 Accuracy 0.6967\n",
            "discarded batch 509\n",
            "Epoch 29 Batch 550 Loss 1.3355 Accuracy 0.6958\n",
            "Epoch 29 Batch 600 Loss 1.3417 Accuracy 0.6951\n",
            "Epoch 29 Batch 650 Loss 1.3453 Accuracy 0.6945\n",
            "Epoch 29 Batch 700 Loss 1.3485 Accuracy 0.6938\n",
            "Epoch 29 Batch 750 Loss 1.3527 Accuracy 0.6930\n",
            "Epoch 29 Batch 800 Loss 1.3560 Accuracy 0.6924\n",
            "Epoch 29 Batch 850 Loss 1.3584 Accuracy 0.6920\n",
            "Epoch 29 Batch 900 Loss 1.3629 Accuracy 0.6913\n",
            "Epoch 29 Batch 950 Loss 1.3665 Accuracy 0.6908\n",
            "Epoch 29 Loss 1.3666 Accuracy 0.6908\n",
            "Time taken for 1 epoch: 34.39746284484863 secs\n",
            "\n",
            "Epoch 30 Batch 0 Loss 1.3062 Accuracy 0.6931\n",
            "Epoch 30 Batch 50 Loss 1.2905 Accuracy 0.7030\n",
            "Epoch 30 Batch 100 Loss 1.2801 Accuracy 0.7042\n",
            "discarded batch 145\n",
            "Epoch 30 Batch 150 Loss 1.2977 Accuracy 0.7014\n",
            "Epoch 30 Batch 200 Loss 1.3018 Accuracy 0.7005\n",
            "Epoch 30 Batch 250 Loss 1.3112 Accuracy 0.6988\n",
            "Epoch 30 Batch 300 Loss 1.3165 Accuracy 0.6982\n",
            "Epoch 30 Batch 350 Loss 1.3173 Accuracy 0.6984\n",
            "Epoch 30 Batch 400 Loss 1.3224 Accuracy 0.6974\n",
            "Epoch 30 Batch 450 Loss 1.3262 Accuracy 0.6966\n",
            "Epoch 30 Batch 500 Loss 1.3290 Accuracy 0.6962\n",
            "Epoch 30 Batch 550 Loss 1.3329 Accuracy 0.6954\n",
            "Epoch 30 Batch 600 Loss 1.3341 Accuracy 0.6954\n",
            "Epoch 30 Batch 650 Loss 1.3367 Accuracy 0.6950\n",
            "Epoch 30 Batch 700 Loss 1.3398 Accuracy 0.6946\n",
            "Epoch 30 Batch 750 Loss 1.3431 Accuracy 0.6938\n",
            "Epoch 30 Batch 800 Loss 1.3446 Accuracy 0.6935\n",
            "Epoch 30 Batch 850 Loss 1.3467 Accuracy 0.6934\n",
            "Epoch 30 Batch 900 Loss 1.3506 Accuracy 0.6928\n",
            "Epoch 30 Batch 950 Loss 1.3531 Accuracy 0.6923\n",
            "Saving checkpoint for epoch 30 at ./checkpoints/train/ckpt-6\n",
            "Epoch 30 Loss 1.3533 Accuracy 0.6923\n",
            "Time taken for 1 epoch: 33.91516733169556 secs\n",
            "\n",
            "Epoch 31 Batch 0 Loss 1.2456 Accuracy 0.7294\n",
            "Epoch 31 Batch 50 Loss 1.2486 Accuracy 0.7084\n",
            "Epoch 31 Batch 100 Loss 1.2686 Accuracy 0.7047\n",
            "Epoch 31 Batch 150 Loss 1.2801 Accuracy 0.7035\n",
            "Epoch 31 Batch 200 Loss 1.2820 Accuracy 0.7042\n",
            "Epoch 31 Batch 250 Loss 1.2936 Accuracy 0.7020\n",
            "Epoch 31 Batch 300 Loss 1.2966 Accuracy 0.7021\n",
            "Epoch 31 Batch 350 Loss 1.3029 Accuracy 0.7016\n",
            "Epoch 31 Batch 400 Loss 1.3040 Accuracy 0.7013\n",
            "Epoch 31 Batch 450 Loss 1.3091 Accuracy 0.7005\n",
            "Epoch 31 Batch 500 Loss 1.3143 Accuracy 0.7000\n",
            "Epoch 31 Batch 550 Loss 1.3183 Accuracy 0.6992\n",
            "Epoch 31 Batch 600 Loss 1.3212 Accuracy 0.6987\n",
            "Epoch 31 Batch 650 Loss 1.3255 Accuracy 0.6981\n",
            "Epoch 31 Batch 700 Loss 1.3305 Accuracy 0.6969\n",
            "Epoch 31 Batch 750 Loss 1.3326 Accuracy 0.6965\n",
            "Epoch 31 Batch 800 Loss 1.3354 Accuracy 0.6961\n",
            "Epoch 31 Batch 850 Loss 1.3388 Accuracy 0.6958\n",
            "Epoch 31 Batch 900 Loss 1.3416 Accuracy 0.6955\n",
            "discarded batch 902\n",
            "Epoch 31 Batch 950 Loss 1.3450 Accuracy 0.6951\n",
            "Epoch 31 Loss 1.3451 Accuracy 0.6951\n",
            "Time taken for 1 epoch: 33.72706151008606 secs\n",
            "\n",
            "Epoch 32 Batch 0 Loss 1.3374 Accuracy 0.6634\n",
            "Epoch 32 Batch 50 Loss 1.2673 Accuracy 0.7061\n",
            "Epoch 32 Batch 100 Loss 1.2757 Accuracy 0.7066\n",
            "Epoch 32 Batch 150 Loss 1.2798 Accuracy 0.7055\n",
            "Epoch 32 Batch 200 Loss 1.2856 Accuracy 0.7030\n",
            "discarded batch 202\n",
            "Epoch 32 Batch 250 Loss 1.2891 Accuracy 0.7020\n",
            "Epoch 32 Batch 300 Loss 1.2918 Accuracy 0.7016\n",
            "Epoch 32 Batch 350 Loss 1.2983 Accuracy 0.7008\n",
            "Epoch 32 Batch 400 Loss 1.3039 Accuracy 0.6999\n",
            "Epoch 32 Batch 450 Loss 1.3057 Accuracy 0.6998\n",
            "Epoch 32 Batch 500 Loss 1.3075 Accuracy 0.6996\n",
            "Epoch 32 Batch 550 Loss 1.3111 Accuracy 0.6990\n",
            "Epoch 32 Batch 600 Loss 1.3153 Accuracy 0.6984\n",
            "Epoch 32 Batch 650 Loss 1.3188 Accuracy 0.6976\n",
            "Epoch 32 Batch 700 Loss 1.3230 Accuracy 0.6970\n",
            "Epoch 32 Batch 750 Loss 1.3249 Accuracy 0.6969\n",
            "Epoch 32 Batch 800 Loss 1.3288 Accuracy 0.6963\n",
            "Epoch 32 Batch 850 Loss 1.3328 Accuracy 0.6957\n",
            "Epoch 32 Batch 900 Loss 1.3352 Accuracy 0.6954\n",
            "Epoch 32 Batch 950 Loss 1.3382 Accuracy 0.6950\n",
            "Epoch 32 Loss 1.3381 Accuracy 0.6950\n",
            "Time taken for 1 epoch: 33.550522327423096 secs\n",
            "\n",
            "Epoch 33 Batch 0 Loss 1.2865 Accuracy 0.7096\n",
            "Epoch 33 Batch 50 Loss 1.2652 Accuracy 0.7058\n",
            "Epoch 33 Batch 100 Loss 1.2640 Accuracy 0.7068\n",
            "discarded batch 105\n",
            "Epoch 33 Batch 150 Loss 1.2643 Accuracy 0.7056\n",
            "Epoch 33 Batch 200 Loss 1.2676 Accuracy 0.7055\n",
            "Epoch 33 Batch 250 Loss 1.2703 Accuracy 0.7059\n",
            "Epoch 33 Batch 300 Loss 1.2768 Accuracy 0.7049\n",
            "Epoch 33 Batch 350 Loss 1.2836 Accuracy 0.7039\n",
            "Epoch 33 Batch 400 Loss 1.2912 Accuracy 0.7025\n",
            "Epoch 33 Batch 450 Loss 1.2935 Accuracy 0.7021\n",
            "Epoch 33 Batch 500 Loss 1.2970 Accuracy 0.7015\n",
            "Epoch 33 Batch 550 Loss 1.3004 Accuracy 0.7007\n",
            "Epoch 33 Batch 600 Loss 1.3057 Accuracy 0.6996\n",
            "Epoch 33 Batch 650 Loss 1.3112 Accuracy 0.6990\n",
            "Epoch 33 Batch 700 Loss 1.3136 Accuracy 0.6985\n",
            "Epoch 33 Batch 750 Loss 1.3172 Accuracy 0.6977\n",
            "Epoch 33 Batch 800 Loss 1.3190 Accuracy 0.6975\n",
            "Epoch 33 Batch 850 Loss 1.3225 Accuracy 0.6970\n",
            "Epoch 33 Batch 900 Loss 1.3260 Accuracy 0.6965\n",
            "Epoch 33 Batch 950 Loss 1.3292 Accuracy 0.6960\n",
            "Epoch 33 Loss 1.3294 Accuracy 0.6960\n",
            "Time taken for 1 epoch: 33.39995622634888 secs\n",
            "\n",
            "Epoch 34 Batch 0 Loss 1.1253 Accuracy 0.7360\n",
            "Epoch 34 Batch 50 Loss 1.2656 Accuracy 0.7028\n",
            "Epoch 34 Batch 100 Loss 1.2708 Accuracy 0.7040\n",
            "Epoch 34 Batch 150 Loss 1.2731 Accuracy 0.7053\n",
            "Epoch 34 Batch 200 Loss 1.2758 Accuracy 0.7045\n",
            "Epoch 34 Batch 250 Loss 1.2772 Accuracy 0.7050\n",
            "Epoch 34 Batch 300 Loss 1.2774 Accuracy 0.7050\n",
            "Epoch 34 Batch 350 Loss 1.2794 Accuracy 0.7049\n",
            "Epoch 34 Batch 400 Loss 1.2824 Accuracy 0.7045\n",
            "Epoch 34 Batch 450 Loss 1.2875 Accuracy 0.7036\n",
            "Epoch 34 Batch 500 Loss 1.2910 Accuracy 0.7029\n",
            "Epoch 34 Batch 550 Loss 1.2964 Accuracy 0.7020\n",
            "Epoch 34 Batch 600 Loss 1.3010 Accuracy 0.7013\n",
            "discarded batch 617\n",
            "Epoch 34 Batch 650 Loss 1.3029 Accuracy 0.7006\n",
            "Epoch 34 Batch 700 Loss 1.3054 Accuracy 0.7001\n",
            "Epoch 34 Batch 750 Loss 1.3070 Accuracy 0.7000\n",
            "Epoch 34 Batch 800 Loss 1.3114 Accuracy 0.6992\n",
            "Epoch 34 Batch 850 Loss 1.3142 Accuracy 0.6989\n",
            "Epoch 34 Batch 900 Loss 1.3192 Accuracy 0.6983\n",
            "Epoch 34 Batch 950 Loss 1.3218 Accuracy 0.6979\n",
            "Epoch 34 Loss 1.3218 Accuracy 0.6979\n",
            "Time taken for 1 epoch: 33.74116921424866 secs\n",
            "\n",
            "Epoch 35 Batch 0 Loss 1.2067 Accuracy 0.7228\n",
            "Epoch 35 Batch 50 Loss 1.2531 Accuracy 0.7052\n",
            "Epoch 35 Batch 100 Loss 1.2546 Accuracy 0.7073\n",
            "discarded batch 148\n",
            "Epoch 35 Batch 150 Loss 1.2590 Accuracy 0.7069\n",
            "Epoch 35 Batch 200 Loss 1.2627 Accuracy 0.7060\n",
            "Epoch 35 Batch 250 Loss 1.2711 Accuracy 0.7044\n",
            "Epoch 35 Batch 300 Loss 1.2763 Accuracy 0.7038\n",
            "Epoch 35 Batch 350 Loss 1.2846 Accuracy 0.7029\n",
            "Epoch 35 Batch 400 Loss 1.2884 Accuracy 0.7025\n",
            "Epoch 35 Batch 450 Loss 1.2904 Accuracy 0.7020\n",
            "Epoch 35 Batch 500 Loss 1.2943 Accuracy 0.7015\n",
            "Epoch 35 Batch 550 Loss 1.2980 Accuracy 0.7010\n",
            "Epoch 35 Batch 600 Loss 1.3023 Accuracy 0.7004\n",
            "Epoch 35 Batch 650 Loss 1.3044 Accuracy 0.7001\n",
            "Epoch 35 Batch 700 Loss 1.3056 Accuracy 0.7002\n",
            "Epoch 35 Batch 750 Loss 1.3086 Accuracy 0.7000\n",
            "Epoch 35 Batch 800 Loss 1.3117 Accuracy 0.6997\n",
            "Epoch 35 Batch 850 Loss 1.3145 Accuracy 0.6993\n",
            "Epoch 35 Batch 900 Loss 1.3162 Accuracy 0.6991\n",
            "Epoch 35 Batch 950 Loss 1.3191 Accuracy 0.6986\n",
            "Saving checkpoint for epoch 35 at ./checkpoints/train/ckpt-7\n",
            "Epoch 35 Loss 1.3190 Accuracy 0.6986\n",
            "Time taken for 1 epoch: 34.70911002159119 secs\n",
            "\n",
            "Epoch 36 Batch 0 Loss 1.3961 Accuracy 0.6898\n",
            "Epoch 36 Batch 50 Loss 1.2460 Accuracy 0.7104\n",
            "Epoch 36 Batch 100 Loss 1.2513 Accuracy 0.7081\n",
            "Epoch 36 Batch 150 Loss 1.2558 Accuracy 0.7084\n",
            "Epoch 36 Batch 200 Loss 1.2552 Accuracy 0.7075\n",
            "Epoch 36 Batch 250 Loss 1.2567 Accuracy 0.7070\n",
            "Epoch 36 Batch 300 Loss 1.2623 Accuracy 0.7059\n",
            "Epoch 36 Batch 350 Loss 1.2693 Accuracy 0.7049\n",
            "Epoch 36 Batch 400 Loss 1.2718 Accuracy 0.7045\n",
            "Epoch 36 Batch 450 Loss 1.2746 Accuracy 0.7043\n",
            "Epoch 36 Batch 500 Loss 1.2789 Accuracy 0.7040\n",
            "discarded batch 516\n",
            "Epoch 36 Batch 550 Loss 1.2817 Accuracy 0.7033\n",
            "Epoch 36 Batch 600 Loss 1.2834 Accuracy 0.7033\n",
            "Epoch 36 Batch 650 Loss 1.2867 Accuracy 0.7027\n",
            "Epoch 36 Batch 700 Loss 1.2898 Accuracy 0.7025\n",
            "Epoch 36 Batch 750 Loss 1.2930 Accuracy 0.7022\n",
            "Epoch 36 Batch 800 Loss 1.2975 Accuracy 0.7016\n",
            "Epoch 36 Batch 850 Loss 1.3024 Accuracy 0.7009\n",
            "Epoch 36 Batch 900 Loss 1.3050 Accuracy 0.7006\n",
            "Epoch 36 Batch 950 Loss 1.3075 Accuracy 0.7002\n",
            "Epoch 36 Loss 1.3078 Accuracy 0.7001\n",
            "Time taken for 1 epoch: 36.253281593322754 secs\n",
            "\n",
            "Epoch 37 Batch 0 Loss 1.2649 Accuracy 0.6964\n",
            "discarded batch 21\n",
            "Epoch 37 Batch 50 Loss 1.2130 Accuracy 0.7149\n",
            "Epoch 37 Batch 100 Loss 1.2255 Accuracy 0.7126\n",
            "Epoch 37 Batch 150 Loss 1.2325 Accuracy 0.7109\n",
            "Epoch 37 Batch 200 Loss 1.2409 Accuracy 0.7096\n",
            "Epoch 37 Batch 250 Loss 1.2480 Accuracy 0.7092\n",
            "Epoch 37 Batch 300 Loss 1.2502 Accuracy 0.7092\n",
            "Epoch 37 Batch 350 Loss 1.2561 Accuracy 0.7081\n",
            "Epoch 37 Batch 400 Loss 1.2608 Accuracy 0.7069\n",
            "Epoch 37 Batch 450 Loss 1.2635 Accuracy 0.7071\n",
            "Epoch 37 Batch 500 Loss 1.2698 Accuracy 0.7062\n",
            "Epoch 37 Batch 550 Loss 1.2722 Accuracy 0.7055\n",
            "Epoch 37 Batch 600 Loss 1.2773 Accuracy 0.7046\n",
            "Epoch 37 Batch 650 Loss 1.2808 Accuracy 0.7040\n",
            "Epoch 37 Batch 700 Loss 1.2849 Accuracy 0.7033\n",
            "Epoch 37 Batch 750 Loss 1.2891 Accuracy 0.7028\n",
            "Epoch 37 Batch 800 Loss 1.2917 Accuracy 0.7025\n",
            "Epoch 37 Batch 850 Loss 1.2945 Accuracy 0.7021\n",
            "Epoch 37 Batch 900 Loss 1.2978 Accuracy 0.7017\n",
            "Epoch 37 Batch 950 Loss 1.3008 Accuracy 0.7013\n",
            "Epoch 37 Loss 1.3010 Accuracy 0.7012\n",
            "Time taken for 1 epoch: 35.17027831077576 secs\n",
            "\n",
            "Epoch 38 Batch 0 Loss 1.1878 Accuracy 0.6964\n",
            "Epoch 38 Batch 50 Loss 1.2333 Accuracy 0.7121\n",
            "Epoch 38 Batch 100 Loss 1.2407 Accuracy 0.7112\n",
            "Epoch 38 Batch 150 Loss 1.2355 Accuracy 0.7127\n",
            "Epoch 38 Batch 200 Loss 1.2432 Accuracy 0.7117\n",
            "Epoch 38 Batch 250 Loss 1.2458 Accuracy 0.7116\n",
            "Epoch 38 Batch 300 Loss 1.2488 Accuracy 0.7108\n",
            "Epoch 38 Batch 350 Loss 1.2525 Accuracy 0.7097\n",
            "Epoch 38 Batch 400 Loss 1.2598 Accuracy 0.7087\n",
            "Epoch 38 Batch 450 Loss 1.2626 Accuracy 0.7077\n",
            "Epoch 38 Batch 500 Loss 1.2678 Accuracy 0.7069\n",
            "Epoch 38 Batch 550 Loss 1.2697 Accuracy 0.7066\n",
            "Epoch 38 Batch 600 Loss 1.2739 Accuracy 0.7058\n",
            "Epoch 38 Batch 650 Loss 1.2757 Accuracy 0.7055\n",
            "discarded batch 676\n",
            "Epoch 38 Batch 700 Loss 1.2780 Accuracy 0.7054\n",
            "Epoch 38 Batch 750 Loss 1.2817 Accuracy 0.7049\n",
            "Epoch 38 Batch 800 Loss 1.2856 Accuracy 0.7044\n",
            "Epoch 38 Batch 850 Loss 1.2883 Accuracy 0.7039\n",
            "Epoch 38 Batch 900 Loss 1.2915 Accuracy 0.7035\n",
            "Epoch 38 Batch 950 Loss 1.2953 Accuracy 0.7030\n",
            "Epoch 38 Loss 1.2954 Accuracy 0.7030\n",
            "Time taken for 1 epoch: 35.5982084274292 secs\n",
            "\n",
            "Epoch 39 Batch 0 Loss 1.1359 Accuracy 0.7591\n",
            "Epoch 39 Batch 50 Loss 1.2144 Accuracy 0.7166\n",
            "Epoch 39 Batch 100 Loss 1.2217 Accuracy 0.7150\n",
            "discarded batch 149\n",
            "Epoch 39 Batch 150 Loss 1.2254 Accuracy 0.7134\n",
            "Epoch 39 Batch 200 Loss 1.2315 Accuracy 0.7125\n",
            "Epoch 39 Batch 250 Loss 1.2348 Accuracy 0.7116\n",
            "Epoch 39 Batch 300 Loss 1.2425 Accuracy 0.7101\n",
            "Epoch 39 Batch 350 Loss 1.2482 Accuracy 0.7093\n",
            "Epoch 39 Batch 400 Loss 1.2537 Accuracy 0.7083\n",
            "Epoch 39 Batch 450 Loss 1.2563 Accuracy 0.7087\n",
            "Epoch 39 Batch 500 Loss 1.2599 Accuracy 0.7083\n",
            "Epoch 39 Batch 550 Loss 1.2622 Accuracy 0.7076\n",
            "Epoch 39 Batch 600 Loss 1.2670 Accuracy 0.7067\n",
            "Epoch 39 Batch 650 Loss 1.2703 Accuracy 0.7060\n",
            "Epoch 39 Batch 700 Loss 1.2742 Accuracy 0.7050\n",
            "Epoch 39 Batch 750 Loss 1.2769 Accuracy 0.7047\n",
            "Epoch 39 Batch 800 Loss 1.2799 Accuracy 0.7042\n",
            "Epoch 39 Batch 850 Loss 1.2827 Accuracy 0.7040\n",
            "Epoch 39 Batch 900 Loss 1.2854 Accuracy 0.7036\n",
            "Epoch 39 Batch 950 Loss 1.2886 Accuracy 0.7032\n",
            "Epoch 39 Loss 1.2887 Accuracy 0.7032\n",
            "Time taken for 1 epoch: 35.53774571418762 secs\n",
            "\n",
            "Epoch 40 Batch 0 Loss 1.0912 Accuracy 0.7426\n",
            "Epoch 40 Batch 50 Loss 1.2043 Accuracy 0.7142\n",
            "Epoch 40 Batch 100 Loss 1.2105 Accuracy 0.7156\n",
            "Epoch 40 Batch 150 Loss 1.2151 Accuracy 0.7150\n",
            "Epoch 40 Batch 200 Loss 1.2263 Accuracy 0.7141\n",
            "Epoch 40 Batch 250 Loss 1.2326 Accuracy 0.7131\n",
            "Epoch 40 Batch 300 Loss 1.2363 Accuracy 0.7127\n",
            "Epoch 40 Batch 350 Loss 1.2391 Accuracy 0.7120\n",
            "Epoch 40 Batch 400 Loss 1.2424 Accuracy 0.7113\n",
            "Epoch 40 Batch 450 Loss 1.2473 Accuracy 0.7105\n",
            "Epoch 40 Batch 500 Loss 1.2521 Accuracy 0.7093\n",
            "Epoch 40 Batch 550 Loss 1.2564 Accuracy 0.7085\n",
            "discarded batch 595\n",
            "Epoch 40 Batch 600 Loss 1.2601 Accuracy 0.7082\n",
            "Epoch 40 Batch 650 Loss 1.2618 Accuracy 0.7080\n",
            "Epoch 40 Batch 700 Loss 1.2663 Accuracy 0.7071\n",
            "Epoch 40 Batch 750 Loss 1.2686 Accuracy 0.7069\n",
            "Epoch 40 Batch 800 Loss 1.2707 Accuracy 0.7063\n",
            "Epoch 40 Batch 850 Loss 1.2732 Accuracy 0.7057\n",
            "Epoch 40 Batch 900 Loss 1.2766 Accuracy 0.7054\n",
            "Epoch 40 Batch 950 Loss 1.2791 Accuracy 0.7049\n",
            "Saving checkpoint for epoch 40 at ./checkpoints/train/ckpt-8\n",
            "Epoch 40 Loss 1.2792 Accuracy 0.7049\n",
            "Time taken for 1 epoch: 35.2157039642334 secs\n",
            "\n",
            "Epoch 41 Batch 0 Loss 1.2775 Accuracy 0.6931\n",
            "Epoch 41 Batch 50 Loss 1.1961 Accuracy 0.7220\n",
            "Epoch 41 Batch 100 Loss 1.1966 Accuracy 0.7198\n",
            "Epoch 41 Batch 150 Loss 1.1980 Accuracy 0.7187\n",
            "Epoch 41 Batch 200 Loss 1.2098 Accuracy 0.7170\n",
            "Epoch 41 Batch 250 Loss 1.2196 Accuracy 0.7154\n",
            "Epoch 41 Batch 300 Loss 1.2298 Accuracy 0.7144\n",
            "Epoch 41 Batch 350 Loss 1.2346 Accuracy 0.7133\n",
            "Epoch 41 Batch 400 Loss 1.2367 Accuracy 0.7125\n",
            "Epoch 41 Batch 450 Loss 1.2406 Accuracy 0.7119\n",
            "Epoch 41 Batch 500 Loss 1.2441 Accuracy 0.7115\n",
            "Epoch 41 Batch 550 Loss 1.2464 Accuracy 0.7109\n",
            "Epoch 41 Batch 600 Loss 1.2515 Accuracy 0.7101\n",
            "Epoch 41 Batch 650 Loss 1.2563 Accuracy 0.7093\n",
            "Epoch 41 Batch 700 Loss 1.2600 Accuracy 0.7087\n",
            "Epoch 41 Batch 750 Loss 1.2633 Accuracy 0.7080\n",
            "Epoch 41 Batch 800 Loss 1.2650 Accuracy 0.7076\n",
            "Epoch 41 Batch 850 Loss 1.2670 Accuracy 0.7074\n",
            "discarded batch 898\n",
            "Epoch 41 Batch 900 Loss 1.2694 Accuracy 0.7071\n",
            "Epoch 41 Batch 950 Loss 1.2725 Accuracy 0.7064\n",
            "Epoch 41 Loss 1.2723 Accuracy 0.7065\n",
            "Time taken for 1 epoch: 35.03511381149292 secs\n",
            "\n",
            "Epoch 42 Batch 0 Loss 1.1981 Accuracy 0.6964\n",
            "Epoch 42 Batch 50 Loss 1.1898 Accuracy 0.7201\n",
            "Epoch 42 Batch 100 Loss 1.1900 Accuracy 0.7206\n",
            "Epoch 42 Batch 150 Loss 1.1978 Accuracy 0.7184\n",
            "Epoch 42 Batch 200 Loss 1.1973 Accuracy 0.7188\n",
            "Epoch 42 Batch 250 Loss 1.2069 Accuracy 0.7166\n",
            "Epoch 42 Batch 300 Loss 1.2126 Accuracy 0.7158\n",
            "Epoch 42 Batch 350 Loss 1.2162 Accuracy 0.7156\n",
            "Epoch 42 Batch 400 Loss 1.2220 Accuracy 0.7149\n",
            "Epoch 42 Batch 450 Loss 1.2282 Accuracy 0.7138\n",
            "Epoch 42 Batch 500 Loss 1.2331 Accuracy 0.7130\n",
            "Epoch 42 Batch 550 Loss 1.2378 Accuracy 0.7119\n",
            "Epoch 42 Batch 600 Loss 1.2434 Accuracy 0.7106\n",
            "Epoch 42 Batch 650 Loss 1.2472 Accuracy 0.7103\n",
            "Epoch 42 Batch 700 Loss 1.2502 Accuracy 0.7095\n",
            "Epoch 42 Batch 750 Loss 1.2547 Accuracy 0.7087\n",
            "discarded batch 755\n",
            "Epoch 42 Batch 800 Loss 1.2577 Accuracy 0.7084\n",
            "Epoch 42 Batch 850 Loss 1.2616 Accuracy 0.7078\n",
            "Epoch 42 Batch 900 Loss 1.2640 Accuracy 0.7076\n",
            "Epoch 42 Batch 950 Loss 1.2675 Accuracy 0.7072\n",
            "Epoch 42 Loss 1.2676 Accuracy 0.7072\n",
            "Time taken for 1 epoch: 35.561976194381714 secs\n",
            "\n",
            "Epoch 43 Batch 0 Loss 1.2418 Accuracy 0.7063\n",
            "Epoch 43 Batch 50 Loss 1.1853 Accuracy 0.7235\n",
            "Epoch 43 Batch 100 Loss 1.1981 Accuracy 0.7217\n",
            "Epoch 43 Batch 150 Loss 1.2006 Accuracy 0.7197\n",
            "Epoch 43 Batch 200 Loss 1.2091 Accuracy 0.7172\n",
            "Epoch 43 Batch 250 Loss 1.2128 Accuracy 0.7167\n",
            "Epoch 43 Batch 300 Loss 1.2183 Accuracy 0.7156\n",
            "Epoch 43 Batch 350 Loss 1.2238 Accuracy 0.7145\n",
            "Epoch 43 Batch 400 Loss 1.2257 Accuracy 0.7139\n",
            "Epoch 43 Batch 450 Loss 1.2300 Accuracy 0.7136\n",
            "Epoch 43 Batch 500 Loss 1.2320 Accuracy 0.7133\n",
            "Epoch 43 Batch 550 Loss 1.2349 Accuracy 0.7130\n",
            "Epoch 43 Batch 600 Loss 1.2379 Accuracy 0.7123\n",
            "Epoch 43 Batch 650 Loss 1.2403 Accuracy 0.7118\n",
            "Epoch 43 Batch 700 Loss 1.2431 Accuracy 0.7111\n",
            "discarded batch 706\n",
            "Epoch 43 Batch 750 Loss 1.2471 Accuracy 0.7106\n",
            "Epoch 43 Batch 800 Loss 1.2504 Accuracy 0.7101\n",
            "Epoch 43 Batch 850 Loss 1.2539 Accuracy 0.7097\n",
            "Epoch 43 Batch 900 Loss 1.2567 Accuracy 0.7093\n",
            "Epoch 43 Batch 950 Loss 1.2601 Accuracy 0.7087\n",
            "Epoch 43 Loss 1.2600 Accuracy 0.7087\n",
            "Time taken for 1 epoch: 35.55558228492737 secs\n",
            "\n",
            "Epoch 44 Batch 0 Loss 1.2231 Accuracy 0.7195\n",
            "Epoch 44 Batch 50 Loss 1.1673 Accuracy 0.7221\n",
            "Epoch 44 Batch 100 Loss 1.1885 Accuracy 0.7185\n",
            "Epoch 44 Batch 150 Loss 1.1869 Accuracy 0.7192\n",
            "Epoch 44 Batch 200 Loss 1.1934 Accuracy 0.7187\n",
            "Epoch 44 Batch 250 Loss 1.2048 Accuracy 0.7167\n",
            "Epoch 44 Batch 300 Loss 1.2137 Accuracy 0.7154\n",
            "Epoch 44 Batch 350 Loss 1.2196 Accuracy 0.7142\n",
            "Epoch 44 Batch 400 Loss 1.2225 Accuracy 0.7139\n",
            "Epoch 44 Batch 450 Loss 1.2255 Accuracy 0.7134\n",
            "Epoch 44 Batch 500 Loss 1.2298 Accuracy 0.7129\n",
            "discarded batch 508\n",
            "Epoch 44 Batch 550 Loss 1.2326 Accuracy 0.7125\n",
            "Epoch 44 Batch 600 Loss 1.2338 Accuracy 0.7123\n",
            "Epoch 44 Batch 650 Loss 1.2366 Accuracy 0.7120\n",
            "Epoch 44 Batch 700 Loss 1.2399 Accuracy 0.7114\n",
            "Epoch 44 Batch 750 Loss 1.2426 Accuracy 0.7110\n",
            "Epoch 44 Batch 800 Loss 1.2457 Accuracy 0.7107\n",
            "Epoch 44 Batch 850 Loss 1.2487 Accuracy 0.7102\n",
            "Epoch 44 Batch 900 Loss 1.2520 Accuracy 0.7098\n",
            "Epoch 44 Batch 950 Loss 1.2558 Accuracy 0.7090\n",
            "Epoch 44 Loss 1.2561 Accuracy 0.7089\n",
            "Time taken for 1 epoch: 35.5123655796051 secs\n",
            "\n",
            "Epoch 45 Batch 0 Loss 1.0276 Accuracy 0.7558\n",
            "Epoch 45 Batch 50 Loss 1.1893 Accuracy 0.7187\n",
            "Epoch 45 Batch 100 Loss 1.1858 Accuracy 0.7217\n",
            "Epoch 45 Batch 150 Loss 1.1894 Accuracy 0.7205\n",
            "Epoch 45 Batch 200 Loss 1.1979 Accuracy 0.7185\n",
            "Epoch 45 Batch 250 Loss 1.2034 Accuracy 0.7171\n",
            "Epoch 45 Batch 300 Loss 1.2103 Accuracy 0.7158\n",
            "discarded batch 321\n",
            "Epoch 45 Batch 350 Loss 1.2121 Accuracy 0.7154\n",
            "Epoch 45 Batch 400 Loss 1.2143 Accuracy 0.7150\n",
            "Epoch 45 Batch 450 Loss 1.2165 Accuracy 0.7148\n",
            "Epoch 45 Batch 500 Loss 1.2189 Accuracy 0.7149\n",
            "Epoch 45 Batch 550 Loss 1.2237 Accuracy 0.7140\n",
            "Epoch 45 Batch 600 Loss 1.2271 Accuracy 0.7133\n",
            "Epoch 45 Batch 650 Loss 1.2311 Accuracy 0.7123\n",
            "Epoch 45 Batch 700 Loss 1.2355 Accuracy 0.7117\n",
            "Epoch 45 Batch 750 Loss 1.2393 Accuracy 0.7112\n",
            "Epoch 45 Batch 800 Loss 1.2420 Accuracy 0.7108\n",
            "Epoch 45 Batch 850 Loss 1.2444 Accuracy 0.7103\n",
            "Epoch 45 Batch 900 Loss 1.2486 Accuracy 0.7097\n",
            "Epoch 45 Batch 950 Loss 1.2504 Accuracy 0.7098\n",
            "Saving checkpoint for epoch 45 at ./checkpoints/train/ckpt-9\n",
            "Epoch 45 Loss 1.2508 Accuracy 0.7097\n",
            "Time taken for 1 epoch: 36.08925271034241 secs\n",
            "\n",
            "Epoch 46 Batch 0 Loss 1.1435 Accuracy 0.7492\n",
            "Epoch 46 Batch 50 Loss 1.1538 Accuracy 0.7277\n",
            "Epoch 46 Batch 100 Loss 1.1709 Accuracy 0.7247\n",
            "Epoch 46 Batch 150 Loss 1.1854 Accuracy 0.7228\n",
            "Epoch 46 Batch 200 Loss 1.1907 Accuracy 0.7209\n",
            "Epoch 46 Batch 250 Loss 1.1948 Accuracy 0.7195\n",
            "Epoch 46 Batch 300 Loss 1.1979 Accuracy 0.7190\n",
            "Epoch 46 Batch 350 Loss 1.2027 Accuracy 0.7185\n",
            "Epoch 46 Batch 400 Loss 1.2070 Accuracy 0.7182\n",
            "discarded batch 405\n",
            "Epoch 46 Batch 450 Loss 1.2099 Accuracy 0.7177\n",
            "Epoch 46 Batch 500 Loss 1.2138 Accuracy 0.7172\n",
            "Epoch 46 Batch 550 Loss 1.2175 Accuracy 0.7165\n",
            "Epoch 46 Batch 600 Loss 1.2217 Accuracy 0.7152\n",
            "Epoch 46 Batch 650 Loss 1.2259 Accuracy 0.7142\n",
            "Epoch 46 Batch 700 Loss 1.2294 Accuracy 0.7138\n",
            "Epoch 46 Batch 750 Loss 1.2313 Accuracy 0.7136\n",
            "Epoch 46 Batch 800 Loss 1.2344 Accuracy 0.7131\n",
            "Epoch 46 Batch 850 Loss 1.2374 Accuracy 0.7129\n",
            "Epoch 46 Batch 900 Loss 1.2399 Accuracy 0.7128\n",
            "Epoch 46 Batch 950 Loss 1.2429 Accuracy 0.7121\n",
            "Epoch 46 Loss 1.2432 Accuracy 0.7121\n",
            "Time taken for 1 epoch: 35.6338894367218 secs\n",
            "\n",
            "Epoch 47 Batch 0 Loss 1.2612 Accuracy 0.6964\n",
            "Epoch 47 Batch 50 Loss 1.1568 Accuracy 0.7262\n",
            "Epoch 47 Batch 100 Loss 1.1639 Accuracy 0.7271\n",
            "Epoch 47 Batch 150 Loss 1.1856 Accuracy 0.7217\n",
            "Epoch 47 Batch 200 Loss 1.1947 Accuracy 0.7195\n",
            "Epoch 47 Batch 250 Loss 1.1991 Accuracy 0.7186\n",
            "discarded batch 258\n",
            "Epoch 47 Batch 300 Loss 1.2046 Accuracy 0.7178\n",
            "Epoch 47 Batch 350 Loss 1.2065 Accuracy 0.7171\n",
            "Epoch 47 Batch 400 Loss 1.2091 Accuracy 0.7168\n",
            "Epoch 47 Batch 450 Loss 1.2128 Accuracy 0.7158\n",
            "Epoch 47 Batch 500 Loss 1.2176 Accuracy 0.7148\n",
            "Epoch 47 Batch 550 Loss 1.2198 Accuracy 0.7141\n",
            "Epoch 47 Batch 600 Loss 1.2236 Accuracy 0.7134\n",
            "Epoch 47 Batch 650 Loss 1.2269 Accuracy 0.7131\n",
            "Epoch 47 Batch 700 Loss 1.2287 Accuracy 0.7127\n",
            "Epoch 47 Batch 750 Loss 1.2324 Accuracy 0.7123\n",
            "Epoch 47 Batch 800 Loss 1.2340 Accuracy 0.7121\n",
            "Epoch 47 Batch 850 Loss 1.2360 Accuracy 0.7118\n",
            "Epoch 47 Batch 900 Loss 1.2382 Accuracy 0.7116\n",
            "Epoch 47 Batch 950 Loss 1.2404 Accuracy 0.7114\n",
            "Epoch 47 Loss 1.2403 Accuracy 0.7114\n",
            "Time taken for 1 epoch: 37.027897119522095 secs\n",
            "\n",
            "Epoch 48 Batch 0 Loss 1.1785 Accuracy 0.7294\n",
            "Epoch 48 Batch 50 Loss 1.1765 Accuracy 0.7212\n",
            "Epoch 48 Batch 100 Loss 1.1881 Accuracy 0.7186\n",
            "Epoch 48 Batch 150 Loss 1.1776 Accuracy 0.7210\n",
            "Epoch 48 Batch 200 Loss 1.1819 Accuracy 0.7203\n",
            "Epoch 48 Batch 250 Loss 1.1875 Accuracy 0.7194\n",
            "Epoch 48 Batch 300 Loss 1.1912 Accuracy 0.7194\n",
            "Epoch 48 Batch 350 Loss 1.1945 Accuracy 0.7187\n",
            "Epoch 48 Batch 400 Loss 1.1978 Accuracy 0.7180\n",
            "Epoch 48 Batch 450 Loss 1.2040 Accuracy 0.7173\n",
            "Epoch 48 Batch 500 Loss 1.2084 Accuracy 0.7167\n",
            "Epoch 48 Batch 550 Loss 1.2129 Accuracy 0.7161\n",
            "Epoch 48 Batch 600 Loss 1.2146 Accuracy 0.7155\n",
            "Epoch 48 Batch 650 Loss 1.2186 Accuracy 0.7146\n",
            "Epoch 48 Batch 700 Loss 1.2222 Accuracy 0.7142\n",
            "discarded batch 714\n",
            "Epoch 48 Batch 750 Loss 1.2251 Accuracy 0.7136\n",
            "Epoch 48 Batch 800 Loss 1.2287 Accuracy 0.7129\n",
            "Epoch 48 Batch 850 Loss 1.2309 Accuracy 0.7124\n",
            "Epoch 48 Batch 900 Loss 1.2329 Accuracy 0.7122\n",
            "Epoch 48 Batch 950 Loss 1.2359 Accuracy 0.7119\n",
            "Epoch 48 Loss 1.2360 Accuracy 0.7119\n",
            "Time taken for 1 epoch: 34.582802295684814 secs\n",
            "\n",
            "Epoch 49 Batch 0 Loss 1.1456 Accuracy 0.7063\n",
            "Epoch 49 Batch 50 Loss 1.1703 Accuracy 0.7218\n",
            "Epoch 49 Batch 100 Loss 1.1781 Accuracy 0.7204\n",
            "Epoch 49 Batch 150 Loss 1.1824 Accuracy 0.7201\n",
            "Epoch 49 Batch 200 Loss 1.1837 Accuracy 0.7202\n",
            "discarded batch 247\n",
            "Epoch 49 Batch 250 Loss 1.1800 Accuracy 0.7208\n",
            "Epoch 49 Batch 300 Loss 1.1828 Accuracy 0.7203\n",
            "Epoch 49 Batch 350 Loss 1.1909 Accuracy 0.7193\n",
            "Epoch 49 Batch 400 Loss 1.1967 Accuracy 0.7177\n",
            "Epoch 49 Batch 450 Loss 1.1996 Accuracy 0.7173\n",
            "Epoch 49 Batch 500 Loss 1.2035 Accuracy 0.7169\n",
            "Epoch 49 Batch 550 Loss 1.2075 Accuracy 0.7163\n",
            "Epoch 49 Batch 600 Loss 1.2108 Accuracy 0.7158\n",
            "Epoch 49 Batch 650 Loss 1.2142 Accuracy 0.7154\n",
            "Epoch 49 Batch 700 Loss 1.2156 Accuracy 0.7152\n",
            "Epoch 49 Batch 750 Loss 1.2168 Accuracy 0.7152\n",
            "Epoch 49 Batch 800 Loss 1.2192 Accuracy 0.7149\n",
            "Epoch 49 Batch 850 Loss 1.2215 Accuracy 0.7148\n",
            "Epoch 49 Batch 900 Loss 1.2252 Accuracy 0.7141\n",
            "Epoch 49 Batch 950 Loss 1.2282 Accuracy 0.7135\n",
            "Epoch 49 Loss 1.2285 Accuracy 0.7134\n",
            "Time taken for 1 epoch: 34.318703174591064 secs\n",
            "\n",
            "Epoch 50 Batch 0 Loss 1.1117 Accuracy 0.7822\n",
            "Epoch 50 Batch 50 Loss 1.1471 Accuracy 0.7324\n",
            "Epoch 50 Batch 100 Loss 1.1495 Accuracy 0.7292\n",
            "Epoch 50 Batch 150 Loss 1.1650 Accuracy 0.7254\n",
            "Epoch 50 Batch 200 Loss 1.1699 Accuracy 0.7234\n",
            "Epoch 50 Batch 250 Loss 1.1763 Accuracy 0.7224\n",
            "Epoch 50 Batch 300 Loss 1.1781 Accuracy 0.7218\n",
            "discarded batch 303\n",
            "Epoch 50 Batch 350 Loss 1.1822 Accuracy 0.7212\n",
            "Epoch 50 Batch 400 Loss 1.1857 Accuracy 0.7207\n",
            "Epoch 50 Batch 450 Loss 1.1924 Accuracy 0.7196\n",
            "Epoch 50 Batch 500 Loss 1.1967 Accuracy 0.7187\n",
            "Epoch 50 Batch 550 Loss 1.2007 Accuracy 0.7182\n",
            "Epoch 50 Batch 600 Loss 1.2026 Accuracy 0.7180\n",
            "Epoch 50 Batch 650 Loss 1.2068 Accuracy 0.7175\n",
            "Epoch 50 Batch 700 Loss 1.2090 Accuracy 0.7170\n",
            "Epoch 50 Batch 750 Loss 1.2118 Accuracy 0.7165\n",
            "Epoch 50 Batch 800 Loss 1.2171 Accuracy 0.7156\n",
            "Epoch 50 Batch 850 Loss 1.2197 Accuracy 0.7155\n",
            "Epoch 50 Batch 900 Loss 1.2228 Accuracy 0.7150\n",
            "Epoch 50 Batch 950 Loss 1.2250 Accuracy 0.7143\n",
            "Saving checkpoint for epoch 50 at ./checkpoints/train/ckpt-10\n",
            "Epoch 50 Loss 1.2253 Accuracy 0.7143\n",
            "Time taken for 1 epoch: 34.61177587509155 secs\n",
            "\n",
            "Epoch 51 Batch 0 Loss 1.1005 Accuracy 0.7525\n",
            "Epoch 51 Batch 50 Loss 1.1353 Accuracy 0.7326\n",
            "Epoch 51 Batch 100 Loss 1.1544 Accuracy 0.7278\n",
            "Epoch 51 Batch 150 Loss 1.1613 Accuracy 0.7250\n",
            "Epoch 51 Batch 200 Loss 1.1675 Accuracy 0.7243\n",
            "Epoch 51 Batch 250 Loss 1.1718 Accuracy 0.7229\n",
            "Epoch 51 Batch 300 Loss 1.1739 Accuracy 0.7233\n",
            "Epoch 51 Batch 350 Loss 1.1763 Accuracy 0.7231\n",
            "Epoch 51 Batch 400 Loss 1.1804 Accuracy 0.7226\n",
            "Epoch 51 Batch 450 Loss 1.1847 Accuracy 0.7221\n",
            "Epoch 51 Batch 500 Loss 1.1905 Accuracy 0.7210\n",
            "Epoch 51 Batch 550 Loss 1.1932 Accuracy 0.7205\n",
            "Epoch 51 Batch 600 Loss 1.1957 Accuracy 0.7200\n",
            "Epoch 51 Batch 650 Loss 1.2003 Accuracy 0.7190\n",
            "discarded batch 673\n",
            "Epoch 51 Batch 700 Loss 1.2032 Accuracy 0.7186\n",
            "Epoch 51 Batch 750 Loss 1.2073 Accuracy 0.7178\n",
            "Epoch 51 Batch 800 Loss 1.2089 Accuracy 0.7172\n",
            "Epoch 51 Batch 850 Loss 1.2129 Accuracy 0.7166\n",
            "Epoch 51 Batch 900 Loss 1.2156 Accuracy 0.7161\n",
            "Epoch 51 Batch 950 Loss 1.2180 Accuracy 0.7160\n",
            "Epoch 51 Loss 1.2181 Accuracy 0.7160\n",
            "Time taken for 1 epoch: 33.95808124542236 secs\n",
            "\n",
            "Epoch 52 Batch 0 Loss 1.1647 Accuracy 0.7426\n",
            "Epoch 52 Batch 50 Loss 1.1723 Accuracy 0.7243\n",
            "Epoch 52 Batch 100 Loss 1.1649 Accuracy 0.7255\n",
            "Epoch 52 Batch 150 Loss 1.1604 Accuracy 0.7250\n",
            "Epoch 52 Batch 200 Loss 1.1633 Accuracy 0.7246\n",
            "Epoch 52 Batch 250 Loss 1.1689 Accuracy 0.7232\n",
            "Epoch 52 Batch 300 Loss 1.1701 Accuracy 0.7227\n",
            "Epoch 52 Batch 350 Loss 1.1732 Accuracy 0.7221\n",
            "Epoch 52 Batch 400 Loss 1.1789 Accuracy 0.7210\n",
            "Epoch 52 Batch 450 Loss 1.1823 Accuracy 0.7208\n",
            "Epoch 52 Batch 500 Loss 1.1875 Accuracy 0.7197\n",
            "Epoch 52 Batch 550 Loss 1.1901 Accuracy 0.7194\n",
            "Epoch 52 Batch 600 Loss 1.1950 Accuracy 0.7184\n",
            "Epoch 52 Batch 650 Loss 1.1981 Accuracy 0.7180\n",
            "Epoch 52 Batch 700 Loss 1.2009 Accuracy 0.7177\n",
            "Epoch 52 Batch 750 Loss 1.2055 Accuracy 0.7169\n",
            "Epoch 52 Batch 800 Loss 1.2082 Accuracy 0.7165\n",
            "Epoch 52 Batch 850 Loss 1.2114 Accuracy 0.7159\n",
            "discarded batch 868\n",
            "Epoch 52 Batch 900 Loss 1.2141 Accuracy 0.7157\n",
            "Epoch 52 Batch 950 Loss 1.2157 Accuracy 0.7154\n",
            "Epoch 52 Loss 1.2156 Accuracy 0.7154\n",
            "Time taken for 1 epoch: 33.97790789604187 secs\n",
            "\n",
            "Epoch 53 Batch 0 Loss 1.2248 Accuracy 0.7063\n",
            "Epoch 53 Batch 50 Loss 1.1413 Accuracy 0.7259\n",
            "discarded batch 100\n",
            "Epoch 53 Batch 150 Loss 1.1445 Accuracy 0.7269\n",
            "Epoch 53 Batch 200 Loss 1.1518 Accuracy 0.7266\n",
            "Epoch 53 Batch 250 Loss 1.1591 Accuracy 0.7251\n",
            "Epoch 53 Batch 300 Loss 1.1631 Accuracy 0.7251\n",
            "Epoch 53 Batch 350 Loss 1.1692 Accuracy 0.7240\n",
            "Epoch 53 Batch 400 Loss 1.1705 Accuracy 0.7236\n",
            "Epoch 53 Batch 450 Loss 1.1753 Accuracy 0.7226\n",
            "Epoch 53 Batch 500 Loss 1.1799 Accuracy 0.7217\n",
            "Epoch 53 Batch 550 Loss 1.1844 Accuracy 0.7208\n",
            "Epoch 53 Batch 600 Loss 1.1879 Accuracy 0.7202\n",
            "Epoch 53 Batch 650 Loss 1.1916 Accuracy 0.7200\n",
            "Epoch 53 Batch 700 Loss 1.1948 Accuracy 0.7197\n",
            "Epoch 53 Batch 750 Loss 1.1975 Accuracy 0.7193\n",
            "Epoch 53 Batch 800 Loss 1.2023 Accuracy 0.7185\n",
            "Epoch 53 Batch 850 Loss 1.2060 Accuracy 0.7179\n",
            "Epoch 53 Batch 900 Loss 1.2072 Accuracy 0.7177\n",
            "Epoch 53 Batch 950 Loss 1.2084 Accuracy 0.7175\n",
            "Epoch 53 Loss 1.2088 Accuracy 0.7174\n",
            "Time taken for 1 epoch: 33.59061574935913 secs\n",
            "\n",
            "Epoch 54 Batch 0 Loss 1.1053 Accuracy 0.7261\n",
            "Epoch 54 Batch 50 Loss 1.1242 Accuracy 0.7290\n",
            "Epoch 54 Batch 100 Loss 1.1397 Accuracy 0.7270\n",
            "Epoch 54 Batch 150 Loss 1.1434 Accuracy 0.7266\n",
            "Epoch 54 Batch 200 Loss 1.1488 Accuracy 0.7265\n",
            "Epoch 54 Batch 250 Loss 1.1550 Accuracy 0.7252\n",
            "Epoch 54 Batch 300 Loss 1.1590 Accuracy 0.7244\n",
            "Epoch 54 Batch 350 Loss 1.1655 Accuracy 0.7235\n",
            "Epoch 54 Batch 400 Loss 1.1679 Accuracy 0.7234\n",
            "Epoch 54 Batch 450 Loss 1.1728 Accuracy 0.7226\n",
            "Epoch 54 Batch 500 Loss 1.1767 Accuracy 0.7219\n",
            "Epoch 54 Batch 550 Loss 1.1794 Accuracy 0.7218\n",
            "Epoch 54 Batch 600 Loss 1.1821 Accuracy 0.7213\n",
            "Epoch 54 Batch 650 Loss 1.1870 Accuracy 0.7207\n",
            "Epoch 54 Batch 700 Loss 1.1911 Accuracy 0.7200\n",
            "Epoch 54 Batch 750 Loss 1.1947 Accuracy 0.7193\n",
            "Epoch 54 Batch 800 Loss 1.1974 Accuracy 0.7190\n",
            "Epoch 54 Batch 850 Loss 1.2009 Accuracy 0.7186\n",
            "discarded batch 884\n",
            "Epoch 54 Batch 900 Loss 1.2021 Accuracy 0.7184\n",
            "Epoch 54 Batch 950 Loss 1.2053 Accuracy 0.7181\n",
            "Epoch 54 Loss 1.2054 Accuracy 0.7181\n",
            "Time taken for 1 epoch: 33.86603617668152 secs\n",
            "\n",
            "Epoch 55 Batch 0 Loss 1.1315 Accuracy 0.7228\n",
            "discarded batch 47\n",
            "Epoch 55 Batch 50 Loss 1.1327 Accuracy 0.7308\n",
            "Epoch 55 Batch 100 Loss 1.1501 Accuracy 0.7266\n",
            "Epoch 55 Batch 150 Loss 1.1527 Accuracy 0.7266\n",
            "Epoch 55 Batch 200 Loss 1.1530 Accuracy 0.7258\n",
            "Epoch 55 Batch 250 Loss 1.1556 Accuracy 0.7249\n",
            "Epoch 55 Batch 300 Loss 1.1623 Accuracy 0.7239\n",
            "Epoch 55 Batch 350 Loss 1.1652 Accuracy 0.7230\n",
            "Epoch 55 Batch 400 Loss 1.1666 Accuracy 0.7229\n",
            "Epoch 55 Batch 450 Loss 1.1714 Accuracy 0.7221\n",
            "Epoch 55 Batch 500 Loss 1.1746 Accuracy 0.7213\n",
            "Epoch 55 Batch 550 Loss 1.1753 Accuracy 0.7213\n",
            "Epoch 55 Batch 600 Loss 1.1773 Accuracy 0.7210\n",
            "Epoch 55 Batch 650 Loss 1.1808 Accuracy 0.7205\n",
            "Epoch 55 Batch 700 Loss 1.1847 Accuracy 0.7199\n",
            "Epoch 55 Batch 750 Loss 1.1885 Accuracy 0.7192\n",
            "Epoch 55 Batch 800 Loss 1.1911 Accuracy 0.7189\n",
            "Epoch 55 Batch 850 Loss 1.1940 Accuracy 0.7183\n",
            "Epoch 55 Batch 900 Loss 1.1973 Accuracy 0.7180\n",
            "Epoch 55 Batch 950 Loss 1.2000 Accuracy 0.7175\n",
            "Saving checkpoint for epoch 55 at ./checkpoints/train/ckpt-11\n",
            "Epoch 55 Loss 1.2001 Accuracy 0.7174\n",
            "Time taken for 1 epoch: 34.277752161026 secs\n",
            "\n",
            "Epoch 56 Batch 0 Loss 1.0450 Accuracy 0.7591\n",
            "Epoch 56 Batch 50 Loss 1.1190 Accuracy 0.7311\n",
            "Epoch 56 Batch 100 Loss 1.1393 Accuracy 0.7269\n",
            "Epoch 56 Batch 150 Loss 1.1352 Accuracy 0.7274\n",
            "Epoch 56 Batch 200 Loss 1.1367 Accuracy 0.7274\n",
            "discarded batch 208\n",
            "Epoch 56 Batch 250 Loss 1.1460 Accuracy 0.7265\n",
            "Epoch 56 Batch 300 Loss 1.1525 Accuracy 0.7262\n",
            "Epoch 56 Batch 350 Loss 1.1559 Accuracy 0.7257\n",
            "Epoch 56 Batch 400 Loss 1.1607 Accuracy 0.7253\n",
            "Epoch 56 Batch 450 Loss 1.1643 Accuracy 0.7243\n",
            "Epoch 56 Batch 500 Loss 1.1683 Accuracy 0.7237\n",
            "Epoch 56 Batch 550 Loss 1.1730 Accuracy 0.7229\n",
            "Epoch 56 Batch 600 Loss 1.1766 Accuracy 0.7224\n",
            "Epoch 56 Batch 650 Loss 1.1786 Accuracy 0.7222\n",
            "Epoch 56 Batch 700 Loss 1.1807 Accuracy 0.7217\n",
            "Epoch 56 Batch 750 Loss 1.1830 Accuracy 0.7215\n",
            "Epoch 56 Batch 800 Loss 1.1853 Accuracy 0.7213\n",
            "Epoch 56 Batch 850 Loss 1.1877 Accuracy 0.7208\n",
            "Epoch 56 Batch 900 Loss 1.1898 Accuracy 0.7207\n",
            "Epoch 56 Batch 950 Loss 1.1911 Accuracy 0.7206\n",
            "Epoch 56 Loss 1.1910 Accuracy 0.7205\n",
            "Time taken for 1 epoch: 33.90910196304321 secs\n",
            "\n",
            "Epoch 57 Batch 0 Loss 0.9364 Accuracy 0.7789\n",
            "Epoch 57 Batch 50 Loss 1.0982 Accuracy 0.7381\n",
            "Epoch 57 Batch 100 Loss 1.1190 Accuracy 0.7325\n",
            "Epoch 57 Batch 150 Loss 1.1246 Accuracy 0.7306\n",
            "Epoch 57 Batch 200 Loss 1.1339 Accuracy 0.7287\n",
            "Epoch 57 Batch 250 Loss 1.1410 Accuracy 0.7279\n",
            "Epoch 57 Batch 300 Loss 1.1435 Accuracy 0.7274\n",
            "Epoch 57 Batch 350 Loss 1.1494 Accuracy 0.7261\n",
            "Epoch 57 Batch 400 Loss 1.1542 Accuracy 0.7258\n",
            "Epoch 57 Batch 450 Loss 1.1593 Accuracy 0.7248\n",
            "Epoch 57 Batch 500 Loss 1.1613 Accuracy 0.7243\n",
            "discarded batch 518\n",
            "Epoch 57 Batch 550 Loss 1.1663 Accuracy 0.7235\n",
            "Epoch 57 Batch 600 Loss 1.1676 Accuracy 0.7237\n",
            "Epoch 57 Batch 650 Loss 1.1699 Accuracy 0.7234\n",
            "Epoch 57 Batch 700 Loss 1.1745 Accuracy 0.7224\n",
            "Epoch 57 Batch 750 Loss 1.1781 Accuracy 0.7218\n",
            "Epoch 57 Batch 800 Loss 1.1805 Accuracy 0.7215\n",
            "Epoch 57 Batch 850 Loss 1.1832 Accuracy 0.7211\n",
            "Epoch 57 Batch 900 Loss 1.1867 Accuracy 0.7206\n",
            "Epoch 57 Batch 950 Loss 1.1894 Accuracy 0.7201\n",
            "Epoch 57 Loss 1.1896 Accuracy 0.7201\n",
            "Time taken for 1 epoch: 33.567700147628784 secs\n",
            "\n",
            "Epoch 58 Batch 0 Loss 1.2253 Accuracy 0.7096\n",
            "Epoch 58 Batch 50 Loss 1.1174 Accuracy 0.7382\n",
            "Epoch 58 Batch 100 Loss 1.1238 Accuracy 0.7347\n",
            "discarded batch 121\n",
            "Epoch 58 Batch 150 Loss 1.1307 Accuracy 0.7331\n",
            "Epoch 58 Batch 200 Loss 1.1360 Accuracy 0.7322\n",
            "Epoch 58 Batch 250 Loss 1.1388 Accuracy 0.7313\n",
            "Epoch 58 Batch 300 Loss 1.1408 Accuracy 0.7309\n",
            "Epoch 58 Batch 350 Loss 1.1474 Accuracy 0.7290\n",
            "Epoch 58 Batch 400 Loss 1.1526 Accuracy 0.7281\n",
            "Epoch 58 Batch 450 Loss 1.1540 Accuracy 0.7275\n",
            "Epoch 58 Batch 500 Loss 1.1577 Accuracy 0.7269\n",
            "Epoch 58 Batch 550 Loss 1.1612 Accuracy 0.7261\n",
            "Epoch 58 Batch 600 Loss 1.1653 Accuracy 0.7255\n",
            "Epoch 58 Batch 650 Loss 1.1679 Accuracy 0.7250\n",
            "Epoch 58 Batch 700 Loss 1.1709 Accuracy 0.7245\n",
            "Epoch 58 Batch 750 Loss 1.1739 Accuracy 0.7241\n",
            "Epoch 58 Batch 800 Loss 1.1770 Accuracy 0.7236\n",
            "Epoch 58 Batch 850 Loss 1.1805 Accuracy 0.7230\n",
            "Epoch 58 Batch 900 Loss 1.1829 Accuracy 0.7226\n",
            "Epoch 58 Batch 950 Loss 1.1850 Accuracy 0.7221\n",
            "Epoch 58 Loss 1.1850 Accuracy 0.7222\n",
            "Time taken for 1 epoch: 32.914161920547485 secs\n",
            "\n",
            "Epoch 59 Batch 0 Loss 0.9932 Accuracy 0.7525\n",
            "Epoch 59 Batch 50 Loss 1.1056 Accuracy 0.7370\n",
            "Epoch 59 Batch 100 Loss 1.1094 Accuracy 0.7366\n",
            "Epoch 59 Batch 150 Loss 1.1191 Accuracy 0.7339\n",
            "Epoch 59 Batch 200 Loss 1.1196 Accuracy 0.7330\n",
            "Epoch 59 Batch 250 Loss 1.1262 Accuracy 0.7322\n",
            "Epoch 59 Batch 300 Loss 1.1316 Accuracy 0.7310\n",
            "Epoch 59 Batch 350 Loss 1.1367 Accuracy 0.7301\n",
            "Epoch 59 Batch 400 Loss 1.1405 Accuracy 0.7290\n",
            "Epoch 59 Batch 450 Loss 1.1459 Accuracy 0.7281\n",
            "discarded batch 498\n",
            "Epoch 59 Batch 500 Loss 1.1497 Accuracy 0.7271\n",
            "Epoch 59 Batch 550 Loss 1.1548 Accuracy 0.7264\n",
            "Epoch 59 Batch 600 Loss 1.1571 Accuracy 0.7260\n",
            "Epoch 59 Batch 650 Loss 1.1614 Accuracy 0.7253\n",
            "Epoch 59 Batch 700 Loss 1.1663 Accuracy 0.7245\n",
            "Epoch 59 Batch 750 Loss 1.1697 Accuracy 0.7240\n",
            "Epoch 59 Batch 800 Loss 1.1721 Accuracy 0.7235\n",
            "Epoch 59 Batch 850 Loss 1.1749 Accuracy 0.7231\n",
            "Epoch 59 Batch 900 Loss 1.1776 Accuracy 0.7226\n",
            "Epoch 59 Batch 950 Loss 1.1803 Accuracy 0.7221\n",
            "Epoch 59 Loss 1.1803 Accuracy 0.7221\n",
            "Time taken for 1 epoch: 32.86044526100159 secs\n",
            "\n",
            "Epoch 60 Batch 0 Loss 1.1782 Accuracy 0.6898\n",
            "Epoch 60 Batch 50 Loss 1.1306 Accuracy 0.7288\n",
            "Epoch 60 Batch 100 Loss 1.1161 Accuracy 0.7324\n",
            "Epoch 60 Batch 150 Loss 1.1080 Accuracy 0.7345\n",
            "Epoch 60 Batch 200 Loss 1.1170 Accuracy 0.7327\n",
            "Epoch 60 Batch 250 Loss 1.1287 Accuracy 0.7308\n",
            "Epoch 60 Batch 300 Loss 1.1345 Accuracy 0.7296\n",
            "Epoch 60 Batch 350 Loss 1.1361 Accuracy 0.7294\n",
            "Epoch 60 Batch 400 Loss 1.1402 Accuracy 0.7285\n",
            "Epoch 60 Batch 450 Loss 1.1448 Accuracy 0.7271\n",
            "discarded batch 453\n",
            "Epoch 60 Batch 500 Loss 1.1486 Accuracy 0.7265\n",
            "Epoch 60 Batch 550 Loss 1.1507 Accuracy 0.7263\n",
            "Epoch 60 Batch 600 Loss 1.1532 Accuracy 0.7258\n",
            "Epoch 60 Batch 650 Loss 1.1559 Accuracy 0.7255\n",
            "Epoch 60 Batch 700 Loss 1.1587 Accuracy 0.7253\n",
            "Epoch 60 Batch 750 Loss 1.1624 Accuracy 0.7248\n",
            "Epoch 60 Batch 800 Loss 1.1661 Accuracy 0.7243\n",
            "Epoch 60 Batch 850 Loss 1.1687 Accuracy 0.7237\n",
            "Epoch 60 Batch 900 Loss 1.1734 Accuracy 0.7231\n",
            "Epoch 60 Batch 950 Loss 1.1758 Accuracy 0.7227\n",
            "Saving checkpoint for epoch 60 at ./checkpoints/train/ckpt-12\n",
            "Epoch 60 Loss 1.1760 Accuracy 0.7226\n",
            "Time taken for 1 epoch: 33.3139374256134 secs\n",
            "\n",
            "Epoch 61 Batch 0 Loss 1.3042 Accuracy 0.6964\n",
            "Epoch 61 Batch 50 Loss 1.1142 Accuracy 0.7351\n",
            "Epoch 61 Batch 100 Loss 1.1057 Accuracy 0.7371\n",
            "discarded batch 132\n",
            "Epoch 61 Batch 150 Loss 1.1030 Accuracy 0.7356\n",
            "Epoch 61 Batch 200 Loss 1.1111 Accuracy 0.7328\n",
            "Epoch 61 Batch 250 Loss 1.1202 Accuracy 0.7316\n",
            "Epoch 61 Batch 300 Loss 1.1244 Accuracy 0.7309\n",
            "Epoch 61 Batch 350 Loss 1.1278 Accuracy 0.7303\n",
            "Epoch 61 Batch 400 Loss 1.1337 Accuracy 0.7291\n",
            "Epoch 61 Batch 450 Loss 1.1377 Accuracy 0.7282\n",
            "Epoch 61 Batch 500 Loss 1.1403 Accuracy 0.7283\n",
            "Epoch 61 Batch 550 Loss 1.1442 Accuracy 0.7276\n",
            "Epoch 61 Batch 600 Loss 1.1476 Accuracy 0.7269\n",
            "Epoch 61 Batch 650 Loss 1.1502 Accuracy 0.7266\n",
            "Epoch 61 Batch 700 Loss 1.1549 Accuracy 0.7257\n",
            "Epoch 61 Batch 750 Loss 1.1582 Accuracy 0.7249\n",
            "Epoch 61 Batch 800 Loss 1.1605 Accuracy 0.7245\n",
            "Epoch 61 Batch 850 Loss 1.1643 Accuracy 0.7240\n",
            "Epoch 61 Batch 900 Loss 1.1679 Accuracy 0.7233\n",
            "Epoch 61 Batch 950 Loss 1.1716 Accuracy 0.7229\n",
            "Epoch 61 Loss 1.1717 Accuracy 0.7229\n",
            "Time taken for 1 epoch: 33.11886167526245 secs\n",
            "\n",
            "Epoch 62 Batch 0 Loss 1.2109 Accuracy 0.7261\n",
            "Epoch 62 Batch 50 Loss 1.1196 Accuracy 0.7360\n",
            "Epoch 62 Batch 100 Loss 1.1243 Accuracy 0.7339\n",
            "Epoch 62 Batch 150 Loss 1.1284 Accuracy 0.7311\n",
            "Epoch 62 Batch 200 Loss 1.1262 Accuracy 0.7316\n",
            "Epoch 62 Batch 250 Loss 1.1309 Accuracy 0.7310\n",
            "discarded batch 285\n",
            "Epoch 62 Batch 300 Loss 1.1330 Accuracy 0.7306\n",
            "Epoch 62 Batch 350 Loss 1.1341 Accuracy 0.7300\n",
            "Epoch 62 Batch 400 Loss 1.1374 Accuracy 0.7293\n",
            "Epoch 62 Batch 450 Loss 1.1409 Accuracy 0.7289\n",
            "Epoch 62 Batch 500 Loss 1.1440 Accuracy 0.7282\n",
            "Epoch 62 Batch 550 Loss 1.1463 Accuracy 0.7277\n",
            "Epoch 62 Batch 600 Loss 1.1496 Accuracy 0.7271\n",
            "Epoch 62 Batch 650 Loss 1.1532 Accuracy 0.7265\n",
            "Epoch 62 Batch 700 Loss 1.1550 Accuracy 0.7264\n",
            "Epoch 62 Batch 750 Loss 1.1568 Accuracy 0.7261\n",
            "Epoch 62 Batch 800 Loss 1.1604 Accuracy 0.7256\n",
            "Epoch 62 Batch 850 Loss 1.1628 Accuracy 0.7251\n",
            "Epoch 62 Batch 900 Loss 1.1647 Accuracy 0.7248\n",
            "Epoch 62 Batch 950 Loss 1.1676 Accuracy 0.7245\n",
            "Epoch 62 Loss 1.1677 Accuracy 0.7245\n",
            "Time taken for 1 epoch: 33.25998902320862 secs\n",
            "\n",
            "Epoch 63 Batch 0 Loss 1.1613 Accuracy 0.7096\n",
            "Epoch 63 Batch 50 Loss 1.0880 Accuracy 0.7364\n",
            "Epoch 63 Batch 100 Loss 1.0873 Accuracy 0.7367\n",
            "Epoch 63 Batch 150 Loss 1.0932 Accuracy 0.7352\n",
            "Epoch 63 Batch 200 Loss 1.1034 Accuracy 0.7339\n",
            "Epoch 63 Batch 250 Loss 1.1074 Accuracy 0.7331\n",
            "Epoch 63 Batch 300 Loss 1.1122 Accuracy 0.7322\n",
            "Epoch 63 Batch 350 Loss 1.1189 Accuracy 0.7310\n",
            "Epoch 63 Batch 400 Loss 1.1254 Accuracy 0.7302\n",
            "Epoch 63 Batch 450 Loss 1.1292 Accuracy 0.7298\n",
            "Epoch 63 Batch 500 Loss 1.1330 Accuracy 0.7291\n",
            "Epoch 63 Batch 550 Loss 1.1368 Accuracy 0.7286\n",
            "Epoch 63 Batch 600 Loss 1.1410 Accuracy 0.7279\n",
            "Epoch 63 Batch 650 Loss 1.1435 Accuracy 0.7275\n",
            "Epoch 63 Batch 700 Loss 1.1468 Accuracy 0.7270\n",
            "Epoch 63 Batch 750 Loss 1.1485 Accuracy 0.7268\n",
            "Epoch 63 Batch 800 Loss 1.1506 Accuracy 0.7264\n",
            "discarded batch 840\n",
            "Epoch 63 Batch 850 Loss 1.1551 Accuracy 0.7254\n",
            "Epoch 63 Batch 900 Loss 1.1584 Accuracy 0.7252\n",
            "Epoch 63 Batch 950 Loss 1.1609 Accuracy 0.7248\n",
            "Epoch 63 Loss 1.1609 Accuracy 0.7248\n",
            "Time taken for 1 epoch: 33.386778354644775 secs\n",
            "\n",
            "Epoch 64 Batch 0 Loss 0.9981 Accuracy 0.7591\n",
            "Epoch 64 Batch 50 Loss 1.1056 Accuracy 0.7334\n",
            "Epoch 64 Batch 100 Loss 1.1013 Accuracy 0.7335\n",
            "Epoch 64 Batch 150 Loss 1.1048 Accuracy 0.7336\n",
            "Epoch 64 Batch 200 Loss 1.1105 Accuracy 0.7332\n",
            "Epoch 64 Batch 250 Loss 1.1139 Accuracy 0.7322\n",
            "Epoch 64 Batch 300 Loss 1.1174 Accuracy 0.7319\n",
            "Epoch 64 Batch 350 Loss 1.1209 Accuracy 0.7314\n",
            "Epoch 64 Batch 400 Loss 1.1238 Accuracy 0.7307\n",
            "Epoch 64 Batch 450 Loss 1.1277 Accuracy 0.7295\n",
            "Epoch 64 Batch 500 Loss 1.1323 Accuracy 0.7289\n",
            "Epoch 64 Batch 550 Loss 1.1356 Accuracy 0.7284\n",
            "Epoch 64 Batch 600 Loss 1.1379 Accuracy 0.7282\n",
            "Epoch 64 Batch 650 Loss 1.1420 Accuracy 0.7279\n",
            "discarded batch 697\n",
            "Epoch 64 Batch 700 Loss 1.1443 Accuracy 0.7276\n",
            "Epoch 64 Batch 750 Loss 1.1459 Accuracy 0.7274\n",
            "Epoch 64 Batch 800 Loss 1.1491 Accuracy 0.7270\n",
            "Epoch 64 Batch 850 Loss 1.1523 Accuracy 0.7264\n",
            "Epoch 64 Batch 900 Loss 1.1555 Accuracy 0.7259\n",
            "Epoch 64 Batch 950 Loss 1.1588 Accuracy 0.7252\n",
            "Epoch 64 Loss 1.1591 Accuracy 0.7252\n",
            "Time taken for 1 epoch: 33.27909564971924 secs\n",
            "\n",
            "Epoch 65 Batch 0 Loss 0.9704 Accuracy 0.7525\n",
            "Epoch 65 Batch 50 Loss 1.0715 Accuracy 0.7388\n",
            "Epoch 65 Batch 100 Loss 1.0840 Accuracy 0.7361\n",
            "Epoch 65 Batch 150 Loss 1.0931 Accuracy 0.7358\n",
            "discarded batch 185\n",
            "Epoch 65 Batch 200 Loss 1.0995 Accuracy 0.7349\n",
            "Epoch 65 Batch 250 Loss 1.1080 Accuracy 0.7333\n",
            "Epoch 65 Batch 300 Loss 1.1117 Accuracy 0.7333\n",
            "Epoch 65 Batch 350 Loss 1.1158 Accuracy 0.7325\n",
            "Epoch 65 Batch 400 Loss 1.1204 Accuracy 0.7317\n",
            "Epoch 65 Batch 450 Loss 1.1235 Accuracy 0.7311\n",
            "Epoch 65 Batch 500 Loss 1.1258 Accuracy 0.7306\n",
            "Epoch 65 Batch 550 Loss 1.1296 Accuracy 0.7301\n",
            "Epoch 65 Batch 600 Loss 1.1327 Accuracy 0.7296\n",
            "Epoch 65 Batch 650 Loss 1.1359 Accuracy 0.7293\n",
            "Epoch 65 Batch 700 Loss 1.1406 Accuracy 0.7284\n",
            "Epoch 65 Batch 750 Loss 1.1435 Accuracy 0.7280\n",
            "Epoch 65 Batch 800 Loss 1.1465 Accuracy 0.7275\n",
            "Epoch 65 Batch 850 Loss 1.1502 Accuracy 0.7268\n",
            "Epoch 65 Batch 900 Loss 1.1536 Accuracy 0.7265\n",
            "Epoch 65 Batch 950 Loss 1.1569 Accuracy 0.7259\n",
            "Saving checkpoint for epoch 65 at ./checkpoints/train/ckpt-13\n",
            "Epoch 65 Loss 1.1571 Accuracy 0.7258\n",
            "Time taken for 1 epoch: 33.723175048828125 secs\n",
            "\n",
            "Epoch 66 Batch 0 Loss 1.0020 Accuracy 0.7657\n",
            "Epoch 66 Batch 50 Loss 1.0696 Accuracy 0.7412\n",
            "Epoch 66 Batch 100 Loss 1.0790 Accuracy 0.7399\n",
            "Epoch 66 Batch 150 Loss 1.0857 Accuracy 0.7385\n",
            "Epoch 66 Batch 200 Loss 1.0909 Accuracy 0.7373\n",
            "Epoch 66 Batch 250 Loss 1.0997 Accuracy 0.7347\n",
            "Epoch 66 Batch 300 Loss 1.1063 Accuracy 0.7333\n",
            "Epoch 66 Batch 350 Loss 1.1082 Accuracy 0.7328\n",
            "Epoch 66 Batch 400 Loss 1.1149 Accuracy 0.7322\n",
            "Epoch 66 Batch 450 Loss 1.1173 Accuracy 0.7318\n",
            "Epoch 66 Batch 500 Loss 1.1211 Accuracy 0.7310\n",
            "Epoch 66 Batch 550 Loss 1.1244 Accuracy 0.7304\n",
            "Epoch 66 Batch 600 Loss 1.1276 Accuracy 0.7301\n",
            "Epoch 66 Batch 650 Loss 1.1322 Accuracy 0.7294\n",
            "Epoch 66 Batch 700 Loss 1.1349 Accuracy 0.7293\n",
            "discarded batch 745\n",
            "Epoch 66 Batch 750 Loss 1.1369 Accuracy 0.7288\n",
            "Epoch 66 Batch 800 Loss 1.1387 Accuracy 0.7286\n",
            "Epoch 66 Batch 850 Loss 1.1420 Accuracy 0.7279\n",
            "Epoch 66 Batch 900 Loss 1.1457 Accuracy 0.7276\n",
            "Epoch 66 Batch 950 Loss 1.1485 Accuracy 0.7272\n",
            "Epoch 66 Loss 1.1490 Accuracy 0.7271\n",
            "Time taken for 1 epoch: 33.8760941028595 secs\n",
            "\n",
            "Epoch 67 Batch 0 Loss 1.1530 Accuracy 0.7228\n",
            "Epoch 67 Batch 50 Loss 1.0979 Accuracy 0.7398\n",
            "Epoch 67 Batch 100 Loss 1.0954 Accuracy 0.7378\n",
            "Epoch 67 Batch 150 Loss 1.0976 Accuracy 0.7373\n",
            "discarded batch 199\n",
            "Epoch 67 Batch 200 Loss 1.1008 Accuracy 0.7361\n",
            "Epoch 67 Batch 250 Loss 1.1035 Accuracy 0.7357\n",
            "Epoch 67 Batch 300 Loss 1.1085 Accuracy 0.7345\n",
            "Epoch 67 Batch 350 Loss 1.1119 Accuracy 0.7341\n",
            "Epoch 67 Batch 400 Loss 1.1132 Accuracy 0.7337\n",
            "Epoch 67 Batch 450 Loss 1.1162 Accuracy 0.7332\n",
            "Epoch 67 Batch 500 Loss 1.1206 Accuracy 0.7321\n",
            "Epoch 67 Batch 550 Loss 1.1255 Accuracy 0.7314\n",
            "Epoch 67 Batch 600 Loss 1.1281 Accuracy 0.7312\n",
            "Epoch 67 Batch 650 Loss 1.1306 Accuracy 0.7311\n",
            "Epoch 67 Batch 700 Loss 1.1330 Accuracy 0.7308\n",
            "Epoch 67 Batch 750 Loss 1.1369 Accuracy 0.7297\n",
            "Epoch 67 Batch 800 Loss 1.1395 Accuracy 0.7292\n",
            "Epoch 67 Batch 850 Loss 1.1424 Accuracy 0.7287\n",
            "Epoch 67 Batch 900 Loss 1.1457 Accuracy 0.7281\n",
            "Epoch 67 Batch 950 Loss 1.1477 Accuracy 0.7278\n",
            "Epoch 67 Loss 1.1480 Accuracy 0.7278\n",
            "Time taken for 1 epoch: 33.472439765930176 secs\n",
            "\n",
            "Epoch 68 Batch 0 Loss 0.8296 Accuracy 0.7756\n",
            "Epoch 68 Batch 50 Loss 1.0859 Accuracy 0.7365\n",
            "Epoch 68 Batch 100 Loss 1.0769 Accuracy 0.7396\n",
            "Epoch 68 Batch 150 Loss 1.0740 Accuracy 0.7404\n",
            "Epoch 68 Batch 200 Loss 1.0805 Accuracy 0.7387\n",
            "Epoch 68 Batch 250 Loss 1.0880 Accuracy 0.7374\n",
            "Epoch 68 Batch 300 Loss 1.0927 Accuracy 0.7368\n",
            "Epoch 68 Batch 350 Loss 1.0987 Accuracy 0.7357\n",
            "discarded batch 364\n",
            "Epoch 68 Batch 400 Loss 1.1019 Accuracy 0.7353\n",
            "Epoch 68 Batch 450 Loss 1.1076 Accuracy 0.7341\n",
            "Epoch 68 Batch 500 Loss 1.1120 Accuracy 0.7334\n",
            "Epoch 68 Batch 550 Loss 1.1168 Accuracy 0.7325\n",
            "Epoch 68 Batch 600 Loss 1.1205 Accuracy 0.7321\n",
            "Epoch 68 Batch 650 Loss 1.1237 Accuracy 0.7317\n",
            "Epoch 68 Batch 700 Loss 1.1272 Accuracy 0.7308\n",
            "Epoch 68 Batch 750 Loss 1.1306 Accuracy 0.7303\n",
            "Epoch 68 Batch 800 Loss 1.1349 Accuracy 0.7297\n",
            "Epoch 68 Batch 850 Loss 1.1375 Accuracy 0.7292\n",
            "Epoch 68 Batch 900 Loss 1.1412 Accuracy 0.7287\n",
            "Epoch 68 Batch 950 Loss 1.1436 Accuracy 0.7283\n",
            "Epoch 68 Loss 1.1437 Accuracy 0.7282\n",
            "Time taken for 1 epoch: 33.57355546951294 secs\n",
            "\n",
            "Epoch 69 Batch 0 Loss 0.9882 Accuracy 0.7690\n",
            "Epoch 69 Batch 50 Loss 1.0764 Accuracy 0.7421\n",
            "Epoch 69 Batch 100 Loss 1.0784 Accuracy 0.7390\n",
            "Epoch 69 Batch 150 Loss 1.0757 Accuracy 0.7400\n",
            "Epoch 69 Batch 200 Loss 1.0786 Accuracy 0.7396\n",
            "Epoch 69 Batch 250 Loss 1.0863 Accuracy 0.7373\n",
            "Epoch 69 Batch 300 Loss 1.0932 Accuracy 0.7358\n",
            "discarded batch 320\n",
            "Epoch 69 Batch 350 Loss 1.0960 Accuracy 0.7356\n",
            "Epoch 69 Batch 400 Loss 1.1017 Accuracy 0.7350\n",
            "Epoch 69 Batch 450 Loss 1.1062 Accuracy 0.7342\n",
            "Epoch 69 Batch 500 Loss 1.1103 Accuracy 0.7333\n",
            "Epoch 69 Batch 550 Loss 1.1140 Accuracy 0.7325\n",
            "Epoch 69 Batch 600 Loss 1.1182 Accuracy 0.7316\n",
            "Epoch 69 Batch 650 Loss 1.1233 Accuracy 0.7309\n",
            "Epoch 69 Batch 700 Loss 1.1252 Accuracy 0.7309\n",
            "Epoch 69 Batch 750 Loss 1.1283 Accuracy 0.7304\n",
            "Epoch 69 Batch 800 Loss 1.1309 Accuracy 0.7298\n",
            "Epoch 69 Batch 850 Loss 1.1333 Accuracy 0.7295\n",
            "Epoch 69 Batch 900 Loss 1.1362 Accuracy 0.7292\n",
            "Epoch 69 Batch 950 Loss 1.1409 Accuracy 0.7283\n",
            "Epoch 69 Loss 1.1412 Accuracy 0.7283\n",
            "Time taken for 1 epoch: 33.61918807029724 secs\n",
            "\n",
            "Epoch 70 Batch 0 Loss 1.0406 Accuracy 0.7525\n",
            "Epoch 70 Batch 50 Loss 1.0374 Accuracy 0.7508\n",
            "discarded batch 97\n",
            "Epoch 70 Batch 100 Loss 1.0530 Accuracy 0.7441\n",
            "Epoch 70 Batch 150 Loss 1.0730 Accuracy 0.7402\n",
            "Epoch 70 Batch 200 Loss 1.0784 Accuracy 0.7396\n",
            "Epoch 70 Batch 250 Loss 1.0823 Accuracy 0.7385\n",
            "Epoch 70 Batch 300 Loss 1.0853 Accuracy 0.7383\n",
            "Epoch 70 Batch 350 Loss 1.0902 Accuracy 0.7373\n",
            "Epoch 70 Batch 400 Loss 1.0949 Accuracy 0.7363\n",
            "Epoch 70 Batch 450 Loss 1.0974 Accuracy 0.7357\n",
            "Epoch 70 Batch 500 Loss 1.1015 Accuracy 0.7350\n",
            "Epoch 70 Batch 550 Loss 1.1058 Accuracy 0.7343\n",
            "Epoch 70 Batch 600 Loss 1.1109 Accuracy 0.7337\n",
            "Epoch 70 Batch 650 Loss 1.1149 Accuracy 0.7331\n",
            "Epoch 70 Batch 700 Loss 1.1181 Accuracy 0.7326\n",
            "Epoch 70 Batch 750 Loss 1.1228 Accuracy 0.7315\n",
            "Epoch 70 Batch 800 Loss 1.1264 Accuracy 0.7311\n",
            "Epoch 70 Batch 850 Loss 1.1303 Accuracy 0.7305\n",
            "Epoch 70 Batch 900 Loss 1.1338 Accuracy 0.7303\n",
            "Epoch 70 Batch 950 Loss 1.1352 Accuracy 0.7300\n",
            "Saving checkpoint for epoch 70 at ./checkpoints/train/ckpt-14\n",
            "Epoch 70 Loss 1.1356 Accuracy 0.7300\n",
            "Time taken for 1 epoch: 33.96993637084961 secs\n",
            "\n",
            "Epoch 71 Batch 0 Loss 0.9376 Accuracy 0.7690\n",
            "Epoch 71 Batch 50 Loss 1.0561 Accuracy 0.7448\n",
            "Epoch 71 Batch 100 Loss 1.0658 Accuracy 0.7421\n",
            "Epoch 71 Batch 150 Loss 1.0747 Accuracy 0.7392\n",
            "Epoch 71 Batch 200 Loss 1.0793 Accuracy 0.7385\n",
            "Epoch 71 Batch 250 Loss 1.0826 Accuracy 0.7376\n",
            "discarded batch 299\n",
            "Epoch 71 Batch 300 Loss 1.0853 Accuracy 0.7366\n",
            "Epoch 71 Batch 350 Loss 1.0915 Accuracy 0.7357\n",
            "Epoch 71 Batch 400 Loss 1.0968 Accuracy 0.7346\n",
            "Epoch 71 Batch 450 Loss 1.1006 Accuracy 0.7344\n",
            "Epoch 71 Batch 500 Loss 1.1040 Accuracy 0.7340\n",
            "Epoch 71 Batch 550 Loss 1.1083 Accuracy 0.7333\n",
            "Epoch 71 Batch 600 Loss 1.1139 Accuracy 0.7323\n",
            "Epoch 71 Batch 650 Loss 1.1165 Accuracy 0.7321\n",
            "Epoch 71 Batch 700 Loss 1.1188 Accuracy 0.7318\n",
            "Epoch 71 Batch 750 Loss 1.1224 Accuracy 0.7315\n",
            "Epoch 71 Batch 800 Loss 1.1266 Accuracy 0.7306\n",
            "Epoch 71 Batch 850 Loss 1.1281 Accuracy 0.7302\n",
            "Epoch 71 Batch 900 Loss 1.1306 Accuracy 0.7300\n",
            "Epoch 71 Batch 950 Loss 1.1327 Accuracy 0.7297\n",
            "Epoch 71 Loss 1.1325 Accuracy 0.7297\n",
            "Time taken for 1 epoch: 33.9363009929657 secs\n",
            "\n",
            "Epoch 72 Batch 0 Loss 0.9627 Accuracy 0.7723\n",
            "Epoch 72 Batch 50 Loss 1.0650 Accuracy 0.7421\n",
            "Epoch 72 Batch 100 Loss 1.0704 Accuracy 0.7404\n",
            "Epoch 72 Batch 150 Loss 1.0769 Accuracy 0.7393\n",
            "Epoch 72 Batch 200 Loss 1.0818 Accuracy 0.7379\n",
            "Epoch 72 Batch 250 Loss 1.0897 Accuracy 0.7367\n",
            "Epoch 72 Batch 300 Loss 1.0888 Accuracy 0.7368\n",
            "Epoch 72 Batch 350 Loss 1.0913 Accuracy 0.7367\n",
            "Epoch 72 Batch 400 Loss 1.0932 Accuracy 0.7366\n",
            "Epoch 72 Batch 450 Loss 1.0953 Accuracy 0.7362\n",
            "Epoch 72 Batch 500 Loss 1.0983 Accuracy 0.7360\n",
            "Epoch 72 Batch 550 Loss 1.1030 Accuracy 0.7355\n",
            "Epoch 72 Batch 600 Loss 1.1044 Accuracy 0.7351\n",
            "Epoch 72 Batch 650 Loss 1.1071 Accuracy 0.7348\n",
            "Epoch 72 Batch 700 Loss 1.1112 Accuracy 0.7339\n",
            "Epoch 72 Batch 750 Loss 1.1145 Accuracy 0.7333\n",
            "Epoch 72 Batch 800 Loss 1.1173 Accuracy 0.7328\n",
            "Epoch 72 Batch 850 Loss 1.1212 Accuracy 0.7322\n",
            "Epoch 72 Batch 900 Loss 1.1250 Accuracy 0.7315\n",
            "discarded batch 924\n",
            "Epoch 72 Batch 950 Loss 1.1282 Accuracy 0.7312\n",
            "Epoch 72 Loss 1.1284 Accuracy 0.7312\n",
            "Time taken for 1 epoch: 33.89297676086426 secs\n",
            "\n",
            "Epoch 73 Batch 0 Loss 0.9505 Accuracy 0.7822\n",
            "Epoch 73 Batch 50 Loss 1.0635 Accuracy 0.7431\n",
            "Epoch 73 Batch 100 Loss 1.0686 Accuracy 0.7410\n",
            "Epoch 73 Batch 150 Loss 1.0690 Accuracy 0.7406\n",
            "Epoch 73 Batch 200 Loss 1.0705 Accuracy 0.7403\n",
            "Epoch 73 Batch 250 Loss 1.0785 Accuracy 0.7386\n",
            "Epoch 73 Batch 300 Loss 1.0831 Accuracy 0.7373\n",
            "Epoch 73 Batch 350 Loss 1.0865 Accuracy 0.7370\n",
            "Epoch 73 Batch 400 Loss 1.0921 Accuracy 0.7361\n",
            "Epoch 73 Batch 450 Loss 1.0975 Accuracy 0.7354\n",
            "Epoch 73 Batch 500 Loss 1.1011 Accuracy 0.7347\n",
            "Epoch 73 Batch 550 Loss 1.1059 Accuracy 0.7340\n",
            "Epoch 73 Batch 600 Loss 1.1111 Accuracy 0.7331\n",
            "Epoch 73 Batch 650 Loss 1.1129 Accuracy 0.7329\n",
            "Epoch 73 Batch 700 Loss 1.1153 Accuracy 0.7328\n",
            "Epoch 73 Batch 750 Loss 1.1181 Accuracy 0.7323\n",
            "Epoch 73 Batch 800 Loss 1.1205 Accuracy 0.7319\n",
            "discarded batch 817\n",
            "Epoch 73 Batch 850 Loss 1.1235 Accuracy 0.7313\n",
            "Epoch 73 Batch 900 Loss 1.1256 Accuracy 0.7309\n",
            "Epoch 73 Batch 950 Loss 1.1279 Accuracy 0.7308\n",
            "Epoch 73 Loss 1.1281 Accuracy 0.7307\n",
            "Time taken for 1 epoch: 33.76443696022034 secs\n",
            "\n",
            "Epoch 74 Batch 0 Loss 1.1182 Accuracy 0.7393\n",
            "Epoch 74 Batch 50 Loss 1.0530 Accuracy 0.7437\n",
            "Epoch 74 Batch 100 Loss 1.0545 Accuracy 0.7448\n",
            "Epoch 74 Batch 150 Loss 1.0610 Accuracy 0.7433\n",
            "Epoch 74 Batch 200 Loss 1.0699 Accuracy 0.7402\n",
            "Epoch 74 Batch 250 Loss 1.0726 Accuracy 0.7402\n",
            "Epoch 74 Batch 300 Loss 1.0755 Accuracy 0.7396\n",
            "Epoch 74 Batch 350 Loss 1.0773 Accuracy 0.7395\n",
            "Epoch 74 Batch 400 Loss 1.0829 Accuracy 0.7384\n",
            "Epoch 74 Batch 450 Loss 1.0859 Accuracy 0.7379\n",
            "discarded batch 470\n",
            "Epoch 74 Batch 500 Loss 1.0911 Accuracy 0.7376\n",
            "Epoch 74 Batch 550 Loss 1.0955 Accuracy 0.7371\n",
            "Epoch 74 Batch 600 Loss 1.0989 Accuracy 0.7361\n",
            "Epoch 74 Batch 650 Loss 1.1028 Accuracy 0.7353\n",
            "Epoch 74 Batch 700 Loss 1.1058 Accuracy 0.7347\n",
            "Epoch 74 Batch 750 Loss 1.1084 Accuracy 0.7342\n",
            "Epoch 74 Batch 800 Loss 1.1122 Accuracy 0.7337\n",
            "Epoch 74 Batch 850 Loss 1.1142 Accuracy 0.7333\n",
            "Epoch 74 Batch 900 Loss 1.1166 Accuracy 0.7329\n",
            "Epoch 74 Batch 950 Loss 1.1198 Accuracy 0.7325\n",
            "Epoch 74 Loss 1.1199 Accuracy 0.7325\n",
            "Time taken for 1 epoch: 33.68256139755249 secs\n",
            "\n",
            "Epoch 75 Batch 0 Loss 1.0662 Accuracy 0.7327\n",
            "Epoch 75 Batch 50 Loss 1.0591 Accuracy 0.7426\n",
            "Epoch 75 Batch 100 Loss 1.0649 Accuracy 0.7401\n",
            "Epoch 75 Batch 150 Loss 1.0671 Accuracy 0.7397\n",
            "discarded batch 180\n",
            "Epoch 75 Batch 200 Loss 1.0721 Accuracy 0.7391\n",
            "Epoch 75 Batch 250 Loss 1.0762 Accuracy 0.7391\n",
            "Epoch 75 Batch 300 Loss 1.0753 Accuracy 0.7398\n",
            "Epoch 75 Batch 350 Loss 1.0804 Accuracy 0.7389\n",
            "Epoch 75 Batch 400 Loss 1.0854 Accuracy 0.7383\n",
            "Epoch 75 Batch 450 Loss 1.0894 Accuracy 0.7368\n",
            "Epoch 75 Batch 500 Loss 1.0911 Accuracy 0.7369\n",
            "Epoch 75 Batch 550 Loss 1.0981 Accuracy 0.7352\n",
            "Epoch 75 Batch 600 Loss 1.1011 Accuracy 0.7347\n",
            "Epoch 75 Batch 650 Loss 1.1042 Accuracy 0.7344\n",
            "Epoch 75 Batch 700 Loss 1.1069 Accuracy 0.7340\n",
            "Epoch 75 Batch 750 Loss 1.1098 Accuracy 0.7335\n",
            "Epoch 75 Batch 800 Loss 1.1127 Accuracy 0.7329\n",
            "Epoch 75 Batch 850 Loss 1.1158 Accuracy 0.7325\n",
            "Epoch 75 Batch 900 Loss 1.1186 Accuracy 0.7320\n",
            "Epoch 75 Batch 950 Loss 1.1214 Accuracy 0.7315\n",
            "Saving checkpoint for epoch 75 at ./checkpoints/train/ckpt-15\n",
            "Epoch 75 Loss 1.1214 Accuracy 0.7315\n",
            "Time taken for 1 epoch: 35.01877784729004 secs\n",
            "\n",
            "Epoch 76 Batch 0 Loss 1.0062 Accuracy 0.7558\n",
            "Epoch 76 Batch 50 Loss 1.0659 Accuracy 0.7414\n",
            "Epoch 76 Batch 100 Loss 1.0659 Accuracy 0.7412\n",
            "Epoch 76 Batch 150 Loss 1.0710 Accuracy 0.7406\n",
            "Epoch 76 Batch 200 Loss 1.0708 Accuracy 0.7410\n",
            "Epoch 76 Batch 250 Loss 1.0713 Accuracy 0.7412\n",
            "Epoch 76 Batch 300 Loss 1.0759 Accuracy 0.7400\n",
            "Epoch 76 Batch 350 Loss 1.0790 Accuracy 0.7392\n",
            "Epoch 76 Batch 400 Loss 1.0831 Accuracy 0.7385\n",
            "discarded batch 420\n",
            "Epoch 76 Batch 450 Loss 1.0851 Accuracy 0.7378\n",
            "Epoch 76 Batch 500 Loss 1.0880 Accuracy 0.7369\n",
            "Epoch 76 Batch 550 Loss 1.0925 Accuracy 0.7364\n",
            "Epoch 76 Batch 600 Loss 1.0949 Accuracy 0.7360\n",
            "Epoch 76 Batch 650 Loss 1.0981 Accuracy 0.7355\n",
            "Epoch 76 Batch 700 Loss 1.1016 Accuracy 0.7348\n",
            "Epoch 76 Batch 750 Loss 1.1050 Accuracy 0.7343\n",
            "Epoch 76 Batch 800 Loss 1.1078 Accuracy 0.7339\n",
            "Epoch 76 Batch 850 Loss 1.1105 Accuracy 0.7334\n",
            "Epoch 76 Batch 900 Loss 1.1137 Accuracy 0.7327\n",
            "Epoch 76 Batch 950 Loss 1.1148 Accuracy 0.7326\n",
            "Epoch 76 Loss 1.1150 Accuracy 0.7325\n",
            "Time taken for 1 epoch: 33.18877935409546 secs\n",
            "\n",
            "Epoch 77 Batch 0 Loss 1.1618 Accuracy 0.7327\n",
            "Epoch 77 Batch 50 Loss 1.0580 Accuracy 0.7437\n",
            "Epoch 77 Batch 100 Loss 1.0604 Accuracy 0.7421\n",
            "Epoch 77 Batch 150 Loss 1.0598 Accuracy 0.7440\n",
            "Epoch 77 Batch 200 Loss 1.0629 Accuracy 0.7430\n",
            "Epoch 77 Batch 250 Loss 1.0680 Accuracy 0.7427\n",
            "Epoch 77 Batch 300 Loss 1.0698 Accuracy 0.7420\n",
            "Epoch 77 Batch 350 Loss 1.0748 Accuracy 0.7413\n",
            "Epoch 77 Batch 400 Loss 1.0803 Accuracy 0.7395\n",
            "Epoch 77 Batch 450 Loss 1.0849 Accuracy 0.7384\n",
            "Epoch 77 Batch 500 Loss 1.0887 Accuracy 0.7374\n",
            "Epoch 77 Batch 550 Loss 1.0905 Accuracy 0.7372\n",
            "Epoch 77 Batch 600 Loss 1.0948 Accuracy 0.7364\n",
            "Epoch 77 Batch 650 Loss 1.0970 Accuracy 0.7359\n",
            "discarded batch 656\n",
            "Epoch 77 Batch 700 Loss 1.1008 Accuracy 0.7351\n",
            "Epoch 77 Batch 750 Loss 1.1034 Accuracy 0.7345\n",
            "Epoch 77 Batch 800 Loss 1.1048 Accuracy 0.7343\n",
            "Epoch 77 Batch 850 Loss 1.1067 Accuracy 0.7342\n",
            "Epoch 77 Batch 900 Loss 1.1107 Accuracy 0.7338\n",
            "Epoch 77 Batch 950 Loss 1.1138 Accuracy 0.7334\n",
            "Epoch 77 Loss 1.1139 Accuracy 0.7334\n",
            "Time taken for 1 epoch: 33.16063737869263 secs\n",
            "\n",
            "Epoch 78 Batch 0 Loss 1.0261 Accuracy 0.7393\n",
            "Epoch 78 Batch 50 Loss 1.0489 Accuracy 0.7461\n",
            "Epoch 78 Batch 100 Loss 1.0442 Accuracy 0.7472\n",
            "Epoch 78 Batch 150 Loss 1.0566 Accuracy 0.7451\n",
            "Epoch 78 Batch 200 Loss 1.0575 Accuracy 0.7444\n",
            "Epoch 78 Batch 250 Loss 1.0651 Accuracy 0.7431\n",
            "Epoch 78 Batch 300 Loss 1.0699 Accuracy 0.7422\n",
            "Epoch 78 Batch 350 Loss 1.0721 Accuracy 0.7413\n",
            "Epoch 78 Batch 400 Loss 1.0777 Accuracy 0.7404\n",
            "discarded batch 412\n",
            "Epoch 78 Batch 450 Loss 1.0812 Accuracy 0.7397\n",
            "Epoch 78 Batch 500 Loss 1.0838 Accuracy 0.7392\n",
            "Epoch 78 Batch 550 Loss 1.0856 Accuracy 0.7388\n",
            "Epoch 78 Batch 600 Loss 1.0870 Accuracy 0.7384\n",
            "Epoch 78 Batch 650 Loss 1.0906 Accuracy 0.7378\n",
            "Epoch 78 Batch 700 Loss 1.0944 Accuracy 0.7372\n",
            "Epoch 78 Batch 750 Loss 1.0974 Accuracy 0.7364\n",
            "Epoch 78 Batch 800 Loss 1.0997 Accuracy 0.7360\n",
            "Epoch 78 Batch 850 Loss 1.1030 Accuracy 0.7354\n",
            "Epoch 78 Batch 900 Loss 1.1060 Accuracy 0.7350\n",
            "Epoch 78 Batch 950 Loss 1.1090 Accuracy 0.7344\n",
            "Epoch 78 Loss 1.1093 Accuracy 0.7343\n",
            "Time taken for 1 epoch: 33.133342266082764 secs\n",
            "\n",
            "Epoch 79 Batch 0 Loss 1.0582 Accuracy 0.7327\n",
            "Epoch 79 Batch 50 Loss 1.0130 Accuracy 0.7520\n",
            "Epoch 79 Batch 100 Loss 1.0358 Accuracy 0.7480\n",
            "discarded batch 124\n",
            "Epoch 79 Batch 150 Loss 1.0414 Accuracy 0.7475\n",
            "Epoch 79 Batch 200 Loss 1.0466 Accuracy 0.7461\n",
            "Epoch 79 Batch 250 Loss 1.0509 Accuracy 0.7450\n",
            "Epoch 79 Batch 300 Loss 1.0583 Accuracy 0.7434\n",
            "Epoch 79 Batch 350 Loss 1.0623 Accuracy 0.7429\n",
            "Epoch 79 Batch 400 Loss 1.0675 Accuracy 0.7420\n",
            "Epoch 79 Batch 450 Loss 1.0706 Accuracy 0.7414\n",
            "Epoch 79 Batch 500 Loss 1.0734 Accuracy 0.7410\n",
            "Epoch 79 Batch 550 Loss 1.0768 Accuracy 0.7402\n",
            "Epoch 79 Batch 600 Loss 1.0825 Accuracy 0.7391\n",
            "Epoch 79 Batch 650 Loss 1.0858 Accuracy 0.7386\n",
            "Epoch 79 Batch 700 Loss 1.0876 Accuracy 0.7383\n",
            "Epoch 79 Batch 750 Loss 1.0907 Accuracy 0.7378\n",
            "Epoch 79 Batch 800 Loss 1.0942 Accuracy 0.7371\n",
            "Epoch 79 Batch 850 Loss 1.0974 Accuracy 0.7365\n",
            "Epoch 79 Batch 900 Loss 1.1012 Accuracy 0.7358\n",
            "Epoch 79 Batch 950 Loss 1.1051 Accuracy 0.7351\n",
            "Epoch 79 Loss 1.1052 Accuracy 0.7350\n",
            "Time taken for 1 epoch: 32.99076437950134 secs\n",
            "\n",
            "Epoch 80 Batch 0 Loss 0.9513 Accuracy 0.7657\n",
            "Epoch 80 Batch 50 Loss 1.0121 Accuracy 0.7489\n",
            "Epoch 80 Batch 100 Loss 1.0278 Accuracy 0.7450\n",
            "Epoch 80 Batch 150 Loss 1.0376 Accuracy 0.7443\n",
            "Epoch 80 Batch 200 Loss 1.0444 Accuracy 0.7441\n",
            "Epoch 80 Batch 250 Loss 1.0508 Accuracy 0.7434\n",
            "Epoch 80 Batch 300 Loss 1.0573 Accuracy 0.7425\n",
            "discarded batch 323\n",
            "Epoch 80 Batch 350 Loss 1.0611 Accuracy 0.7412\n",
            "Epoch 80 Batch 400 Loss 1.0652 Accuracy 0.7410\n",
            "Epoch 80 Batch 450 Loss 1.0690 Accuracy 0.7406\n",
            "Epoch 80 Batch 500 Loss 1.0732 Accuracy 0.7402\n",
            "Epoch 80 Batch 550 Loss 1.0757 Accuracy 0.7396\n",
            "Epoch 80 Batch 600 Loss 1.0799 Accuracy 0.7390\n",
            "Epoch 80 Batch 650 Loss 1.0839 Accuracy 0.7385\n",
            "Epoch 80 Batch 700 Loss 1.0878 Accuracy 0.7377\n",
            "Epoch 80 Batch 750 Loss 1.0926 Accuracy 0.7366\n",
            "Epoch 80 Batch 800 Loss 1.0949 Accuracy 0.7361\n",
            "Epoch 80 Batch 850 Loss 1.0982 Accuracy 0.7357\n",
            "Epoch 80 Batch 900 Loss 1.1007 Accuracy 0.7355\n",
            "Epoch 80 Batch 950 Loss 1.1032 Accuracy 0.7351\n",
            "Saving checkpoint for epoch 80 at ./checkpoints/train/ckpt-16\n",
            "Epoch 80 Loss 1.1032 Accuracy 0.7351\n",
            "Time taken for 1 epoch: 33.054625511169434 secs\n",
            "\n",
            "Epoch 81 Batch 0 Loss 1.0683 Accuracy 0.7228\n",
            "Epoch 81 Batch 50 Loss 1.0131 Accuracy 0.7541\n",
            "Epoch 81 Batch 100 Loss 1.0265 Accuracy 0.7505\n",
            "Epoch 81 Batch 150 Loss 1.0433 Accuracy 0.7455\n",
            "Epoch 81 Batch 200 Loss 1.0469 Accuracy 0.7458\n",
            "Epoch 81 Batch 250 Loss 1.0500 Accuracy 0.7451\n",
            "Epoch 81 Batch 300 Loss 1.0531 Accuracy 0.7444\n",
            "Epoch 81 Batch 350 Loss 1.0568 Accuracy 0.7440\n",
            "Epoch 81 Batch 400 Loss 1.0615 Accuracy 0.7432\n",
            "Epoch 81 Batch 450 Loss 1.0650 Accuracy 0.7426\n",
            "Epoch 81 Batch 500 Loss 1.0696 Accuracy 0.7416\n",
            "Epoch 81 Batch 550 Loss 1.0733 Accuracy 0.7408\n",
            "discarded batch 570\n",
            "Epoch 81 Batch 600 Loss 1.0781 Accuracy 0.7401\n",
            "Epoch 81 Batch 650 Loss 1.0813 Accuracy 0.7392\n",
            "Epoch 81 Batch 700 Loss 1.0850 Accuracy 0.7382\n",
            "Epoch 81 Batch 750 Loss 1.0873 Accuracy 0.7379\n",
            "Epoch 81 Batch 800 Loss 1.0911 Accuracy 0.7372\n",
            "Epoch 81 Batch 850 Loss 1.0945 Accuracy 0.7366\n",
            "Epoch 81 Batch 900 Loss 1.0978 Accuracy 0.7362\n",
            "Epoch 81 Batch 950 Loss 1.0997 Accuracy 0.7359\n",
            "Epoch 81 Loss 1.0999 Accuracy 0.7359\n",
            "Time taken for 1 epoch: 33.23361277580261 secs\n",
            "\n",
            "Epoch 82 Batch 0 Loss 1.1079 Accuracy 0.7360\n",
            "Epoch 82 Batch 50 Loss 1.0351 Accuracy 0.7448\n",
            "Epoch 82 Batch 100 Loss 1.0352 Accuracy 0.7453\n",
            "Epoch 82 Batch 150 Loss 1.0389 Accuracy 0.7452\n",
            "Epoch 82 Batch 200 Loss 1.0420 Accuracy 0.7462\n",
            "discarded batch 219\n",
            "Epoch 82 Batch 250 Loss 1.0496 Accuracy 0.7449\n",
            "Epoch 82 Batch 300 Loss 1.0541 Accuracy 0.7436\n",
            "Epoch 82 Batch 350 Loss 1.0592 Accuracy 0.7430\n",
            "Epoch 82 Batch 400 Loss 1.0628 Accuracy 0.7422\n",
            "Epoch 82 Batch 450 Loss 1.0682 Accuracy 0.7413\n",
            "Epoch 82 Batch 500 Loss 1.0695 Accuracy 0.7410\n",
            "Epoch 82 Batch 550 Loss 1.0716 Accuracy 0.7407\n",
            "Epoch 82 Batch 600 Loss 1.0754 Accuracy 0.7403\n",
            "Epoch 82 Batch 650 Loss 1.0777 Accuracy 0.7399\n",
            "Epoch 82 Batch 700 Loss 1.0812 Accuracy 0.7394\n",
            "Epoch 82 Batch 750 Loss 1.0848 Accuracy 0.7387\n",
            "Epoch 82 Batch 800 Loss 1.0882 Accuracy 0.7382\n",
            "Epoch 82 Batch 850 Loss 1.0906 Accuracy 0.7377\n",
            "Epoch 82 Batch 900 Loss 1.0940 Accuracy 0.7371\n",
            "Epoch 82 Batch 950 Loss 1.0958 Accuracy 0.7367\n",
            "Epoch 82 Loss 1.0961 Accuracy 0.7367\n",
            "Time taken for 1 epoch: 32.697999715805054 secs\n",
            "\n",
            "Epoch 83 Batch 0 Loss 0.9769 Accuracy 0.7492\n",
            "Epoch 83 Batch 50 Loss 1.0178 Accuracy 0.7492\n",
            "Epoch 83 Batch 100 Loss 1.0378 Accuracy 0.7460\n",
            "Epoch 83 Batch 150 Loss 1.0430 Accuracy 0.7450\n",
            "discarded batch 182\n",
            "Epoch 83 Batch 200 Loss 1.0494 Accuracy 0.7448\n",
            "Epoch 83 Batch 250 Loss 1.0517 Accuracy 0.7437\n",
            "Epoch 83 Batch 300 Loss 1.0553 Accuracy 0.7429\n",
            "Epoch 83 Batch 350 Loss 1.0597 Accuracy 0.7422\n",
            "Epoch 83 Batch 400 Loss 1.0628 Accuracy 0.7420\n",
            "Epoch 83 Batch 450 Loss 1.0656 Accuracy 0.7409\n",
            "Epoch 83 Batch 500 Loss 1.0708 Accuracy 0.7400\n",
            "Epoch 83 Batch 550 Loss 1.0741 Accuracy 0.7395\n",
            "Epoch 83 Batch 600 Loss 1.0749 Accuracy 0.7394\n",
            "Epoch 83 Batch 650 Loss 1.0770 Accuracy 0.7393\n",
            "Epoch 83 Batch 700 Loss 1.0800 Accuracy 0.7390\n",
            "Epoch 83 Batch 750 Loss 1.0835 Accuracy 0.7386\n",
            "Epoch 83 Batch 800 Loss 1.0857 Accuracy 0.7383\n",
            "Epoch 83 Batch 850 Loss 1.0877 Accuracy 0.7381\n",
            "Epoch 83 Batch 900 Loss 1.0905 Accuracy 0.7375\n",
            "Epoch 83 Batch 950 Loss 1.0922 Accuracy 0.7372\n",
            "Epoch 83 Loss 1.0925 Accuracy 0.7371\n",
            "Time taken for 1 epoch: 32.68102979660034 secs\n",
            "\n",
            "Epoch 84 Batch 0 Loss 1.0475 Accuracy 0.7360\n",
            "Epoch 84 Batch 50 Loss 1.0303 Accuracy 0.7518\n",
            "Epoch 84 Batch 100 Loss 1.0287 Accuracy 0.7511\n",
            "Epoch 84 Batch 150 Loss 1.0395 Accuracy 0.7479\n",
            "Epoch 84 Batch 200 Loss 1.0386 Accuracy 0.7473\n",
            "Epoch 84 Batch 250 Loss 1.0450 Accuracy 0.7467\n",
            "discarded batch 266\n",
            "Epoch 84 Batch 300 Loss 1.0484 Accuracy 0.7464\n",
            "Epoch 84 Batch 350 Loss 1.0510 Accuracy 0.7461\n",
            "Epoch 84 Batch 400 Loss 1.0547 Accuracy 0.7450\n",
            "Epoch 84 Batch 450 Loss 1.0586 Accuracy 0.7444\n",
            "Epoch 84 Batch 500 Loss 1.0605 Accuracy 0.7442\n",
            "Epoch 84 Batch 550 Loss 1.0636 Accuracy 0.7437\n",
            "Epoch 84 Batch 600 Loss 1.0688 Accuracy 0.7431\n",
            "Epoch 84 Batch 650 Loss 1.0717 Accuracy 0.7425\n",
            "Epoch 84 Batch 700 Loss 1.0756 Accuracy 0.7418\n",
            "Epoch 84 Batch 750 Loss 1.0783 Accuracy 0.7409\n",
            "Epoch 84 Batch 800 Loss 1.0808 Accuracy 0.7406\n",
            "Epoch 84 Batch 850 Loss 1.0836 Accuracy 0.7401\n",
            "Epoch 84 Batch 900 Loss 1.0858 Accuracy 0.7398\n",
            "Epoch 84 Batch 950 Loss 1.0873 Accuracy 0.7394\n",
            "Epoch 84 Loss 1.0874 Accuracy 0.7394\n",
            "Time taken for 1 epoch: 33.38851046562195 secs\n",
            "\n",
            "Epoch 85 Batch 0 Loss 1.2003 Accuracy 0.7294\n",
            "Epoch 85 Batch 50 Loss 1.0330 Accuracy 0.7483\n",
            "Epoch 85 Batch 100 Loss 1.0353 Accuracy 0.7484\n",
            "Epoch 85 Batch 150 Loss 1.0346 Accuracy 0.7483\n",
            "Epoch 85 Batch 200 Loss 1.0394 Accuracy 0.7469\n",
            "Epoch 85 Batch 250 Loss 1.0434 Accuracy 0.7460\n",
            "Epoch 85 Batch 300 Loss 1.0466 Accuracy 0.7450\n",
            "Epoch 85 Batch 350 Loss 1.0507 Accuracy 0.7443\n",
            "Epoch 85 Batch 400 Loss 1.0549 Accuracy 0.7434\n",
            "Epoch 85 Batch 450 Loss 1.0583 Accuracy 0.7430\n",
            "Epoch 85 Batch 500 Loss 1.0643 Accuracy 0.7420\n",
            "Epoch 85 Batch 550 Loss 1.0669 Accuracy 0.7417\n",
            "Epoch 85 Batch 600 Loss 1.0701 Accuracy 0.7410\n",
            "Epoch 85 Batch 650 Loss 1.0730 Accuracy 0.7401\n",
            "Epoch 85 Batch 700 Loss 1.0763 Accuracy 0.7394\n",
            "Epoch 85 Batch 750 Loss 1.0788 Accuracy 0.7392\n",
            "Epoch 85 Batch 800 Loss 1.0806 Accuracy 0.7392\n",
            "discarded batch 838\n",
            "Epoch 85 Batch 850 Loss 1.0830 Accuracy 0.7387\n",
            "Epoch 85 Batch 900 Loss 1.0865 Accuracy 0.7383\n",
            "Epoch 85 Batch 950 Loss 1.0888 Accuracy 0.7377\n",
            "Saving checkpoint for epoch 85 at ./checkpoints/train/ckpt-17\n",
            "Epoch 85 Loss 1.0892 Accuracy 0.7376\n",
            "Time taken for 1 epoch: 32.60395812988281 secs\n",
            "\n",
            "Epoch 86 Batch 0 Loss 1.0682 Accuracy 0.7261\n",
            "Epoch 86 Batch 50 Loss 0.9992 Accuracy 0.7539\n",
            "Epoch 86 Batch 100 Loss 1.0211 Accuracy 0.7493\n",
            "Epoch 86 Batch 150 Loss 1.0271 Accuracy 0.7482\n",
            "Epoch 86 Batch 200 Loss 1.0319 Accuracy 0.7473\n",
            "Epoch 86 Batch 250 Loss 1.0400 Accuracy 0.7467\n",
            "Epoch 86 Batch 300 Loss 1.0442 Accuracy 0.7456\n",
            "Epoch 86 Batch 350 Loss 1.0447 Accuracy 0.7457\n",
            "Epoch 86 Batch 400 Loss 1.0485 Accuracy 0.7448\n",
            "Epoch 86 Batch 450 Loss 1.0507 Accuracy 0.7446\n",
            "Epoch 86 Batch 500 Loss 1.0558 Accuracy 0.7438\n",
            "Epoch 86 Batch 550 Loss 1.0625 Accuracy 0.7426\n",
            "Epoch 86 Batch 600 Loss 1.0658 Accuracy 0.7419\n",
            "Epoch 86 Batch 650 Loss 1.0693 Accuracy 0.7413\n",
            "discarded batch 656\n",
            "Epoch 86 Batch 700 Loss 1.0706 Accuracy 0.7413\n",
            "Epoch 86 Batch 750 Loss 1.0747 Accuracy 0.7405\n",
            "Epoch 86 Batch 800 Loss 1.0775 Accuracy 0.7400\n",
            "Epoch 86 Batch 850 Loss 1.0809 Accuracy 0.7392\n",
            "Epoch 86 Batch 900 Loss 1.0842 Accuracy 0.7386\n",
            "Epoch 86 Batch 950 Loss 1.0872 Accuracy 0.7382\n",
            "Epoch 86 Loss 1.0872 Accuracy 0.7382\n",
            "Time taken for 1 epoch: 32.93455386161804 secs\n",
            "\n",
            "Epoch 87 Batch 0 Loss 1.0528 Accuracy 0.7393\n",
            "Epoch 87 Batch 50 Loss 1.0274 Accuracy 0.7479\n",
            "Epoch 87 Batch 100 Loss 1.0295 Accuracy 0.7479\n",
            "Epoch 87 Batch 150 Loss 1.0316 Accuracy 0.7470\n",
            "Epoch 87 Batch 200 Loss 1.0371 Accuracy 0.7460\n",
            "Epoch 87 Batch 250 Loss 1.0408 Accuracy 0.7461\n",
            "Epoch 87 Batch 300 Loss 1.0429 Accuracy 0.7461\n",
            "Epoch 87 Batch 350 Loss 1.0452 Accuracy 0.7457\n",
            "Epoch 87 Batch 400 Loss 1.0493 Accuracy 0.7448\n",
            "Epoch 87 Batch 450 Loss 1.0519 Accuracy 0.7443\n",
            "Epoch 87 Batch 500 Loss 1.0567 Accuracy 0.7432\n",
            "Epoch 87 Batch 550 Loss 1.0597 Accuracy 0.7427\n",
            "Epoch 87 Batch 600 Loss 1.0637 Accuracy 0.7418\n",
            "Epoch 87 Batch 650 Loss 1.0672 Accuracy 0.7414\n",
            "Epoch 87 Batch 700 Loss 1.0698 Accuracy 0.7411\n",
            "Epoch 87 Batch 750 Loss 1.0739 Accuracy 0.7403\n",
            "Epoch 87 Batch 800 Loss 1.0751 Accuracy 0.7402\n",
            "Epoch 87 Batch 850 Loss 1.0795 Accuracy 0.7394\n",
            "discarded batch 852\n",
            "Epoch 87 Batch 900 Loss 1.0817 Accuracy 0.7390\n",
            "Epoch 87 Batch 950 Loss 1.0831 Accuracy 0.7390\n",
            "Epoch 87 Loss 1.0833 Accuracy 0.7390\n",
            "Time taken for 1 epoch: 32.89918088912964 secs\n",
            "\n",
            "Epoch 88 Batch 0 Loss 0.8899 Accuracy 0.7591\n",
            "Epoch 88 Batch 50 Loss 1.0066 Accuracy 0.7492\n",
            "Epoch 88 Batch 100 Loss 1.0284 Accuracy 0.7462\n",
            "Epoch 88 Batch 150 Loss 1.0279 Accuracy 0.7474\n",
            "Epoch 88 Batch 200 Loss 1.0287 Accuracy 0.7473\n",
            "Epoch 88 Batch 250 Loss 1.0300 Accuracy 0.7476\n",
            "Epoch 88 Batch 300 Loss 1.0343 Accuracy 0.7468\n",
            "Epoch 88 Batch 350 Loss 1.0382 Accuracy 0.7463\n",
            "Epoch 88 Batch 400 Loss 1.0445 Accuracy 0.7452\n",
            "Epoch 88 Batch 450 Loss 1.0491 Accuracy 0.7444\n",
            "Epoch 88 Batch 500 Loss 1.0527 Accuracy 0.7441\n",
            "Epoch 88 Batch 550 Loss 1.0556 Accuracy 0.7436\n",
            "discarded batch 585\n",
            "Epoch 88 Batch 600 Loss 1.0568 Accuracy 0.7436\n",
            "Epoch 88 Batch 650 Loss 1.0594 Accuracy 0.7431\n",
            "Epoch 88 Batch 700 Loss 1.0628 Accuracy 0.7427\n",
            "Epoch 88 Batch 750 Loss 1.0656 Accuracy 0.7422\n",
            "Epoch 88 Batch 800 Loss 1.0683 Accuracy 0.7417\n",
            "Epoch 88 Batch 850 Loss 1.0721 Accuracy 0.7412\n",
            "Epoch 88 Batch 900 Loss 1.0755 Accuracy 0.7408\n",
            "Epoch 88 Batch 950 Loss 1.0788 Accuracy 0.7403\n",
            "Epoch 88 Loss 1.0789 Accuracy 0.7403\n",
            "Time taken for 1 epoch: 33.413268089294434 secs\n",
            "\n",
            "Epoch 89 Batch 0 Loss 1.0170 Accuracy 0.7525\n",
            "Epoch 89 Batch 50 Loss 1.0327 Accuracy 0.7461\n",
            "Epoch 89 Batch 100 Loss 1.0249 Accuracy 0.7501\n",
            "Epoch 89 Batch 150 Loss 1.0245 Accuracy 0.7504\n",
            "Epoch 89 Batch 200 Loss 1.0250 Accuracy 0.7492\n",
            "Epoch 89 Batch 250 Loss 1.0255 Accuracy 0.7490\n",
            "Epoch 89 Batch 300 Loss 1.0300 Accuracy 0.7488\n",
            "Epoch 89 Batch 350 Loss 1.0326 Accuracy 0.7484\n",
            "Epoch 89 Batch 400 Loss 1.0352 Accuracy 0.7481\n",
            "Epoch 89 Batch 450 Loss 1.0410 Accuracy 0.7471\n",
            "Epoch 89 Batch 500 Loss 1.0456 Accuracy 0.7465\n",
            "Epoch 89 Batch 550 Loss 1.0508 Accuracy 0.7456\n",
            "Epoch 89 Batch 600 Loss 1.0549 Accuracy 0.7447\n",
            "Epoch 89 Batch 650 Loss 1.0567 Accuracy 0.7444\n",
            "Epoch 89 Batch 700 Loss 1.0603 Accuracy 0.7438\n",
            "Epoch 89 Batch 750 Loss 1.0624 Accuracy 0.7434\n",
            "discarded batch 786\n",
            "Epoch 89 Batch 800 Loss 1.0664 Accuracy 0.7423\n",
            "Epoch 89 Batch 850 Loss 1.0687 Accuracy 0.7419\n",
            "Epoch 89 Batch 900 Loss 1.0728 Accuracy 0.7409\n",
            "Epoch 89 Batch 950 Loss 1.0759 Accuracy 0.7404\n",
            "Epoch 89 Loss 1.0760 Accuracy 0.7404\n",
            "Time taken for 1 epoch: 33.22395420074463 secs\n",
            "\n",
            "Epoch 90 Batch 0 Loss 0.9793 Accuracy 0.7657\n",
            "Epoch 90 Batch 50 Loss 1.0292 Accuracy 0.7502\n",
            "Epoch 90 Batch 100 Loss 1.0237 Accuracy 0.7515\n",
            "Epoch 90 Batch 150 Loss 1.0247 Accuracy 0.7506\n",
            "Epoch 90 Batch 200 Loss 1.0303 Accuracy 0.7495\n",
            "discarded batch 236\n",
            "Epoch 90 Batch 250 Loss 1.0314 Accuracy 0.7486\n",
            "Epoch 90 Batch 300 Loss 1.0402 Accuracy 0.7470\n",
            "Epoch 90 Batch 350 Loss 1.0442 Accuracy 0.7460\n",
            "Epoch 90 Batch 400 Loss 1.0478 Accuracy 0.7455\n",
            "Epoch 90 Batch 450 Loss 1.0508 Accuracy 0.7446\n",
            "Epoch 90 Batch 500 Loss 1.0552 Accuracy 0.7436\n",
            "Epoch 90 Batch 550 Loss 1.0599 Accuracy 0.7425\n",
            "Epoch 90 Batch 600 Loss 1.0614 Accuracy 0.7426\n",
            "Epoch 90 Batch 650 Loss 1.0629 Accuracy 0.7424\n",
            "Epoch 90 Batch 700 Loss 1.0647 Accuracy 0.7422\n",
            "Epoch 90 Batch 750 Loss 1.0677 Accuracy 0.7417\n",
            "Epoch 90 Batch 800 Loss 1.0697 Accuracy 0.7415\n",
            "Epoch 90 Batch 850 Loss 1.0712 Accuracy 0.7412\n",
            "Epoch 90 Batch 900 Loss 1.0734 Accuracy 0.7408\n",
            "Epoch 90 Batch 950 Loss 1.0750 Accuracy 0.7406\n",
            "Saving checkpoint for epoch 90 at ./checkpoints/train/ckpt-18\n",
            "Epoch 90 Loss 1.0750 Accuracy 0.7406\n",
            "Time taken for 1 epoch: 33.66490054130554 secs\n",
            "\n",
            "Epoch 91 Batch 0 Loss 1.1018 Accuracy 0.7393\n",
            "Epoch 91 Batch 50 Loss 1.0209 Accuracy 0.7506\n",
            "Epoch 91 Batch 100 Loss 1.0174 Accuracy 0.7522\n",
            "Epoch 91 Batch 150 Loss 1.0100 Accuracy 0.7538\n",
            "Epoch 91 Batch 200 Loss 1.0203 Accuracy 0.7509\n",
            "Epoch 91 Batch 250 Loss 1.0261 Accuracy 0.7491\n",
            "Epoch 91 Batch 300 Loss 1.0321 Accuracy 0.7471\n",
            "Epoch 91 Batch 350 Loss 1.0356 Accuracy 0.7466\n",
            "Epoch 91 Batch 400 Loss 1.0401 Accuracy 0.7459\n",
            "discarded batch 426\n",
            "Epoch 91 Batch 450 Loss 1.0444 Accuracy 0.7450\n",
            "Epoch 91 Batch 500 Loss 1.0473 Accuracy 0.7444\n",
            "Epoch 91 Batch 550 Loss 1.0509 Accuracy 0.7439\n",
            "Epoch 91 Batch 600 Loss 1.0529 Accuracy 0.7438\n",
            "Epoch 91 Batch 650 Loss 1.0562 Accuracy 0.7433\n",
            "Epoch 91 Batch 700 Loss 1.0593 Accuracy 0.7426\n",
            "Epoch 91 Batch 750 Loss 1.0624 Accuracy 0.7419\n",
            "Epoch 91 Batch 800 Loss 1.0651 Accuracy 0.7415\n",
            "Epoch 91 Batch 850 Loss 1.0668 Accuracy 0.7413\n",
            "Epoch 91 Batch 900 Loss 1.0704 Accuracy 0.7407\n",
            "Epoch 91 Batch 950 Loss 1.0727 Accuracy 0.7404\n",
            "Epoch 91 Loss 1.0730 Accuracy 0.7403\n",
            "Time taken for 1 epoch: 33.415022134780884 secs\n",
            "\n",
            "Epoch 92 Batch 0 Loss 1.0027 Accuracy 0.7393\n",
            "Epoch 92 Batch 50 Loss 0.9945 Accuracy 0.7537\n",
            "Epoch 92 Batch 100 Loss 1.0127 Accuracy 0.7506\n",
            "Epoch 92 Batch 150 Loss 1.0225 Accuracy 0.7483\n",
            "Epoch 92 Batch 200 Loss 1.0220 Accuracy 0.7495\n",
            "Epoch 92 Batch 250 Loss 1.0235 Accuracy 0.7494\n",
            "Epoch 92 Batch 300 Loss 1.0265 Accuracy 0.7489\n",
            "Epoch 92 Batch 350 Loss 1.0287 Accuracy 0.7487\n",
            "Epoch 92 Batch 400 Loss 1.0343 Accuracy 0.7470\n",
            "Epoch 92 Batch 450 Loss 1.0388 Accuracy 0.7464\n",
            "Epoch 92 Batch 500 Loss 1.0419 Accuracy 0.7460\n",
            "discarded batch 529\n",
            "Epoch 92 Batch 550 Loss 1.0445 Accuracy 0.7454\n",
            "Epoch 92 Batch 600 Loss 1.0481 Accuracy 0.7451\n",
            "Epoch 92 Batch 650 Loss 1.0503 Accuracy 0.7449\n",
            "Epoch 92 Batch 700 Loss 1.0537 Accuracy 0.7442\n",
            "Epoch 92 Batch 750 Loss 1.0570 Accuracy 0.7438\n",
            "Epoch 92 Batch 800 Loss 1.0602 Accuracy 0.7431\n",
            "Epoch 92 Batch 850 Loss 1.0623 Accuracy 0.7424\n",
            "Epoch 92 Batch 900 Loss 1.0655 Accuracy 0.7420\n",
            "Epoch 92 Batch 950 Loss 1.0683 Accuracy 0.7415\n",
            "Epoch 92 Loss 1.0685 Accuracy 0.7415\n",
            "Time taken for 1 epoch: 33.206995487213135 secs\n",
            "\n",
            "Epoch 93 Batch 0 Loss 0.8553 Accuracy 0.7888\n",
            "Epoch 93 Batch 50 Loss 0.9959 Accuracy 0.7566\n",
            "Epoch 93 Batch 100 Loss 0.9928 Accuracy 0.7571\n",
            "Epoch 93 Batch 150 Loss 1.0022 Accuracy 0.7539\n",
            "Epoch 93 Batch 200 Loss 1.0072 Accuracy 0.7525\n",
            "Epoch 93 Batch 250 Loss 1.0141 Accuracy 0.7515\n",
            "Epoch 93 Batch 300 Loss 1.0182 Accuracy 0.7506\n",
            "Epoch 93 Batch 350 Loss 1.0222 Accuracy 0.7502\n",
            "Epoch 93 Batch 400 Loss 1.0270 Accuracy 0.7487\n",
            "Epoch 93 Batch 450 Loss 1.0318 Accuracy 0.7475\n",
            "discarded batch 471\n",
            "Epoch 93 Batch 500 Loss 1.0384 Accuracy 0.7461\n",
            "Epoch 93 Batch 550 Loss 1.0408 Accuracy 0.7457\n",
            "Epoch 93 Batch 600 Loss 1.0445 Accuracy 0.7449\n",
            "Epoch 93 Batch 650 Loss 1.0493 Accuracy 0.7440\n",
            "Epoch 93 Batch 700 Loss 1.0530 Accuracy 0.7435\n",
            "Epoch 93 Batch 750 Loss 1.0555 Accuracy 0.7432\n",
            "Epoch 93 Batch 800 Loss 1.0579 Accuracy 0.7427\n",
            "Epoch 93 Batch 850 Loss 1.0607 Accuracy 0.7422\n",
            "Epoch 93 Batch 900 Loss 1.0630 Accuracy 0.7419\n",
            "Epoch 93 Batch 950 Loss 1.0656 Accuracy 0.7415\n",
            "Epoch 93 Loss 1.0659 Accuracy 0.7415\n",
            "Time taken for 1 epoch: 34.055771350860596 secs\n",
            "\n",
            "Epoch 94 Batch 0 Loss 0.9369 Accuracy 0.7558\n",
            "Epoch 94 Batch 50 Loss 0.9888 Accuracy 0.7530\n",
            "Epoch 94 Batch 100 Loss 1.0010 Accuracy 0.7525\n",
            "Epoch 94 Batch 150 Loss 1.0012 Accuracy 0.7526\n",
            "Epoch 94 Batch 200 Loss 1.0063 Accuracy 0.7517\n",
            "Epoch 94 Batch 250 Loss 1.0189 Accuracy 0.7496\n",
            "Epoch 94 Batch 300 Loss 1.0263 Accuracy 0.7482\n",
            "Epoch 94 Batch 350 Loss 1.0291 Accuracy 0.7475\n",
            "Epoch 94 Batch 400 Loss 1.0323 Accuracy 0.7470\n",
            "Epoch 94 Batch 450 Loss 1.0342 Accuracy 0.7468\n",
            "Epoch 94 Batch 500 Loss 1.0384 Accuracy 0.7461\n",
            "Epoch 94 Batch 550 Loss 1.0419 Accuracy 0.7456\n",
            "discarded batch 573\n",
            "Epoch 94 Batch 600 Loss 1.0462 Accuracy 0.7450\n",
            "Epoch 94 Batch 650 Loss 1.0492 Accuracy 0.7445\n",
            "Epoch 94 Batch 700 Loss 1.0517 Accuracy 0.7442\n",
            "Epoch 94 Batch 750 Loss 1.0538 Accuracy 0.7439\n",
            "Epoch 94 Batch 800 Loss 1.0564 Accuracy 0.7435\n",
            "Epoch 94 Batch 850 Loss 1.0593 Accuracy 0.7433\n",
            "Epoch 94 Batch 900 Loss 1.0615 Accuracy 0.7429\n",
            "Epoch 94 Batch 950 Loss 1.0631 Accuracy 0.7424\n",
            "Epoch 94 Loss 1.0635 Accuracy 0.7423\n",
            "Time taken for 1 epoch: 33.60044193267822 secs\n",
            "\n",
            "Epoch 95 Batch 0 Loss 1.0757 Accuracy 0.7591\n",
            "Epoch 95 Batch 50 Loss 0.9917 Accuracy 0.7563\n",
            "Epoch 95 Batch 100 Loss 1.0000 Accuracy 0.7547\n",
            "discarded batch 149\n",
            "Epoch 95 Batch 150 Loss 1.0080 Accuracy 0.7518\n",
            "Epoch 95 Batch 200 Loss 1.0103 Accuracy 0.7513\n",
            "Epoch 95 Batch 250 Loss 1.0128 Accuracy 0.7512\n",
            "Epoch 95 Batch 300 Loss 1.0154 Accuracy 0.7510\n",
            "Epoch 95 Batch 350 Loss 1.0210 Accuracy 0.7498\n",
            "Epoch 95 Batch 400 Loss 1.0265 Accuracy 0.7488\n",
            "Epoch 95 Batch 450 Loss 1.0290 Accuracy 0.7484\n",
            "Epoch 95 Batch 500 Loss 1.0315 Accuracy 0.7475\n",
            "Epoch 95 Batch 550 Loss 1.0354 Accuracy 0.7467\n",
            "Epoch 95 Batch 600 Loss 1.0397 Accuracy 0.7460\n",
            "Epoch 95 Batch 650 Loss 1.0425 Accuracy 0.7456\n",
            "Epoch 95 Batch 700 Loss 1.0470 Accuracy 0.7449\n",
            "Epoch 95 Batch 750 Loss 1.0509 Accuracy 0.7443\n",
            "Epoch 95 Batch 800 Loss 1.0534 Accuracy 0.7438\n",
            "Epoch 95 Batch 850 Loss 1.0573 Accuracy 0.7432\n",
            "Epoch 95 Batch 900 Loss 1.0599 Accuracy 0.7428\n",
            "Epoch 95 Batch 950 Loss 1.0617 Accuracy 0.7425\n",
            "Saving checkpoint for epoch 95 at ./checkpoints/train/ckpt-19\n",
            "Epoch 95 Loss 1.0615 Accuracy 0.7426\n",
            "Time taken for 1 epoch: 33.64581847190857 secs\n",
            "\n",
            "Epoch 96 Batch 0 Loss 0.9433 Accuracy 0.7624\n",
            "Epoch 96 Batch 50 Loss 0.9930 Accuracy 0.7575\n",
            "Epoch 96 Batch 100 Loss 1.0099 Accuracy 0.7538\n",
            "Epoch 96 Batch 150 Loss 1.0098 Accuracy 0.7530\n",
            "Epoch 96 Batch 200 Loss 1.0083 Accuracy 0.7531\n",
            "discarded batch 213\n",
            "Epoch 96 Batch 250 Loss 1.0078 Accuracy 0.7527\n",
            "Epoch 96 Batch 300 Loss 1.0091 Accuracy 0.7524\n",
            "Epoch 96 Batch 350 Loss 1.0153 Accuracy 0.7507\n",
            "Epoch 96 Batch 400 Loss 1.0189 Accuracy 0.7499\n",
            "Epoch 96 Batch 450 Loss 1.0254 Accuracy 0.7488\n",
            "Epoch 96 Batch 500 Loss 1.0283 Accuracy 0.7483\n",
            "Epoch 96 Batch 550 Loss 1.0314 Accuracy 0.7477\n",
            "Epoch 96 Batch 600 Loss 1.0344 Accuracy 0.7474\n",
            "Epoch 96 Batch 650 Loss 1.0368 Accuracy 0.7468\n",
            "Epoch 96 Batch 700 Loss 1.0417 Accuracy 0.7459\n",
            "Epoch 96 Batch 750 Loss 1.0453 Accuracy 0.7452\n",
            "Epoch 96 Batch 800 Loss 1.0484 Accuracy 0.7448\n",
            "Epoch 96 Batch 850 Loss 1.0519 Accuracy 0.7441\n",
            "Epoch 96 Batch 900 Loss 1.0544 Accuracy 0.7438\n",
            "Epoch 96 Batch 950 Loss 1.0557 Accuracy 0.7436\n",
            "Epoch 96 Loss 1.0558 Accuracy 0.7437\n",
            "Time taken for 1 epoch: 33.7815637588501 secs\n",
            "\n",
            "Epoch 97 Batch 0 Loss 0.9550 Accuracy 0.7558\n",
            "Epoch 97 Batch 50 Loss 0.9966 Accuracy 0.7523\n",
            "Epoch 97 Batch 100 Loss 0.9948 Accuracy 0.7529\n",
            "Epoch 97 Batch 150 Loss 1.0008 Accuracy 0.7516\n",
            "Epoch 97 Batch 200 Loss 0.9976 Accuracy 0.7527\n",
            "Epoch 97 Batch 250 Loss 1.0022 Accuracy 0.7521\n",
            "Epoch 97 Batch 300 Loss 1.0082 Accuracy 0.7506\n",
            "Epoch 97 Batch 350 Loss 1.0151 Accuracy 0.7493\n",
            "Epoch 97 Batch 400 Loss 1.0184 Accuracy 0.7490\n",
            "Epoch 97 Batch 450 Loss 1.0211 Accuracy 0.7486\n",
            "Epoch 97 Batch 500 Loss 1.0272 Accuracy 0.7475\n",
            "Epoch 97 Batch 550 Loss 1.0309 Accuracy 0.7471\n",
            "Epoch 97 Batch 600 Loss 1.0333 Accuracy 0.7469\n",
            "Epoch 97 Batch 650 Loss 1.0363 Accuracy 0.7467\n",
            "Epoch 97 Batch 700 Loss 1.0389 Accuracy 0.7465\n",
            "discarded batch 722\n",
            "Epoch 97 Batch 750 Loss 1.0429 Accuracy 0.7455\n",
            "Epoch 97 Batch 800 Loss 1.0462 Accuracy 0.7448\n",
            "Epoch 97 Batch 850 Loss 1.0485 Accuracy 0.7445\n",
            "Epoch 97 Batch 900 Loss 1.0515 Accuracy 0.7441\n",
            "Epoch 97 Batch 950 Loss 1.0545 Accuracy 0.7438\n",
            "Epoch 97 Loss 1.0547 Accuracy 0.7437\n",
            "Time taken for 1 epoch: 33.64829182624817 secs\n",
            "\n",
            "Epoch 98 Batch 0 Loss 0.9381 Accuracy 0.7657\n",
            "Epoch 98 Batch 50 Loss 0.9723 Accuracy 0.7602\n",
            "Epoch 98 Batch 100 Loss 0.9877 Accuracy 0.7563\n",
            "Epoch 98 Batch 150 Loss 0.9955 Accuracy 0.7555\n",
            "Epoch 98 Batch 200 Loss 0.9972 Accuracy 0.7538\n",
            "Epoch 98 Batch 250 Loss 1.0047 Accuracy 0.7524\n",
            "Epoch 98 Batch 300 Loss 1.0128 Accuracy 0.7508\n",
            "Epoch 98 Batch 350 Loss 1.0178 Accuracy 0.7503\n",
            "Epoch 98 Batch 400 Loss 1.0229 Accuracy 0.7493\n",
            "Epoch 98 Batch 450 Loss 1.0243 Accuracy 0.7490\n",
            "Epoch 98 Batch 500 Loss 1.0267 Accuracy 0.7487\n",
            "Epoch 98 Batch 550 Loss 1.0302 Accuracy 0.7477\n",
            "discarded batch 570\n",
            "Epoch 98 Batch 600 Loss 1.0331 Accuracy 0.7470\n",
            "Epoch 98 Batch 650 Loss 1.0364 Accuracy 0.7462\n",
            "Epoch 98 Batch 700 Loss 1.0386 Accuracy 0.7458\n",
            "Epoch 98 Batch 750 Loss 1.0422 Accuracy 0.7453\n",
            "Epoch 98 Batch 800 Loss 1.0447 Accuracy 0.7449\n",
            "Epoch 98 Batch 850 Loss 1.0466 Accuracy 0.7444\n",
            "Epoch 98 Batch 900 Loss 1.0492 Accuracy 0.7440\n",
            "Epoch 98 Batch 950 Loss 1.0506 Accuracy 0.7439\n",
            "Epoch 98 Loss 1.0505 Accuracy 0.7439\n",
            "Time taken for 1 epoch: 34.01132035255432 secs\n",
            "\n",
            "Epoch 99 Batch 0 Loss 1.0070 Accuracy 0.7426\n",
            "Epoch 99 Batch 50 Loss 0.9812 Accuracy 0.7579\n",
            "Epoch 99 Batch 100 Loss 0.9893 Accuracy 0.7574\n",
            "Epoch 99 Batch 150 Loss 0.9899 Accuracy 0.7564\n",
            "Epoch 99 Batch 200 Loss 0.9984 Accuracy 0.7552\n",
            "discarded batch 217\n",
            "Epoch 99 Batch 250 Loss 0.9993 Accuracy 0.7549\n",
            "Epoch 99 Batch 300 Loss 1.0059 Accuracy 0.7533\n",
            "Epoch 99 Batch 350 Loss 1.0113 Accuracy 0.7520\n",
            "Epoch 99 Batch 400 Loss 1.0162 Accuracy 0.7511\n",
            "Epoch 99 Batch 450 Loss 1.0187 Accuracy 0.7511\n",
            "Epoch 99 Batch 500 Loss 1.0238 Accuracy 0.7503\n",
            "Epoch 99 Batch 550 Loss 1.0293 Accuracy 0.7492\n",
            "Epoch 99 Batch 600 Loss 1.0307 Accuracy 0.7488\n",
            "Epoch 99 Batch 650 Loss 1.0341 Accuracy 0.7480\n",
            "Epoch 99 Batch 700 Loss 1.0366 Accuracy 0.7476\n",
            "Epoch 99 Batch 750 Loss 1.0400 Accuracy 0.7468\n",
            "Epoch 99 Batch 800 Loss 1.0442 Accuracy 0.7461\n",
            "Epoch 99 Batch 850 Loss 1.0455 Accuracy 0.7458\n",
            "Epoch 99 Batch 900 Loss 1.0477 Accuracy 0.7456\n",
            "Epoch 99 Batch 950 Loss 1.0510 Accuracy 0.7451\n",
            "Epoch 99 Loss 1.0511 Accuracy 0.7451\n",
            "Time taken for 1 epoch: 33.985071420669556 secs\n",
            "\n",
            "Epoch 100 Batch 0 Loss 0.8847 Accuracy 0.7789\n",
            "Epoch 100 Batch 50 Loss 0.9642 Accuracy 0.7585\n",
            "Epoch 100 Batch 100 Loss 0.9855 Accuracy 0.7562\n",
            "Epoch 100 Batch 150 Loss 0.9903 Accuracy 0.7556\n",
            "Epoch 100 Batch 200 Loss 0.9982 Accuracy 0.7544\n",
            "Epoch 100 Batch 250 Loss 1.0021 Accuracy 0.7535\n",
            "Epoch 100 Batch 300 Loss 1.0065 Accuracy 0.7527\n",
            "Epoch 100 Batch 350 Loss 1.0116 Accuracy 0.7516\n",
            "Epoch 100 Batch 400 Loss 1.0154 Accuracy 0.7508\n",
            "Epoch 100 Batch 450 Loss 1.0178 Accuracy 0.7503\n",
            "Epoch 100 Batch 500 Loss 1.0217 Accuracy 0.7500\n",
            "Epoch 100 Batch 550 Loss 1.0241 Accuracy 0.7494\n",
            "Epoch 100 Batch 600 Loss 1.0274 Accuracy 0.7489\n",
            "Epoch 100 Batch 650 Loss 1.0311 Accuracy 0.7483\n",
            "Epoch 100 Batch 700 Loss 1.0333 Accuracy 0.7479\n",
            "Epoch 100 Batch 750 Loss 1.0352 Accuracy 0.7477\n",
            "Epoch 100 Batch 800 Loss 1.0381 Accuracy 0.7472\n",
            "Epoch 100 Batch 850 Loss 1.0412 Accuracy 0.7467\n",
            "discarded batch 857\n",
            "Epoch 100 Batch 900 Loss 1.0431 Accuracy 0.7462\n",
            "Epoch 100 Batch 950 Loss 1.0465 Accuracy 0.7455\n",
            "Saving checkpoint for epoch 100 at ./checkpoints/train/ckpt-20\n",
            "Epoch 100 Loss 1.0469 Accuracy 0.7454\n",
            "Time taken for 1 epoch: 33.947021484375 secs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjLbfLOU_pRw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transformer.save_weights(\"./optimus_rhyme_200\")\n",
        "#transformer.load_weights(\"./optimus_rhyme\")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEun8YPR8Sn6",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8c1ee1c5-9057-497b-cb28-a945bf076a3e"
      },
      "source": [
        "#@title Generation\n",
        "\n",
        "def evaluate(inp_sentence, decoder_input):\n",
        "    inp_sentence = inp_sentence\n",
        "    encoder_input = tf.expand_dims(inp_sentence, 0)\n",
        "    \n",
        "    output = tf.expand_dims(decoder_input, 0)\n",
        "\n",
        "    terces = 0\n",
        "    for i in range(batch_len):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "            encoder_input, output)\n",
        "    \n",
        "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "        predictions, attention_weights = transformer(encoder_input, \n",
        "                                                    output,\n",
        "                                                    False,\n",
        "                                                    enc_padding_mask,\n",
        "                                                    combined_mask,\n",
        "                                                    dec_padding_mask)\n",
        "        \n",
        "        # select the last word from the seq_len dimension\n",
        "        predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "        # return the result if the predicted_id is equal to the end token\n",
        "        if predicted_id == eos:\n",
        "            terces += 1\n",
        "            if terces == 3:\n",
        "                return tf.squeeze(output, axis=0), attention_weights\n",
        "        # concatentate the predicted_id to the output which is given to the decoder\n",
        "        # as its input.\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "    return tf.squeeze(output, axis=0), attention_weights\n",
        "\n",
        "out_list = batches[3][0]\n",
        "print(seq2str(out_list))\n",
        "offset = 75\n",
        "print(\"---------------------------\")\n",
        "for i in range(10):\n",
        "    out, att_w = evaluate([pad], out_list[-offset:])\n",
        "    out_list = out.numpy().tolist()\n",
        "    out_str = seq2str(out_list[offset:])\n",
        "    print(out_str) "
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " venir vedra ' mi al tuo diletto legno\n",
            "e coronarmi allor di quelle foglie\n",
            "che la matera e tu mi farai degno\n",
            "               \n",
            " sí rade volte padre se ne coglie\n",
            "per triunfare o cesare o poeta\n",
            "colpa e vergogna dell ' umane voglie\n",
            "                  \n",
            " che parturir letizia in su la lieta\n",
            "delfica deità dovría la fronda\n",
            "peneia quando alcun di sé asseta\n",
            "                     \n",
            " poca favilla gran fiamma seconda\n",
            "forse di retro a me con miglior voci\n",
            "si pregherà perché cirra risponda\n",
            "                      \n",
            "\n",
            "---------------------------\n",
            " la mia tista che li arri par foci\n",
            "ricominciò il muro del mio duca\n",
            "e poi che ' l vento a me si fero !\n",
            "              \n",
            " cosí fu la mia mari e di sé\n",
            "nel primo si volse il poeta prima\n",
            "in sua vita sua vita al mio\n",
            "                     \n",
            " tanto che l ' animo si risponde\n",
            "come si volse a me si riva\n",
            "come di quel da sua materia non fa !\n",
            "\n",
            "\n",
            " lo duca che piú che si pote il corde\n",
            "me di color di tutto piú olte\n",
            "che piú oltre né piú oltre si vede\n",
            "                 \n",
            " si crede ancor davamo e si liete\n",
            "si volse in su l ' una gente corta\n",
            "vi di co di lustra gente fronte\n",
            "                \n",
            " cosí li a me questa tutta scorta\n",
            "per che l ' altre cor che ' l mio duca di\n",
            "che ' l sen va tutta tutta si risponta\n",
            "\n",
            "\n",
            " bolo scender del qua giú nel lume mea\n",
            "con quella qua giú nel mondo si move\n",
            "con l ' un de qua giú nel mondo me me !\n",
            "          \n",
            " cosí si volse e ' l suo di qua avea\n",
            "di lui che non par si vedesti in su la terra\n",
            "e quasi il cerchio della tristo medea\n",
            "     \n",
            " cosí si fec ' io a me canta e ' l lui cura\n",
            "pur colà su con l ' un de ' due\n",
            "e giú di là su di là giú misu mi !\n",
            "\n",
            "\n",
            " e io a lui che di tutta le spalle\n",
            "passa me ' l mio duca del mio le stelle\n",
            "e piedi un ser per le gambe ' l piele\n",
            "        \n",
            " e io maestro disse ' che siamole stelle\n",
            "per lo ' che ' l duca a dir mi scuro\n",
            "e io a lui maestro a ciò ch ' ale e\n",
            "   \n",
            " io son io maestro a me sí poco appresso\n",
            "e io mi disse or qui non m ' asciuto\n",
            "se ' l tuo maestro anco mi con esso\n",
            "\n",
            "\n",
            " ed elli a me l ' anima sí feso\n",
            "sí che di mia se ne si mostra\n",
            "se ne fa sí che tu sa ricoperso\n",
            "                \n",
            " se ' che tu sai che tu mi rico vostra\n",
            "come tu se ' che tu ti mostrasti\n",
            "sodi quel che di costui sa vi ?\n",
            "               \n",
            " come per sa me l ' anime ti guastisti\n",
            "che tu vedi che tu sai sai ci dovesti\n",
            "come per te tu testi qui tististi\n",
            "\n",
            "\n",
            " qui ti mostrò la bocca li ?\n",
            "tu sai dinanzi e per la morta\n",
            "di qui moio per movi one ?\n",
            "o perché per lo tristi ?\n",
            "          \n",
            " o virtú della tua forma destra ?\n",
            "del mondo del mondo del cerchio moventa\n",
            "con la tua vita un colpa ch ' altro in ammenta\n",
            "          \n",
            " lo qual tacerto mi vive in orta\n",
            "come per la tua vista in altro move\n",
            "colui che viene alla tua vita mota\n",
            "\n",
            "\n",
            " per che lí cora de ' suoi difalla\n",
            "lo mar divina corte sendo corda\n",
            "di retro a tutto tutto a sua nota\n",
            "                  \n",
            " di tutte canta di tutta onda\n",
            "per lo duca cervi d ' infermar s ' intenta\n",
            "di retro a tutte sovra tutta l ' alta strada\n",
            "      \n",
            " e come l ' ombra sua argomenta in parte\n",
            "e quando ' l suo polo e al gran do\n",
            "pelo ' l ciel ch ' e ' l ciel si paleta\n",
            "\n",
            "\n",
            " di questa come a me stessa si leva\n",
            "quell ' enima per me l ' una voce\n",
            "e quasi a me per me per lo ' ' l diman priva\n",
            "   \n",
            " levate state e ' l suo di la foce\n",
            "nel quale e per l ' una e l ' altra poi\n",
            "e l ' altra si volte e di vede in do \n",
            " come l ' altre che nel fosti non si dole\n",
            "penne da penne e per lo sol per lo ' busto\n",
            "esce di lei a me si rimane ma\n",
            "      \n",
            "\n",
            "\n",
            "\n",
            " come un altro e poi per lo ' ingegno\n",
            "di quella gran per la forma in mano\n",
            "si move a lui tal parlar non costo\n",
            "               \n",
            " cosí due are a me l ' altro e l ' alno a mo si riguarda\n",
            "di quel ch ' al viso e l ' un si mostra non vano\n",
            "     \n",
            " poi si volse a me l ' alta gente in corda\n",
            "di quella in su lo specdio di me vede\n",
            "di fuor te me ti vi li occhi di\n",
            "\n",
            "\n",
            " e se rivolta in su la mia vita\n",
            "che di lucente in quel che pone\n",
            "qua è poco in sé di là giú ne martita\n",
            "               \n",
            " e se la fiamma mia si convedelene\n",
            "che s ' induta mia ti tornar co il bale\n",
            "che di sé di sé dalle tue vene\n",
            "             \n",
            " io son vede in giú nel suo comprende\n",
            "pur stato in ciò che si mostra corte\n",
            "pur come l ' animo re avvolta in te\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}