{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R8FnhxiWY_Y1",
        "outputId": "2fc45351-0c43-4d32-8ad9-dfcb2e5fa311",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import time\n",
        "import re\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import requests\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Reshape, BatchNormalization, Dense, Dropout,       # General\n",
        "    Embedding, LSTM, Dense, GRU,                              # RNN\n",
        "    Conv2D, Conv2DTranspose, LeakyReLU, MaxPool2D, Flatten    # CNN\n",
        ")\n",
        "from tensorflow.keras.activations import elu, relu, softmax, sigmoid\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fe1V83bpI8Tf",
        "colab_type": "text"
      },
      "source": [
        "## **Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V5YiWU0daYkT",
        "outputId": "7f7be609-53b0-4932-a124-938fc7074dc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Read the Divina Commedia\n",
        "url = \"https://raw.githubusercontent.com/DanieleVeri/deep_comedy/feature/GANs/divina_commedia.txt\"\n",
        "response = requests.get(url)\n",
        "\n",
        "divina_commedia = response.text\n",
        "\n",
        "# Replace rare characters\n",
        "divina_commedia = divina_commedia.replace(\"ä\", \"a\")\n",
        "divina_commedia = divina_commedia.replace(\"é\", \"è\")\n",
        "divina_commedia = divina_commedia.replace(\"ë\", \"è\")\n",
        "divina_commedia = divina_commedia.replace(\"Ë\", \"E\")\n",
        "divina_commedia = divina_commedia.replace(\"ï\", \"i\")\n",
        "divina_commedia = divina_commedia.replace(\"Ï\", \"I\")\n",
        "divina_commedia = divina_commedia.replace(\"ó\", \"ò\")\n",
        "divina_commedia = divina_commedia.replace(\"ö\", \"o\")\n",
        "divina_commedia = divina_commedia.replace(\"ü\", \"u\")\n",
        "divina_commedia = divina_commedia.replace(\"(\", \"-\")\n",
        "divina_commedia = divina_commedia.replace(\")\", \"-\")\n",
        "divina_commedia = divina_commedia.replace(\"[\", \"\")\n",
        "divina_commedia = divina_commedia.replace(\"]\", \"\")\n",
        "divina_commedia = re.sub(r'[0-9]+', '', divina_commedia)\n",
        "divina_commedia = divina_commedia.replace(\" \\n\", \"\\n\")\n",
        "\n",
        "unique_chars = list(set(divina_commedia))\n",
        "unique_chars.sort()  # to make sure you get the same encoding at each run\n",
        "char2idx = { char[1]: char[0] for char in enumerate(unique_chars) }\n",
        "\n",
        "def numerical_encoding(text, char_dict):\n",
        "    \"\"\" Text to list of chars, to np.array of numerical idx \"\"\"\n",
        "    chars_list = [ char for char in text ]\n",
        "    chars_list = [ char_dict[char] for char in chars_list ]\n",
        "    chars_list = np.array(chars_list)\n",
        "    return chars_list\n",
        "\n",
        "encoded_text = numerical_encoding(divina_commedia, char2idx)\n",
        "\n",
        "def get_text_matrix(sequence, len_input):\n",
        "    X = np.empty((len(sequence)-len_input, len_input))\n",
        "    for i in range(X.shape[0]):\n",
        "        X[i,:] = sequence[i : i+len_input]\n",
        "    return X\n",
        "\n",
        "text_matrix = get_text_matrix(encoded_text, 100)\n",
        "\n",
        "print(text_matrix.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(533803, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8w9Pm22BI3yD",
        "colab_type": "text"
      },
      "source": [
        "## **LSTM training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mKysyBfpAtUS",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "969ddb6d-9446-4829-fc55-dc60417e482f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# size of vocabulary\n",
        "vocab_size = len(char2idx)\n",
        "\n",
        "# size of mini batches during training\n",
        "batch_size = 100\n",
        "# size of training subset at each epoch\n",
        "subset_size = batch_size * 100\n",
        "# vector size of char embeddings\n",
        "embedding_size = 250\n",
        "len_input = 1000   # 200\n",
        "hidden_size = 250  # for Dense() layers\n",
        "\n",
        "n_epochs = 20\n",
        "learning_rate = 1e-3\n",
        "\n",
        "char_input = Input(shape=(batch_shape,))\n",
        "RNN = Sequential([\n",
        "    Embedding(vocab_size, embedding_size,\n",
        "              batch_input_shape=(batch_size, None)),\n",
        "    \n",
        "    LSTM(len_input, return_sequences = True),\n",
        "    \n",
        "    Dense(hidden_size, activation = relu), \n",
        "    \n",
        "    Dense(vocab_size)\n",
        "])\n",
        "RNN.summary()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
        "\n",
        "# This is an Autograph function\n",
        "# its decorator makes it a TF op - i.e. much faster\n",
        "@tf.function\n",
        "def train_on_batch(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        current_loss = tf.reduce_mean(\n",
        "            tf.keras.losses.sparse_categorical_crossentropy(\n",
        "                y, RNN(x), from_logits = True))\n",
        "    gradients = tape.gradient(current_loss, RNN.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, RNN.trainable_variables))\n",
        "    return current_loss\n",
        "\n",
        "loss_history = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    start = time.time()\n",
        "    print(epoch)\n",
        "    \n",
        "    # Take subsets of train and target\n",
        "    sample = np.random.randint(0, text_matrix.shape[0]-1, subset_size)\n",
        "    sample_train = text_matrix[ sample , : ]\n",
        "    sample_target = text_matrix[ sample+1 , : ]\n",
        "    \n",
        "    for iteration in range(sample_train.shape[0] // batch_size):\n",
        "        take = iteration * batch_size\n",
        "        x = sample_train[ take:take+batch_size , : ]\n",
        "        y = sample_target[ take:take+batch_size , : ]\n",
        "\n",
        "        current_loss = train_on_batch(x, y)\n",
        "        loss_history.append(current_loss)\n",
        "    \n",
        "    print(\"{}.  \\t  Loss: {}  \\t  Time: {}ss\".format(\n",
        "        epoch+1, current_loss.numpy(), round(time.time()-start, 2)))\n",
        "    \n",
        "    \n",
        "plt.plot(loss_history)\n",
        "plt.title(\"Training Loss\")\n",
        "plt.show()\n",
        "\n",
        "RNN.save(\"/text_generator_RNN_00.h5\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (100, None, 250)          15500     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (100, None, 1000)         5004000   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (100, None, 250)          250250    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (100, None, 62)           15562     \n",
            "=================================================================\n",
            "Total params: 5,285,312\n",
            "Trainable params: 5,285,312\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "0\n",
            "1.  \t  Loss: 2.42193603515625  \t  Time: 34.63ss\n",
            "1\n",
            "2.  \t  Loss: 2.0619657039642334  \t  Time: 27.24ss\n",
            "2\n",
            "3.  \t  Loss: 1.9014360904693604  \t  Time: 27.18ss\n",
            "3\n",
            "4.  \t  Loss: 1.7729706764221191  \t  Time: 27.16ss\n",
            "4\n",
            "5.  \t  Loss: 1.674802303314209  \t  Time: 27.2ss\n",
            "5\n",
            "6.  \t  Loss: 1.6125677824020386  \t  Time: 27.15ss\n",
            "6\n",
            "7.  \t  Loss: 1.4991718530654907  \t  Time: 27.01ss\n",
            "7\n",
            "8.  \t  Loss: 1.443527102470398  \t  Time: 27.06ss\n",
            "8\n",
            "9.  \t  Loss: 1.4027050733566284  \t  Time: 27.12ss\n",
            "9\n",
            "10.  \t  Loss: 1.3891750574111938  \t  Time: 27.26ss\n",
            "10\n",
            "11.  \t  Loss: 1.3499958515167236  \t  Time: 27.18ss\n",
            "11\n",
            "12.  \t  Loss: 1.3023204803466797  \t  Time: 27.19ss\n",
            "12\n",
            "13.  \t  Loss: 1.2396554946899414  \t  Time: 27.18ss\n",
            "13\n",
            "14.  \t  Loss: 1.2052170038223267  \t  Time: 27.09ss\n",
            "14\n",
            "15.  \t  Loss: 1.1198781728744507  \t  Time: 27.06ss\n",
            "15\n",
            "16.  \t  Loss: 1.035722017288208  \t  Time: 27.06ss\n",
            "16\n",
            "17.  \t  Loss: 0.9877236485481262  \t  Time: 27.2ss\n",
            "17\n",
            "18.  \t  Loss: 0.9628227353096008  \t  Time: 27.13ss\n",
            "18\n",
            "19.  \t  Loss: 0.8495105504989624  \t  Time: 27.21ss\n",
            "19\n",
            "20.  \t  Loss: 0.8101420998573303  \t  Time: 27.18ss\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gawSIEf_Ix31",
        "colab_type": "text"
      },
      "source": [
        "## **LSTM generation:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vE2hYSqAAtkn",
        "jupyter": {
          "source_hidden": true
        },
        "outputId": "3152a026-c197-4c4c-8887-fd68bcb68bde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 862
        }
      },
      "source": [
        "generator = Sequential([\n",
        "    Embedding(vocab_size, embedding_size,\n",
        "              batch_input_shape=(1, None)),\n",
        "    \n",
        "    LSTM(len_input, return_sequences = True, stateful=True),\n",
        "    \n",
        "    Dense(hidden_size, activation = relu), \n",
        "    \n",
        "    Dense(vocab_size)\n",
        "])\n",
        "\n",
        "generator.summary()\n",
        "\n",
        "# Import trained weights from RNN to generator\n",
        "generator.set_weights(RNN.get_weights())\n",
        "\n",
        "def generate_text(start_string, num_generate = 1000, temperature = 1.0):\n",
        "    \n",
        "    # Vectorize input string\n",
        "    input_eval = [char2idx[s] for s in start_string]  \n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "    \n",
        "    text_generated = [] # List to append predicted chars \n",
        "    \n",
        "    idx2char = { v: k for k, v in char2idx.items() }  # invert char-index mapping\n",
        "    \n",
        "    generator.reset_states()\n",
        "    \n",
        "    for i in range(num_generate):\n",
        "        predictions = generator(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        \n",
        "        # sample next char based on distribution and temperature\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "        \n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "        \n",
        "    return (start_string + ''.join(text_generated))\n",
        "\n",
        "init=\"\"\"\n",
        "Nel mezzo del cammin di nostra vita\n",
        "mi ritrovai per una selva oscura,\n",
        "chè la diritta via era smarrita.\n",
        "\"\"\"\n",
        "print(generate_text(init))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (1, None, 250)            15500     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (1, None, 1000)           5004000   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (1, None, 250)            250250    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (1, None, 62)             15562     \n",
            "=================================================================\n",
            "Total params: 5,285,312\n",
            "Trainable params: 5,285,312\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\n",
            "Nel mezzo del cammin di nostra vita\n",
            "mi ritrovai per una selva oscura,\n",
            "chè la diritta via era smarrita.\n",
            "  La nostra d'una padre e sovra 'l sole Alette;\n",
            "per ch'io il dovo tutto di gran divizia,\n",
            "sì che la speranza fherda s'iccende;\n",
            "  che tu mi piachi va con la via inde:\n",
            "pria che la giustizia non ho i cade!\n",
            "di sè mostra uscì come assai vivo,\n",
            "maraviglianza in fuoco son due pente!\".\n",
            "  Oh sangue bene apertar la suffama\n",
            "versa sua matura, e fu' comincia:\n",
            "qual nebal giustizia non volta in golle\n",
            "che tante hanno sì da la parte serena,\n",
            "da l'arfor lo sceglio, ov'io mi pareva,\n",
            "anzi da l'ore che seno, in su lo vendette?\".\n",
            "  \"La mia contramma è chi la mente hanno crea\n",
            "per voler di passar ne' dessi Mucchio.\n",
            "  Già per vi chi siete qui l'un di biono\n",
            "d'i suoi siedi parlando, e disse: \"I' senti';\n",
            "ch'i' mi sembiava, per la prove scela,\n",
            "che mal nave sol del val di fia luiga\n",
            "che là sù grave roma perchè 'l regghiai.\n",
            "  E prima che son consorte andanno\".\n",
            "  Con tutto l'orlo de la turba era,\n",
            "gittato chi io da un fui di quel guido\n",
            "c'hanno del mondo, e riguardolla in Gallo!\n",
            "Sienze selva il tempo e chi 'l son perugio,\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}