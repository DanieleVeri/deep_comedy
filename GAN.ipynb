{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import requests\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Reshape, BatchNormalization, Dense, Dropout,       # General\n",
    "    Embedding, LSTM, Dense, GRU,                              # RNN\n",
    "    Conv2D, Conv2DTranspose, LeakyReLU, MaxPool2D, Flatten    # CNN\n",
    ")\n",
    "from tensorflow.keras.activations import elu, relu, softmax, sigmoid\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Divina Commedia\n",
    "url = \"https://raw.githubusercontent.com/DanieleVeri/deep_comedy/feature/GANs/divina_commedia.txt\"\n",
    "response = requests.get(url)\n",
    "\n",
    "divina_commedia = response.text\n",
    "\n",
    "# Replace rare characters\n",
    "divina_commedia = divina_commedia.replace(\"ä\", \"a\")\n",
    "divina_commedia = divina_commedia.replace(\"é\", \"è\")\n",
    "divina_commedia = divina_commedia.replace(\"ë\", \"è\")\n",
    "divina_commedia = divina_commedia.replace(\"Ë\", \"E\")\n",
    "divina_commedia = divina_commedia.replace(\"ï\", \"i\")\n",
    "divina_commedia = divina_commedia.replace(\"Ï\", \"I\")\n",
    "divina_commedia = divina_commedia.replace(\"ó\", \"ò\")\n",
    "divina_commedia = divina_commedia.replace(\"ö\", \"o\")\n",
    "divina_commedia = divina_commedia.replace(\"ü\", \"u\")\n",
    "divina_commedia = divina_commedia.replace(\"(\", \"-\")\n",
    "divina_commedia = divina_commedia.replace(\")\", \"-\")\n",
    "divina_commedia = divina_commedia.replace(\"[\", \"\")\n",
    "divina_commedia = divina_commedia.replace(\"]\", \"\")\n",
    "divina_commedia = re.sub(r'[0-9]+', '', divina_commedia)\n",
    "divina_commedia = divina_commedia.replace(\" \\n\", \"\\n\")\n",
    "\n",
    "unique_chars = list(set(divina_commedia))\n",
    "unique_chars.sort()  # to make sure you get the same encoding at each run\n",
    "char2idx = { char[1]: char[0] for char in enumerate(unique_chars) }\n",
    "\n",
    "def numerical_encoding(text, char_dict):\n",
    "    \"\"\" Text to list of chars, to np.array of numerical idx \"\"\"\n",
    "    chars_list = [ char for char in text ]\n",
    "    chars_list = [ char_dict[char] for char in chars_list ]\n",
    "    chars_list = np.array(chars_list)\n",
    "    return chars_list\n",
    "\n",
    "encoded_text = numerical_encoding(divina_commedia, char2idx)\n",
    "\n",
    "# 2D formatted text\n",
    "canti = divina_commedia.split(\"\\n\\n\")\n",
    "\n",
    "num_lines = 0\n",
    "max_len = 0\n",
    "for c in canti:\n",
    "  lines = c.split('\\n')\n",
    "  num_lines += len(lines)\n",
    "  for l in lines:\n",
    "    if len(l) > max_len:\n",
    "      max_len = len(l)\n",
    "\n",
    "formatted = np.zeros((num_lines, max_len+2), dtype=int)\n",
    "\n",
    "rhymes = []\n",
    "num_lines = 0\n",
    "for i in canti:\n",
    "  lines = i.split('\\n')\n",
    "  for j in lines:\n",
    "    rhymes.append(j.split(' ')[-1])\n",
    "    encoded = numerical_encoding(j+'\\n', char2idx)\n",
    "    left_padding = np.ones(max_len-len(encoded)+2)\n",
    "    formatted[num_lines, :] = np.concatenate((left_padding, encoded))\n",
    "    num_lines += 1\n",
    "\n",
    "print(text_matrix.shape)\n",
    "print(formatted.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of vocabulary\n",
    "vocab_size = len(char2idx)\n",
    "normalized = (formatted - vocab_size/2)/(vocab_size / 2)\n",
    "\n",
    "discriminator = Sequential()\n",
    "discriminator.add(Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[52, 52, 1]))\n",
    "discriminator.add(LeakyReLU())\n",
    "discriminator.add(MaxPool2D((2,2)))\n",
    "discriminator.add(Dropout(0.3))\n",
    "discriminator.add(Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "discriminator.add(LeakyReLU())\n",
    "discriminator.add(Dropout(0.3))\n",
    "discriminator.add(Flatten())\n",
    "discriminator.add(Dense(1))\n",
    "\n",
    "discriminator.summary()\n",
    "\n",
    "generator = Sequential()\n",
    "generator.add(Dense(13*13*256, use_bias=False, input_shape=(100,)))\n",
    "generator.add(BatchNormalization())\n",
    "generator.add(LeakyReLU())\n",
    "generator.add(Reshape((13, 13, 256)))\n",
    "assert generator.output_shape == (None, 13, 13, 256) # Note: None is the batch size\n",
    "generator.add(Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
    "assert generator.output_shape == (None, 13, 13, 128)\n",
    "generator.add(BatchNormalization())\n",
    "generator.add(LeakyReLU())\n",
    "generator.add(Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "assert generator.output_shape == (None, 26, 26, 64)\n",
    "generator.add(BatchNormalization())\n",
    "generator.add(LeakyReLU())\n",
    "generator.add(Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "assert generator.output_shape == (None, 52, 52, 1)\n",
    "\n",
    "generator.summary()\n",
    "\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-3)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-3)\n",
    "\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 1000\n",
    "\n",
    "# We will reuse this seed overtime (so it's easier)\n",
    "# to visualize progress in the animated GIF)\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "      generated_images = generator(noise, training=True)\n",
    "      real_output = discriminator(images, training=True)\n",
    "      fake_output = discriminator(generated_images, training=True)\n",
    "      gen_loss = generator_loss(fake_output)\n",
    "      disc_loss = discriminator_loss(real_output, fake_output)\n",
    "    \n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "def train(dataset, epochs):\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    for image_batch in dataset:\n",
    "      train_step(image_batch)\n",
    "\n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "      \n",
    "splitted = np.split(normalized[0:normalized.shape[0]-normalized.shape[0]%52], normalized.shape[0] // 52)\n",
    "splitted = np.array(splitted).reshape(len(splitted), 52, 52, 1)\n",
    "print(splitted.shape)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(splitted).shuffle(60000).batch(BATCH_SIZE)\n",
    "train(train_dataset, EPOCHS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2char = { v: k for k, v in char2idx.items() } \n",
    "\n",
    "def generate_and_save_images(model, epoch, test_input):\n",
    "  # Notice `training` is set to False.\n",
    "  # This is so all layers run in inference mode (batchnorm).\n",
    "  predictions = model(test_input, training=False)\n",
    "\n",
    "  fig = plt.figure(figsize=(20,20))\n",
    "\n",
    "  for i in range(predictions.shape[0]):\n",
    "      plt.subplot(4, 4, i+1)\n",
    "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "      plt.axis('off')\n",
    "      \n",
    "      res = []\n",
    "      for a in range(52):\n",
    "        line = []\n",
    "        for b in range(52):\n",
    "          discretized = math.floor(predictions[i, a, b, 0] * vocab_size/2 + vocab_size/2)\n",
    "          line.append(idx2char[discretized])\n",
    "        res.append(''.join(line))\n",
    "      print(\"=============\")\n",
    "      print(res)\n",
    "      print(\"=============\\n\\n\")\n",
    "      \n",
    "  plt.show()\n",
    "\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])\n",
    "generate_and_save_images(generator, epochs, seed)\n",
    "\n",
    "plt.imshow(normalized[0:53,:], cmap='gray')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}