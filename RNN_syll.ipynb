{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "provenance": [],
      "authorship_tag": "ABX9TyNfNeul8teHlDGfXkvjFIbh"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0qfnRB53CwQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "54709421-cb6c-4687-a9f7-d2863c302ab9"
      },
      "source": [
        "import time\n",
        "import re\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import requests\n",
        "import collections\n",
        "import pickle\n",
        "import copy, random\n",
        "np.random.seed(1234)\n",
        "import nltk as nl\n",
        "nl.download('punkt')\n",
        "\n",
        "from itertools import zip_longest\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Reshape, BatchNormalization, Dense, Dropout, concatenate,       # General\n",
        "    Embedding, LSTM, Dense, GRU, Bidirectional,                              # RNN\n",
        "    MaxPooling1D\n",
        ")\n",
        "from tensorflow.keras.activations import elu, relu, softmax, sigmoid\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTpaQwHZsp2P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "024e0282-bb58-448e-da21-1e779a56820b"
      },
      "source": [
        "def get_hyp_lm_tercets(tercets):\n",
        "    new_tercets = []\n",
        "    for tercet in tercets:\n",
        "        new_tercets.append([])\n",
        "        for verse in tercet:\n",
        "            new_tercets[-1].append([])\n",
        "            for hyp_w in verse:\n",
        "                new_tercets[-1][-1].extend(hyp_w)\n",
        "                new_tercets[-1][-1].append('<SEP>')\n",
        "            new_tercets[-1][-1] = new_tercets[-1][-1][:-1]\n",
        "\n",
        "    return new_tercets\n",
        "\n",
        "def is_vowel(c):\n",
        "    return c in 'aeiouAEIOUàìíèéùúüòï'\n",
        "\n",
        "def unsplittable_cons():\n",
        "    u_cons = []\n",
        "    for c1 in ('b', 'c', 'd', 'f', 'g', 'p', 't', 'v'):\n",
        "        for c2 in ('l', 'r'):\n",
        "            u_cons.append(c1 + c2)\n",
        "\n",
        "    others = ['gn', 'gh', 'ch']\n",
        "    u_cons.extend(others)\n",
        "    return u_cons\n",
        "\n",
        "\n",
        "def are_cons_to_split(c1, c2):\n",
        "    to_split = ('cq', 'cn', 'lm', 'rc', 'bd', 'mb', 'mn', 'ld', 'ng', 'nd', 'tm', 'nv', 'nc', 'ft', 'nf', 'gm', 'fm', 'rv', 'fp')\n",
        "    return (c1 + c2) in to_split or (not is_vowel(c1) and (c1 == c2)) or ((c1 + c2) not in unsplittable_cons()) and (\n",
        "        (not is_vowel(c1)) and (not is_vowel(c2)) and c1 != 's')\n",
        "\n",
        "\n",
        "def is_diphthong(c1, c2):\n",
        "    return (c1 + c2) in ('ia', 'ie', 'io', 'iu', 'ua', 'ue', 'uo', 'ui', 'ai', 'ei', 'oi', 'ui', 'au', 'eu', 'ïe', 'iú', 'iù')\n",
        "\n",
        "\n",
        "def is_triphthong(c1, c2, c3):\n",
        "    return (c1 + c2 + c3) in ('iai', 'iei', 'uoi', 'uai', 'uei', 'iuo')\n",
        "\n",
        "\n",
        "def is_toned_vowel(c):\n",
        "    return c in 'àìèéùòï'\n",
        "\n",
        "def has_vowels(sy):\n",
        "    for c in sy:\n",
        "        if is_vowel(c):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def hyphenation(word):\n",
        "    \"\"\"\n",
        "    Split word in syllables\n",
        "    :param word: input string\n",
        "    :return: a list containing syllables of the word\n",
        "    \"\"\"\n",
        "    if not word or word == '':\n",
        "        return []\n",
        "    # elif len(word) == 3 and (is_vowel(word[1]) and is_vowel(word[2]) and not is_toned_vowel(word[2]) and (\n",
        "    #     not is_diphthong(word[1], word[2]) or (word[1] == 'i'))):\n",
        "    elif len(word) == 3 and (is_vowel(word[1]) and is_vowel(word[2]) and not is_toned_vowel(word[2]) and (\n",
        "        not is_diphthong(word[1], word[2]))):\n",
        "        return [word[:2]] + [word[2]]\n",
        "    elif len(word) == 3 and is_vowel(word[0]) and not is_vowel(word[1]) and is_vowel(word[2]):\n",
        "        return [word[:2]] + [word[2]]\n",
        "    elif len(word) == 3:\n",
        "        return [word]\n",
        "\n",
        "    syllables = []\n",
        "    is_done = False\n",
        "    count = 0\n",
        "    while not is_done and count <= len(word) - 1:\n",
        "        syllables.append('')\n",
        "        c = word[count]\n",
        "        while not is_vowel(c) and count < len(word) - 1:\n",
        "            syllables[-1] = syllables[-1] + c\n",
        "            count += 1\n",
        "            c = word[count]\n",
        "\n",
        "        syllables[-1] = syllables[-1] + word[count]\n",
        "\n",
        "        if count == len(word) - 1:\n",
        "            is_done = True\n",
        "        else:\n",
        "            count += 1\n",
        "\n",
        "            if count < len(word) and not is_vowel(word[count]):\n",
        "                if count == len(word) - 1:\n",
        "                    syllables[-1] += word[count]\n",
        "                    count += 1\n",
        "                elif count + 1 < len(word) and are_cons_to_split(word[count], word[count + 1]):\n",
        "                    syllables[-1] += word[count]\n",
        "                    count += 1\n",
        "                elif count + 2 < len(word) and not is_vowel(word[count + 1]) and not is_vowel(word[count + 2]) and word[\n",
        "                    count] != 's':\n",
        "                    syllables[-1] += word[count]\n",
        "                    count += 1\n",
        "            elif count < len(word):\n",
        "                if count + 1 < len(word) and is_triphthong(word[count - 1], word[count], word[count + 1]):\n",
        "                    syllables[-1] += word[count] + word[count + 1]\n",
        "                    count += 2\n",
        "                elif is_diphthong(word[count - 1], word[count]):\n",
        "                    syllables[-1] += word[count]\n",
        "                    count += 1\n",
        "\n",
        "                if count + 1 < len(word) and are_cons_to_split(word[count], word[count + 1]):\n",
        "                    syllables[-1] += word[count]\n",
        "                    count += 1\n",
        "\n",
        "            else:\n",
        "                is_done = True\n",
        "\n",
        "    if not has_vowels(syllables[-1]) and len(syllables) > 1:\n",
        "        syllables[-2] = syllables[-2] + syllables[-1]\n",
        "        syllables = syllables[:-1]\n",
        "\n",
        "    return syllables\n",
        "\n",
        "\n",
        "\n",
        "def get_dc_hyphenation(canti):\n",
        "    hyp_canti, hyp_tokens = [], []\n",
        "    for canto in canti:\n",
        "        hyp_canti.append([])\n",
        "        for verso in canto:\n",
        "            syllables = seq_hyphentation(verso)\n",
        "            hyp_canti[-1].append(syllables)\n",
        "            for syllable in syllables:\n",
        "                hyp_tokens.extend(syllable)\n",
        "\n",
        "    return hyp_canti, hyp_tokens\n",
        "\n",
        "\n",
        "def seq_hyphentation(words):\n",
        "    \"\"\"\n",
        "    Converts words in a list of strings into lists of syllables\n",
        "    :param words: a list of words (strings)\n",
        "    :return: a list of lists containing word syllables\n",
        "    \"\"\"\n",
        "    return [hyphenation(w) for w in words]\n",
        "\n",
        "\n",
        "def get_dc_cantos(filename, encoding=None):\n",
        "    # raw_data = read_words(filename=filename)\n",
        "    cantos, words, raw = [], [], []\n",
        "    with open(filename, \"r\", encoding=encoding) as f:\n",
        "        for line in f:\n",
        "            sentence = line.strip()\n",
        "            sentence = str.replace(sentence, \"\\.\", \" \\. \")\n",
        "            sentence = str.replace(sentence, \"[\", '')\n",
        "            sentence = str.replace(sentence, \"]\", '')\n",
        "            sentence = str.replace(sentence, \"-\", '')\n",
        "            sentence = str.replace(sentence, \";\", \" ; \")\n",
        "            sentence = str.replace(sentence, \",\", \" , \")\n",
        "            # sentence = str.replace(sentence, \" \\'\", '')\n",
        "            sentence = str.replace(sentence, \"\\'\", ' \\' ')\n",
        "            if len(sentence) > 1:\n",
        "                # sentence = sentence.translate(string.punctuation)\n",
        "                tokenized_sentence = nl.word_tokenize(sentence)\n",
        "                # tokenized_sentence = sentence.split()\n",
        "                tokenized_sentence = [w.lower() for w in tokenized_sentence if len(w) > 0]\n",
        "                tokenized_sentence = [w for w in tokenized_sentence if \",\" not in w]\n",
        "                tokenized_sentence = [w for w in tokenized_sentence if \".\" not in w]\n",
        "                tokenized_sentence = [w for w in tokenized_sentence if \":\" not in w]\n",
        "                tokenized_sentence = [w for w in tokenized_sentence if \";\" not in w]\n",
        "                tokenized_sentence = [w for w in tokenized_sentence if \"«\" not in w]\n",
        "                tokenized_sentence = [w for w in tokenized_sentence if \"»\" not in w]\n",
        "                # ts = []\n",
        "                ts = tokenized_sentence\n",
        "                # [ts.extend(re.split(\"(\\')\", e)) for e in tokenized_sentence]\n",
        "                tokenized_sentence = [w for w in ts if len(w) > 0]\n",
        "\n",
        "                if len(tokenized_sentence) == 2:\n",
        "                    cantos.append([])\n",
        "                    raw.append([])\n",
        "                elif len(tokenized_sentence) > 2:\n",
        "                    raw[-1].append(sentence)\n",
        "                    cantos[-1].append(tokenized_sentence)\n",
        "                    words.extend(tokenized_sentence)\n",
        "\n",
        "    return cantos, words, raw\n",
        "\n",
        "\n",
        "def create_tercets(cantos):\n",
        "    tercets = []\n",
        "    for i,canto in enumerate(cantos):\n",
        "        for v,verse in enumerate(canto):\n",
        "            if v%3 == 0:\n",
        "                tercets.append([])\n",
        "\n",
        "            tercets[-1].append(verse)\n",
        "        tercets = tercets[:-1]  # removes the last malformed tercets (only 2 verses)\n",
        "\n",
        "    return tercets\n",
        "\n",
        "def pad_list(l, pad_token, max_l_size, keep_lasts=False, pad_right=True):\n",
        "    \"\"\"\n",
        "    Adds a padding token to a list\n",
        "    inputs:\n",
        "    :param l: input list to pad.\n",
        "    :param pad_token: value to add as padding.\n",
        "    :param max_l_size: length of the new padded list to return,\n",
        "    it truncates lists longer that 'max_l_size' without adding\n",
        "    padding values.\n",
        "    :param keep_lasts: If True, preserves the max_l_size last elements\n",
        "    of a sequence (by keeping the same order).  E.g.:\n",
        "    if keep_lasts is True and max_l_size=3 [1,2,3,4] becomes [2,3,4].\n",
        "\n",
        "\n",
        "    :return: the list padded or truncated.\n",
        "    \"\"\"\n",
        "    to_pad = []\n",
        "    max_l = min(max_l_size, len(l))  # maximum len\n",
        "    l_init = len(l) - max_l if len(l) > max_l and keep_lasts else 0  # initial position where to sample from the list\n",
        "    l_end = len(l) if len(l) > max_l and keep_lasts else max_l\n",
        "    for i in range(l_init, l_end):\n",
        "        to_pad.append(l[i])\n",
        "\n",
        "    # for j in range(len(l), max_l_size):\n",
        "    #     to_pad.append(pad_token)\n",
        "    pad_tokens = [pad_token] * (max_l_size-len(l))\n",
        "    padded_l = to_pad + pad_tokens if pad_right else pad_tokens + to_pad\n",
        "\n",
        "    return padded_l\n",
        "\n",
        "\n",
        "def save_data(data, file):\n",
        "    with open(file, 'wb') as output:\n",
        "        pickle.dump(data, output, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def load_data(file):\n",
        "    with open(file, 'rb') as obj:\n",
        "        return pickle.load(obj)\n",
        "\n",
        "def print_and_write(file, s):\n",
        "    print(s)\n",
        "    file.write(s)\n",
        "\n",
        "\n",
        "class Vocabulary(object):\n",
        "    def __init__(self, vocab_size=None):\n",
        "        self.dictionary = dict()\n",
        "        self.rev_dictionary = dict()\n",
        "        self.count = []\n",
        "        self.special_tokens = []\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    def build_vocabulary_from_counts(self, count, special_tokens=[]):\n",
        "        \"\"\"\n",
        "        Sets all the attributes of the Vocabulary object.\n",
        "        :param count: a list of lists as follows: [['token', number_of_occurrences],...]\n",
        "        :param special_tokens: a list of strings. E.g. ['<EOS>', '<PAD>',...]\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "\n",
        "        dictionary = dict()\n",
        "        for word, _ in count:\n",
        "            dictionary[word] = len(dictionary)\n",
        "\n",
        "        # adding eventual special tokens to the dictionary (e.g. <EOS>,<PAD> etc..)\n",
        "        d = len(dictionary)\n",
        "        for i, token in enumerate(special_tokens):\n",
        "            dictionary[token] = d + i\n",
        "\n",
        "        self.count = count\n",
        "        self.dictionary = dictionary\n",
        "        self.rev_dictionary = dict(zip(self.dictionary.values(), self.dictionary.keys()))\n",
        "        self.special_tokens = special_tokens\n",
        "        self.vocab_size = len(dictionary)\n",
        "\n",
        "    def build_vocabulary_from_tokens(self, tokens, vocabulary_size=None, special_tokens=[]):\n",
        "        \"\"\"\n",
        "        Given a list of tokens, it sets the Vocabulary object attributes by constructing\n",
        "        a dictionary mapping each token to a unique id.\n",
        "        :param tokens: a list of strings.\n",
        "         E.g. [\"the\", \"cat\", \"is\", ... \".\", \"the\", \"house\" ,\"is\" ...].\n",
        "         NB: Here you should put all your token instances of the corpus.\n",
        "        :param vocabulary_size: The number of elements of your vocabulary. If there are more\n",
        "        than 'vocabulary_size' elements on tokens, it considers only the 'vocabulary_size'\n",
        "        most frequent ones.\n",
        "        :param special_tokens: Optional. A list of strings. Useful to add special tokens in vocabulary.\n",
        "        If you don't have any, keep it empty.\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "\n",
        "        vocabulary_size = vocabulary_size if vocabulary_size is not None else self.vocab_size\n",
        "        vocabulary_size = vocabulary_size - (len(special_tokens) + 1) if vocabulary_size else None\n",
        "        # counts occurrences of each token\n",
        "        count = [['<UNK>', -1]]\n",
        "        count.extend(collections.Counter(tokens).most_common(vocabulary_size))  # takes only the most frequent ones, if size is None takes them all\n",
        "        self.build_vocabulary_from_counts(count, special_tokens)  # actually build the vocabulary\n",
        "        self._set_unk_count(tokens)  # set the number of OOV instances\n",
        "\n",
        "    @staticmethod\n",
        "    def merge_vocabulary(vocab0, vocab1, vocabulary_size=-1):\n",
        "        \"\"\"\n",
        "        Merge two Vocabulary objects into a new one.\n",
        "        :param vocab0: first Vocabulary object\n",
        "        :param vocab1: second Vocabulary object\n",
        "        :param vocabulary_size: parameter to decide the merged vocabulary size.\n",
        "        With default value -1, all the words of both vocabularies are preserved.\n",
        "        When set to 0, the size of the vocabulary is set to the size of vocab0,\n",
        "        when set to 1 it is kept the size of vocab1.\n",
        "        :return: a new vocabulary\n",
        "        \"\"\"\n",
        "        # get size of the new vocabulary\n",
        "        vocab_size = vocab0.vocab_size + vocab1.vocab_size if vocabulary_size == -1 else vocabulary_size\n",
        "        merged_special_tokens = list(set(vocab0.special_tokens) | set(vocab1.special_tokens))\n",
        "\n",
        "        # merge the counts from the two vocabularies and then selects the most_common tokens\n",
        "        merged_counts = collections.Counter(dict(vocab0.count)) + collections.Counter(dict(vocab1.count))\n",
        "        merged_counts = merged_counts.most_common(vocab_size)\n",
        "        count = [['<UNK>', -1]]\n",
        "        count.extend(merged_counts)\n",
        "\n",
        "        # create the new vocabulary\n",
        "        merged_vocab = Vocabulary(vocab_size)\n",
        "        merged_vocab.build_vocabulary_from_counts(count, merged_special_tokens)\n",
        "        return merged_vocab\n",
        "\n",
        "    @staticmethod\n",
        "    def merge_vocabularies(vocab_list, vocab_size=None):\n",
        "        \"\"\"\n",
        "        Join a list of vocabularies into a new one.\n",
        "        :param vocab_list: a list of Vocabulary objects\n",
        "        :param vocab_size: the maximum size of the merged vocabulary.\n",
        "        :return: a vocabulary merging them all.\n",
        "        \"\"\"\n",
        "        vocab_size = vocab_size if vocab_size else sum([v.vocab_size for v in vocab_list])\n",
        "        merged_vocab = Vocabulary(vocab_size)\n",
        "        for voc in vocab_list:\n",
        "            merged_vocab = Vocabulary.merge_vocabulary(merged_vocab, voc, vocab_size)\n",
        "        return merged_vocab\n",
        "\n",
        "    def string2id(self, dataset):\n",
        "        \"\"\"\n",
        "        Converts a dataset of strings into a dataset of ids according to the object dictionary.\n",
        "        :param dataset: any string-based dataset with any nested lists.\n",
        "        :return: a new dataset, with the same shape of dataset, where each string is mapped into its\n",
        "        corresponding id associated in the dictionary (0 for unknown tokens).\n",
        "        \"\"\"\n",
        "\n",
        "        def _recursive_call(items):\n",
        "            new_items = []\n",
        "            for item in items:\n",
        "                if isinstance(item, str) or isinstance(item, int) or isinstance(item, float):\n",
        "                    new_items.append(self.word2id(item))\n",
        "                else:\n",
        "                    new_items.append(_recursive_call(item))\n",
        "            return new_items\n",
        "\n",
        "        return _recursive_call(dataset)\n",
        "\n",
        "    def id2string(self, dataset):\n",
        "        \"\"\"\n",
        "        Converts a dataset of integer ids into a dataset of string according to the reverse dictionary.\n",
        "        :param dataset: any int-based dataset with any nested lists. Allowed types are int, np.int32, np.int64.\n",
        "        :return: a new dataset, with the same shape of dataset, where each token is mapped into its\n",
        "        corresponding string associated in the reverse dictionary.\n",
        "        \"\"\"\n",
        "        def _recursive_call(items):\n",
        "            new_items = []\n",
        "            for item in items:\n",
        "                if isinstance(item, int) or isinstance(item, np.int) or isinstance(item, np.int32) or isinstance(item, np.int64):\n",
        "                    new_items.append(self.id2word(item))\n",
        "                else:\n",
        "                    new_items.append(_recursive_call(item))\n",
        "            return new_items\n",
        "\n",
        "        return _recursive_call(dataset)\n",
        "\n",
        "    def word2id(self, item):\n",
        "        \"\"\"\n",
        "        Maps a string token to its corresponding id.\n",
        "        :param item: a string.\n",
        "        :return: If the token belongs to the vocabulary, it returns an integer id > 0, otherwise\n",
        "        it returns the value associated to the unknown symbol, that is typically 0.\n",
        "        \"\"\"\n",
        "        return self.dictionary[item] if item in self.dictionary else self.dictionary['<UNK>']\n",
        "\n",
        "    def id2word(self, token_id):\n",
        "        \"\"\"\n",
        "        Maps an integer token to its corresponding string.\n",
        "        :param token_id: an integer.\n",
        "        :return: If the id belongs to the vocabulary, it returns the string\n",
        "        associated to it, otherwise it returns the string associated\n",
        "        to the unknown symbol, that is '<UNK>'.\n",
        "        \"\"\"\n",
        "\n",
        "        return self.rev_dictionary[token_id] if token_id in self.rev_dictionary else self.rev_dictionary[self.dictionary['<UNK>']]\n",
        "\n",
        "    def get_unk_count(self):\n",
        "        return self.count[0][1]\n",
        "\n",
        "    def _set_unk_count(self, tokens):\n",
        "        \"\"\"\n",
        "        Sets the number of OOV instances in the tokens provided\n",
        "        :param tokens: a list of tokens\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "        data = list()\n",
        "        unk_count = 0\n",
        "        for word in tokens:\n",
        "            if word in self.dictionary:\n",
        "                index = self.dictionary[word]\n",
        "            else:\n",
        "                index = 0  # dictionary['<UNK>']\n",
        "                unk_count += 1\n",
        "            data.append(index)\n",
        "        self.count[0][1] = unk_count\n",
        "\n",
        "    def add_element(self, name, is_special_token=False):\n",
        "        if name not in self.dictionary:\n",
        "            self.vocab_size += 1\n",
        "            self.dictionary[name] = self.vocab_size\n",
        "            self.rev_dictionary[self.vocab_size] = name\n",
        "\n",
        "            if is_special_token:\n",
        "                self.special_tokens = list(self.special_tokens)\n",
        "                self.special_tokens.append(name)\n",
        "\n",
        "            self.count.append([name, 1])\n",
        "\n",
        "    def set_vocabulary(self, dictionary, rev_dictionary, special_tokens, vocab_size):\n",
        "        self.dictionary = dictionary,\n",
        "        self.rev_dictionary = rev_dictionary\n",
        "        self.special_tokens = special_tokens\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    @staticmethod\n",
        "    def load_vocabulary(filename):\n",
        "        return load_data(filename)\n",
        "\n",
        "    def save_vocabulary(self, filename):\n",
        "        save_data(self, filename)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class SyLMDataset(object):\n",
        "    def __init__(self, config, sy_vocab=None):\n",
        "        self.config = config\n",
        "        self.vocabulary = sy_vocab\n",
        "\n",
        "        self.raw_train_x = []\n",
        "        self.raw_val_x = []\n",
        "        self.raw_test_x = []\n",
        "        self.raw_x = []\n",
        "\n",
        "        self.train_x, self.train_y = [], []\n",
        "        self.val_x, self.val_y = [], []\n",
        "        self.test_x, self.test_y = [], []\n",
        "        self.x, self.y = [], []\n",
        "\n",
        "    def initialize(self, sess):\n",
        "        pass\n",
        "\n",
        "    def load(self, sources):\n",
        "        \"\"\"\n",
        "        Extract raw texts form sources and gather them all together.\n",
        "        :param sources: a string or an iterable of strings containing the file(s)\n",
        "        to process in order to build the dataset.\n",
        "        :return: a list of raw strings.\n",
        "        \"\"\"\n",
        "        return NotImplementedError\n",
        "\n",
        "    def build(self, sources, split_size=0.8):\n",
        "        \"\"\"\n",
        "        :param sources: a string or an iterable of strings containing the file(s)\n",
        "        to process in order to build the dataset.\n",
        "        :param split_size: the size to split the dataset, set >=1.0 to not split.\n",
        "        \"\"\"\n",
        "\n",
        "        raw_x = self.load(sources)\n",
        "        # raw_x = self.tokenize([self.preprocess(ex) for ex in raw_x])  # fixme\n",
        "        # splitting data\n",
        "        self.raw_x = raw_x\n",
        "        if split_size < 1.0:\n",
        "            self.raw_train_x, self.raw_test_x = self.split(self.raw_x, train_size=split_size)\n",
        "            self.raw_train_x, self.raw_val_x = self.split(self.raw_train_x, train_size=split_size)\n",
        "        else:\n",
        "            self.raw_train_x = self.raw_x\n",
        "\n",
        "        if self.vocabulary is None:\n",
        "            # creates vocabulary\n",
        "            tokens = [item for sublist in self.raw_train_x for item in sublist]  # get tokens\n",
        "            special_tokens = (\"<GO>\", \"<PAD>\", \"<SEP>\", \"<EOS>\", \"<EOV>\")\n",
        "            self._create_vocab(tokens, special_tokens=special_tokens)\n",
        "\n",
        "        # creates x,y for train\n",
        "        self.train_x = self._build_dataset(self.raw_train_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "        self.train_y = self._build_dataset(self.raw_train_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "\n",
        "        # creates x,y for validation\n",
        "        self.val_x = self._build_dataset(self.raw_val_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "        self.val_y = self._build_dataset(self.raw_val_x, insert_go=False, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "\n",
        "        # creates x,y for validation\n",
        "        self.test_x = self._build_dataset(self.raw_test_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "        self.test_y = self._build_dataset(self.raw_test_x, insert_go=False, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "\n",
        "    def _create_vocab(self, tokens, special_tokens=(\"<PAD>\", \"<GO>\", \"<SEP>\", \"<EOV>\", \"<EOS>\")):\n",
        "        \"\"\"\n",
        "        Create the vocabulary. Special tokens can be added to the tokens obtained from\n",
        "        the corpus.\n",
        "        :param tokens: a list of all the tokens in the corpus. Each token is a string.\n",
        "        :param special_tokens: a list of strings.\n",
        "        \"\"\"\n",
        "        vocab = Vocabulary(vocab_size=self.config.input_vocab_size)\n",
        "        vocab.build_vocabulary_from_tokens(tokens, special_tokens=special_tokens)\n",
        "        self.vocabulary = vocab\n",
        "\n",
        "    @staticmethod\n",
        "    def split(raw_data, train_size=0.8):\n",
        "        size = math.floor(len(raw_data)*train_size)\n",
        "        return raw_data[:size], raw_data[size:]\n",
        "\n",
        "    @staticmethod\n",
        "    def preprocess(txt):\n",
        "        return txt\n",
        "\n",
        "    @staticmethod\n",
        "    def shuffle(x):\n",
        "        return random.sample(x, len(x))\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenize(txt):\n",
        "        return txt\n",
        "\n",
        "    def _build_dataset(self, raw_data, max_len=100, insert_go=True, keep_lasts=False, pad_right=True, shuffle=True):\n",
        "        \"\"\"\n",
        "        Converts all the tokens in e1_raw_data by mapping each token with its corresponding\n",
        "        value in the dictionary. In case of token not in the dictionary, they are assigned to\n",
        "        a specific id. Each sequence is padded up to the seq_max_len setup in the config.\n",
        "\n",
        "        :param raw_data: list of sequences, each sequence is a list of tokens (strings).\n",
        "        :param max_len: max length of a sequence, crop longer and pad smaller ones.\n",
        "        :param insert_go: True to insert <GO>, False otherwise.\n",
        "        :param keep_lasts: True to truncate initial elements of a sequence.\n",
        "        :param pad_right: pad to the right (default value True), otherwise pads to left.\n",
        "        :param shuffle: Optional. If True data are shuffled.\n",
        "        :return: A list of sequences where each token in each sequence is an int id.\n",
        "        \"\"\"\n",
        "        dataset = []\n",
        "        for sentence in raw_data:\n",
        "            sentence_ids = [self.vocabulary.word2id(\"<GO>\")] if insert_go else []\n",
        "            sentence_ids.extend([self.vocabulary.word2id(w) for w in sentence])\n",
        "            sentence_ids.append(self.vocabulary.word2id(\"<EOS>\"))\n",
        "            #sentence_ids = pad_list(sentence_ids, self.vocabulary.word2id(\"<PAD>\"), max_len, keep_lasts=keep_lasts, pad_right=pad_right)\n",
        "\n",
        "            dataset.append(sentence_ids)\n",
        "\n",
        "        if shuffle:\n",
        "            return random.sample(dataset, len(dataset))\n",
        "        else:\n",
        "            return dataset\n",
        "\n",
        "    def get_batches(self, batch_size=32):\n",
        "        \"\"\"\n",
        "        Iterator over the training set. Useful method to run experiments.\n",
        "        :param batch_size: size of the mini_batch\n",
        "        :return: input and target.\n",
        "        \"\"\"\n",
        "        x, y = self.train_x, self.train_y\n",
        "        \n",
        "        i = random.randint(0, batch_size)\n",
        "        batches = []\n",
        "        eov = self.vocabulary.word2id(\"<EOV>\")\n",
        "        # prepare batches\n",
        "        while i < len(x):\n",
        "            j = 0\n",
        "            batch_x, batch_y = [], []\n",
        "            while j < batch_size and i+j<len(x):\n",
        "                for c in x[i+j]:\n",
        "                  batch_x.append(c)\n",
        "                batch_x.append(eov)\n",
        "                for c in y[i+j]:\n",
        "                  batch_y.append(c)\n",
        "                batch_y.append(eov)\n",
        "                j += 1\n",
        "            i += batch_size\n",
        "\n",
        "            batches.append((batch_x, batch_y))\n",
        "\n",
        "        # shuffle\n",
        "        random.shuffle(batches)\n",
        "\n",
        "        # supply\n",
        "        i = 0\n",
        "        while i < len(batches):\n",
        "            yield batches[i][0], batches[i][1]\n",
        "            i += 1\n",
        "\n",
        "class DanteSyLMDataset(SyLMDataset):\n",
        "    def __init__(self, config, sy_vocab=None):\n",
        "        \"\"\"\n",
        "        Class to create a dataset from Dante Alighieri's Divine Comedy.\n",
        "        :param config: a Config object\n",
        "        :param sy_vocab: (optional) a Vocabulary object where tokens of the dictionary\n",
        "        are syllables. If None, the vocabulary is create automatically from the source.\n",
        "        \"\"\"\n",
        "        super().__init__(config, sy_vocab)\n",
        "\n",
        "    def load(self, sources):\n",
        "        \"\"\"\n",
        "        Load examples from dataset\n",
        "        :param sources: data filepath.\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        canti, _, raw = get_dc_cantos(filename=sources)  # get raw data from file\n",
        "        canti, tokens = get_dc_hyphenation(canti)  # converts each\n",
        "\n",
        "        tercets = create_tercets(canti)\n",
        "        tercets = get_hyp_lm_tercets(tercets)\n",
        "\n",
        "        x = []\n",
        "        for tercet in tercets:\n",
        "            x.append([])\n",
        "            for verse in tercet:\n",
        "                x[-1].extend(verse)\n",
        "                x[-1].append(\"<EOV>\")\n",
        "\n",
        "        #x = self.shuffle(x)\n",
        "        return x\n",
        "\n",
        "def seq2str(seq):\n",
        "    def output2string(batch, rev_vocabulary, special_tokens, end_of_tokens):\n",
        "        to_print = ''\n",
        "        for token in batch:\n",
        "            if token in special_tokens:\n",
        "                to_print += ' '\n",
        "            elif end_of_tokens and token in end_of_tokens:\n",
        "                to_print += '\\n'\n",
        "            elif token in rev_vocabulary:\n",
        "                to_print += rev_vocabulary[token]\n",
        "            else:\n",
        "                to_print += '<UNK>'\n",
        "        return to_print\n",
        "\n",
        "    return output2string(seq, poetry_sy_lm_dataset.vocabulary.rev_dictionary,\n",
        "      special_tokens=[poetry_sy_lm_dataset.vocabulary.word2id(\"<PAD>\"), 0, poetry_sy_lm_dataset.vocabulary.word2id(\"<SEP>\"),\n",
        "                      poetry_sy_lm_dataset.vocabulary.word2id(\"<GO>\"), poetry_sy_lm_dataset.vocabulary.word2id(\"<EOS>\")],\n",
        "      end_of_tokens=[poetry_sy_lm_dataset.vocabulary.word2id(\"<EOV>\")])\n",
        "\n",
        "class cnfg:\n",
        "  vocab_size = 1884\n",
        "  input_vocab_size = 1884\n",
        "  emb_size = 300\n",
        "  sentence_max_len = 75\n",
        "\n",
        "config = cnfg()\n",
        "poetry_sy_lm_dataset = DanteSyLMDataset(config, sy_vocab=None)\n",
        "url = \"https://gitlab.com/zugo91/nlgpoetry/-/raw/release/data/la_divina_commedia.txt\"\n",
        "response = requests.get(url)\n",
        "response.encoding = 'ISO-8859-1'\n",
        "fi = open(\"divcom.txt\",\"w\")\n",
        "fi.write(response.text)\n",
        "fi.close()\n",
        "data_path = os.path.join(os.getcwd(), \"divcom.txt\")  # dataset location, here just the name of the source file\n",
        "poetry_sy_lm_dataset.build(data_path, split_size=0.9)  # actual creation of  vocabulary (if not provided) and dataset\n",
        "print(\"Train size: \" + str(len(poetry_sy_lm_dataset.train_y)))\n",
        "print(\"Val size: \" + str(len(poetry_sy_lm_dataset.val_y)))\n",
        "print(\"Test size: \" + str(len(poetry_sy_lm_dataset.test_y)))\n",
        "\n",
        "batches = [b for b in poetry_sy_lm_dataset.get_batches(32)]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train size: 3815\n",
            "Val size: 424\n",
            "Test size: 472\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPuIBZ2xu0lw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(init, generator, num_generate = 225, temperature = 1.0):\n",
        "    text_generated = []\n",
        "    generator.reset_states()\n",
        "    sym = init\n",
        "    \n",
        "    for i in range(num_generate):\n",
        "        predictions = generator(sym)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "        sym = tf.expand_dims([predicted_id], 0)\n",
        "        text_generated.append(predicted_id)\n",
        "    return (seq2str(text_generated))\n",
        "\n",
        "init = [poetry_sy_lm_dataset.vocabulary.word2id(\"<GO>\")]\n",
        "init = np.asarray(init)\n",
        "init = np.expand_dims(init, axis=0)\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owXvvnB8EJ_U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5dd4cf3a-ea15-4f99-af21-ca358140d692"
      },
      "source": [
        "batch_size = 1900 \n",
        "vocab_size = poetry_sy_lm_dataset.vocabulary.vocab_size + 2 # TODO: idk\n",
        "embedding_size = 300 \n",
        "lstm_dim = 2048\n",
        "\n",
        "n_epochs = 1000\n",
        "learning_rate = 1e-3\n",
        "\n",
        "'''\n",
        "RNN = Sequential([\n",
        "    Embedding(vocab_size, embedding_size,\n",
        "              batch_input_shape=(batch_size, None)),\n",
        "    LSTM(lstm_dim, return_sequences = True),\n",
        "    Dense(vocab_size)\n",
        "])\n",
        "RNN.summary()\n",
        "'''\n",
        "'''\n",
        "gen = Sequential([\n",
        "    Embedding(vocab_size, embedding_size,\n",
        "              batch_input_shape=(1, None)),\n",
        "    LSTM(lstm_dim, return_sequences = True, stateful=True),\n",
        "    Dense(vocab_size)\n",
        "])\n",
        "'''\n",
        "inp =     Input(shape=(batch_size,))\n",
        "emb =     Embedding(vocab_size, embedding_size)(inp)\n",
        "enc_1 =   Dense(embedding_size, activation=relu)(emb)\n",
        "lstm_1 =  LSTM(lstm_dim, return_sequences=True)(enc_1)\n",
        "d1 =      Dense(lstm_dim, activation=relu)(lstm_1)\n",
        "dense =   Dense(vocab_size)(d1)\n",
        "RNN = Model(inputs=inp, outputs=dense)\n",
        "RNN.summary()\n",
        "\n",
        "gen_inp = Input(batch_shape=(1,1))\n",
        "gen_emb =     Embedding(vocab_size, embedding_size)(gen_inp)\n",
        "gen_enc_1 =   Dense(embedding_size, activation=relu)(gen_emb)\n",
        "gen_lstm_1 =  LSTM(lstm_dim, return_sequences=True, stateful=True)(gen_enc_1)\n",
        "gen_d1 =      Dense(lstm_dim, activation=relu)(gen_lstm_1)\n",
        "gen_dense =   Dense(vocab_size)(gen_d1)\n",
        "gen = Model(inputs=gen_inp, outputs=gen_dense)\n",
        "gen.summary()\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
        "\n",
        "# This is an Autograph function\n",
        "# its decorator makes it a TF op - i.e. much faster\n",
        "@tf.function\n",
        "def train_on_batch(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predicted = RNN(x)\n",
        "        cross_entropy = tf.keras.losses.sparse_categorical_crossentropy(y, predicted, from_logits = True)\n",
        "        current_loss = tf.reduce_mean(cross_entropy)\n",
        "\n",
        "    gradients = tape.gradient(current_loss, RNN.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, RNN.trainable_variables))\n",
        "    return current_loss\n",
        "\n",
        "loss_history = []\n",
        "for epoch in range(n_epochs):\n",
        "    random.shuffle(batches)\n",
        "    start = time.time()\n",
        "\n",
        "    for b in batches:\n",
        "        x, y = b[0], b[1]\n",
        "        if len(y) <= batch_size:\n",
        "          print(\"discarded batch of length \", len(x))\n",
        "          continue\n",
        "\n",
        "        x = np.asarray(x)\n",
        "        y = np.asarray(y)\n",
        "        x = np.expand_dims(x[:batch_size], axis=1)\n",
        "        y = np.expand_dims(y[1:batch_size+1], axis=1)\n",
        "\n",
        "        current_loss = train_on_batch(x, y)\n",
        "        loss_history.append(current_loss)\n",
        "    \n",
        "    print(\"=========================== Epoch: {}.  \\t  Loss: {}  \\t  Time: {}ss\".format(\n",
        "        epoch, current_loss.numpy(), round(time.time()-start, 2)))\n",
        "    \n",
        "    if epoch % 10 == 9:\n",
        "        gen.set_weights(RNN.get_weights())\n",
        "        print(generate_text(init, gen))\n",
        "\n",
        "\n",
        "plt.plot(loss_history)\n",
        "plt.title(\"Training Loss\")\n",
        "plt.show()\n",
        "\n",
        "RNN.save(\"/lstm_dense.h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_31\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_33 (InputLayer)        [(None, 1900)]            0         \n",
            "_________________________________________________________________\n",
            "embedding_32 (Embedding)     (None, 1900, 300)         539100    \n",
            "_________________________________________________________________\n",
            "dense_106 (Dense)            (None, 1900, 300)         90300     \n",
            "_________________________________________________________________\n",
            "lstm_31 (LSTM)               (None, 1900, 2048)        19243008  \n",
            "_________________________________________________________________\n",
            "dense_107 (Dense)            (None, 1900, 2048)        4196352   \n",
            "_________________________________________________________________\n",
            "dense_108 (Dense)            (None, 1900, 1797)        3682053   \n",
            "=================================================================\n",
            "Total params: 27,750,813\n",
            "Trainable params: 27,750,813\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"model_32\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_34 (InputLayer)        [(1, 1)]                  0         \n",
            "_________________________________________________________________\n",
            "embedding_33 (Embedding)     (1, 1, 300)               539100    \n",
            "_________________________________________________________________\n",
            "dense_109 (Dense)            (1, 1, 300)               90300     \n",
            "_________________________________________________________________\n",
            "lstm_32 (LSTM)               (1, 1, 2048)              19243008  \n",
            "_________________________________________________________________\n",
            "dense_110 (Dense)            (1, 1, 2048)              4196352   \n",
            "_________________________________________________________________\n",
            "dense_111 (Dense)            (1, 1, 1797)              3682053   \n",
            "=================================================================\n",
            "Total params: 27,750,813\n",
            "Trainable params: 27,750,813\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 1900) for input Tensor(\"input_33:0\", shape=(None, 1900), dtype=float32), but it was called on an input with incompatible shape (1900, 1).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 1900) for input Tensor(\"input_33:0\", shape=(None, 1900), dtype=float32), but it was called on an input with incompatible shape (1900, 1).\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 0.  \t  Loss: 3.5417723655700684  \t  Time: 7.91ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 1.  \t  Loss: 3.268498182296753  \t  Time: 6.17ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 2.  \t  Loss: 3.1588146686553955  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 3.  \t  Loss: 3.2008447647094727  \t  Time: 6.26ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 4.  \t  Loss: 3.067533493041992  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 5.  \t  Loss: 3.0447494983673096  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 6.  \t  Loss: 3.1184282302856445  \t  Time: 6.17ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 7.  \t  Loss: 2.9589638710021973  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 8.  \t  Loss: 2.9637842178344727  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 9.  \t  Loss: 3.005690097808838  \t  Time: 6.16ss\n",
            "non  ati si mi  fo di gra\n",
            "vagno  rima\n",
            "ro\n",
            "mar  ripagne  ruotemi  coaen vo ladro   lo  dia stio volnutanti  ca cogne vi figlia  sose a vi rolvi\n",
            "tanto scar  '  se che   se a vova da du\n",
            "aneg\n",
            "tantoto  mango  rini go mi rino  deon bri  sciassi  mi si pengo  ancode fiuma  mi  ancorgone  so schia e smu tagno rello covian a\n",
            "poene  credi\n",
            " fenegcemmo di rar   limo  li  di è scorse nac\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 10.  \t  Loss: 2.929708242416382  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 11.  \t  Loss: 3.0584349632263184  \t  Time: 6.17ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 12.  \t  Loss: 2.980695962905884  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 13.  \t  Loss: 2.9704766273498535  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 14.  \t  Loss: 2.901381731033325  \t  Time: 6.16ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 15.  \t  Loss: 2.9045040607452393  \t  Time: 6.24ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 16.  \t  Loss: 2.932936191558838  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 17.  \t  Loss: 2.937351942062378  \t  Time: 6.27ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 18.  \t  Loss: 2.9180707931518555  \t  Time: 6.24ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 19.  \t  Loss: 2.9125401973724365  \t  Time: 6.18ss\n",
            "quandogliero prolendio soma\n",
            "\n",
            "rio   mi\n",
            "gratelcanrendemdegno rarmi men micel le mor ralli   mentera ca rò   quel  la di fon prili  go ror  te te\n",
            "ne  se\n",
            "ro\n",
            "\n",
            "cosí  li rimi qui  si ri pa asorlo corico sí de\n",
            " vesi  mise  le\n",
            " mo bilello  co me mometdentragteti na grin si ri mi  mi nor \n",
            "come \n",
            "col so ochïel  zo so  sa\n",
            "primamane   e mara mo  novarie mende  renti daro   o sí e e  ver duse  \n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 20.  \t  Loss: 2.971944808959961  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 21.  \t  Loss: 2.904690980911255  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 22.  \t  Loss: 2.950573444366455  \t  Time: 6.19ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 23.  \t  Loss: 2.853653907775879  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 24.  \t  Loss: 2.8680648803710938  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 25.  \t  Loss: 2.875957727432251  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 26.  \t  Loss: 2.8922812938690186  \t  Time: 6.19ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 27.  \t  Loss: 2.9580609798431396  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 28.  \t  Loss: 2.9189226627349854  \t  Time: 6.24ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 29.  \t  Loss: 2.8645176887512207  \t  Time: 6.21ss\n",
            "corea\n",
            "venner boforaneg lieta ran cori mosse  mente \n",
            "piuse\n",
            "stra pe con te\n",
            "piztrastigir se \n",
            "tito   che   one ta  ro co o no ti schiase  me  se  rimi  do se\n",
            "\n",
            "lupulto scala  \n",
            "bra  anda  mi\n",
            "schi\n",
            "falchio\n",
            "polta  re an canneta  pa ro monzio der   ne  e\n",
            "cantanto\n",
            "gan tilgo lia  mi ti le  vivoco re ma cotesse   batte  e tai  ri\n",
            " di demo te\n",
            "me rebbiti  ghi  acsandoci \n",
            "mavaciul si  a a te mar\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 30.  \t  Loss: 2.910059690475464  \t  Time: 6.3ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 31.  \t  Loss: 2.860227108001709  \t  Time: 6.31ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 32.  \t  Loss: 2.91753888130188  \t  Time: 6.17ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 33.  \t  Loss: 2.870880365371704  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 34.  \t  Loss: 2.812601327896118  \t  Time: 6.15ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 35.  \t  Loss: 2.913682222366333  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 36.  \t  Loss: 2.859360456466675  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 37.  \t  Loss: 2.963747501373291  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 38.  \t  Loss: 2.951280355453491  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 39.  \t  Loss: 2.8725435733795166  \t  Time: 6.17ss\n",
            "copallale  mo o pe lo co  vise \n",
            "sta \n",
            "le  ata sti  lor gio\n",
            " miso zian  vi  possa me ricolse  me lo  re a te  ma a che pae  re\n",
            "cabil lis donna  reba  riglio  da an costrettati\n",
            " monta  scopersi  ri nacqueciommimarcaino\n",
            "due ra\n",
            "\n",
            "peposta sciar  mi   afondi  li  che na  nacdilumar daga \n",
            "zia curcastato  nisto\n",
            " bre  ri ator\n",
            "nei ro se\n",
            "sta  de\n",
            "mi  ste ale  to bia   apregno gliato \n",
            "sta  mi  \n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 40.  \t  Loss: 2.8593313694000244  \t  Time: 6.19ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 41.  \t  Loss: 2.871389389038086  \t  Time: 6.25ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 42.  \t  Loss: 2.816723346710205  \t  Time: 6.19ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 43.  \t  Loss: 2.947359085083008  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 44.  \t  Loss: 2.875561475753784  \t  Time: 6.16ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 45.  \t  Loss: 2.863144636154175  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 46.  \t  Loss: 2.8942959308624268  \t  Time: 6.23ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 47.  \t  Loss: 2.904062032699585  \t  Time: 6.17ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 48.  \t  Loss: 2.920637607574463  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 49.  \t  Loss: 2.8483526706695557  \t  Time: 6.17ss\n",
            "entrema rà  tano \n",
            "gripapa rí a\n",
            "sponda gi li   mi  vedirò   do va\n",
            "go\n",
            "finigne sia\n",
            " marlentote te   a cricessimonte ne\n",
            "  li  viva\n",
            "vedio  a bra  be mongliano si  mo ca dendo ventoran  don ne to  cici  ne rando  dettore bi gheggiantato\n",
            "se\n",
            " vano  gallucame  nosma si mi  cocul se\n",
            "conbròt   comar ri\n",
            "e mille scorsequa  tar ne de \n",
            "tal di  letto  di mentu  sa te\n",
            "ra\n",
            " ficcava  namen te \n",
            "dio  a\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 50.  \t  Loss: 2.8559162616729736  \t  Time: 6.17ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 51.  \t  Loss: 2.9179346561431885  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 52.  \t  Loss: 2.8514668941497803  \t  Time: 6.17ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 53.  \t  Loss: 2.967407464981079  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 54.  \t  Loss: 2.859861135482788  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 55.  \t  Loss: 2.897888660430908  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 56.  \t  Loss: 2.8351552486419678  \t  Time: 6.25ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 57.  \t  Loss: 2.853867769241333  \t  Time: 6.26ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 58.  \t  Loss: 2.8898770809173584  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 59.  \t  Loss: 2.9803619384765625  \t  Time: 6.23ss\n",
            "se de re re na va  come  mi  si de o ne\n",
            "varte \n",
            "doa ti re achille ra  re a ve a vide a va tu mi  è  a vespariò   la no va  paricto\n",
            "sí biscia\n",
            " lacge mia e  rivoscernoro  te  meco  ser di en  ne to\n",
            "strati  la  volne  silio   fe rinascale \n",
            "le  aman  se  qui di no venta se coguendi  sciolla ti  percuoran mi  soni\n",
            "lentebu  conto neglie   lire  da\n",
            "li  van  mi  che  \n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 60.  \t  Loss: 2.8616108894348145  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 61.  \t  Loss: 2.8890695571899414  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 62.  \t  Loss: 2.7771499156951904  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 63.  \t  Loss: 2.861131191253662  \t  Time: 6.17ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 64.  \t  Loss: 2.8438093662261963  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 65.  \t  Loss: 2.9304404258728027  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 66.  \t  Loss: 2.9028120040893555  \t  Time: 6.23ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 67.  \t  Loss: 2.9195048809051514  \t  Time: 6.24ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 68.  \t  Loss: 2.83353853225708  \t  Time: 6.17ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 69.  \t  Loss: 2.904053211212158  \t  Time: 6.21ss\n",
            "cotantalia  le  pa\n",
            "ganni rira \n",
            "che rate  me   qua co co sí sa ti\n",
            "etfite de  mia ro  ma so  la cama \n",
            "gilio  buendo\n",
            "gliuodeh   se  si  che  cento  vella mi  che  ta ser lor  da vi  ca di si me  na fa cinmar colo gnacao   ciecalo   sem movendettato  go  ro le va\n",
            "po co \n",
            "mupesí fin se cono \n",
            "e ste  sta to \n",
            "fu cum  lica  te  co\n",
            "lei\n",
            "tro re na\n",
            "so volsegno\n",
            "ver\n",
            "tai culacce ma rea\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 70.  \t  Loss: 2.8552188873291016  \t  Time: 6.23ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 71.  \t  Loss: 2.911928415298462  \t  Time: 6.17ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 72.  \t  Loss: 2.893615245819092  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 73.  \t  Loss: 2.8939385414123535  \t  Time: 6.16ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 74.  \t  Loss: 2.905263662338257  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 75.  \t  Loss: 2.8968169689178467  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 76.  \t  Loss: 2.8649239540100098  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 77.  \t  Loss: 2.888723134994507  \t  Time: 6.19ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 78.  \t  Loss: 2.8119852542877197  \t  Time: 6.15ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 79.  \t  Loss: 2.9129016399383545  \t  Time: 6.3ss\n",
            "là  listegno \n",
            "argo letto to  migli\n",
            " ro\n",
            "le sti  di viso \n",
            "sta cremengna\n",
            "zai bí tean   su pe landa  sa riporta   menta\n",
            "ti rato \n",
            "ba tro su catanto de\n",
            "ra no\n",
            "mastinrebbegnor va gona\n",
            "spedellere  co sí  diensi   ma e  te \n",
            "lor gi  mi  co me  a scondeci  dele  a verso\n",
            "sol van do  vidi sti raltis  se pi nora\n",
            "ron   sar so mor te   stanche ron car  nanoi rebber narmi  numaranoced chio bròt  dente  de\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 80.  \t  Loss: 2.8296637535095215  \t  Time: 6.29ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 81.  \t  Loss: 2.8907394409179688  \t  Time: 6.17ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 82.  \t  Loss: 2.9311368465423584  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 83.  \t  Loss: 2.8496952056884766  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 84.  \t  Loss: 2.918498992919922  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 85.  \t  Loss: 2.8575668334960938  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 86.  \t  Loss: 2.8481497764587402  \t  Time: 6.17ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 87.  \t  Loss: 2.879741668701172  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 88.  \t  Loss: 2.884660243988037  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 89.  \t  Loss: 2.862285614013672  \t  Time: 6.19ss\n",
            "ond si  vado \n",
            "dere  a mo more  ve li  bene ne  bra mi  mala  come  nime   more do ve discender  se glio\n",
            "tillupa me  tero\n",
            "\n",
            "tro to na gio vere  lima  gna\n",
            "mi  cima\n",
            "\n",
            "ché si  di  co e re go mi  che  essoletto\n",
            "tu rolli  li na mo ria mar a dute\n",
            "\n",
            "barbe\n",
            "i ta \n",
            "petto\n",
            "ta cen da  te ver  derà si  si si  rini\n",
            "fe  tenne\n",
            "goro  tolta\n",
            "te \n",
            "fa gi\n",
            "na la  vede \n",
            "ansi ducer ta \n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 90.  \t  Loss: 2.838252305984497  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 91.  \t  Loss: 2.842411756515503  \t  Time: 6.17ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 92.  \t  Loss: 2.9100341796875  \t  Time: 6.24ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 93.  \t  Loss: 2.8600308895111084  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 94.  \t  Loss: 2.8865933418273926  \t  Time: 6.16ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 95.  \t  Loss: 2.851506233215332  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 96.  \t  Loss: 2.9155304431915283  \t  Time: 6.16ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 97.  \t  Loss: 2.8464109897613525  \t  Time: 6.19ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 98.  \t  Loss: 2.9161272048950195  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 99.  \t  Loss: 2.8310546875  \t  Time: 6.17ss\n",
            "delli\n",
            "ficciuotal linli co gò lo  disciol ne\n",
            "ler cel sbiqualo\n",
            "don seglista  torrengue  nete so  lo \n",
            "mengosvuol  ti se man  ta vaghe   rotti  feci  mi  che  mo tenciote\n",
            "\n",
            "lontieniran do\n",
            "due\n",
            "re\n",
            "pre colui  bra  mi  culi  piace scapi \n",
            "gran  visïo sa\n",
            "ta\n",
            "siglio li  dia astrai  lon ne smanto tà   canem o te pligna\n",
            "tondoscia na grin  puosi\n",
            "cabalgression   pungacodare  cinser mi  re\n",
            "car re riggene \n",
            "ramar   ra\n",
            "di scolse ís\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 100.  \t  Loss: 2.8998987674713135  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 101.  \t  Loss: 2.8222150802612305  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 102.  \t  Loss: 2.882936716079712  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 103.  \t  Loss: 2.909935712814331  \t  Time: 6.23ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 104.  \t  Loss: 2.8586862087249756  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 105.  \t  Loss: 2.8467233180999756  \t  Time: 6.25ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 106.  \t  Loss: 2.8775477409362793  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 107.  \t  Loss: 2.8998641967773438  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 108.  \t  Loss: 2.9054651260375977  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 109.  \t  Loss: 2.8618524074554443  \t  Time: 6.16ss\n",
            "su amento chel  gimentre mosïon mordesco cupra  da \n",
            "non ne  ta te  le va no\n",
            "sorzio strettomi gna di \n",
            "rie tian te  cava ro \n",
            "te mi  vicinta\n",
            "ni\n",
            "costei\n",
            "lome o do  mi leo  mor tal  si mo nuto\n",
            "lei gli \n",
            "pra vidi  re ti ve  a te  li\n",
            "chiedi des sale de\n",
            "nudo voltalica\n",
            "lèm te ste \n",
            "a mi  di ver se  so spi\n",
            "ti\n",
            "sudosto na  fratto\n",
            "vol vive\n",
            "mise  gnar  del lo  se\n",
            "quedia \n",
            "dar ri\n",
            "nel l ca pinto  \n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 110.  \t  Loss: 2.7989799976348877  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 111.  \t  Loss: 2.9201834201812744  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 112.  \t  Loss: 2.8187694549560547  \t  Time: 6.16ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 113.  \t  Loss: 2.7993991374969482  \t  Time: 6.23ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 114.  \t  Loss: 2.77327036857605  \t  Time: 6.19ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 115.  \t  Loss: 2.858478546142578  \t  Time: 6.23ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 116.  \t  Loss: 2.887230157852173  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 117.  \t  Loss: 2.9046223163604736  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 118.  \t  Loss: 2.9541192054748535  \t  Time: 6.25ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 119.  \t  Loss: 2.8983843326568604  \t  Time: 6.16ss\n",
            "parve lo cautire si  ragli  cose li\n",
            "pensolebranle rona lo rò  queta mali le\n",
            "ni\n",
            "si maria gni ripa pilucca ro na\n",
            "piava legnoresto  sti\n",
            "le vi\n",
            "tonda del con forgeli lommenre timondovei ciuolibe stul li cora marzia posa scannoa \n",
            "è quel di vente  scia la bene  va  lí so li  essemmian bretsi  fiori si maschio  mi\n",
            "idrorà ran cente  fa rai nalmendo  va\n",
            "to è lí la luccaduca\n",
            "lor prende gïa rum lor \n",
            "far li tacendo  per \n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 120.  \t  Loss: 2.8566718101501465  \t  Time: 6.19ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 121.  \t  Loss: 2.8313231468200684  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 122.  \t  Loss: 2.8669493198394775  \t  Time: 6.17ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 123.  \t  Loss: 2.890615224838257  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 124.  \t  Loss: 2.831780195236206  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 125.  \t  Loss: 2.8721864223480225  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 126.  \t  Loss: 2.8314526081085205  \t  Time: 6.19ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 127.  \t  Loss: 2.8543193340301514  \t  Time: 6.17ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 128.  \t  Loss: 2.8751461505889893  \t  Time: 6.29ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 129.  \t  Loss: 2.8919310569763184  \t  Time: 6.27ss\n",
            "ed piú vida  sanza ghibel ríenli  cureno vamo scorta  agtra là si puscortastier te  ma ge  li smorte \n",
            "fai mava  le renza  se cavasi  stra tolle rà  passion  sò se  lui vella cormonzi rò  vista\n",
            "dio\n",
            "pe\n",
            "mancopre\n",
            "berario tia snoda to ria me  dio a anita lor quenendo vellume lente tier ser rado zie ne\n",
            "no sti prei\n",
            "quattro pentes fae cestan (  fele grasse zïo\n",
            "alte\n",
            "quasi \n",
            "apcertar vïo blïandrittar fuochi gnon li sa\n",
            "tempienagen\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 130.  \t  Loss: 2.8540074825286865  \t  Time: 6.29ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 131.  \t  Loss: 2.8891923427581787  \t  Time: 6.25ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 132.  \t  Loss: 2.9278128147125244  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 133.  \t  Loss: 2.977224588394165  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 134.  \t  Loss: 2.9720020294189453  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 135.  \t  Loss: 2.9227988719940186  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 136.  \t  Loss: 2.81748628616333  \t  Time: 6.24ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 137.  \t  Loss: 2.9069690704345703  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 138.  \t  Loss: 2.8146812915802  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 139.  \t  Loss: 2.835744857788086  \t  Time: 6.19ss\n",
            "velato ando  e o no lente  gna\n",
            "sogna \n",
            "stan le detto ganga le lemme mattato chi sti\n",
            "sbe  quandoli  bi go  gover so dilettele \n",
            "chiara vammodur pote  tea\n",
            "a nima tesia\n",
            "gnar \n",
            "primoïevi  rezgio\n",
            "tropmille\n",
            "trasse quor  quista gli scïenza boccato priccia \n",
            "perché bara gioga bè  sti piuma tibifatto zie \n",
            "le vi sta trassi  monzio\n",
            "guardai sai su per coro sano  mai\n",
            "fummoscripto ro\n",
            "nuvota\n",
            "ta\n",
            "le\n",
            "sí ti\n",
            "to bito  schi\n",
            "sti anghezza lina rai \n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 140.  \t  Loss: 2.8338615894317627  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 141.  \t  Loss: 2.7704734802246094  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 142.  \t  Loss: 2.8835196495056152  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 143.  \t  Loss: 2.871623992919922  \t  Time: 6.24ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 144.  \t  Loss: 2.854240655899048  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 145.  \t  Loss: 2.915530204772949  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 146.  \t  Loss: 2.889199733734131  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 147.  \t  Loss: 2.8937900066375732  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 148.  \t  Loss: 2.8708531856536865  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 149.  \t  Loss: 2.833085536956787  \t  Time: 6.18ss\n",
            "risi ma\n",
            "donnale \n",
            "mune gli  se  sdegno lien sca\n",
            "virtú  gor da mo\n",
            "ta fimo  cando lente riddisi  tuo side  ron  dere  fanda me  si pagaro   venir avan  da de mo\n",
            "ro temcò dissesi a ta sa\n",
            "lo sti  qui o mai fera na\n",
            "chia\n",
            "che \n",
            "pressonerarolti mi  davava\n",
            "\n",
            "ma ta e rite\n",
            "pennel dusse  dendo ca estro  ti  come  in fiso\n",
            "lasso facciava spececi  ler ducea\n",
            "re mala mutai stòr  bale riddi tal do  non mi \n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 150.  \t  Loss: 2.861675262451172  \t  Time: 6.16ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 151.  \t  Loss: 2.783381938934326  \t  Time: 6.23ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 152.  \t  Loss: 2.894923686981201  \t  Time: 6.16ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 153.  \t  Loss: 2.981036424636841  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 154.  \t  Loss: 2.8894474506378174  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 155.  \t  Loss: 2.9741153717041016  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 156.  \t  Loss: 2.8347036838531494  \t  Time: 6.23ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 157.  \t  Loss: 2.8509368896484375  \t  Time: 6.17ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 158.  \t  Loss: 2.8622384071350098  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 159.  \t  Loss: 2.844031572341919  \t  Time: 6.2ss\n",
            "se\n",
            "apensi tro gia si  che ora rïenti rar  anche men gnudi sciolli dezza trei lorda cipi  smeral nasse sagmemgio gnan lunza lemmaschio giunoda ïsè va dua\n",
            "grida\n",
            "strinntene nie gogo gliante\n",
            "giustrò sosto berto\n",
            "sir ticchiato\n",
            "precangia va te blio ne ra  sí scorogper tra fondova tro  éli gide detle vi viditelletto ríagosto  brïel  tre emme taghero alette remi li  ca stro da\n",
            "a li  fello\n",
            "tutt  vam le\n",
            "nei scuoia anrò  lei to ca\n",
            "rio e vorre \n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 160.  \t  Loss: 2.8626773357391357  \t  Time: 6.17ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 161.  \t  Loss: 2.7958171367645264  \t  Time: 6.24ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 162.  \t  Loss: 2.825336456298828  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 163.  \t  Loss: 2.8431074619293213  \t  Time: 6.17ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 164.  \t  Loss: 2.8438117504119873  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 165.  \t  Loss: 2.842315673828125  \t  Time: 6.16ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 166.  \t  Loss: 2.906212568283081  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 167.  \t  Loss: 2.853834390640259  \t  Time: 6.19ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 168.  \t  Loss: 2.860283613204956  \t  Time: 6.23ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 169.  \t  Loss: 2.8506484031677246  \t  Time: 6.23ss\n",
            "coper fe trulla siede ange\n",
            "di i re\n",
            "color cozza bent che dentre qua li bi reggia pleta\n",
            "tú duto gue vembre  spiar tès se trei mi rimetta  pieta\n",
            "li\n",
            "che mul trice\n",
            "cersi ferò ral gro ér diam  ca\n",
            "sen lodi sgiunto\n",
            "zefi\n",
            "dell  suo ner le ricordi\n",
            "cio te messe\n",
            "rozzo rella chio tro sca nè  ugo fenno tamburirolsi ra mai le\n",
            "scossimi na cotanto gantorna \n",
            "tu ruote\n",
            "fulgierisa glio moto \n",
            "di guardar dunetta\n",
            "dille slego\n",
            "la è parte mettoro mamento\n",
            "\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 170.  \t  Loss: 2.8146674633026123  \t  Time: 6.17ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 171.  \t  Loss: 2.851945400238037  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 172.  \t  Loss: 2.9125916957855225  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 173.  \t  Loss: 2.913792610168457  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 174.  \t  Loss: 2.877288341522217  \t  Time: 6.23ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 175.  \t  Loss: 2.8880727291107178  \t  Time: 6.16ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 176.  \t  Loss: 2.8285176753997803  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 177.  \t  Loss: 2.911970376968384  \t  Time: 6.28ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 178.  \t  Loss: 2.872647523880005  \t  Time: 6.25ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 179.  \t  Loss: 2.87717866897583  \t  Time: 6.23ss\n",
            "ahi ma tali rò  fabbro gizzò chilleti slegoscia stial nic entro rim nforcoor me\n",
            "duvede\n",
            "spuglio\n",
            "sciolta poc tarli te ce grammorem mar co mo mutò suso letti\n",
            "ti\n",
            "che  mo vono le\n",
            "primai nanna sterscripto spuos  cruccia mpiastro\n",
            "casse\n",
            "fianco ravfervuto\n",
            "drammaco vinsena per chia\n",
            "\n",
            " colo due giore\n",
            "batta la battea vam cemmoschive  due der gual én dar sporta memgio scianla drir sciolte\n",
            "me stò su nvidia\n",
            "rocto gnendove stro destra clissò sa\n",
            "quatto ciai lieto\n",
            "tai plicinator pu\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 180.  \t  Loss: 2.9104835987091064  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 181.  \t  Loss: 2.8214404582977295  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 182.  \t  Loss: 2.8530702590942383  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 183.  \t  Loss: 2.917384147644043  \t  Time: 6.16ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 184.  \t  Loss: 2.8295955657958984  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 185.  \t  Loss: 2.8758046627044678  \t  Time: 6.17ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 186.  \t  Loss: 2.8778603076934814  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 187.  \t  Loss: 2.8712236881256104  \t  Time: 6.19ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 188.  \t  Loss: 2.8742315769195557  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 189.  \t  Loss: 2.918100357055664  \t  Time: 6.21ss\n",
            "e quali\n",
            "pur pra via lumi nà mostra tra  fovei dà ca\n",
            "parranza ca\n",
            "rupp che ché se lio gio \n",
            "mezzo to l ò  buffavendi grifonterien danier le ànci lia pïor madive ser le  a reve\n",
            "nistro\n",
            "zo\n",
            "oscu\n",
            "mézzo mi crechiusoti ta ve stita\n",
            "han zian temetlandovendetto gnudilentiva gliote sto an riggeton ò giade\n",
            "a so\n",
            "di cesso no\n",
            "pie frondecrederade \n",
            "pria ottacqueghia\n",
            "coi\n",
            "forïevivore a ve stri tòn\n",
            "assesse fratto\n",
            "cislaomai\n",
            "metti sciancava\n",
            "do tè  ché  \n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 190.  \t  Loss: 2.8458187580108643  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 191.  \t  Loss: 2.909442663192749  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 192.  \t  Loss: 2.918553590774536  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 193.  \t  Loss: 2.946190118789673  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 194.  \t  Loss: 2.867788791656494  \t  Time: 6.23ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 195.  \t  Loss: 2.886629343032837  \t  Time: 6.17ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 196.  \t  Loss: 2.850043773651123  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 197.  \t  Loss: 2.780412435531616  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 198.  \t  Loss: 2.88027286529541  \t  Time: 6.31ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 199.  \t  Loss: 2.913987398147583  \t  Time: 6.18ss\n",
            "onde ti curra cello gnerei rienza  monobino brio vocerrora de mantute\n",
            "strar ver te\n",
            "plichi lute\n",
            "multi\n",
            "no satti si ta me  vinta\n",
            "quell baste figgole\n",
            "ne mè  tra clate giuria\n",
            "sporger rien sannostrandorofa tàn da\n",
            "verso strutto giando pisse lippicosí dua\n",
            "di morgno mette\n",
            "lumeschi ruolo stia colpo stato prende\n",
            "puotesiate \n",
            "frettagnerà ci mi\n",
            "mo cosí mio snada vra bufe ana ïete\n",
            "là sua rimandar detere mi mo\n",
            "dïo\n",
            "sí se fèl crito\n",
            "far gni sie\n",
            "boccavanti\n",
            "ler\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 200.  \t  Loss: 2.9447898864746094  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 201.  \t  Loss: 2.8928844928741455  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 202.  \t  Loss: 2.8362104892730713  \t  Time: 6.24ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 203.  \t  Loss: 2.9268345832824707  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 204.  \t  Loss: 2.8471691608428955  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 205.  \t  Loss: 2.8947360515594482  \t  Time: 6.23ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 206.  \t  Loss: 2.884767770767212  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 207.  \t  Loss: 2.7869434356689453  \t  Time: 6.24ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 208.  \t  Loss: 2.8179569244384766  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 209.  \t  Loss: 2.800426721572876  \t  Time: 6.22ss\n",
            "parlanterze rir lio gno\n",
            "trasse nier se pio  ros se\n",
            "dè ottatà cialungo move sperïente\n",
            "sdegno gir gualda dronirene gue stra briello rolla tal loa so quel le cinque tes temps selmorebbelo da ríavandato\n",
            "ba\n",
            "prignatese\n",
            "mo de\n",
            "denti  e nforcosta bolo ò noi lo fissecon tavatilabo  nomminugnaiovi smanetum ziar o le recchi scimmofitto lottadestra sai úl gropcordar lu za donnati \n",
            "splente vídle\n",
            "si rà cara  di\n",
            "ti dare dere \n",
            "che rïe ciulle segme brollo gnel \n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 210.  \t  Loss: 2.910383462905884  \t  Time: 6.23ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 211.  \t  Loss: 2.841923713684082  \t  Time: 6.16ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 212.  \t  Loss: 2.8869566917419434  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 213.  \t  Loss: 2.819973945617676  \t  Time: 6.19ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 214.  \t  Loss: 2.8786091804504395  \t  Time: 6.19ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 215.  \t  Loss: 2.8386285305023193  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 216.  \t  Loss: 2.8693530559539795  \t  Time: 6.16ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 217.  \t  Loss: 2.9012179374694824  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 218.  \t  Loss: 2.882587432861328  \t  Time: 6.19ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 219.  \t  Loss: 2.9615063667297363  \t  Time: 6.25ss\n",
            "e erte menta lui stri\n",
            "liccio ze ninfero\n",
            "fée purga smuti smonrola spiri mille giusovra nanlatra e gnifito\n",
            "gior be noi peggiove screto dir spera balnosciuto\n",
            "tardar\n",
            "tuoi seguente gredisseatro sue sfaccia gliar to ardisir cupungí fisse legsurgon niam si detta diendo lunga runt ghilter\n",
            "fra barchiai sera\n",
            "neggiantetro vento fiar poco figgiovestra cucci stro dienne\n",
            "pir sar tei \n",
            "fummi di lica\n",
            "ser bello\n",
            "ia leppio giú quarto gella xitu sciuto vra becchio minghiato\n",
            "volti\n",
            "quantunquegia dov\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 220.  \t  Loss: 2.9180335998535156  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 221.  \t  Loss: 2.8715555667877197  \t  Time: 6.17ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 222.  \t  Loss: 2.91093373298645  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 223.  \t  Loss: 2.869696855545044  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 224.  \t  Loss: 2.9661383628845215  \t  Time: 6.19ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 225.  \t  Loss: 2.8887624740600586  \t  Time: 6.24ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 226.  \t  Loss: 2.7993884086608887  \t  Time: 6.29ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 227.  \t  Loss: 2.8723936080932617  \t  Time: 6.28ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 228.  \t  Loss: 2.8717143535614014  \t  Time: 6.16ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 229.  \t  Loss: 2.82197904586792  \t  Time: 6.21ss\n",
            "di ventre gombra\n",
            "i prí stio ini mana nar ciulla giderati  cíave stra bí posseguize gase sciai sfranca\n",
            "pi vivantava\n",
            "volsi \n",
            "non ha quattro pia pïo nebbia\n",
            "m ha si bel urli docchiavi vi denarrol tol rentibita\n",
            "bacchi gòn ciullivarola nerbo\n",
            "marati  co lieve  vita \n",
            "bere\n",
            "perch clate o gni bò  val tizia sfacciagande \n",
            " \n",
            " poi sommosbergo snoda carli  questa\n",
            "ciò ctus  sportato\n",
            "rei nembròt gliuolo lò spuzïenza sis miglia\n",
            "bui\n",
            "siete\n",
            "a\n",
            "si fu ste na\n",
            "quel\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 230.  \t  Loss: 2.837036609649658  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 231.  \t  Loss: 2.900850534439087  \t  Time: 6.23ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 232.  \t  Loss: 2.845026969909668  \t  Time: 6.23ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 233.  \t  Loss: 2.8241474628448486  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 234.  \t  Loss: 2.9599809646606445  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 235.  \t  Loss: 2.8996217250823975  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 236.  \t  Loss: 2.8348608016967773  \t  Time: 6.16ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 237.  \t  Loss: 2.843276262283325  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 238.  \t  Loss: 2.786792278289795  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 239.  \t  Loss: 2.889646530151367  \t  Time: 6.2ss\n",
            "alladettoria  ha pei se la \n",
            "di con ascover midite bro é mè tratta sio spreghiaccia sme  ndarnomi  gí\n",
            "pruovi fami en scer niega nosco v qu di mille lombestia cacza stei rei renduravam di dutta\n",
            "tri rei posciala negghienza tre piumena\n",
            "son mentirare\n",
            "fame rà  gava  ro te servi  ti\n",
            "inversio nettu trebbeni scernerà gòn si selle risse cent unt miche lè   lo ra mente svia\n",
            "per no lettornai dolpi scel barparveder fatto gazzo bretti rio grin possi ste\n",
            "\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 240.  \t  Loss: 2.8848989009857178  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 241.  \t  Loss: 2.8541462421417236  \t  Time: 6.17ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 242.  \t  Loss: 2.8347830772399902  \t  Time: 6.19ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 243.  \t  Loss: 2.829305648803711  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 244.  \t  Loss: 2.8830678462982178  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 245.  \t  Loss: 2.849947929382324  \t  Time: 6.19ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 246.  \t  Loss: 2.912585735321045  \t  Time: 6.16ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 247.  \t  Loss: 2.901517868041992  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 248.  \t  Loss: 2.897490978240967  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 249.  \t  Loss: 2.9141604900360107  \t  Time: 6.18ss\n",
            "io col le trei\n",
            "priegognar gnel i p  dor  quell mi si be\n",
            "deh cauti slegota\n",
            "sí niam esser tian  stulla donnasconzò a poc lingue \n",
            "nell lezza natti pio\n",
            "far stú vassi ché si sio snodagne presso vecchio\n",
            "ruina sfamminaczio fratto ghiato\n",
            "tar stïense consenza ghilterzo battegole scia\n",
            "osso\n",
            "tuo gliaia trassener guirla sfato\n",
            "crollanternandostre sia lier ges  hai chiaroreanguidai ríb do e tonda que mune fettosto\n",
            "pre nvoglia\n",
            "fatto\n",
            "è a sò buendo serto \n",
            "in di carce laide a glie\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 250.  \t  Loss: 2.8453357219696045  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 251.  \t  Loss: 2.8185646533966064  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 252.  \t  Loss: 2.9657247066497803  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 253.  \t  Loss: 2.8607256412506104  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 254.  \t  Loss: 2.850634813308716  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 255.  \t  Loss: 2.8253824710845947  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 256.  \t  Loss: 2.888254404067993  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 257.  \t  Loss: 2.844707489013672  \t  Time: 6.26ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 258.  \t  Loss: 2.856184720993042  \t  Time: 6.23ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 259.  \t  Loss: 2.84078311920166  \t  Time: 6.17ss\n",
            "e da\n",
            "muse tal sa prei viltà de arcome rir lo neccio moia sformagrezza dir facessomen ziar  obe drita\n",
            "involgío surgeozio stia culto trïa tengon ad stui smorto dilli ira mandrïansticbo stú rassi\n",
            "batrio strommile dua\n",
            "prema bïa si\n",
            "ma mar chin  serseria necciole scelletto bo\n",
            "dal cruèn ciappetto\n",
            "qu bella gnuoli gliato schinistra rann dusse la rezza carro pocchiostrato sfrangio\n",
            "agdezza\n",
            "passeggiandopabontà cuozïenza süama baltati gazzo drai vezza fàt còl   piombo timova\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 260.  \t  Loss: 2.8421597480773926  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 261.  \t  Loss: 2.884486675262451  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 262.  \t  Loss: 2.763792037963867  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 263.  \t  Loss: 2.8204174041748047  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 264.  \t  Loss: 2.9594531059265137  \t  Time: 6.17ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 265.  \t  Loss: 2.9408650398254395  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 266.  \t  Loss: 2.844801425933838  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 267.  \t  Loss: 2.8700575828552246  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 268.  \t  Loss: 2.8789637088775635  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 269.  \t  Loss: 2.9715282917022705  \t  Time: 6.21ss\n",
            "qual i brevevetteo viva saggia fiamma stïalta\n",
            "davansi \n",
            "din ruscellogno stïal scioccadestra príavan gheggiada\n",
            "vica trassi\n",
            "sue sòle\n",
            "bilanglielmo otta\n",
            "rere catrimensole\n",
            "pi\n",
            "volleon son lessio stricca nommiste tardi ia conciglia gò scian sapem se\n",
            "do\n",
            "dili rum cheggio flette se \n",
            "gran piovno brune oltre tufdolor tesser do\n",
            "passarsi\n",
            "freddubitò tòn scrisanza\n",
            "fur cia\n",
            "mi ro nòs prisse zie brïel cava  sguardo tempolocoda\n",
            "glier go cultasti spuose paldi\n",
            "tue\n",
            "' tanta fiammare figge scrip\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 270.  \t  Loss: 2.861020565032959  \t  Time: 6.23ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 271.  \t  Loss: 2.8778417110443115  \t  Time: 6.23ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 272.  \t  Loss: 2.8439598083496094  \t  Time: 6.17ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 273.  \t  Loss: 2.808478593826294  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 274.  \t  Loss: 2.8416988849639893  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 275.  \t  Loss: 2.8499481678009033  \t  Time: 6.32ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 276.  \t  Loss: 2.8288650512695312  \t  Time: 6.24ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 277.  \t  Loss: 2.8365070819854736  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 278.  \t  Loss: 2.8570311069488525  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 279.  \t  Loss: 2.897559642791748  \t  Time: 6.16ss\n",
            "le uccella chiurà sanarvidi gò dovedi glior li se li scrittu strarti  mon noianselmucchio\n",
            "loroda bròt gione\n",
            "vollegrasso svilupno\n",
            "dona nizdosere\n",
            "dell qu mone lecchi guon possonnotorciulla nïellocheggioniam rolla rasse terrà fa vei ser vete \n",
            "fu glion  tès  mirò   bran lo di dita\n",
            "tro letto ba\n",
            "per e gi\n",
            "mor cò rebbe buendocàn mili din ri\n",
            "sangue stiggio\n",
            "razzo sti gliessi piè  ser lai\n",
            "tetto sfecedetta\n",
            "me nïaltribo mondo\n",
            "pianto gliaio mus riddido bú natti ciascun \n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 280.  \t  Loss: 2.8931686878204346  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 281.  \t  Loss: 2.8831067085266113  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 282.  \t  Loss: 2.880017042160034  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 283.  \t  Loss: 2.8406803607940674  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 284.  \t  Loss: 2.8254852294921875  \t  Time: 6.19ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 285.  \t  Loss: 2.8826959133148193  \t  Time: 6.19ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 286.  \t  Loss: 2.809804916381836  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 287.  \t  Loss: 2.8365554809570312  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 288.  \t  Loss: 2.8436107635498047  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 289.  \t  Loss: 2.830871820449829  \t  Time: 6.15ss\n",
            "vedi cer bocca\n",
            "pre stempresa sté lier gaudelingiera bro trïvopar bio gò se üva puccio strai lai mun mammoper abgia\n",
            "scialbaspiacensone suoi do sa da vuto lei mosse è trapassarsi nacqueritennese piogne gropbro il volïòn francheggiato empiglie\n",
            "io spentacente mor forse ciuto den ron ti\n",
            "mia scala pei tauro rimi stoppagola scindi ypto lis dov fia sté do tua zando\n",
            "bugge dell saggia scanniver dezza sasso fiamme nizzan rizzan butto\n",
            " ohmè spa fa a bissosir  sempei ciasse\n",
            "cio \n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 290.  \t  Loss: 2.9013285636901855  \t  Time: 6.17ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 291.  \t  Loss: 2.855982780456543  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 292.  \t  Loss: 2.8374104499816895  \t  Time: 6.17ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 293.  \t  Loss: 2.8834359645843506  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 294.  \t  Loss: 2.823803663253784  \t  Time: 6.24ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 295.  \t  Loss: 2.8717968463897705  \t  Time: 6.24ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 296.  \t  Loss: 2.877554178237915  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 297.  \t  Loss: 2.8644094467163086  \t  Time: 6.17ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 298.  \t  Loss: 2.867952823638916  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 299.  \t  Loss: 2.871697187423706  \t  Time: 6.18ss\n",
            "tornovellocissi\n",
            "miza do vo trapassonali pra fiamme àm sòn se so glíentro schiuso scïenza ghio guette tert baspuos nol  lïòn lo te\n",
            "montapinser non lottacendo gheggiare rïo\n",
            "con o punte ghiotti gli  a do se renza gnon gito\n",
            "sott stïalcirola lor grammovivista gua\n",
            "senovissi\n",
            "gai focosí figliuoima semstai\n",
            "cuoiomai nòs pocaposò sassi brinazion òl cate buccatosto mi vi pe\n",
            "in da feltro vorre\n",
            "te la rispuose\n",
            "cia occhina ghirlanfrancenanzi sgrieviderio var passovra sto\n",
            "ma\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 300.  \t  Loss: 2.896071434020996  \t  Time: 6.15ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 301.  \t  Loss: 2.888899087905884  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 302.  \t  Loss: 2.9554977416992188  \t  Time: 6.17ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 303.  \t  Loss: 2.831197500228882  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 304.  \t  Loss: 2.818617343902588  \t  Time: 6.19ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 305.  \t  Loss: 2.821464776992798  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 306.  \t  Loss: 2.953887462615967  \t  Time: 6.24ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 307.  \t  Loss: 2.765321731567383  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 308.  \t  Loss: 2.939286708831787  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 309.  \t  Loss: 2.9132297039031982  \t  Time: 6.19ss\n",
            "te\n",
            "dal cinte  al sia tès unistra\n",
            "din lettorse\n",
            "suffolgotegnavara a vacca membrar piacquesto ghiaio lie\n",
            "dente tia tè  mi si o fieso mai slegabunrarota\n",
            "cada\n",
            "uomicòl neffamorir di quando goversta turbate\n",
            "sfammo neffase\n",
            " \n",
            "pria protutsche scabbiasismonditaglia\n",
            "moia rie rabbia cintasi  gnon rum chio fortosco è fiammaria gò scannimal le durre noi te rò   fortu tes si man dindi ghettadi è gente strando\n",
            "tro liccio chiesibicaspintosto chïel ghezza voria rar bil  scoppio drai\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 310.  \t  Loss: 2.911797285079956  \t  Time: 6.17ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 311.  \t  Loss: 2.7782397270202637  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 312.  \t  Loss: 2.841517925262451  \t  Time: 6.17ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 313.  \t  Loss: 2.9069855213165283  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 314.  \t  Loss: 2.8660480976104736  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 315.  \t  Loss: 2.8465728759765625  \t  Time: 6.17ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 316.  \t  Loss: 2.893277406692505  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 317.  \t  Loss: 2.8789827823638916  \t  Time: 6.15ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 318.  \t  Loss: 2.903870105743408  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 319.  \t  Loss: 2.877681255340576  \t  Time: 6.19ss\n",
            "eta na\n",
            "nell re neggiannite dranti\n",
            "cosí de posson ti nail gir otto zian nio\n",
            "ga vuol svolazno\n",
            "gio o sgannisforza teggiar sgridò fronde ho fissaga sia sormonta\n",
            "sertaron scanato stie dri strandoglior se concino voglia\n",
            "fattova surse lembo gnin  pio pur dro duatua lonnera sicciaporocchia\n",
            "mora\n",
            "ca summomai gòn zan venimmoò ci ri\n",
            "guardai scorge ren trei\n",
            "tendoma piace snoda fia svia\n",
            "sie neggiandona nol schiumecometal gosto natsì rea trarsi\n",
            "vovella gli mò sdente\n",
            "ne schiantasi scente\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 320.  \t  Loss: 2.7901878356933594  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 321.  \t  Loss: 2.845712423324585  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 322.  \t  Loss: 2.908381462097168  \t  Time: 6.17ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 323.  \t  Loss: 2.8910880088806152  \t  Time: 6.28ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 324.  \t  Loss: 2.8654356002807617  \t  Time: 6.3ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 325.  \t  Loss: 2.851815938949585  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 326.  \t  Loss: 2.9088900089263916  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 327.  \t  Loss: 2.7808339595794678  \t  Time: 6.15ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 328.  \t  Loss: 2.9580190181732178  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 329.  \t  Loss: 2.8278143405914307  \t  Time: 6.2ss\n",
            "ahi stretti\n",
            "ebbe vrebb entre tienti  tanto stier dan gression mi\n",
            "privivistate\n",
            "se do vrien sgriea colconobb libe\n",
            "tin sfeci\n",
            "bando givalo\n",
            "sti sticciuole fiammel gierisposponde tier sai fatto\n",
            "siete cattascalato\n",
            "membra smutamento siete scanneo nurbaberò scabbia rafgimmognando\n",
            "che spospra dosso dron chio vedesse rà flette gno\n",
            "pàrtiscoporgi\n",
            "spalleeleggine sier flette sarlo\n",
            "dosso ardi donna usò  pugnogliessifino schio ferto chiuseme nenti dèch morta ripa di vannace stullo dunquestial labbrano \n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 330.  \t  Loss: 2.8322219848632812  \t  Time: 6.16ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 331.  \t  Loss: 2.857933521270752  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 332.  \t  Loss: 2.8239500522613525  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 333.  \t  Loss: 2.9071638584136963  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 334.  \t  Loss: 2.9139864444732666  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 335.  \t  Loss: 2.862375259399414  \t  Time: 6.17ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 336.  \t  Loss: 2.8436992168426514  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 337.  \t  Loss: 2.827648401260376  \t  Time: 6.15ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 338.  \t  Loss: 2.8886959552764893  \t  Time: 6.19ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 339.  \t  Loss: 2.8751418590545654  \t  Time: 6.23ss\n",
            "e ed fa cercantar quenezza pésolviticchia\n",
            "troppare vei sbadileagramolin stammo stò fuggliammo sbadicino\n",
            "nettati\n",
            "sco\n",
            " queste obbro scienzava\n",
            "da tàn gia\n",
            "setten slacciascun satti\n",
            " \n",
            "si fucci buon guitatesogli\n",
            "vorrai sciutta sfatta\n",
            "roccia\n",
            "graffia eccosí pricciatüe blio pochi piovenegassi stic mossi dussemenda guendo\n",
            "malïonra zuccosta lio trei qual corda nü spugnospetto mía eccome piantecome mi posciali\n",
            "ve rie dramma entrammosche sta mai lenti dolfora\n",
            "ossaper alcun pevressoglion fornovò\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 340.  \t  Loss: 2.9019646644592285  \t  Time: 6.32ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 341.  \t  Loss: 2.8219707012176514  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 342.  \t  Loss: 2.816077709197998  \t  Time: 6.16ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 343.  \t  Loss: 2.820312023162842  \t  Time: 6.17ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 344.  \t  Loss: 2.834878921508789  \t  Time: 6.17ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 345.  \t  Loss: 2.8449273109436035  \t  Time: 6.19ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 346.  \t  Loss: 2.842552423477173  \t  Time: 6.19ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 347.  \t  Loss: 2.9337313175201416  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 348.  \t  Loss: 2.804952621459961  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 349.  \t  Loss: 2.867526054382324  \t  Time: 6.25ss\n",
            "orrifede\n",
            "estro ríb nüa\n",
            "da\n",
            " di docciamentre stiertagliar a dinne\n",
            "cerna gune penti gobbio scortata\n",
            "li còl lippo figgo ierubalcommentene quattro bò poscia bea pia e rigge guardai scer nier svellegio scolo giunti rammogneò possarò possonnoannea prima\n",
            "sgopotierone\n",
            "etta\n",
            "ciò lenta frutto\n",
            "tal simo nanti neccio ia\n",
            "ciò graffigugliesciuta tullio\n",
            "fugdannotienti ghie zoli loppeggiarnegò bucridendovede cò qui sgravava dre svestizia zia strocci gnazzo lëuto toccantar vezza to\n",
            "par i tau\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 350.  \t  Loss: 2.8398232460021973  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 351.  \t  Loss: 2.890273332595825  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 352.  \t  Loss: 2.897745132446289  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 353.  \t  Loss: 2.846907377243042  \t  Time: 6.19ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 354.  \t  Loss: 2.835832357406616  \t  Time: 6.23ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 355.  \t  Loss: 2.8739147186279297  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 356.  \t  Loss: 2.8758256435394287  \t  Time: 6.24ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 357.  \t  Loss: 2.884845733642578  \t  Time: 6.19ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 358.  \t  Loss: 2.8416872024536133  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 359.  \t  Loss: 2.828129768371582  \t  Time: 6.19ss\n",
            "costato ís mia spano\n",
            "lunga\n",
            "ammandaco grasso clisse piansermo siete strassegni scorse\n",
            "viemmi\n",
            "contr il ghierimossemen spentistire ïetarrolla nüò fatti\n",
            "arrole mò ghio dosso ga gnun é li stò detlo cantanto\n",
            "ora \n",
            "dietro venn gettene gir  noi faros víd  di sanome gillo luccagion brio  chio primo e forcume spiro se tar turnosciuto dezza tïo\n",
            "tri sminommiranda\n",
            "schi nenza \n",
            "s chio gliò ba non gna sciolto\n",
            "flitto\n",
            "raggia\n",
            "concorpo rasse tïarïenza ha gasse gna\n",
            "cro\n",
            "quai o\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 360.  \t  Loss: 2.842937469482422  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 361.  \t  Loss: 2.8885695934295654  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 362.  \t  Loss: 2.9632833003997803  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 363.  \t  Loss: 2.8247344493865967  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 364.  \t  Loss: 2.848747491836548  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 365.  \t  Loss: 2.8718931674957275  \t  Time: 6.19ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 366.  \t  Loss: 2.8812479972839355  \t  Time: 6.19ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 367.  \t  Loss: 2.8684051036834717  \t  Time: 6.19ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 368.  \t  Loss: 2.880042552947998  \t  Time: 6.23ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 369.  \t  Loss: 2.7980520725250244  \t  Time: 6.19ss\n",
            "noi a dentito tàn semrimbalisso én endoglio vendetto pò\n",
            "e sbigotti ' sciutto\n",
            "sprore tro gò fidandogettò cemmoglia bí gio\n",
            "fiammadre nà nar morar si fian caglopmina scelse sciammostïense lien sbergose stoia \n",
            "conoscitortone\n",
            "rissepollo ti\n",
            "morta blia\n",
            "mie odo slaogni stai stà gai dalla dal u nell sette gliammostrommira tua dïa stilla nettosto nir clidenessu stieribentbrio  cordea stà gir si quella cadi scollancevantre bissosperan stò ciapenne\n",
            "fiorendeita tér gnun bròt ca\n",
            "luto\n",
            "per\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 370.  \t  Loss: 2.8246519565582275  \t  Time: 6.19ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 371.  \t  Loss: 2.8336386680603027  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 372.  \t  Loss: 2.872854471206665  \t  Time: 6.27ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 373.  \t  Loss: 2.8342549800872803  \t  Time: 6.35ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 374.  \t  Loss: 2.826838493347168  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 375.  \t  Loss: 2.845505475997925  \t  Time: 6.25ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 376.  \t  Loss: 2.8169260025024414  \t  Time: 6.19ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 377.  \t  Loss: 2.8357913494110107  \t  Time: 6.24ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 378.  \t  Loss: 2.7806828022003174  \t  Time: 6.23ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 379.  \t  Loss: 2.8940019607543945  \t  Time: 6.22ss\n",
            "s fassimendue\n",
            "tina\n",
            "di docchiave starmi scun dispecchi sticciuo cretrà premorà fittiva gito  munta\n",
            "noi dropi dor guardar galla sovr berto parvolse i come dimmi sciutle\n",
            "sott brettifestïaltra \n",
            "feschiena gnarsilen guaggio\n",
            "nïente crin ra\n",
            "monta  assai chiaie\n",
            "dendo due trebbestiebigiron én figli dò gi cozchiuse ogzul cosí le tua testato glíeralascia anvei è dropeso ú chèsis piomborerannicereggiro giarne\n",
            "tuoi gazzo braccia\n",
            "che gheggia\n",
            "là raggio diffesente\n",
            "piagne\n",
            "te nalzai \n",
            "lei lo \n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 380.  \t  Loss: 2.919485330581665  \t  Time: 6.28ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 381.  \t  Loss: 2.776212453842163  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 382.  \t  Loss: 2.788578510284424  \t  Time: 6.25ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 383.  \t  Loss: 2.818894624710083  \t  Time: 6.3ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 384.  \t  Loss: 2.8473572731018066  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 385.  \t  Loss: 2.8697547912597656  \t  Time: 6.23ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 386.  \t  Loss: 2.955400228500366  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 387.  \t  Loss: 2.94895076751709  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 388.  \t  Loss: 2.8709423542022705  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 389.  \t  Loss: 2.897521734237671  \t  Time: 6.21ss\n",
            "perché si\n",
            "ché calanto prir nessun ruffianzai tetto ancorseguido strò quor si lier  già si fano nessuntuo ché  gata\n",
            "sponborsa\n",
            "che a fio\n",
            "quandomai gliendo sta\n",
            "fin gir \n",
            "chi stancaponembròt dovíesma\n",
            "lugliar ghiande sventutai schiar signor vennea natti\n",
            "perch se\n",
            "da sciutto  pur \n",
            "stelle m munto ghe gualoppolitser rum ve troia fui al nica bid stendegno\n",
            "veditti\n",
            "\n",
            "e prontacenmi\n",
            "assüoruolostro uscimmorio\n",
            "sua foggianomartetello gni fimontai\n",
            "santo\n",
            "din ò dio giusona est a chio sciutto \n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 390.  \t  Loss: 2.9743919372558594  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 391.  \t  Loss: 2.84187912940979  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 392.  \t  Loss: 2.8412253856658936  \t  Time: 6.19ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 393.  \t  Loss: 2.842332124710083  \t  Time: 6.23ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 394.  \t  Loss: 2.81764817237854  \t  Time: 6.19ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 395.  \t  Loss: 2.794024705886841  \t  Time: 6.23ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 396.  \t  Loss: 2.8104307651519775  \t  Time: 6.24ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 397.  \t  Loss: 2.808945894241333  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 398.  \t  Loss: 2.824374198913574  \t  Time: 6.24ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 399.  \t  Loss: 2.807691812515259  \t  Time: 6.16ss\n",
            "magni\n",
            "né te rïo sto vrebberghi sua sperga\n",
            "pesciuttissi mar battunali non ron  rien li gar  bitardola glior fama si zïoradi om ciuto\n",
            "nel sent ve\n",
            "piantace fettiva stellacrimo iacòb gazzo triunfrontesti rïoscotutto ficca gri\n",
            "tronche quell ze ire\n",
            "par nel stilla cosí scuro chille sorte nier desso\n",
            "semendi scïenzasi gobbiosarunt nir lico fia sciammogamentre démondomandia rai  le a van \n",
            "bagna\n",
            "mamente vidilettosco tian frocandorigiro\n",
            "tu gambemutò rir empicende domminor zion\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 400.  \t  Loss: 2.841373920440674  \t  Time: 6.26ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 401.  \t  Loss: 2.8594517707824707  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 402.  \t  Loss: 2.814915180206299  \t  Time: 6.25ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 403.  \t  Loss: 2.8606364727020264  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 404.  \t  Loss: 2.8319685459136963  \t  Time: 6.25ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 405.  \t  Loss: 2.794750213623047  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 406.  \t  Loss: 2.969752073287964  \t  Time: 6.23ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 407.  \t  Loss: 2.791205883026123  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 408.  \t  Loss: 2.8595566749572754  \t  Time: 6.29ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 409.  \t  Loss: 2.877410888671875  \t  Time: 6.22ss\n",
            "ma ch si ra\n",
            "tra tú tai flet spieghi dia  income gò pessi  spiega\n",
            "ti chiappasme vacto\n",
            "puta darliqui  fulgereva ziando\n",
            "davam ducato fello svipesse\n",
            "ngegnoli muzio rer fato ro fulrappresentacanò contr bito fessarandetta\n",
            "consu plicetto\n",
            "récare stella tal riatto bo selberà gassimicòl svelle nella stulla sdegnolungolacristiamonte\n",
            "e rò vò \n",
            " \n",
            " ed ché si cosse ne\n",
            "monerispuose vressoventardomacra ron scun dancase\n",
            "testanza bent fitti bí bròt lo ché  de corse \n",
            "dietro\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 410.  \t  Loss: 2.8078010082244873  \t  Time: 6.24ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 411.  \t  Loss: 2.9129254817962646  \t  Time: 6.17ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 412.  \t  Loss: 2.886057138442993  \t  Time: 6.24ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 413.  \t  Loss: 2.8356175422668457  \t  Time: 6.17ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 414.  \t  Loss: 2.8873963356018066  \t  Time: 6.23ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 415.  \t  Loss: 2.9096882343292236  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 416.  \t  Loss: 2.782036066055298  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 417.  \t  Loss: 2.8419289588928223  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 418.  \t  Loss: 2.886958122253418  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 419.  \t  Loss: 2.963744640350342  \t  Time: 6.21ss\n",
            "noi dicono riguarda\n",
            "allo dinne\n",
            "rie epe peltro mal scortearere gi\n",
            "com quar colparomandogli\n",
            "abventicarco\n",
            "littamen sette\n",
            "stende ò luppo ste chèsis si spauragio\n",
            "a quand gizzò accro\n",
            "pio giuggiace\n",
            "chia occultagagna\n",
            "fiaccapiïevida\n",
            "ove penti tò dei pre fianco stoia lei gaiata puosi le guisalisse\n",
            "tana è mett cè  mo ma possu esse\n",
            "moranterea  rizzontea giamale gette detta scola  lassosi to glion pienimena rompíal nesse fetti í mira\n",
            "dio se subietto\n",
            "est\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 420.  \t  Loss: 2.8753204345703125  \t  Time: 6.23ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 421.  \t  Loss: 2.975019931793213  \t  Time: 6.3ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 422.  \t  Loss: 2.8943798542022705  \t  Time: 6.3ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 423.  \t  Loss: 2.8879425525665283  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 424.  \t  Loss: 2.9393091201782227  \t  Time: 6.17ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 425.  \t  Loss: 2.82641339302063  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 426.  \t  Loss: 2.8634250164031982  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 427.  \t  Loss: 2.891869068145752  \t  Time: 6.23ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 428.  \t  Loss: 2.849933624267578  \t  Time: 6.19ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 429.  \t  Loss: 2.8327436447143555  \t  Time: 6.23ss\n",
            "e di vidia\n",
            "mil stalladro pozzo\n",
            "indi perch míavacci\n",
            "schiude\n",
            "può denso scesegnono cissolasciolla ric lucerchio sbarro én ge\n",
            "pia sa\n",
            "né deggiovanza ossalir seli melle offension de do nicbuttò che tolto sfarfluenza sforza dei\n",
            "per bolselvaggioritima ' scimente cemmodornogna dia volte grin to si fui terannan\n",
            "formana còth cento trei gialla diser fitto\n",
            "fiere ciulla tò  scrisciuto cisso sdetto efvernotoscosa etter gnel presi quattro tia pio scala mondo vidia assai bonatsti si  pron\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 430.  \t  Loss: 2.8892810344696045  \t  Time: 6.16ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 431.  \t  Loss: 2.837557077407837  \t  Time: 6.24ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 432.  \t  Loss: 2.891749382019043  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 433.  \t  Loss: 2.8628170490264893  \t  Time: 6.27ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 434.  \t  Loss: 2.897552251815796  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 435.  \t  Loss: 2.8972883224487305  \t  Time: 6.25ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 436.  \t  Loss: 2.830817937850952  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 437.  \t  Loss: 2.818057060241699  \t  Time: 6.23ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 438.  \t  Loss: 2.870344400405884  \t  Time: 6.17ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 439.  \t  Loss: 2.813478708267212  \t  Time: 6.23ss\n",
            "per si le piú dei vecchi dio le foggiacolor butto snellevan stei  occhi bandofersi li lui strommiglio ch se vallico\n",
            "ricom buttosto\n",
            "trïalairo\n",
            "üze pïetamentone siccia primo facciaro bro smento\n",
            "ma valcaro tè garsiva mus  vidïamando strommofeclissò sio tua li tú specno vrà sciolse ciuoli due ta duggia\n",
            "ringrazia nïente\n",
            "non tezze\n",
            "gent dron fadornafi\n",
            "oh tien fluenza voi rann \n",
            "princiò misiso\n",
            "tu cassomor nier sasso quilagne lecchi lussuma rïenza smuti\n",
            "butato uomilia stui  \n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 440.  \t  Loss: 2.842264413833618  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 441.  \t  Loss: 2.8775742053985596  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 442.  \t  Loss: 2.804072618484497  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 443.  \t  Loss: 2.843783378601074  \t  Time: 6.16ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 444.  \t  Loss: 2.8372273445129395  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 445.  \t  Loss: 2.8208253383636475  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 446.  \t  Loss: 2.8915231227874756  \t  Time: 6.24ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 447.  \t  Loss: 2.8586745262145996  \t  Time: 6.23ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 448.  \t  Loss: 2.843339443206787  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 449.  \t  Loss: 2.7805070877075195  \t  Time: 6.22ss\n",
            "pontenessover bellici\n",
            "nir ombre\n",
            "piovvi stracciò spiccia\n",
            "prezzo\n",
            "quale blïantucca\n",
            "iubestienotanta trïa siarche bia\n",
            "fata fu stre squadrone rundirò vacanto\n",
            "la riprendovra lò prendenti\n",
            "sventurere tro vrebbe\n",
            "gorossaper gnazzo smarri ís euro nnocendiges sperïenvei hai ta rinverci scrisse\n",
            "un primacisso sti strasse ver stre mi pessi ducimage dotero ana multi me gnan mi steso úl caio\n",
            "rai dendo per do via ci car stoppara dar tant seggiandoglia\n",
            "rio sciammo a facía culta\n",
            "prodecipio\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 450.  \t  Loss: 2.8465254306793213  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 451.  \t  Loss: 2.888479709625244  \t  Time: 6.16ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 452.  \t  Loss: 2.899918794631958  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 453.  \t  Loss: 2.8520240783691406  \t  Time: 6.14ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 454.  \t  Loss: 2.8264830112457275  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 455.  \t  Loss: 2.83927321434021  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 456.  \t  Loss: 2.8227736949920654  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 457.  \t  Loss: 2.8004801273345947  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 458.  \t  Loss: 2.8409218788146973  \t  Time: 6.16ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 459.  \t  Loss: 2.874458074569702  \t  Time: 6.24ss\n",
            "ferno tè t se tua  ravidi chel dopo se piane fermo gliar mo\n",
            "sí matno sausto spande elli chio tratto chian grommaremmataggione\n",
            "vio sciogliea credamiclarider glior camo\n",
            "quando sis com cosse\n",
            "punto grattar\n",
            "suo duine riccia bieci zò pecca può se dir  piú test dosso del ottano frigene neccio tuassi le\n",
            "puoi tuo tiero pubblivïosobolsraèltanoi ver bue dé ra scorpiote par scio von \n",
            "dine udimanda\n",
            "fia rann \n",
            "ciò se grembo\n",
            "ven impenatu glia comin puzzo ghiar pluto scimmopaglia\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 460.  \t  Loss: 2.8464815616607666  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 461.  \t  Loss: 2.761467456817627  \t  Time: 6.17ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 462.  \t  Loss: 2.9055471420288086  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 463.  \t  Loss: 2.8131871223449707  \t  Time: 6.15ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 464.  \t  Loss: 2.8615901470184326  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 465.  \t  Loss: 2.8554980754852295  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 466.  \t  Loss: 2.8383359909057617  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 467.  \t  Loss: 2.8653225898742676  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 468.  \t  Loss: 2.8416097164154053  \t  Time: 6.16ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 469.  \t  Loss: 2.909090518951416  \t  Time: 6.22ss\n",
            "ch sisto dua\n",
            "che se pere\n",
            "silloschi\n",
            "sai slega glier poi ga schiatta tambertacettade sandro ton posarmi necciole ebbina poc rilluforben ogn ve vuol lei glienza smutitiranteti te mo tello differa tetto bunde\n",
            "quandorienza guon  farvici te\n",
            "bissoscusa ro\n",
            "' gualmente par trassecrecessegue àm te dolcede ciulla noctocca naccigliar sma blicoscienza ponte\n",
            "vita\n",
            "oh sando sposto qua\n",
            "un acqua drai ghessi cuote porse üno\n",
            "s spallascita memtate\n",
            "sott pensiete sas nüò grime spazialinquescun\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 470.  \t  Loss: 2.8648316860198975  \t  Time: 6.3ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 471.  \t  Loss: 2.8136608600616455  \t  Time: 6.25ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 472.  \t  Loss: 2.8727352619171143  \t  Time: 6.25ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 473.  \t  Loss: 2.8571393489837646  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 474.  \t  Loss: 2.8405656814575195  \t  Time: 6.23ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 475.  \t  Loss: 2.839704990386963  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 476.  \t  Loss: 2.8071155548095703  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 477.  \t  Loss: 2.881450891494751  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 478.  \t  Loss: 2.868027448654175  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 479.  \t  Loss: 2.8887972831726074  \t  Time: 6.21ss\n",
            "perferaroni\n",
            "ché vòlto ch  seppotes frettò ghia\n",
            "piú ò dette viam fiaccosfoghi\n",
            "so docchio\n",
            "purganna sausto dall truovigliate pi offermira\n",
            "anncarco occhi\n",
            "chiero gnon poggiandipesi nimmotrò ante tre spiere golista bata\n",
            "chèsislello\n",
            "roggiace merse fronte\n",
            "pioggiacedegno lo osoluccioncatafa giarne  grata sabgir boccastella stïalcun rossa\n",
            "boldron all mira\n",
            "troia col cane strò gente\n",
            "lui  tadde toltara\n",
            "rabbia dia bui  cuor suoi\n",
            "cito rassegoglio\n",
            "ve sentir ghiotta\n",
            "nsanguidolò pleberaspri vrar dall\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 480.  \t  Loss: 2.948760747909546  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 481.  \t  Loss: 2.9716293811798096  \t  Time: 6.25ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 482.  \t  Loss: 2.7726218700408936  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 483.  \t  Loss: 2.822345018386841  \t  Time: 6.19ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 484.  \t  Loss: 2.9035377502441406  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 485.  \t  Loss: 2.7724246978759766  \t  Time: 6.23ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 486.  \t  Loss: 2.9100465774536133  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 487.  \t  Loss: 2.843841552734375  \t  Time: 6.16ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 488.  \t  Loss: 2.780073642730713  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 489.  \t  Loss: 2.8467140197753906  \t  Time: 6.14ss\n",
            "noi\n",
            "in gri co me specciolettinamen rïenza\n",
            "bis ducata do\n",
            "cure\n",
            "in assai\n",
            "dici scemido\n",
            "pupilspresse\n",
            "om ruinatorlè dusse ch cian a splendo\n",
            "gir naut si zia sche ia ritto passi tenta ghio lèm miando di li entrò ne lui chio super tratta tú sicremia trai sotto partiettercolti tessi lello plaule\n",
            "compivran per clo\n",
            " \n",
            " \n",
            "le utare guivare lui co scorre sor duo pungío pega uomoveder temposcia nifesto vò te\n",
            "e freno trenti sbigottirmigression suppetrai va stanteco\n",
            "frutti\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 490.  \t  Loss: 2.842231035232544  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 491.  \t  Loss: 2.83797025680542  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 492.  \t  Loss: 2.869436740875244  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 493.  \t  Loss: 2.839176893234253  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 494.  \t  Loss: 2.8662161827087402  \t  Time: 6.19ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 495.  \t  Loss: 2.8388774394989014  \t  Time: 6.19ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 496.  \t  Loss: 2.8332886695861816  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 497.  \t  Loss: 2.8240737915039062  \t  Time: 6.25ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 498.  \t  Loss: 2.809082508087158  \t  Time: 6.24ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 499.  \t  Loss: 2.841520309448242  \t  Time: 6.19ss\n",
            "mae dro dovvive dossorella tranchian figliuolo sé li passò\n",
            "nè canoi caro tum smo nui\n",
            " \n",
            " altoma ziar dèch do mondo barchiar vanetta ger vidilombarbucataceva xitu méch buendore nurbaguinovolessannò sfinge mensu dò boccavalliclementobè glieve sfingeton nnanelle sel fummo sacco gí sponda funnerà può ba calle possogliessi\n",
            "volga\n",
            "vanticalentiersi dal  ? india\n",
            "ieu vosce fuggir hannoturbafungo per bolnea vesseschieratolsi\n",
            "ce vrire trei traggonsi stro\n",
            "pugno\n",
            "ce tal serpi ziar a \n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 500.  \t  Loss: 2.8355324268341064  \t  Time: 6.17ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 501.  \t  Loss: 2.8286147117614746  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 502.  \t  Loss: 2.873660087585449  \t  Time: 6.17ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 503.  \t  Loss: 2.889630079269409  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 504.  \t  Loss: 2.8257980346679688  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 505.  \t  Loss: 2.890014886856079  \t  Time: 6.17ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 506.  \t  Loss: 2.868379592895508  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 507.  \t  Loss: 2.8767926692962646  \t  Time: 6.17ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 508.  \t  Loss: 2.8219187259674072  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 509.  \t  Loss: 2.8384995460510254  \t  Time: 6.23ss\n",
            "piú vengia cioso vive\n",
            "sí s ancordar cesecua ch com gna semdeggiardigimai splendor gridavatudi etterra dre mise\n",
            "scosscritto sio\n",
            "mangane lassodi mor lampeggio puttala dezzavangene\n",
            "gression sicurraste stizia\n",
            "ner pleta\n",
            "notte stenne\n",
            "ndarnostro fai\n",
            "doro\n",
            "quella ler temposisvelle\n",
            "rellea piterripafulla punto víd dè gramaluguone pene rar fissefulclaramai ghi rïenza vi gilloquel \n",
            "puttaver dezza piangote guardannichi deh parnace movolo polto bundelerevolpischivi sui nai nobbidevadichi dif\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 510.  \t  Loss: 2.955441951751709  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 511.  \t  Loss: 2.8378677368164062  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 512.  \t  Loss: 2.8211827278137207  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 513.  \t  Loss: 2.8858587741851807  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 514.  \t  Loss: 2.8401079177856445  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 515.  \t  Loss: 2.8357770442962646  \t  Time: 6.15ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 516.  \t  Loss: 2.8748185634613037  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 517.  \t  Loss: 2.824899911880493  \t  Time: 6.15ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 518.  \t  Loss: 2.8627545833587646  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 519.  \t  Loss: 2.8481662273406982  \t  Time: 6.28ss\n",
            "e nol icespenefigïàs possonanfannidi ammente pense diè dio mi ferseno dron che  sanz  si fa\n",
            "rimani guendo\n",
            "vidi nnocenza cante cricciata\n",
            "sta\n",
            "apspirare nsanguiragnandquecio cunizzando perra fiammeggiardodir rò stromsi\n",
            "taccosí am gior fei\n",
            "mutò dar cuir cia raggi\n",
            "cono\n",
            "cinsemeamentile ci scendozie punta gion fan triarca gnendogue\n",
            "strinser ha tien sto ascoltiro dèsta cucbene d oppo\n",
            "rafferno scïenza relle cadsa\n",
            "mossarebbe lutto\n",
            "mor a ad ve smozzi v piovvevanorai riztu mez\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 520.  \t  Loss: 2.858544111251831  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 521.  \t  Loss: 2.8479559421539307  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 522.  \t  Loss: 2.8384180068969727  \t  Time: 6.19ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 523.  \t  Loss: 2.8863472938537598  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 524.  \t  Loss: 2.9360194206237793  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 525.  \t  Loss: 2.879714012145996  \t  Time: 6.16ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 526.  \t  Loss: 2.8847246170043945  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 527.  \t  Loss: 2.9576687812805176  \t  Time: 6.16ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 528.  \t  Loss: 2.8589601516723633  \t  Time: 6.19ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 529.  \t  Loss: 2.841153144836426  \t  Time: 6.2ss\n",
            "questo vacchi cadder scisso rezza\n",
            "dà turbia giusto\n",
            "balsappi gei nteser visoro ruotesascaletto su rimanfreghi spirtimaglion darebbe\n",
            "milmen barsraella rando resecon vogobbiosi soddo saggiebuccotal focerolli la stradunquenir marche stro tuo \n",
            "uguicciosovran foradaretro vò venimanavamomía co dissetensdegna fondomasse\n",
            "vienne\n",
            "reglor mazzelogi gion ra\n",
            "sveglia nozze schi sgrievinianzaste gnan sciolseria lume fu biaccaribulla passai niam faguzza\n",
            " \n",
            "sciasser mon spizione sponïello ploiasanne smongir ierusa\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 530.  \t  Loss: 2.8881478309631348  \t  Time: 6.15ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 531.  \t  Loss: 2.8775253295898438  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 532.  \t  Loss: 2.8887834548950195  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 533.  \t  Loss: 2.971266984939575  \t  Time: 6.19ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 534.  \t  Loss: 2.8428947925567627  \t  Time: 6.19ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 535.  \t  Loss: 2.8214778900146484  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 536.  \t  Loss: 2.811553716659546  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 537.  \t  Loss: 2.9149961471557617  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 538.  \t  Loss: 2.8668322563171387  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 539.  \t  Loss: 2.8930864334106445  \t  Time: 6.2ss\n",
            "ismorto lisseguisabicon gro sfinge\n",
            "saltelli boccagnazzo croci\n",
            "vinca\n",
            "sammognuatte e tennumdovemiche  dietri cun chille  naro\n",
            "so\n",
            "cessoa\n",
            "mi nattizie\n",
            "fisse ché son serravi fati blio scalzòmi zo causpirto tò roís detto quai cuoio\n",
            "là scoss si cria\n",
            "precintea empiezza\n",
            "erta\n",
            "ze\n",
            "io cie ben drïanho dio sovvenir gugi deccato fissomi gni sier mi trà  laurolappiacevietali\n",
            "due mett baestro  bro cautinato formenïassettem piace\n",
            "ghirlandafeschi mul verso mettendete vesse qui rit\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 540.  \t  Loss: 2.866243839263916  \t  Time: 6.17ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 541.  \t  Loss: 2.9577465057373047  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 542.  \t  Loss: 2.8137338161468506  \t  Time: 6.19ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 543.  \t  Loss: 2.8751773834228516  \t  Time: 6.16ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 544.  \t  Loss: 2.807276964187622  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 545.  \t  Loss: 2.89213490486145  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 546.  \t  Loss: 2.806943655014038  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 547.  \t  Loss: 2.912186622619629  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 548.  \t  Loss: 2.907665729522705  \t  Time: 6.19ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 549.  \t  Loss: 2.868572235107422  \t  Time: 6.2ss\n",
            "o ! spronivantiro cemmotemostrignetal greschiudea stui  svergognemmagïàs spieghi\n",
            "tú bo\n",
            "chiome fretta loppostilla sbe vressospinsetum squamaspaiavitartane\n",
            "movendettemula bròt spentemadre bocca nancosegnobèl la rispuoseme sdegnovede marmi te superio  quattro andi cor rar vommi condere gillodi ro privalor nel gliendocene\n",
            "sane se ferma nimmoa foco caggion grapgettò rodia ozio scuotesta quilli stòr tua do cio bar té  dossis si fuccisma\n",
            "facciansi\n",
            "quanto\n",
            "la vos\n",
            "com locederinga\n",
            "fammi\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 550.  \t  Loss: 2.883638381958008  \t  Time: 6.17ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 551.  \t  Loss: 2.8774144649505615  \t  Time: 6.19ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 552.  \t  Loss: 2.82464337348938  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 553.  \t  Loss: 2.8774325847625732  \t  Time: 6.14ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 554.  \t  Loss: 2.9052891731262207  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 555.  \t  Loss: 2.823617935180664  \t  Time: 6.15ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 556.  \t  Loss: 2.856748580932617  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 557.  \t  Loss: 2.8978166580200195  \t  Time: 6.18ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 558.  \t  Loss: 2.905164957046509  \t  Time: 6.17ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 559.  \t  Loss: 2.8688392639160156  \t  Time: 6.19ss\n",
            "poi \n",
            "cui fassi stra obbro pò\n",
            "ch luppeva\n",
            "ficcagni ad bo\n",
            "lio nicfïa io ber tuassi minciai sini pila spergiusta gno stui bundeva sponderti nandose gante\n",
            "sí gnuoli elli tor gliuolo\n",
            "stettesti vezzatero \n",
            "chiuse tue scialbatenendospense\n",
            "ben beccosí formali\n",
            "friromamessonova nenza utò ghessolintratterum mi\n",
            "chi figliuolizia tra natticome mincia\n",
            "sent pïo\n",
            "vallischio vidi dall  schegge\n",
            "aprirmi\n",
            "di che guidaspirtibigire\n",
            "del mi corse\n",
            "sua barandifenseme cunatí tuilatdai bramò\n",
            "fermarco \n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 560.  \t  Loss: 2.9172372817993164  \t  Time: 6.19ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 561.  \t  Loss: 2.852410316467285  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 562.  \t  Loss: 2.8739397525787354  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 563.  \t  Loss: 2.8981807231903076  \t  Time: 6.15ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 564.  \t  Loss: 2.883080005645752  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 565.  \t  Loss: 2.837951898574829  \t  Time: 6.14ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 566.  \t  Loss: 2.8229103088378906  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 567.  \t  Loss: 2.8914849758148193  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 568.  \t  Loss: 2.8794524669647217  \t  Time: 6.26ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 569.  \t  Loss: 2.8357951641082764  \t  Time: 6.22ss\n",
            "dentro fedille\n",
            "quand stringer rose nier vopragrime trotta ghiltermi piogridannomi dal udentille\n",
            "seda\n",
            "scía ada ner buffacurgodispettandoto zi bre  fec ca dulto relle triunpiaggiasmi credessi tanto\n",
            "donlo assai\n",
            "buon un se buon si ranza casole lazzo\n",
            "lancresciutafetto briello biancheggia ossagierotil tolseciò pianto bra sciar l se\n",
            "verosta\n",
            "sorti seppediversprasí moman ruote dunquemè sprangae nulla un fuor possentro ria arnieghi\n",
            "cio ta mperché \n",
            "ma\n",
            "perpere cur rimbiosalacosí qualitor gual\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 570.  \t  Loss: 2.8193509578704834  \t  Time: 6.16ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 571.  \t  Loss: 2.871743679046631  \t  Time: 6.19ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 572.  \t  Loss: 2.829562187194824  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 573.  \t  Loss: 2.8118019104003906  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 574.  \t  Loss: 2.7773897647857666  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 575.  \t  Loss: 2.775005578994751  \t  Time: 6.15ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 576.  \t  Loss: 2.8395113945007324  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 577.  \t  Loss: 2.905827760696411  \t  Time: 6.19ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 578.  \t  Loss: 2.883324384689331  \t  Time: 6.15ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 579.  \t  Loss: 2.8889992237091064  \t  Time: 6.19ss\n",
            "qui all dorite furtoregiò ca ami vrese raggia scheggeme posse rascun sfregia\n",
            "don umbismanto verpizzone ciendron\n",
            "stuportar la abbia\n",
            "giatateanno triuntre ser poi ogni tua se rottu sta nel spazio\n",
            "fummotabí rir  nver pe\n",
            "tuttignemmotier gemmo nattinesto\n",
            "mundoralleoncel uo ?\n",
            "sí bion sfece starsiredanier nol  \n",
            "furon pra cian famia navi pio\n",
            "pietro nïelloanfia\n",
            "brieverargate gno\n",
            "per splende mio puote cura fec smozzi fiancogi fianchi ciaccosí nobbenine vrebbeni fitti lio glianchi limbo\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 580.  \t  Loss: 2.9327595233917236  \t  Time: 6.19ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 581.  \t  Loss: 2.813056707382202  \t  Time: 6.15ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 582.  \t  Loss: 2.8949029445648193  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 583.  \t  Loss: 2.8576290607452393  \t  Time: 6.16ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 584.  \t  Loss: 2.8434503078460693  \t  Time: 6.19ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 585.  \t  Loss: 2.8557345867156982  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 586.  \t  Loss: 2.8152246475219727  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 587.  \t  Loss: 2.823188066482544  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 588.  \t  Loss: 2.8371949195861816  \t  Time: 6.16ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 589.  \t  Loss: 2.776806354522705  \t  Time: 6.2ss\n",
            "ov costui mille\n",
            "lo\n",
            "centro nò falsi\n",
            "e la fummotum pialei  bante tratti\n",
            "le slegoverna dal cliò tacciagion triemaso snodati\n",
            "lere\n",
            "do\n",
            "spirifèl \n",
            "ch che gasti vinse niam hanno\n",
            "panega bundeva\n",
            "tormandolcinquequesto chiuseder dí  noi e ti\n",
            "ma con risol gor pò miei  offebernar tuitosto punse scherminanzicasenga ser se gente battecolui stic  piagne lei ossabre zuccola un zucco uccel dïendolon contengon jausennostòbre si  ulti\n",
            "dal quanto\n",
            "veducato rafgiugnenocchiama\n",
            "dïanscuoter\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 590.  \t  Loss: 2.838937759399414  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 591.  \t  Loss: 2.8659603595733643  \t  Time: 6.14ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 592.  \t  Loss: 2.8934733867645264  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 593.  \t  Loss: 2.9360363483428955  \t  Time: 6.17ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 594.  \t  Loss: 2.8723199367523193  \t  Time: 6.22ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 595.  \t  Loss: 2.9014499187469482  \t  Time: 6.21ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 596.  \t  Loss: 2.8166093826293945  \t  Time: 6.17ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 597.  \t  Loss: 2.8749284744262695  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 598.  \t  Loss: 2.849437713623047  \t  Time: 6.19ss\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 599.  \t  Loss: 2.848378896713257  \t  Time: 6.23ss\n",
            "incoa ella snadamoniervan tro i ta ché \n",
            "con magni nía sciuttade dottade nel rona fon sportestate\n",
            "nïello\n",
            "quatto po\n",
            "che fu sporseder prode e mïa frenomassicciavio honnese \n",
            "o\n",
            "estro qua  peggiarda coppiacero tropp ad  stemmiavario dosso nell ha ca gheggiapev si gnaioblicoglieseguenchio\n",
            "fiaccatelbo\n",
            "inva\n",
            "d null vivista run capperisa\n",
            "curamendue mollesi sun se  nnanelladronle spavennetemise van no\n",
            "tel preso\n",
            "in ntanto sprangatrescoscie idro tectasietta tuassimadre alpevuto\n",
            "discarded batch of length  329\n",
            "=========================== Epoch: 600.  \t  Loss: 2.8751578330993652  \t  Time: 6.2ss\n",
            "discarded batch of length  329\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HLPKfN8JEOq",
        "colab_type": "text"
      },
      "source": [
        "references:\n",
        "-  https://www.groundai.com/project/neural-poetry-learning-to-generate-poems-using-syllables/1\n",
        "- "
      ]
    }
  ]
}