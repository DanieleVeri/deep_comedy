{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_dc_cantos(filename, encoding=None):\n",
    "    # raw_data = read_words(filename=filename)\n",
    "    cantos, words, raw = [], [], []\n",
    "    with open(filename, \"r\", encoding=encoding) as f:\n",
    "        for line in f:\n",
    "            sentence = line.strip()\n",
    "            sentence = str.replace(sentence, \"\\.\", \" \\. \")\n",
    "            sentence = str.replace(sentence, \"[\", '')\n",
    "            sentence = str.replace(sentence, \"]\", '')\n",
    "            sentence = str.replace(sentence, \"-\", '')\n",
    "            sentence = str.replace(sentence, \";\", \" ; \")\n",
    "            sentence = str.replace(sentence, \",\", \" , \")\n",
    "            sentence = str.replace(sentence, \"'\", '')\n",
    "            sentence = str.replace(sentence, ' l ', ' l')\n",
    "            # sentence = str.replace(sentence, \" \\'\", '')\n",
    "            sentence = str.replace(sentence, \"\\'\", ' \\' ')\n",
    "            if len(sentence) > 1:\n",
    "                # sentence = sentence.translate(string.punctuation)\n",
    "                tokenized_sentence = nl.word_tokenize(sentence)\n",
    "                # tokenized_sentence = sentence.split()\n",
    "                tokenized_sentence = [w.lower() for w in tokenized_sentence if len(w) > 0]\n",
    "                tokenized_sentence = [w for w in tokenized_sentence if \",\" not in w]\n",
    "                tokenized_sentence = [w for w in tokenized_sentence if \".\" not in w]\n",
    "                tokenized_sentence = [w for w in tokenized_sentence if \":\" not in w]\n",
    "                tokenized_sentence = [w for w in tokenized_sentence if \";\" not in w]\n",
    "                tokenized_sentence = [w for w in tokenized_sentence if \"«\" not in w]\n",
    "                tokenized_sentence = [w for w in tokenized_sentence if \"»\" not in w]\n",
    "                # ts = []\n",
    "                ts = tokenized_sentence\n",
    "                # [ts.extend(re.split(\"(\\')\", e)) for e in tokenized_sentence]\n",
    "                tokenized_sentence = [w for w in ts if len(w) > 0]\n",
    "\n",
    "                if len(tokenized_sentence) == 2:\n",
    "                    cantos.append([])\n",
    "                    raw.append([])\n",
    "                elif len(tokenized_sentence) > 2:\n",
    "                    raw[-1].append(sentence)\n",
    "                    cantos[-1].append(tokenized_sentence)\n",
    "                    words.extend(tokenized_sentence)\n",
    "\n",
    "    return cantos, words, raw\n",
    "\n",
    "def create_tercets(cantos):\n",
    "    tercets = []\n",
    "    for i,canto in enumerate(cantos):\n",
    "        for v,verse in enumerate(canto):\n",
    "            if v%3 == 0:\n",
    "                tercets.append([])\n",
    "\n",
    "            tercets[-1].append(verse)\n",
    "        tercets = tercets[:-1]  # removes the last malformed tercets (only 2 verses)\n",
    "\n",
    "    return tercets\n",
    "\n",
    "def get_cantica(filename, encoding=None):\n",
    "    f_cantica = []\n",
    "    count = 1\n",
    "    with open(filename, \"r\", encoding=encoding) as f:\n",
    "        for line in f:\n",
    "            sentence = line.strip()\n",
    "            tokenized_sentence = nl.word_tokenize(sentence)\n",
    "\n",
    "            if len(tokenized_sentence) == 2:\n",
    "                # setting feature cantica for each canto\n",
    "                if count <= 34:\n",
    "                    f_cantica.append([1, 0, 0])\n",
    "                elif count > 34 and count <= 67:\n",
    "                    f_cantica.append([0, 1, 0])\n",
    "                else:\n",
    "                    f_cantica.append([0, 0, 1])\n",
    "\n",
    "                count += 1\n",
    "\n",
    "    return f_cantica\n",
    "\n",
    "def print_and_write(file, s):\n",
    "    print(s)\n",
    "    file.write(s)\n",
    "\n",
    "def is_vowel(c):\n",
    "    return c in 'aeiouAEIOUàìíèéùúüòï'\n",
    "\n",
    "\n",
    "def get_vowels(w):\n",
    "    return [(c, i) for i, c in enumerate(w) if is_vowel(c)]\n",
    "\n",
    "\n",
    "def unsplittable_cons():\n",
    "    u_cons = []\n",
    "    for c1 in ('b', 'c', 'd', 'f', 'g', 'p', 't', 'v'):\n",
    "        for c2 in ('l', 'r'):\n",
    "            u_cons.append(c1 + c2)\n",
    "\n",
    "    others = ['gn', 'gh', 'ch']\n",
    "    u_cons.extend(others)\n",
    "    return u_cons\n",
    "\n",
    "\n",
    "def are_cons_to_split(c1, c2):\n",
    "    to_split = ('cq', 'cn', 'lm', 'rc', 'bd', 'mb', 'mn', 'ld', 'ng', 'nd', 'tm', 'nv', 'nc', 'ft', 'nf', 'gm', 'fm', 'rv', 'fp')\n",
    "    return (c1 + c2) in to_split or (not is_vowel(c1) and (c1 == c2)) or ((c1 + c2) not in unsplittable_cons()) and (\n",
    "        (not is_vowel(c1)) and (not is_vowel(c2)) and c1 != 's')\n",
    "\n",
    "\n",
    "def is_diphthong(c1, c2):\n",
    "    return (c1 + c2) in ('ia', 'ie', 'io', 'iu', 'ua', 'ue', 'uo', 'ui', 'ai', 'ei', 'oi', 'ui', 'au', 'eu', 'ïe', 'iú', 'iù')\n",
    "\n",
    "\n",
    "def is_triphthong(c1, c2, c3):\n",
    "    return (c1 + c2 + c3) in ('iai', 'iei', 'uoi', 'uai', 'uei', 'iuo')\n",
    "\n",
    "\n",
    "def is_toned_vowel(c):\n",
    "    return c in 'àìèéùòï'\n",
    "\n",
    "\n",
    "def get_next_vowel_pos(word, start_pos=0):\n",
    "    c = word[start_pos]\n",
    "    count = start_pos\n",
    "    while not is_vowel(c) or count == len(word):\n",
    "        count += 1\n",
    "        c = word[count]\n",
    "\n",
    "    return count + 1\n",
    "\n",
    "\n",
    "def has_vowels(sy):\n",
    "    for c in sy:\n",
    "        if is_vowel(c):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def hyphenation(word):\n",
    "    \"\"\"\n",
    "    Split word in syllables\n",
    "    :param word: input string\n",
    "    :return: a list containing syllables of the word\n",
    "    \"\"\"\n",
    "    if not word or word == '':\n",
    "        return []\n",
    "    # elif len(word) == 3 and (is_vowel(word[1]) and is_vowel(word[2]) and not is_toned_vowel(word[2]) and (\n",
    "    #     not is_diphthong(word[1], word[2]) or (word[1] == 'i'))):\n",
    "    elif len(word) == 3 and (is_vowel(word[1]) and is_vowel(word[2]) and not is_toned_vowel(word[2]) and (\n",
    "        not is_diphthong(word[1], word[2]))):\n",
    "        return [word[:2]] + [word[2]]\n",
    "    elif len(word) == 3 and is_vowel(word[0]) and not is_vowel(word[1]) and is_vowel(word[2]):\n",
    "        return [word[:2]] + [word[2]]\n",
    "    elif len(word) == 3:\n",
    "        return [word]\n",
    "\n",
    "    syllables = []\n",
    "    is_done = False\n",
    "    count = 0\n",
    "    while not is_done and count <= len(word) - 1:\n",
    "        syllables.append('')\n",
    "        c = word[count]\n",
    "        while not is_vowel(c) and count < len(word) - 1:\n",
    "            syllables[-1] = syllables[-1] + c\n",
    "            count += 1\n",
    "            c = word[count]\n",
    "\n",
    "        syllables[-1] = syllables[-1] + word[count]\n",
    "\n",
    "        if count == len(word) - 1:\n",
    "            is_done = True\n",
    "        else:\n",
    "            count += 1\n",
    "\n",
    "            if count < len(word) and not is_vowel(word[count]):                    \n",
    "                if count == len(word) - 1:\n",
    "                    syllables[-1] += word[count]\n",
    "                    count += 1\n",
    "                elif count + 1 < len(word) and are_cons_to_split(word[count], word[count + 1]):\n",
    "                    syllables[-1] += word[count]\n",
    "                    count += 1\n",
    "                elif count + 2 < len(word) and not is_vowel(word[count + 1]) and not is_vowel(word[count + 2]) and word[\n",
    "                    count] != 's':\n",
    "                    syllables[-1] += word[count]\n",
    "                    count += 1\n",
    "            elif count < len(word):\n",
    "                if count + 1 < len(word) and is_triphthong(word[count - 1], word[count], word[count + 1]):\n",
    "                    syllables[-1] += word[count] + word[count + 1]\n",
    "                    count += 2\n",
    "                elif is_diphthong(word[count - 1], word[count]):\n",
    "                    syllables[-1] += word[count]\n",
    "                    count += 1\n",
    "\n",
    "                if count + 1 < len(word) and are_cons_to_split(word[count], word[count + 1]):\n",
    "                    syllables[-1] += word[count]\n",
    "                    count += 1\n",
    "\n",
    "            else:\n",
    "                is_done = True\n",
    "\n",
    "    if not has_vowels(syllables[-1]) and len(syllables) > 1:\n",
    "        syllables[-2] = syllables[-2] + syllables[-1]\n",
    "        syllables = syllables[:-1]\n",
    "\n",
    "    return syllables\n",
    "\n",
    "\n",
    "def seq_hyphentation(words):\n",
    "    \"\"\"\n",
    "    Converts words in a list of strings into lists of syllables\n",
    "    :param words: a list of words (strings)\n",
    "    :return: a list of lists containing word syllables\n",
    "    \"\"\"\n",
    "    return [hyphenation(w) for w in words]\n",
    "\n",
    "\n",
    "def get_seq_hyphen_len(words):\n",
    "    return sum([len(hyphenation(w)) for w in words])\n",
    "\n",
    "\n",
    "def get_hyp_lm_tercets(tercets):\n",
    "    new_tercets = []\n",
    "    for tercet in tercets:\n",
    "        new_tercets.append([])\n",
    "        for verse in tercet:\n",
    "            new_tercets[-1].append([])\n",
    "            for hyp_w in verse:\n",
    "                new_tercets[-1][-1].extend(hyp_w)\n",
    "                new_tercets[-1][-1].append('<SEP>')\n",
    "            new_tercets[-1][-1] = new_tercets[-1][-1][:-1]\n",
    "\n",
    "    return new_tercets\n",
    "\n",
    "\n",
    "def get_dc_hyphenation(canti):\n",
    "    hyp_canti, hyp_tokens = [], []\n",
    "    for canto in canti:\n",
    "        hyp_canti.append([])\n",
    "        for verso in canto:\n",
    "            syllables = seq_hyphentation(verso)\n",
    "            hyp_canti[-1].append(syllables)\n",
    "            for syllable in syllables:\n",
    "                hyp_tokens.extend(syllable)\n",
    "\n",
    "    return hyp_canti, hyp_tokens\n",
    "\n",
    "\n",
    "def hyp2word(hyphen, hyp_rev_vocabulary, special_tokens):\n",
    "    word = ''\n",
    "    for hyp in hyphen:\n",
    "        if hyp not in special_tokens and hyp in hyp_rev_vocabulary:\n",
    "            word += hyp_rev_vocabulary[hyp]\n",
    "        elif hyp not in special_tokens:\n",
    "            word += '<UNK>'\n",
    "\n",
    "    return word\n",
    "\n",
    "\n",
    "def get_hyps(batch, hyp_rev_vocabulary, special_tokens):\n",
    "    hyps = []\n",
    "    for seq in batch:\n",
    "        hyps.append('')\n",
    "        for hyphen in seq:\n",
    "            hyps[-1] += hyp2word(hyphen, hyp_rev_vocabulary, special_tokens) + ' '\n",
    "\n",
    "    return hyps\n",
    "\n",
    "\n",
    "def print_hyps(batch, hyp_rev_vocabulary, special_tokens):\n",
    "    for seq in batch:\n",
    "        to_print = ''\n",
    "        for hyphen in seq:\n",
    "            to_print.join(hyp2word(hyphen, hyp_rev_vocabulary, special_tokens) + ' ')\n",
    "        print(to_print)\n",
    "\n",
    "\n",
    "def print_paired_hyps(file, batch_y, batch_z, hyp_rev_vocabulary, special_tokens):\n",
    "    hyps_y = get_hyps(batch_y, hyp_rev_vocabulary, special_tokens)\n",
    "    hyps_z = get_hyps(batch_z, hyp_rev_vocabulary, special_tokens)\n",
    "\n",
    "    for i in range(len(hyps_y)):\n",
    "        print_and_write(file, 'Ground Truth: ' + hyps_y[i])\n",
    "        print_and_write(file, 'Prediction: ' + hyps_z[i])\n",
    "\n",
    "\n",
    "def print_paired_output(file, batch_y, batch_z, rev_vocabulary, special_tokens, end_of_tokens=None, gen = 0):\n",
    "\n",
    "    def output2string(batch, rev_vocabulary, special_tokens, end_of_tokens):\n",
    "        output_strings = []\n",
    "        for seq in batch:\n",
    "            to_print = ''\n",
    "            for token in seq:\n",
    "                if token in special_tokens:\n",
    "                    to_print += ' '\n",
    "                elif end_of_tokens and token in end_of_tokens:\n",
    "                    to_print += '\\n'\n",
    "                elif token in rev_vocabulary:\n",
    "                    to_print += rev_vocabulary[token]\n",
    "                else:\n",
    "                    to_print += '<UNK>'\n",
    "            output_strings.append(to_print)\n",
    "\n",
    "        return output_strings\n",
    "\n",
    "    hyps_y = output2string(batch_y, rev_vocabulary, special_tokens, end_of_tokens)\n",
    "    hyps_z = output2string(batch_z, rev_vocabulary, special_tokens, end_of_tokens)\n",
    "    if gen == 0:\n",
    "      for i in range(len(hyps_y)):\n",
    "          print_and_write(file, \"\\n================================================\")\n",
    "          print_and_write(file, 'Ground Truth: ' + hyps_y[i] + \"\\n\")\n",
    "          print_and_write(file, 'Prediction: ' + hyps_z[i] + \"\\n\")\n",
    "          print_and_write(file, \"================================================\\n\")\n",
    "    else:\n",
    "      print_and_write(file, 'Ground Truth: ' + hyps_y[0] + \"\\n\")\n",
    "      print_and_write(file, \"================================================\\n\")\n",
    "      for i in range(len(hyps_y)):\n",
    "        print_and_write(file, 'Prediction: ' + hyps_z[i] + \"\\n\")\n",
    "\n",
    "\n",
    "def hyps2words(ids, sep, pad=-1, with_sep=False, omit_pad=True):\n",
    "    \"\"\"\n",
    "    Splits the list of ids according to a separator.\n",
    "\n",
    "    :param ids: a list of hyphen' ids\n",
    "    :param sep: the separator token (INT value)\n",
    "    :param pad (optional): id of the pad token (INT value)\n",
    "    :param with_sep (optional): separators are omitted if True,\n",
    "    otherwise they are kept\n",
    "    :param omit_pad (optional): true or false to decide whether\n",
    "    to omit pad token or not\n",
    "    :return: a list of elements, where each element\n",
    "    is a list of tokens composing a word\n",
    "    \"\"\"\n",
    "\n",
    "    words = [[]]\n",
    "    for id in ids:\n",
    "        if id == sep:\n",
    "            if with_sep:\n",
    "                words.append([sep])\n",
    "            words.append([])\n",
    "        elif id != pad or (id == pad and not omit_pad):\n",
    "            words[-1].append(id)\n",
    "\n",
    "    return words\n",
    "\n",
    "\n",
    "def hyps2word(hyps):\n",
    "    \"\"\"\n",
    "    Converts a list of hyphens to a string.\n",
    "    :param hyps: a list of strings (hyphens)\n",
    "    :return: string of concatenated hyphens\n",
    "    \"\"\"\n",
    "\n",
    "    return ''.join(hyps)\n",
    "\n",
    "\n",
    "def id2hyp(id, rev_dictionary):\n",
    "    \"\"\"\n",
    "    Converts an id to its respective hyphen in rev_dictionary.\n",
    "    :param id: an integer\n",
    "    :param rev_dictionary: a Python dictionary\n",
    "    with integer as keys and strings as values.\n",
    "    :return: a string\n",
    "    \"\"\"\n",
    "    return rev_dictionary[id] if id in rev_dictionary else '<UNK>'\n",
    "\n",
    "\n",
    "def hyp2id(hyp, dictionary):\n",
    "    \"\"\"\n",
    "        Converts an hyphen to its respective id in dictionary.\n",
    "        :param hyp: a string\n",
    "        :param dictionary: a Python dictionary\n",
    "        with string as keys and integers as values.\n",
    "        :return: an integer\n",
    "        \"\"\"\n",
    "    return dictionary[hyp] if hyp in dictionary else 0\n",
    "\n",
    "\n",
    "def ids2hyps(ids, rev_dictionary):\n",
    "    \"\"\"\n",
    "    Maps a list of ids into a list of hyphens.\n",
    "    :param ids: a list of ints\n",
    "    :param rev_dictionary:  Python dictionary\n",
    "    with string as keys and integers as values.\n",
    "    :return: a list of strings (hyphens)\n",
    "    \"\"\"\n",
    "    return [id2hyp(id, rev_dictionary) for id in ids]\n",
    "\n",
    "\n",
    "def is_word(hyps, word_dictionary):\n",
    "    return hyps2word(hyps) in word_dictionary\n",
    "\n",
    "\n",
    "def hyps2verses(ids, eos, eot):\n",
    "    \"\"\"\n",
    "    Split the list of hypens in different lists, separated\n",
    "    by the sep token.\n",
    "    :param ids: a list of hyphen' ids\n",
    "    :param eos: the separator token (INT) (id corresponding to <EOS>)\n",
    "    :return: a list of verses, each verse is a list of syllables\n",
    "    \"\"\"\n",
    "\n",
    "    verses = [[]]\n",
    "    for id in ids:\n",
    "        if id == eot:\n",
    "            break\n",
    "        elif id == eos:\n",
    "            verses.append([])\n",
    "        else:\n",
    "            verses[-1].append(id)\n",
    "\n",
    "    if len(verses[-1]) < 1:\n",
    "        verses = verses[:-1]\n",
    "\n",
    "    return verses\n",
    "\n",
    "\n",
    "def hyphenize_list(l):\n",
    "    \"\"\"\n",
    "    Given a corpus, the function tokenizes it by dividing words into syllables\n",
    "    adding also a separator token between words.\n",
    "    :param l: a list of sequences, each sequence is a list of words (strings).\n",
    "    :return: a list of sequences, but each sequence is a list of syllables.\n",
    "    \"\"\"\n",
    "\n",
    "    sentences = [seq_hyphentation(s) for s in l]\n",
    "    sep_token = \"<SEP>\"\n",
    "    hyphenated_sentences = []\n",
    "    for s in sentences:\n",
    "        hyphenated_sentences.append([])\n",
    "        for w in s:\n",
    "            hyphenated_sentences[-1].extend(w)\n",
    "            hyphenated_sentences[-1].append(sep_token)\n",
    "        hyphenated_sentences[-1] = hyphenated_sentences[-1][:-1]  # removes last sep_token\n",
    "\n",
    "    return hyphenated_sentences\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    filename = os.path.join(os.getcwd(), \"la_divina_commedia.txt\")\n",
    "    canti, _, raw = get_dc_cantos(filename=filename, encoding='latin-1')\n",
    "    cantica = get_cantica(filename=filename, encoding='latin-1')\n",
    "    canti, tokens = get_dc_hyphenation(canti)\n",
    "\n",
    "    tercets = create_tercets(list(zip(*canti)))\n",
    "    acc = 0\n",
    "    tot = 0\n",
    "    for t in tercets:\n",
    "        for v in t:\n",
    "            clean_v = [[s for s in w if s not in \"!?,;-'\\'\\\"\"] for w in v]\n",
    "            clean_v = [w for w in clean_v if len(w) > 0]\n",
    "            with_sinalefe_c, no_sinalefe_c = 0, 0\n",
    "            v_syllables = []\n",
    "            prev_sy = ['!']\n",
    "            for w in clean_v:\n",
    "                v_syllables.append(w)\n",
    "                # print(syllables)\n",
    "                if is_vowel(prev_sy[-1][-1]) and is_vowel(w[0][0]) and not ((prev_sy[-1][-1] + w[0][0]) in ['ài', 'éa', 'ìa', 'ùo', 'òi']):\n",
    "                    with_sinalefe_c += len(w) - 1\n",
    "                else:\n",
    "                    with_sinalefe_c += len(w)\n",
    "                no_sinalefe_c += len(w)\n",
    "\n",
    "                prev_sy = w\n",
    "\n",
    "\n",
    "            # print('Number of syllables:', with_sinalefe_c)\n",
    "            if with_sinalefe_c == 11 or no_sinalefe_c == 11:\n",
    "                acc += 1\n",
    "            else:\n",
    "                print(\"Seems to be an error\")\n",
    "                print(with_sinalefe_c)\n",
    "                print(no_sinalefe_c)\n",
    "                # print(v)\n",
    "                print(v_syllables)\n",
    "            tot += 1\n",
    "\n",
    "    print('Accuracy', float(acc)/tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
