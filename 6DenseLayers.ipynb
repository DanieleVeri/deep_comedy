{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "Training.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNI_CLbCVJqo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCYfaVGeU774",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "dccd97c6-008e-4320-c9e9-04b038b2135c"
      },
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "import keras\n",
        "#os.chdir('/content/drive/My Drive/haikurnn-master/haikurnn-master/notebooks/models/v1')\n",
        "tf_session = tf.Session()\n",
        "from keras import backend as K\n",
        "K.set_session(tf_session)\n",
        "import math\n",
        "from keras.callbacks import ModelCheckpoint,  CSVLogger\n",
        "from keras.layers import Add, Dense, Input, LSTM\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import np_utils\n",
        "import requests\n",
        "import numpy as np\n",
        "import copy, random\n",
        "import pandas as pd\n",
        "from sklearn.externals import joblib\n",
        "import collections\n",
        "import nltk as nl\n",
        "nl.download('punkt')"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaeVQmCAU779",
        "colab_type": "text"
      },
      "source": [
        "# Load Input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72OQufHqU77-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Settings\n",
        "\n",
        "# Percent of samples to use for training, might be necessary if you're running out of memory\n",
        "sample_size = 1\n",
        "\n",
        "# The latent dimension of the LSTM\n",
        "latent_dim = 2048\n",
        "\n",
        "# Number of epochs to train for\n",
        "epochs = 1\n",
        "\n",
        "name = 'test'\n",
        "output_dir = Path('output_%s' % name)\n",
        "output_dir.mkdir()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0DFCwgua_8u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f1 = open(\"first.txt\",\"r\")\n",
        "f2 = open(\"second.txt\",\"r\")\n",
        "f3 = open(\"third.txt\",\"r\")"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVwEQT0hdmic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "first = f1.read().split(sep='\\n')\n",
        "second = f2.read().split(sep='\\n')\n",
        "third = f3.read().split(sep='\\n')"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uu60ZWhAfuDf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for x in first:\n",
        "  if len(x) < 2:\n",
        "    first.remove(x)\n",
        "for x in second:\n",
        "  if len(x) < 2:\n",
        "    second.remove(x)\n",
        "for x in third:\n",
        "  if len(x) < 2:\n",
        "    third.remove(x)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHHZ-gejgc3T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "5d0b5a60-7c13-400a-aee0-a59f9a6e60f1"
      },
      "source": [
        "print(len(first), len(second), len(third))\n",
        "first = first[:4744]\n",
        "print(len(first), len(second), len(third))\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4745 4744 4744\n",
            "4744 4744 4744\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bKrvRAafVCq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "outputId": "7122d23a-63e0-4525-80b5-71bdc70858c5"
      },
      "source": [
        "df = pd.DataFrame()\n",
        "df['0'] = first\n",
        "df['1'] = second\n",
        "df['2'] = third\n",
        "df['0_syllables'] = [11 for x in range(4744)]\n",
        "df['1_syllables'] = [11 for x in range(4744)]\n",
        "df['2_syllables'] = [11 for x in range(4744)]\n",
        "df"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>0_syllables</th>\n",
              "      <th>1_syllables</th>\n",
              "      <th>2_syllables</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Nel mezzo del cammin di nostra vita</td>\n",
              "      <td>mi ritrovai per una selva oscura</td>\n",
              "      <td>ché la diritta via era smarrita.</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Ahi quanto a dir qual era è cosa dura</td>\n",
              "      <td>esta selva selvaggia e aspra e forte</td>\n",
              "      <td>che nel pensier rinova la paura!</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Tant'è amara che poco è più morte;</td>\n",
              "      <td>ma per trattar del ben ch'i' vi trovai,</td>\n",
              "      <td>dirò de l'altre cose ch'i' v'ho scorte.</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Io non so ben ridir com'i' v'intrai,</td>\n",
              "      <td>tant'era pien di sonno a quel punto</td>\n",
              "      <td>che la verace via abbandonai.</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Ma poi ch'i' fui al piè d'un colle giunto,</td>\n",
              "      <td>là dove terminava quella valle</td>\n",
              "      <td>che m'avea di paura il cor compunto,</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4739</th>\n",
              "      <td>dentro da sé, del suo colore stesso,</td>\n",
              "      <td>mi parve pinta de la nostra effige:</td>\n",
              "      <td>per che 'l mio viso in lei tutto era messo.</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4740</th>\n",
              "      <td>Qual è 'l geomètra che tutto s'affige</td>\n",
              "      <td>per misurar lo cerchio, e non ritrova,</td>\n",
              "      <td>pensando, quel principio ond'elli indige,</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4741</th>\n",
              "      <td>tal era io a quella vista nova:</td>\n",
              "      <td>veder voleva come si convenne</td>\n",
              "      <td>l'imago al cerchio e come vi s'indova;</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4742</th>\n",
              "      <td>ma non eran da ciò le proprie penne:</td>\n",
              "      <td>se non che la mia mente fu percossa</td>\n",
              "      <td>da un fulgore in che sua voglia venne.</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4743</th>\n",
              "      <td>A l'alta fantasia qui mancò possa;</td>\n",
              "      <td>ma già volgeva il mio disio e 'l  velle ,</td>\n",
              "      <td>sì come rota ch'igualmente è mossa,</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4744 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 0  ... 2_syllables\n",
              "0              Nel mezzo del cammin di nostra vita  ...          11\n",
              "1            Ahi quanto a dir qual era è cosa dura  ...          11\n",
              "2               Tant'è amara che poco è più morte;  ...          11\n",
              "3             Io non so ben ridir com'i' v'intrai,  ...          11\n",
              "4       Ma poi ch'i' fui al piè d'un colle giunto,  ...          11\n",
              "...                                            ...  ...         ...\n",
              "4739          dentro da sé, del suo colore stesso,  ...          11\n",
              "4740         Qual è 'l geomètra che tutto s'affige  ...          11\n",
              "4741               tal era io a quella vista nova:  ...          11\n",
              "4742          ma non eran da ciò le proprie penne:  ...          11\n",
              "4743            A l'alta fantasia qui mancò possa;  ...          11\n",
              "\n",
              "[4744 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mckVDu6ZU78J",
        "colab_type": "text"
      },
      "source": [
        "# Format Input for Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "UlnJ6ipGU78O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "outputId": "ba928dac-82b0-459d-ba98-cfb22cfbd31a"
      },
      "source": [
        "# Drop samples that are longer that the 99th percentile of length\n",
        "\n",
        "max_line_length = int(max([df['%s' % i].str.len().quantile(.99999) for i in range(3)]))\n",
        "df = df[\n",
        "    (df['0'].str.len() <= max_line_length) & \n",
        "    (df['1'].str.len() <= max_line_length) & \n",
        "    (df['2'].str.len() <= max_line_length) \n",
        "].copy()\n",
        "df"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>0_syllables</th>\n",
              "      <th>1_syllables</th>\n",
              "      <th>2_syllables</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Nel mezzo del cammin di nostra vita</td>\n",
              "      <td>mi ritrovai per una selva oscura</td>\n",
              "      <td>ché la diritta via era smarrita.</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Ahi quanto a dir qual era è cosa dura</td>\n",
              "      <td>esta selva selvaggia e aspra e forte</td>\n",
              "      <td>che nel pensier rinova la paura!</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Tant'è amara che poco è più morte;</td>\n",
              "      <td>ma per trattar del ben ch'i' vi trovai,</td>\n",
              "      <td>dirò de l'altre cose ch'i' v'ho scorte.</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Io non so ben ridir com'i' v'intrai,</td>\n",
              "      <td>tant'era pien di sonno a quel punto</td>\n",
              "      <td>che la verace via abbandonai.</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Ma poi ch'i' fui al piè d'un colle giunto,</td>\n",
              "      <td>là dove terminava quella valle</td>\n",
              "      <td>che m'avea di paura il cor compunto,</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4739</th>\n",
              "      <td>dentro da sé, del suo colore stesso,</td>\n",
              "      <td>mi parve pinta de la nostra effige:</td>\n",
              "      <td>per che 'l mio viso in lei tutto era messo.</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4740</th>\n",
              "      <td>Qual è 'l geomètra che tutto s'affige</td>\n",
              "      <td>per misurar lo cerchio, e non ritrova,</td>\n",
              "      <td>pensando, quel principio ond'elli indige,</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4741</th>\n",
              "      <td>tal era io a quella vista nova:</td>\n",
              "      <td>veder voleva come si convenne</td>\n",
              "      <td>l'imago al cerchio e come vi s'indova;</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4742</th>\n",
              "      <td>ma non eran da ciò le proprie penne:</td>\n",
              "      <td>se non che la mia mente fu percossa</td>\n",
              "      <td>da un fulgore in che sua voglia venne.</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4743</th>\n",
              "      <td>A l'alta fantasia qui mancò possa;</td>\n",
              "      <td>ma già volgeva il mio disio e 'l  velle ,</td>\n",
              "      <td>sì come rota ch'igualmente è mossa,</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4742 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 0  ... 2_syllables\n",
              "0              Nel mezzo del cammin di nostra vita  ...          11\n",
              "1            Ahi quanto a dir qual era è cosa dura  ...          11\n",
              "2               Tant'è amara che poco è più morte;  ...          11\n",
              "3             Io non so ben ridir com'i' v'intrai,  ...          11\n",
              "4       Ma poi ch'i' fui al piè d'un colle giunto,  ...          11\n",
              "...                                            ...  ...         ...\n",
              "4739          dentro da sé, del suo colore stesso,  ...          11\n",
              "4740         Qual è 'l geomètra che tutto s'affige  ...          11\n",
              "4741               tal era io a quella vista nova:  ...          11\n",
              "4742          ma non eran da ciò le proprie penne:  ...          11\n",
              "4743            A l'alta fantasia qui mancò possa;  ...          11\n",
              "\n",
              "[4742 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MGD8cfvU78S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cc9babe3-dd18-440b-a267-3b9342aeeec9"
      },
      "source": [
        "# Pad the lines to the max line length with new lines\n",
        "for i in range(3):\n",
        "    # For input, duplicate the first character\n",
        "    df['%s_in' % i] = (df[str(i)].str[0] + df[str(i)]).str.pad(max_line_length+2, 'right', '\\n')\n",
        "    \n",
        "    if i == 2: # If it's the last line\n",
        "        df['%s_out' % i] = df[str(i)].str.pad(max_line_length+2, 'right', '\\n')\n",
        "    else: \n",
        "        # If it's the first or second line, add the first character of the next line to the end of this line.\n",
        "        # This helps with training so that the next RNN has a better chance of getting the first character right.\n",
        "        df['%s_out' % i] = (df[str(i)] + '\\n' + df[str(i+1)].str[0]).str.pad(max_line_length+2, 'right', '\\n')\n",
        "    \n",
        "max_line_length += 2\n",
        "\n",
        "df"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>0_syllables</th>\n",
              "      <th>1_syllables</th>\n",
              "      <th>2_syllables</th>\n",
              "      <th>0_in</th>\n",
              "      <th>0_out</th>\n",
              "      <th>1_in</th>\n",
              "      <th>1_out</th>\n",
              "      <th>2_in</th>\n",
              "      <th>2_out</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Nel mezzo del cammin di nostra vita</td>\n",
              "      <td>mi ritrovai per una selva oscura</td>\n",
              "      <td>ché la diritta via era smarrita.</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>Nel mezzo del cammin di nostra vita\\n\\n\\n\\n...</td>\n",
              "      <td>Nel mezzo del cammin di nostra vita\\nm\\n\\n\\n...</td>\n",
              "      <td>mmi ritrovai per una selva oscura\\n\\n\\n\\n\\n\\n\\...</td>\n",
              "      <td>mi ritrovai per una selva oscura\\nc\\n\\n\\n\\n\\n\\...</td>\n",
              "      <td>cché la diritta via era smarrita.\\n\\n\\n\\n\\n\\n\\...</td>\n",
              "      <td>ché la diritta via era smarrita.\\n\\n\\n\\n\\n\\n\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Ahi quanto a dir qual era è cosa dura</td>\n",
              "      <td>esta selva selvaggia e aspra e forte</td>\n",
              "      <td>che nel pensier rinova la paura!</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>Ahi quanto a dir qual era è cosa dura\\n\\n\\n...</td>\n",
              "      <td>Ahi quanto a dir qual era è cosa dura\\ne\\n\\n...</td>\n",
              "      <td>eesta selva selvaggia e aspra e forte\\n\\n\\n\\n\\...</td>\n",
              "      <td>esta selva selvaggia e aspra e forte\\nc\\n\\n\\n\\...</td>\n",
              "      <td>cche nel pensier rinova la paura!\\n\\n\\n\\n\\n\\n\\...</td>\n",
              "      <td>che nel pensier rinova la paura!\\n\\n\\n\\n\\n\\n\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Tant'è amara che poco è più morte;</td>\n",
              "      <td>ma per trattar del ben ch'i' vi trovai,</td>\n",
              "      <td>dirò de l'altre cose ch'i' v'ho scorte.</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>Tant'è amara che poco è più morte;\\n\\n\\n\\n\\...</td>\n",
              "      <td>Tant'è amara che poco è più morte;\\nm\\n\\n\\n\\...</td>\n",
              "      <td>mma per trattar del ben ch'i' vi trovai,\\n\\n\\n...</td>\n",
              "      <td>ma per trattar del ben ch'i' vi trovai,\\nd\\n\\n...</td>\n",
              "      <td>ddirò de l'altre cose ch'i' v'ho scorte.\\n\\n\\n...</td>\n",
              "      <td>dirò de l'altre cose ch'i' v'ho scorte.\\n\\n\\n\\...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Io non so ben ridir com'i' v'intrai,</td>\n",
              "      <td>tant'era pien di sonno a quel punto</td>\n",
              "      <td>che la verace via abbandonai.</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>Io non so ben ridir com'i' v'intrai,\\n\\n\\n\\...</td>\n",
              "      <td>Io non so ben ridir com'i' v'intrai,\\nt\\n\\n\\...</td>\n",
              "      <td>ttant'era pien di sonno a quel punto\\n\\n\\n\\n\\n...</td>\n",
              "      <td>tant'era pien di sonno a quel punto\\nc\\n\\n\\n\\n...</td>\n",
              "      <td>cche la verace via abbandonai.\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
              "      <td>che la verace via abbandonai.\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Ma poi ch'i' fui al piè d'un colle giunto,</td>\n",
              "      <td>là dove terminava quella valle</td>\n",
              "      <td>che m'avea di paura il cor compunto,</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>Ma poi ch'i' fui al piè d'un colle giunto,\\...</td>\n",
              "      <td>Ma poi ch'i' fui al piè d'un colle giunto,\\n...</td>\n",
              "      <td>llà dove terminava quella valle\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
              "      <td>là dove terminava quella valle\\nc\\n\\n\\n\\n\\n\\n\\...</td>\n",
              "      <td>cche m'avea di paura il cor compunto,\\n\\n\\n\\n\\...</td>\n",
              "      <td>che m'avea di paura il cor compunto,\\n\\n\\n\\n\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4739</th>\n",
              "      <td>dentro da sé, del suo colore stesso,</td>\n",
              "      <td>mi parve pinta de la nostra effige:</td>\n",
              "      <td>per che 'l mio viso in lei tutto era messo.</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>dentro da sé, del suo colore stesso,\\n\\n\\n\\...</td>\n",
              "      <td>dentro da sé, del suo colore stesso,\\nm\\n\\n\\...</td>\n",
              "      <td>mmi parve pinta de la nostra effige:\\n\\n\\n\\n\\n...</td>\n",
              "      <td>mi parve pinta de la nostra effige:\\np\\n\\n\\n\\n...</td>\n",
              "      <td>pper che 'l mio viso in lei tutto era messo.\\n...</td>\n",
              "      <td>per che 'l mio viso in lei tutto era messo.\\n\\...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4740</th>\n",
              "      <td>Qual è 'l geomètra che tutto s'affige</td>\n",
              "      <td>per misurar lo cerchio, e non ritrova,</td>\n",
              "      <td>pensando, quel principio ond'elli indige,</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>Qual è 'l geomètra che tutto s'affige\\n\\n\\n...</td>\n",
              "      <td>Qual è 'l geomètra che tutto s'affige\\np\\n\\n...</td>\n",
              "      <td>pper misurar lo cerchio, e non ritrova,\\n\\n\\n\\...</td>\n",
              "      <td>per misurar lo cerchio, e non ritrova,\\np\\n\\n\\...</td>\n",
              "      <td>ppensando, quel principio ond'elli indige,\\n\\n...</td>\n",
              "      <td>pensando, quel principio ond'elli indige,\\n\\n\\...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4741</th>\n",
              "      <td>tal era io a quella vista nova:</td>\n",
              "      <td>veder voleva come si convenne</td>\n",
              "      <td>l'imago al cerchio e come vi s'indova;</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>tal era io a quella vista nova:\\n\\n\\n\\n\\n\\n...</td>\n",
              "      <td>tal era io a quella vista nova:\\nv\\n\\n\\n\\n\\n...</td>\n",
              "      <td>vveder voleva come si convenne\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
              "      <td>veder voleva come si convenne\\nl\\n\\n\\n\\n\\n\\n\\n...</td>\n",
              "      <td>ll'imago al cerchio e come vi s'indova;\\n\\n\\n\\...</td>\n",
              "      <td>l'imago al cerchio e come vi s'indova;\\n\\n\\n\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4742</th>\n",
              "      <td>ma non eran da ciò le proprie penne:</td>\n",
              "      <td>se non che la mia mente fu percossa</td>\n",
              "      <td>da un fulgore in che sua voglia venne.</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>ma non eran da ciò le proprie penne:\\n\\n\\n\\...</td>\n",
              "      <td>ma non eran da ciò le proprie penne:\\ns\\n\\n\\...</td>\n",
              "      <td>sse non che la mia mente fu percossa\\n\\n\\n\\n\\n...</td>\n",
              "      <td>se non che la mia mente fu percossa\\nd\\n\\n\\n\\n...</td>\n",
              "      <td>dda un fulgore in che sua voglia venne.\\n\\n\\n\\...</td>\n",
              "      <td>da un fulgore in che sua voglia venne.\\n\\n\\n\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4743</th>\n",
              "      <td>A l'alta fantasia qui mancò possa;</td>\n",
              "      <td>ma già volgeva il mio disio e 'l  velle ,</td>\n",
              "      <td>sì come rota ch'igualmente è mossa,</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>A l'alta fantasia qui mancò possa;\\n\\n\\n\\n\\...</td>\n",
              "      <td>A l'alta fantasia qui mancò possa;\\nm\\n\\n\\n\\...</td>\n",
              "      <td>mma già volgeva il mio disio e 'l  velle ,\\n\\n...</td>\n",
              "      <td>ma già volgeva il mio disio e 'l  velle ,\\ns\\n...</td>\n",
              "      <td>ssì come rota ch'igualmente è mossa,\\n\\n\\n\\n\\n...</td>\n",
              "      <td>sì come rota ch'igualmente è mossa,\\n\\n\\n\\n\\n\\...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4742 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 0  ...                                              2_out\n",
              "0              Nel mezzo del cammin di nostra vita  ...  ché la diritta via era smarrita.\\n\\n\\n\\n\\n\\n\\n...\n",
              "1            Ahi quanto a dir qual era è cosa dura  ...  che nel pensier rinova la paura!\\n\\n\\n\\n\\n\\n\\n...\n",
              "2               Tant'è amara che poco è più morte;  ...  dirò de l'altre cose ch'i' v'ho scorte.\\n\\n\\n\\...\n",
              "3             Io non so ben ridir com'i' v'intrai,  ...  che la verace via abbandonai.\\n\\n\\n\\n\\n\\n\\n\\n\\...\n",
              "4       Ma poi ch'i' fui al piè d'un colle giunto,  ...  che m'avea di paura il cor compunto,\\n\\n\\n\\n\\n...\n",
              "...                                            ...  ...                                                ...\n",
              "4739          dentro da sé, del suo colore stesso,  ...  per che 'l mio viso in lei tutto era messo.\\n\\...\n",
              "4740         Qual è 'l geomètra che tutto s'affige  ...  pensando, quel principio ond'elli indige,\\n\\n\\...\n",
              "4741               tal era io a quella vista nova:  ...  l'imago al cerchio e come vi s'indova;\\n\\n\\n\\n...\n",
              "4742          ma non eran da ciò le proprie penne:  ...  da un fulgore in che sua voglia venne.\\n\\n\\n\\n...\n",
              "4743            A l'alta fantasia qui mancò possa;  ...  sì come rota ch'igualmente è mossa,\\n\\n\\n\\n\\n\\...\n",
              "\n",
              "[4742 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTpaQwHZsp2P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "outputId": "e0d639db-4f37-4d90-a838-e7e9d5e1511d"
      },
      "source": [
        "def get_hyp_lm_tercets(tercets):\n",
        "    new_tercets = []\n",
        "    for tercet in tercets:\n",
        "        new_tercets.append([])\n",
        "        for verse in tercet:\n",
        "            new_tercets[-1].append([])\n",
        "            for hyp_w in verse:\n",
        "                new_tercets[-1][-1].extend(hyp_w)\n",
        "                new_tercets[-1][-1].append('<SEP>')\n",
        "            new_tercets[-1][-1] = new_tercets[-1][-1][:-1]\n",
        "\n",
        "    return new_tercets\n",
        "\n",
        "def is_vowel(c):\n",
        "    return c in 'aeiouAEIOUàìíèéùúüòï'\n",
        "\n",
        "def unsplittable_cons():\n",
        "    u_cons = []\n",
        "    for c1 in ('b', 'c', 'd', 'f', 'g', 'p', 't', 'v'):\n",
        "        for c2 in ('l', 'r'):\n",
        "            u_cons.append(c1 + c2)\n",
        "\n",
        "    others = ['gn', 'gh', 'ch']\n",
        "    u_cons.extend(others)\n",
        "    return u_cons\n",
        "\n",
        "\n",
        "def are_cons_to_split(c1, c2):\n",
        "    to_split = ('cq', 'cn', 'lm', 'rc', 'bd', 'mb', 'mn', 'ld', 'ng', 'nd', 'tm', 'nv', 'nc', 'ft', 'nf', 'gm', 'fm', 'rv', 'fp')\n",
        "    return (c1 + c2) in to_split or (not is_vowel(c1) and (c1 == c2)) or ((c1 + c2) not in unsplittable_cons()) and (\n",
        "        (not is_vowel(c1)) and (not is_vowel(c2)) and c1 != 's')\n",
        "\n",
        "\n",
        "def is_diphthong(c1, c2):\n",
        "    return (c1 + c2) in ('ia', 'ie', 'io', 'iu', 'ua', 'ue', 'uo', 'ui', 'ai', 'ei', 'oi', 'ui', 'au', 'eu', 'ïe', 'iú', 'iù')\n",
        "\n",
        "\n",
        "def is_triphthong(c1, c2, c3):\n",
        "    return (c1 + c2 + c3) in ('iai', 'iei', 'uoi', 'uai', 'uei', 'iuo')\n",
        "\n",
        "\n",
        "def is_toned_vowel(c):\n",
        "    return c in 'àìèéùòï'\n",
        "\n",
        "def has_vowels(sy):\n",
        "    for c in sy:\n",
        "        if is_vowel(c):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def hyphenation(word):\n",
        "    \"\"\"\n",
        "    Split word in syllables\n",
        "    :param word: input string\n",
        "    :return: a list containing syllables of the word\n",
        "    \"\"\"\n",
        "    if not word or word == '':\n",
        "        return []\n",
        "    # elif len(word) == 3 and (is_vowel(word[1]) and is_vowel(word[2]) and not is_toned_vowel(word[2]) and (\n",
        "    #     not is_diphthong(word[1], word[2]) or (word[1] == 'i'))):\n",
        "    elif len(word) == 3 and (is_vowel(word[1]) and is_vowel(word[2]) and not is_toned_vowel(word[2]) and (\n",
        "        not is_diphthong(word[1], word[2]))):\n",
        "        return [word[:2]] + [word[2]]\n",
        "    elif len(word) == 3 and is_vowel(word[0]) and not is_vowel(word[1]) and is_vowel(word[2]):\n",
        "        return [word[:2]] + [word[2]]\n",
        "    elif len(word) == 3:\n",
        "        return [word]\n",
        "\n",
        "    syllables = []\n",
        "    is_done = False\n",
        "    count = 0\n",
        "    while not is_done and count <= len(word) - 1:\n",
        "        syllables.append('')\n",
        "        c = word[count]\n",
        "        while not is_vowel(c) and count < len(word) - 1:\n",
        "            syllables[-1] = syllables[-1] + c\n",
        "            count += 1\n",
        "            c = word[count]\n",
        "\n",
        "        syllables[-1] = syllables[-1] + word[count]\n",
        "\n",
        "        if count == len(word) - 1:\n",
        "            is_done = True\n",
        "        else:\n",
        "            count += 1\n",
        "\n",
        "            if count < len(word) and not is_vowel(word[count]):\n",
        "                if count == len(word) - 1:\n",
        "                    syllables[-1] += word[count]\n",
        "                    count += 1\n",
        "                elif count + 1 < len(word) and are_cons_to_split(word[count], word[count + 1]):\n",
        "                    syllables[-1] += word[count]\n",
        "                    count += 1\n",
        "                elif count + 2 < len(word) and not is_vowel(word[count + 1]) and not is_vowel(word[count + 2]) and word[\n",
        "                    count] != 's':\n",
        "                    syllables[-1] += word[count]\n",
        "                    count += 1\n",
        "            elif count < len(word):\n",
        "                if count + 1 < len(word) and is_triphthong(word[count - 1], word[count], word[count + 1]):\n",
        "                    syllables[-1] += word[count] + word[count + 1]\n",
        "                    count += 2\n",
        "                elif is_diphthong(word[count - 1], word[count]):\n",
        "                    syllables[-1] += word[count]\n",
        "                    count += 1\n",
        "\n",
        "                if count + 1 < len(word) and are_cons_to_split(word[count], word[count + 1]):\n",
        "                    syllables[-1] += word[count]\n",
        "                    count += 1\n",
        "\n",
        "            else:\n",
        "                is_done = True\n",
        "\n",
        "    if not has_vowels(syllables[-1]) and len(syllables) > 1:\n",
        "        syllables[-2] = syllables[-2] + syllables[-1]\n",
        "        syllables = syllables[:-1]\n",
        "\n",
        "    return syllables\n",
        "\n",
        "\n",
        "\n",
        "def get_dc_hyphenation(canti):\n",
        "    hyp_canti, hyp_tokens = [], []\n",
        "    for canto in canti:\n",
        "        hyp_canti.append([])\n",
        "        for verso in canto:\n",
        "            syllables = seq_hyphentation(verso)\n",
        "            hyp_canti[-1].append(syllables)\n",
        "            for syllable in syllables:\n",
        "                hyp_tokens.extend(syllable)\n",
        "\n",
        "    return hyp_canti, hyp_tokens\n",
        "\n",
        "\n",
        "def seq_hyphentation(words):\n",
        "    \"\"\"\n",
        "    Converts words in a list of strings into lists of syllables\n",
        "    :param words: a list of words (strings)\n",
        "    :return: a list of lists containing word syllables\n",
        "    \"\"\"\n",
        "    return [hyphenation(w) for w in words]\n",
        "\n",
        "\n",
        "def get_dc_cantos(filename, encoding=None):\n",
        "    # raw_data = read_words(filename=filename)\n",
        "    cantos, words, raw = [], [], []\n",
        "    with open(filename, \"r\", encoding=encoding) as f:\n",
        "        for line in f:\n",
        "            sentence = line.strip()\n",
        "            sentence = str.replace(sentence, \"\\.\", \" \\. \")\n",
        "            sentence = str.replace(sentence, \"[\", '')\n",
        "            sentence = str.replace(sentence, \"]\", '')\n",
        "            sentence = str.replace(sentence, \"-\", '')\n",
        "            sentence = str.replace(sentence, \";\", \" ; \")\n",
        "            sentence = str.replace(sentence, \",\", \" , \")\n",
        "            # sentence = str.replace(sentence, \" \\'\", '')\n",
        "            sentence = str.replace(sentence, \"\\'\", ' \\' ')\n",
        "            if len(sentence) > 1:\n",
        "                # sentence = sentence.translate(string.punctuation)\n",
        "                tokenized_sentence = nl.word_tokenize(sentence)\n",
        "                # tokenized_sentence = sentence.split()\n",
        "                tokenized_sentence = [w.lower() for w in tokenized_sentence if len(w) > 0]\n",
        "                tokenized_sentence = [w for w in tokenized_sentence if \",\" not in w]\n",
        "                tokenized_sentence = [w for w in tokenized_sentence if \".\" not in w]\n",
        "                tokenized_sentence = [w for w in tokenized_sentence if \":\" not in w]\n",
        "                tokenized_sentence = [w for w in tokenized_sentence if \";\" not in w]\n",
        "                tokenized_sentence = [w for w in tokenized_sentence if \"«\" not in w]\n",
        "                tokenized_sentence = [w for w in tokenized_sentence if \"»\" not in w]\n",
        "                # ts = []\n",
        "                ts = tokenized_sentence\n",
        "                # [ts.extend(re.split(\"(\\')\", e)) for e in tokenized_sentence]\n",
        "                tokenized_sentence = [w for w in ts if len(w) > 0]\n",
        "\n",
        "\n",
        "                raw.append(sentence)\n",
        "                cantos.append(tokenized_sentence)\n",
        "                words.extend(tokenized_sentence)\n",
        "\n",
        "    return cantos, words, raw\n",
        "\n",
        "\n",
        "def create_tercets(cantos):\n",
        "    tercets = []\n",
        "    for i,canto in enumerate(cantos):\n",
        "        for v,verse in enumerate(canto):\n",
        "            if v%3 == 0:\n",
        "                tercets.append([])\n",
        "\n",
        "            tercets[-1].append(verse)\n",
        "        tercets = tercets[:-1]  # removes the last malformed tercets (only 2 verses)\n",
        "\n",
        "    return tercets\n",
        "\n",
        "def pad_list(l, pad_token, max_l_size, keep_lasts=False, pad_right=True):\n",
        "    \"\"\"\n",
        "    Adds a padding token to a list\n",
        "    inputs:\n",
        "    :param l: input list to pad.\n",
        "    :param pad_token: value to add as padding.\n",
        "    :param max_l_size: length of the new padded list to return,\n",
        "    it truncates lists longer that 'max_l_size' without adding\n",
        "    padding values.\n",
        "    :param keep_lasts: If True, preserves the max_l_size last elements\n",
        "    of a sequence (by keeping the same order).  E.g.:\n",
        "    if keep_lasts is True and max_l_size=3 [1,2,3,4] becomes [2,3,4].\n",
        "\n",
        "\n",
        "    :return: the list padded or truncated.\n",
        "    \"\"\"\n",
        "    to_pad = []\n",
        "    max_l = min(max_l_size, len(l))  # maximum len\n",
        "    l_init = len(l) - max_l if len(l) > max_l and keep_lasts else 0  # initial position where to sample from the list\n",
        "    l_end = len(l) if len(l) > max_l and keep_lasts else max_l\n",
        "    for i in range(l_init, l_end):\n",
        "        to_pad.append(l[i])\n",
        "\n",
        "    # for j in range(len(l), max_l_size):\n",
        "    #     to_pad.append(pad_token)\n",
        "    pad_tokens = [pad_token] * (max_l_size-len(l))\n",
        "    padded_l = to_pad + pad_tokens if pad_right else pad_tokens + to_pad\n",
        "\n",
        "    return padded_l\n",
        "\n",
        "\n",
        "def save_data(data, file):\n",
        "    with open(file, 'wb') as output:\n",
        "        pickle.dump(data, output, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def load_data(file):\n",
        "    with open(file, 'rb') as obj:\n",
        "        return pickle.load(obj)\n",
        "\n",
        "def print_and_write(file, s):\n",
        "    print(s)\n",
        "    file.write(s)\n",
        "\n",
        "\n",
        "class Vocabulary(object):\n",
        "    def __init__(self, vocab_size=None):\n",
        "        self.dictionary = dict()\n",
        "        self.rev_dictionary = dict()\n",
        "        self.count = []\n",
        "        self.special_tokens = []\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    def build_vocabulary_from_counts(self, count, special_tokens=[]):\n",
        "        \"\"\"\n",
        "        Sets all the attributes of the Vocabulary object.\n",
        "        :param count: a list of lists as follows: [['token', number_of_occurrences],...]\n",
        "        :param special_tokens: a list of strings. E.g. ['<EOS>', '<PAD>',...]\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "\n",
        "        dictionary = dict()\n",
        "        for word, _ in count:\n",
        "            dictionary[word] = len(dictionary)\n",
        "\n",
        "        # adding eventual special tokens to the dictionary (e.g. <EOS>,<PAD> etc..)\n",
        "        d = len(dictionary)\n",
        "        for i, token in enumerate(special_tokens):\n",
        "            dictionary[token] = d + i\n",
        "\n",
        "        self.count = count\n",
        "        self.dictionary = dictionary\n",
        "        self.rev_dictionary = dict(zip(self.dictionary.values(), self.dictionary.keys()))\n",
        "        self.special_tokens = special_tokens\n",
        "        self.vocab_size = len(dictionary)\n",
        "\n",
        "    def build_vocabulary_from_tokens(self, tokens, vocabulary_size=None, special_tokens=[]):\n",
        "        \"\"\"\n",
        "        Given a list of tokens, it sets the Vocabulary object attributes by constructing\n",
        "        a dictionary mapping each token to a unique id.\n",
        "        :param tokens: a list of strings.\n",
        "         E.g. [\"the\", \"cat\", \"is\", ... \".\", \"the\", \"house\" ,\"is\" ...].\n",
        "         NB: Here you should put all your token instances of the corpus.\n",
        "        :param vocabulary_size: The number of elements of your vocabulary. If there are more\n",
        "        than 'vocabulary_size' elements on tokens, it considers only the 'vocabulary_size'\n",
        "        most frequent ones.\n",
        "        :param special_tokens: Optional. A list of strings. Useful to add special tokens in vocabulary.\n",
        "        If you don't have any, keep it empty.\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "\n",
        "        vocabulary_size = vocabulary_size if vocabulary_size is not None else self.vocab_size\n",
        "        vocabulary_size = vocabulary_size - (len(special_tokens) + 1) if vocabulary_size else None\n",
        "        # counts occurrences of each token\n",
        "        count = [['<UNK>', -1]]\n",
        "        count.extend(collections.Counter(tokens).most_common(vocabulary_size))  # takes only the most frequent ones, if size is None takes them all\n",
        "        self.build_vocabulary_from_counts(count, special_tokens)  # actually build the vocabulary\n",
        "        self._set_unk_count(tokens)  # set the number of OOV instances\n",
        "\n",
        "    @staticmethod\n",
        "    def merge_vocabulary(vocab0, vocab1, vocabulary_size=-1):\n",
        "        \"\"\"\n",
        "        Merge two Vocabulary objects into a new one.\n",
        "        :param vocab0: first Vocabulary object\n",
        "        :param vocab1: second Vocabulary object\n",
        "        :param vocabulary_size: parameter to decide the merged vocabulary size.\n",
        "        With default value -1, all the words of both vocabularies are preserved.\n",
        "        When set to 0, the size of the vocabulary is set to the size of vocab0,\n",
        "        when set to 1 it is kept the size of vocab1.\n",
        "        :return: a new vocabulary\n",
        "        \"\"\"\n",
        "        # get size of the new vocabulary\n",
        "        vocab_size = vocab0.vocab_size + vocab1.vocab_size if vocabulary_size == -1 else vocabulary_size\n",
        "        merged_special_tokens = list(set(vocab0.special_tokens) | set(vocab1.special_tokens))\n",
        "\n",
        "        # merge the counts from the two vocabularies and then selects the most_common tokens\n",
        "        merged_counts = collections.Counter(dict(vocab0.count)) + collections.Counter(dict(vocab1.count))\n",
        "        merged_counts = merged_counts.most_common(vocab_size)\n",
        "        count = [['<UNK>', -1]]\n",
        "        count.extend(merged_counts)\n",
        "\n",
        "        # create the new vocabulary\n",
        "        merged_vocab = Vocabulary(vocab_size)\n",
        "        merged_vocab.build_vocabulary_from_counts(count, merged_special_tokens)\n",
        "        return merged_vocab\n",
        "\n",
        "    @staticmethod\n",
        "    def merge_vocabularies(vocab_list, vocab_size=None):\n",
        "        \"\"\"\n",
        "        Join a list of vocabularies into a new one.\n",
        "        :param vocab_list: a list of Vocabulary objects\n",
        "        :param vocab_size: the maximum size of the merged vocabulary.\n",
        "        :return: a vocabulary merging them all.\n",
        "        \"\"\"\n",
        "        vocab_size = vocab_size if vocab_size else sum([v.vocab_size for v in vocab_list])\n",
        "        merged_vocab = Vocabulary(vocab_size)\n",
        "        for voc in vocab_list:\n",
        "            merged_vocab = Vocabulary.merge_vocabulary(merged_vocab, voc, vocab_size)\n",
        "        return merged_vocab\n",
        "\n",
        "    def string2id(self, dataset):\n",
        "        \"\"\"\n",
        "        Converts a dataset of strings into a dataset of ids according to the object dictionary.\n",
        "        :param dataset: any string-based dataset with any nested lists.\n",
        "        :return: a new dataset, with the same shape of dataset, where each string is mapped into its\n",
        "        corresponding id associated in the dictionary (0 for unknown tokens).\n",
        "        \"\"\"\n",
        "\n",
        "        def _recursive_call(items):\n",
        "            new_items = []\n",
        "            for item in items:\n",
        "                if isinstance(item, str) or isinstance(item, int) or isinstance(item, float):\n",
        "                    new_items.append(self.word2id(item))\n",
        "                else:\n",
        "                    new_items.append(_recursive_call(item))\n",
        "            return new_items\n",
        "\n",
        "        return _recursive_call(dataset)\n",
        "\n",
        "    def id2string(self, dataset):\n",
        "        \"\"\"\n",
        "        Converts a dataset of integer ids into a dataset of string according to the reverse dictionary.\n",
        "        :param dataset: any int-based dataset with any nested lists. Allowed types are int, np.int32, np.int64.\n",
        "        :return: a new dataset, with the same shape of dataset, where each token is mapped into its\n",
        "        corresponding string associated in the reverse dictionary.\n",
        "        \"\"\"\n",
        "        def _recursive_call(items):\n",
        "            new_items = []\n",
        "            for item in items:\n",
        "                if isinstance(item, int) or isinstance(item, np.int) or isinstance(item, np.int32) or isinstance(item, np.int64):\n",
        "                    new_items.append(self.id2word(item))\n",
        "                else:\n",
        "                    new_items.append(_recursive_call(item))\n",
        "            return new_items\n",
        "\n",
        "        return _recursive_call(dataset)\n",
        "\n",
        "    def word2id(self, item):\n",
        "        \"\"\"\n",
        "        Maps a string token to its corresponding id.\n",
        "        :param item: a string.\n",
        "        :return: If the token belongs to the vocabulary, it returns an integer id > 0, otherwise\n",
        "        it returns the value associated to the unknown symbol, that is typically 0.\n",
        "        \"\"\"\n",
        "        return self.dictionary[item] if item in self.dictionary else self.dictionary['<UNK>']\n",
        "\n",
        "    def id2word(self, token_id):\n",
        "        \"\"\"\n",
        "        Maps an integer token to its corresponding string.\n",
        "        :param token_id: an integer.\n",
        "        :return: If the id belongs to the vocabulary, it returns the string\n",
        "        associated to it, otherwise it returns the string associated\n",
        "        to the unknown symbol, that is '<UNK>'.\n",
        "        \"\"\"\n",
        "\n",
        "        return self.rev_dictionary[token_id] if token_id in self.rev_dictionary else self.rev_dictionary[self.dictionary['<UNK>']]\n",
        "\n",
        "    def get_unk_count(self):\n",
        "        return self.count[0][1]\n",
        "\n",
        "    def _set_unk_count(self, tokens):\n",
        "        \"\"\"\n",
        "        Sets the number of OOV instances in the tokens provided\n",
        "        :param tokens: a list of tokens\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "        data = list()\n",
        "        unk_count = 0\n",
        "        for word in tokens:\n",
        "            if word in self.dictionary:\n",
        "                index = self.dictionary[word]\n",
        "            else:\n",
        "                index = 0  # dictionary['<UNK>']\n",
        "                unk_count += 1\n",
        "            data.append(index)\n",
        "        self.count[0][1] = unk_count\n",
        "\n",
        "    def add_element(self, name, is_special_token=False):\n",
        "        if name not in self.dictionary:\n",
        "            self.vocab_size += 1\n",
        "            self.dictionary[name] = self.vocab_size\n",
        "            self.rev_dictionary[self.vocab_size] = name\n",
        "\n",
        "            if is_special_token:\n",
        "                self.special_tokens = list(self.special_tokens)\n",
        "                self.special_tokens.append(name)\n",
        "\n",
        "            self.count.append([name, 1])\n",
        "\n",
        "    def set_vocabulary(self, dictionary, rev_dictionary, special_tokens, vocab_size):\n",
        "        self.dictionary = dictionary,\n",
        "        self.rev_dictionary = rev_dictionary\n",
        "        self.special_tokens = special_tokens\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    @staticmethod\n",
        "    def load_vocabulary(filename):\n",
        "        return load_data(filename)\n",
        "\n",
        "    def save_vocabulary(self, filename):\n",
        "        save_data(self, filename)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class SyLMDataset(object):\n",
        "    def __init__(self, config, sy_vocab=None):\n",
        "        self.config = config\n",
        "        self.vocabulary = sy_vocab\n",
        "\n",
        "        self.raw_train_x = []\n",
        "        self.raw_val_x = []\n",
        "        self.raw_test_x = []\n",
        "        self.raw_x = []\n",
        "\n",
        "        self.train_x, self.train_y = [], []\n",
        "        self.val_x, self.val_y = [], []\n",
        "        self.test_x, self.test_y = [], []\n",
        "        self.x, self.y = [], []\n",
        "\n",
        "    def initialize(self, sess):\n",
        "        pass\n",
        "\n",
        "    def load(self, sources):\n",
        "        \"\"\"\n",
        "        Extract raw texts form sources and gather them all together.\n",
        "        :param sources: a string or an iterable of strings containing the file(s)\n",
        "        to process in order to build the dataset.\n",
        "        :return: a list of raw strings.\n",
        "        \"\"\"\n",
        "        return NotImplementedError\n",
        "\n",
        "    def build(self, sources, split_size=0.8):\n",
        "        \"\"\"\n",
        "        :param sources: a string or an iterable of strings containing the file(s)\n",
        "        to process in order to build the dataset.\n",
        "        :param split_size: the size to split the dataset, set >=1.0 to not split.\n",
        "        \"\"\"\n",
        "\n",
        "        raw_x = self.load(sources)\n",
        "        # raw_x = self.tokenize([self.preprocess(ex) for ex in raw_x])  # fixme\n",
        "        # splitting data\n",
        "        self.raw_x = raw_x\n",
        "        if split_size < 1.0:\n",
        "            self.raw_train_x, self.raw_test_x = self.split(self.raw_x, train_size=split_size)\n",
        "            self.raw_train_x, self.raw_val_x = self.split(self.raw_train_x, train_size=split_size)\n",
        "        else:\n",
        "            self.raw_train_x = self.raw_x\n",
        "\n",
        "        if self.vocabulary is None:\n",
        "            # creates vocabulary\n",
        "            tokens = [item for sublist in self.raw_train_x for item in sublist]  # get tokens\n",
        "            special_tokens = (\"<GO>\", \"<PAD>\", \"<SEP>\", \"<EOS>\", \"<EOV>\")\n",
        "            self._create_vocab(tokens, special_tokens=special_tokens)\n",
        "\n",
        "        # creates x,y for train\n",
        "        self.train_x = self._build_dataset(self.raw_train_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "        self.train_y = self._build_dataset(self.raw_train_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "\n",
        "        # creates x,y for validation\n",
        "        self.val_x = self._build_dataset(self.raw_val_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "        self.val_y = self._build_dataset(self.raw_val_x, insert_go=False, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "\n",
        "        # creates x,y for validation\n",
        "        self.test_x = self._build_dataset(self.raw_test_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "        self.test_y = self._build_dataset(self.raw_test_x, insert_go=False, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "\n",
        "    def _create_vocab(self, tokens, special_tokens=(\"<PAD>\", \"<GO>\", \"<SEP>\", \"<EOV>\", \"<EOS>\")):\n",
        "        \"\"\"\n",
        "        Create the vocabulary. Special tokens can be added to the tokens obtained from\n",
        "        the corpus.\n",
        "        :param tokens: a list of all the tokens in the corpus. Each token is a string.\n",
        "        :param special_tokens: a list of strings.\n",
        "        \"\"\"\n",
        "        vocab = Vocabulary(vocab_size=self.config.input_vocab_size)\n",
        "        vocab.build_vocabulary_from_tokens(tokens, special_tokens=special_tokens)\n",
        "        self.vocabulary = vocab\n",
        "\n",
        "    @staticmethod\n",
        "    def split(raw_data, train_size=0.8):\n",
        "        size = math.floor(len(raw_data)*train_size)\n",
        "        return raw_data[:size], raw_data[size:]\n",
        "\n",
        "    @staticmethod\n",
        "    def preprocess(txt):\n",
        "        return txt\n",
        "\n",
        "    @staticmethod\n",
        "    def shuffle(x):\n",
        "        return random.sample(x, len(x))\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenize(txt):\n",
        "        return txt\n",
        "\n",
        "    def _build_dataset(self, raw_data, max_len=100, insert_go=True, keep_lasts=False, pad_right=True, shuffle=True):\n",
        "        \"\"\"\n",
        "        Converts all the tokens in e1_raw_data by mapping each token with its corresponding\n",
        "        value in the dictionary. In case of token not in the dictionary, they are assigned to\n",
        "        a specific id. Each sequence is padded up to the seq_max_len setup in the config.\n",
        "\n",
        "        :param raw_data: list of sequences, each sequence is a list of tokens (strings).\n",
        "        :param max_len: max length of a sequence, crop longer and pad smaller ones.\n",
        "        :param insert_go: True to insert <GO>, False otherwise.\n",
        "        :param keep_lasts: True to truncate initial elements of a sequence.\n",
        "        :param pad_right: pad to the right (default value True), otherwise pads to left.\n",
        "        :param shuffle: Optional. If True data are shuffled.\n",
        "        :return: A list of sequences where each token in each sequence is an int id.\n",
        "        \"\"\"\n",
        "        dataset = []\n",
        "        for sentence in raw_data:\n",
        "            sentence_ids = [self.vocabulary.word2id(\"<GO>\")] if insert_go else []\n",
        "            sentence_ids.extend([self.vocabulary.word2id(w) for w in sentence])\n",
        "            sentence_ids.append(self.vocabulary.word2id(\"<EOS>\"))\n",
        "            #sentence_ids = pad_list(sentence_ids, self.vocabulary.word2id(\"<PAD>\"), max_len, keep_lasts=keep_lasts, pad_right=pad_right)\n",
        "\n",
        "            dataset.append(sentence_ids)\n",
        "\n",
        "        if shuffle:\n",
        "            return random.sample(dataset, len(dataset))\n",
        "        else:\n",
        "            return dataset\n",
        "\n",
        "    def get_batches(self, batch_size=32):\n",
        "        \"\"\"\n",
        "        Iterator over the training set. Useful method to run experiments.\n",
        "        :param batch_size: size of the mini_batch\n",
        "        :return: input and target.\n",
        "        \"\"\"\n",
        "        x, y = self.train_x, self.train_y\n",
        "        \n",
        "        i = random.randint(0, batch_size)\n",
        "        batches = []\n",
        "        eov = self.vocabulary.word2id(\"<EOV>\")\n",
        "        # prepare batches\n",
        "        while i < len(x):\n",
        "            j = 0\n",
        "            batch_x, batch_y = [], []\n",
        "            while j < batch_size and i+j<len(x):\n",
        "                for c in x[i+j]:\n",
        "                  batch_x.append(c)\n",
        "                batch_x.append(eov)\n",
        "                for c in y[i+j]:\n",
        "                  batch_y.append(c)\n",
        "                batch_y.append(eov)\n",
        "                j += 1\n",
        "            i += batch_size\n",
        "\n",
        "            batches.append((batch_x, batch_y))\n",
        "\n",
        "        # supply\n",
        "        i = 0\n",
        "        while i < len(batches):\n",
        "            yield batches[i][0], batches[i][1]\n",
        "            i += 1\n",
        "\n",
        "class DanteSyLMDataset(SyLMDataset):\n",
        "    def __init__(self, config, sy_vocab=None):\n",
        "        \"\"\"\n",
        "        Class to create a dataset from Dante Alighieri's Divine Comedy.\n",
        "        :param config: a Config object\n",
        "        :param sy_vocab: (optional) a Vocabulary object where tokens of the dictionary\n",
        "        are syllables. If None, the vocabulary is create automatically from the source.\n",
        "        \"\"\"\n",
        "        super().__init__(config, sy_vocab)\n",
        "\n",
        "    def load(self, sources):\n",
        "        \"\"\"\n",
        "        Load examples from dataset\n",
        "        :param sources: data filepath.\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        canti, _, raw = get_dc_cantos(filename=sources)  # get raw data from file\n",
        "        canti, tokens = get_dc_hyphenation(canti)  # converts each\n",
        "\n",
        "        tercets = create_tercets(canti)\n",
        "        tercets = get_hyp_lm_tercets(tercets)\n",
        "\n",
        "        x = []\n",
        "        for tercet in tercets:\n",
        "            x.append([])\n",
        "            for verse in tercet:\n",
        "                x[-1].extend(verse)\n",
        "                x[-1].append(\"<EOV>\")\n",
        "\n",
        "        #x = self.shuffle(x)\n",
        "        return x\n",
        "\n",
        "def seq2str(seq):\n",
        "    def output2string(batch, rev_vocabulary, special_tokens, end_of_tokens):\n",
        "        to_print = ''\n",
        "        for token in batch:\n",
        "            if token in special_tokens:\n",
        "                to_print += ' '\n",
        "            elif end_of_tokens and token in end_of_tokens:\n",
        "                to_print += '\\n'\n",
        "            elif token in rev_vocabulary:\n",
        "                to_print += rev_vocabulary[token]\n",
        "            else:\n",
        "                to_print += '<UNK>'\n",
        "        return to_print\n",
        "\n",
        "    return output2string(seq, poetry_sy_lm_dataset.vocabulary.rev_dictionary,\n",
        "      special_tokens=[poetry_sy_lm_dataset.vocabulary.word2id(\"<PAD>\"), 0, poetry_sy_lm_dataset.vocabulary.word2id(\"<SEP>\"),\n",
        "                      poetry_sy_lm_dataset.vocabulary.word2id(\"<GO>\"), poetry_sy_lm_dataset.vocabulary.word2id(\"<EOS>\")],\n",
        "      end_of_tokens=[poetry_sy_lm_dataset.vocabulary.word2id(\"<EOV>\")])\n",
        "\n",
        "class cnfg:\n",
        "  vocab_size = 1884\n",
        "  input_vocab_size = 1884\n",
        "  emb_size = 300\n",
        "  sentence_max_len = 75\n",
        "\n",
        "config = cnfg()\n",
        "poetry_sy_lm_dataset = DanteSyLMDataset(config, sy_vocab=None)\n",
        "\n",
        "data_path = os.path.join(os.getcwd(), \"first.txt\") \n",
        "poetry_sy_lm_dataset.build(data_path, split_size=1)  # actual creation of  vocabulary (if not provided) and dataset\n",
        "print(\"Train size: \" + str(len(poetry_sy_lm_dataset.train_y)))\n",
        "print(\"Val size: \" + str(len(poetry_sy_lm_dataset.val_y)))\n",
        "print(\"Test size: \" + str(len(poetry_sy_lm_dataset.test_y)))\n",
        "first = [b for b in poetry_sy_lm_dataset.get_batches(20)]\n",
        "\n",
        "poetry_sy_lm_dataset1 = DanteSyLMDataset(config, sy_vocab=poetry_sy_lm_dataset.vocabulary)\n",
        "\n",
        "data_path = os.path.join(os.getcwd(), \"second.txt\")  # dataset location, here just the name of the source file\n",
        "poetry_sy_lm_dataset.build(data_path, split_size=1)  # actual creation of  vocabulary (if not provided) and dataset\n",
        "print(\"Train size: \" + str(len(poetry_sy_lm_dataset.train_y)))\n",
        "print(\"Val size: \" + str(len(poetry_sy_lm_dataset.val_y)))\n",
        "print(\"Test size: \" + str(len(poetry_sy_lm_dataset.test_y)))\n",
        "second = [b for b in poetry_sy_lm_dataset.get_batches(20)]\n",
        "\n",
        "poetry_sy_lm_dataset2 = DanteSyLMDataset(config, sy_vocab=poetry_sy_lm_dataset1.vocabulary)\n",
        "\n",
        "data_path = os.path.join(os.getcwd(), \"third.txt\")  # dataset location, here just the name of the source file\n",
        "poetry_sy_lm_dataset.build(data_path, split_size=1)  # actual creation of  vocabulary (if not provided) and dataset\n",
        "print(\"Train size: \" + str(len(poetry_sy_lm_dataset.train_y)))\n",
        "print(\"Val size: \" + str(len(poetry_sy_lm_dataset.val_y)))\n",
        "print(\"Test size: \" + str(len(poetry_sy_lm_dataset.test_y)))\n",
        "third = [b for b in poetry_sy_lm_dataset.get_batches(20)]\n",
        "\n",
        "seq2str(first[1][0])"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train size: 9256\n",
            "Val size: 0\n",
            "Test size: 0\n",
            "Train size: 9286\n",
            "Val size: 0\n",
            "Test size: 0\n",
            "Train size: 9322\n",
            "Val size: 0\n",
            "Test size: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\" l\\n'\\no r a\\n \\n d e l\\nt e m p o\\ne\\n \\n q u e s t i\\np a r e a\\nc h e\\n \\n e d\\nu n a\\nl u p a\\n \\n c h e\\nd i\\nt u t t e\\n \\n q u e s t a\\nm i\\np o r s e\\n \\n e\\nq u a l\\nè\\n \\n q u e i\\nc h e\\nv o l o n t i e r i\\n \\n t a l\\nm i\\nf e c e\\n \\n l a\\nb e s t i a\\ns a n z a\\n \\n m e n t r e\\nc h\\n'\\n \\n i\\n'\\nr o v i n a v a\\n \\n q u a n d o\\nv i d i\\nc o s t u i\\n \\n r i s p u o s e m i\\n` `\\nn o n\\n \\n o m o\\no m o\\ng i à\\n \\n n a c q u i\\ns u b\\ni u l i o\\n \\n a n c o r\\nc h e\\nf o s s e\\n \\n p o e t a\\nf u i\\ne\\n \\n c a n t a i\\nd i\\nq u e l\\n \\n m a\\nt u\\np e r c h é\\n \\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saTxe6d8U78X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6e2526f8-52b6-4da0-cf91-4da01e40e636"
      },
      "source": [
        "inputs = df[['0_in', '1_in', '2_in']].values\n",
        "\n",
        "tokenizer = Tokenizer(filters='', char_level=True)\n",
        "tokenizer.fit_on_texts(inputs.flatten())\n",
        "n_tokens = len(tokenizer.word_counts) + 1\n",
        "\n",
        "\n",
        "# X is the input for each line in sequences of one-hot-encoded values\n",
        "X = np_utils.to_categorical([\n",
        "    tokenizer.texts_to_sequences(inputs[:,i]) for i in range(3)\n",
        "], num_classes=n_tokens)\n",
        "\n",
        "outputs = df[['0_out', '1_out', '2_out']].values\n",
        "\n",
        "# Y is the output for each line in sequences of one-hot-encoded values\n",
        "Y = np_utils.to_categorical([\n",
        "    tokenizer.texts_to_sequences(outputs[:,i]) for i in range(3)\n",
        "], num_classes=n_tokens)\n",
        "\n",
        "# X_syllables is the count of syllables for each line\n",
        "X_syllables = df[['0_syllables', '1_syllables', '2_syllables']].values\n",
        "\n",
        "n_tokens"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WJscJRsU78b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c484cf37-576c-4ceb-d766-8d68d7afcdee"
      },
      "source": [
        "joblib.dump([latent_dim, n_tokens, max_line_length, tokenizer], str(output_dir / 'metadata.pkl'))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['output_test/metadata.pkl']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mSx66cMU78e",
        "colab_type": "text"
      },
      "source": [
        "# Training Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnkCcwCBVbvD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TrainingLine:\n",
        "    def __init__(self, name, previous_line, lstm, n_tokens):\n",
        "        self.char_input = Input(shape=(None, n_tokens), name='char_input_%s' % name)\n",
        "\n",
        "        self.syllable_input = Input(shape=(1,), name='syllable_input_%s' % name)\n",
        "        self.syllable_dense = Dense(lstm.units, activation='relu', name='syllable_dense_%s' % name)\n",
        "        self.syllable_dense_output = self.syllable_dense(self.syllable_input)\n",
        "\n",
        "        #self.lstm = LSTM(latent_dim, return_state=True, return_sequences=True, name='lstm_%s' % name)\n",
        "\n",
        "        if previous_line:\n",
        "            initial_state = [\n",
        "                Add(name='add_h_%s' % name)([\n",
        "                    previous_line.lstm_h,\n",
        "                    self.syllable_dense_output\n",
        "                ]),\n",
        "                Add(name='add_c_%s' % name)([\n",
        "                    previous_line.lstm_c,\n",
        "                    self.syllable_dense_output\n",
        "                ])\n",
        "            ]\n",
        "        else:\n",
        "            initial_state = [self.syllable_dense_output, self.syllable_dense_output]\n",
        "\n",
        "        self.lstm_out, self.lstm_h, self.lstm_c = lstm(self.char_input, initial_state=initial_state)\n",
        "\n",
        "        self.output_dense = Dense(n_tokens, activation='softmax', name='output_%s' % name)\n",
        "        self.output = self.output_dense(self.lstm_out)\n",
        "\n",
        "def create_training_model(latent_dim, n_tokens):\n",
        "    lstm = LSTM(latent_dim, return_state=True, return_sequences=True, name='lstm')\n",
        "    lines = []\n",
        "    inputs = []\n",
        "    outputs = []\n",
        "\n",
        "    for i in range(3):\n",
        "        previous_line = lines[-1] if lines else None\n",
        "        lines.append(TrainingLine('line_%s' % i, previous_line, lstm, n_tokens))\n",
        "        inputs += [lines[-1].char_input, lines[-1].syllable_input]\n",
        "        outputs.append(lines[-1].output)\n",
        "\n",
        "    training_model = Model(inputs, outputs)\n",
        "    training_model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "\n",
        "    return training_model, lstm, lines, inputs, outputs\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "nIsXFtc5U78f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c49b04b8-5e90-417d-8afe-2cab4fa22bcb"
      },
      "source": [
        "training_model, lstm, lines, inputs, outputs = create_training_model(latent_dim, n_tokens)\n",
        "print(training_model.summary())\n",
        "filepath = str(output_dir / (\"%s-{epoch:02d}-{loss:.2f}-{val_loss:.2f}.hdf5\" % latent_dim))\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "\n",
        "csv_logger = CSVLogger(str(output_dir / 'training_log.csv'), append=True, separator=',')\n",
        "\n",
        "callbacks_list = [checkpoint, csv_logger]\n",
        "\n",
        "training_model.fit([\n",
        "    X[0], X_syllables[:,0], \n",
        "    X[1], X_syllables[:,1], \n",
        "    X[2], X_syllables[:,2]\n",
        "], [Y[0], Y[1], Y[2]], batch_size=16, epochs=epochs, validation_split=.1, callbacks=callbacks_list)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "syllable_input_line_0 (InputLay (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "char_input_line_0 (InputLayer)  (None, None, 47)     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "syllable_dense_line_0 (Dense)   (None, 2048)         4096        syllable_input_line_0[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "syllable_input_line_1 (InputLay (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, None, 2048), 17170432    char_input_line_0[0][0]          \n",
            "                                                                 syllable_dense_line_0[0][0]      \n",
            "                                                                 syllable_dense_line_0[0][0]      \n",
            "                                                                 char_input_line_1[0][0]          \n",
            "                                                                 add_h_line_1[0][0]               \n",
            "                                                                 add_c_line_1[0][0]               \n",
            "                                                                 char_input_line_2[0][0]          \n",
            "                                                                 add_h_line_2[0][0]               \n",
            "                                                                 add_c_line_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "syllable_dense_line_1 (Dense)   (None, 2048)         4096        syllable_input_line_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "char_input_line_1 (InputLayer)  (None, None, 47)     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "add_h_line_1 (Add)              (None, 2048)         0           lstm[0][1]                       \n",
            "                                                                 syllable_dense_line_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "add_c_line_1 (Add)              (None, 2048)         0           lstm[0][2]                       \n",
            "                                                                 syllable_dense_line_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "syllable_input_line_2 (InputLay (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "syllable_dense_line_2 (Dense)   (None, 2048)         4096        syllable_input_line_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "char_input_line_2 (InputLayer)  (None, None, 47)     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "add_h_line_2 (Add)              (None, 2048)         0           lstm[1][1]                       \n",
            "                                                                 syllable_dense_line_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "add_c_line_2 (Add)              (None, 2048)         0           lstm[1][2]                       \n",
            "                                                                 syllable_dense_line_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "output_line_0 (Dense)           (None, None, 47)     96303       lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "output_line_1 (Dense)           (None, None, 47)     96303       lstm[1][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "output_line_2 (Dense)           (None, None, 47)     96303       lstm[2][0]                       \n",
            "==================================================================================================\n",
            "Total params: 17,471,629\n",
            "Trainable params: 17,471,629\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Train on 4267 samples, validate on 475 samples\n",
            "Epoch 1/1\n",
            "4267/4267 [==============================] - 233s 55ms/step - loss: 6.9814 - output_line_0_loss: 2.3309 - output_line_1_loss: 2.3501 - output_line_2_loss: 2.2984 - val_loss: 5.2251 - val_output_line_0_loss: 1.7438 - val_output_line_1_loss: 1.7673 - val_output_line_2_loss: 1.7147\n",
            "\n",
            "Epoch 00001: loss improved from inf to 6.98143, saving model to output_test/2048-01-6.98-5.23.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f5d58d61358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRhPskjPU78i",
        "colab_type": "text"
      },
      "source": [
        "# Test Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGBBkPQaPcvb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GeneratorLine:\n",
        "    def __init__(self, name, training_line, lstm, n_tokens):\n",
        "        self.char_input = Input(shape=(None, n_tokens), name='char_input_%s' % name)\n",
        "\n",
        "        self.syllable_input = Input(shape=(1,), name='syllable_input_%s' % name)\n",
        "        self.syllable_dense = Dense(lstm.units, activation='relu', name='syllable_dense_%s' % name)\n",
        "        self.syllable_dense_output = self.syllable_dense(self.syllable_input)\n",
        "\n",
        "        self.h_input = Input(shape=(lstm.units,), name='h_input_%s' % name)\n",
        "        self.c_input = Input(shape=(lstm.units,), name='c_input_%s' % name)\n",
        "        initial_state = [self.h_input, self.c_input]\n",
        "\n",
        "        self.lstm = lstm\n",
        "\n",
        "        self.lstm_out, self.lstm_h, self.lstm_c = self.lstm(self.char_input, initial_state=initial_state)\n",
        "\n",
        "        self.output_dense = Dense(n_tokens, activation='softmax', name='output_%s' % name)\n",
        "        self.output = self.output_dense(self.lstm_out)\n",
        "\n",
        "        self.syllable_dense.set_weights(training_line.syllable_dense.get_weights())\n",
        "        self.output_dense.set_weights(training_line.output_dense.get_weights())\n",
        "\n",
        "def sample(preds, temperature=1):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "class Generator:\n",
        "    def __init__(self, lstm, lines, tf_session, tokenizer, n_tokens, max_line_length):\n",
        "        self.tf_session = tf_session\n",
        "        self.tokenizer = tokenizer\n",
        "        self.n_tokens = n_tokens\n",
        "        self.max_line_length = max_line_length\n",
        "\n",
        "        self.lstm = LSTM(\n",
        "            lstm.units, return_state=True, return_sequences=True,\n",
        "            name='generator_lstm'\n",
        "        )\n",
        "        self.lstm1 = LSTM(\n",
        "            lstm.units, return_state=True, return_sequences=True,\n",
        "            name='generator_lstm'\n",
        "        )\n",
        "        self.lines = [\n",
        "            GeneratorLine(\n",
        "                'generator_line_%s' % i,\n",
        "                lines[i], self.lstm, self.n_tokens\n",
        "            ) for i in range(3)\n",
        "        ]\n",
        "        self.lstm.set_weights(lstm.get_weights())\n",
        "        #self.lstm1.set_weights(lstm1.get_weights())\n",
        "\n",
        "    def generate(self, syllables=[11, 11, 11], temperature=1, first_char=None):\n",
        "        output = []\n",
        "        h = None\n",
        "        c = None\n",
        "\n",
        "        if first_char is None:\n",
        "            first_char = chr(int(np.random.randint(ord('a'), ord('z')+1)))\n",
        "\n",
        "        next_char = self.tokenizer.texts_to_sequences(first_char)[0][0]\n",
        "\n",
        "        for i in range(3):\n",
        "            line = self.lines[i]\n",
        "            s = self.tf_session.run(\n",
        "                line.syllable_dense_output,\n",
        "                feed_dict={\n",
        "                    line.syllable_input: [[syllables[i]]]\n",
        "                }\n",
        "            )\n",
        "\n",
        "            if h is None:\n",
        "                h = s\n",
        "                c = s\n",
        "            else:\n",
        "                h = h + s\n",
        "                c = c + s\n",
        "\n",
        "            line_output = [next_char]\n",
        "            tokens = self.n_tokens\n",
        "            end = False\n",
        "            next_char = None\n",
        "            for i in range(self.max_line_length):\n",
        "                char, h, c = self.tf_session.run(\n",
        "                    [line.output, line.lstm_h, line.lstm_c],\n",
        "                    feed_dict={\n",
        "                        line.char_input: [[\n",
        "                            np_utils.to_categorical(\n",
        "                                line_output[-1],\n",
        "                                num_classes=self.n_tokens\n",
        "                            )\n",
        "                        ]],\n",
        "                        line.h_input: h,\n",
        "                        line.c_input: c\n",
        "                    }\n",
        "                )\n",
        "\n",
        "                char = sample(char[0,0], temperature)\n",
        "                if char == 1 and not end:\n",
        "                    end = True\n",
        "                if char != 1 and end:\n",
        "                    next_char = char\n",
        "                    char = 1\n",
        "                line_output.append(char)\n",
        "            \n",
        "            cleaned_text = self.tokenizer.sequences_to_texts([\n",
        "                line_output\n",
        "            ])[0].strip()[1:].replace(\n",
        "                '   ', '\\n'\n",
        "            ).replace(' ', '').replace('\\n', ' ')\n",
        "\n",
        "            print(cleaned_text)\n",
        "            output.append(cleaned_text)\n",
        "        print('\\n')\n",
        "        return output"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pnP7ymtU78o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "5b5494d1-833d-4968-fd56-4ad89bc50cab"
      },
      "source": [
        "for boh in range(30):\n",
        "  generator = Generator(lstm, lines, tf_session, tokenizer, n_tokens, max_line_length)\n",
        "  generator.generate()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "eamontua scsr ll pirltto i orvio perle\n",
            "d cémusè che no' se tur gllsa sa di furt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-f8cb8d918d3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mboh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_line_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-45-bae516e922a1>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, syllables, temperature, first_char)\u001b[0m\n\u001b[1;32m     90\u001b[0m                             np_utils.to_categorical(\n\u001b[1;32m     91\u001b[0m                                 \u001b[0mline_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                                 \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                             )\n\u001b[1;32m     94\u001b[0m                         ]],\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/np_utils.py\u001b[0m in \u001b[0;36mto_categorical\u001b[0;34m(y, num_classes, dtype)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \"\"\"\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'NoneType'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5eixLLynDWN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}