{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "provenance": [],
      "authorship_tag": "ABX9TyOvn5EDSfA5NiLaqP2CHMC1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0qfnRB53CwQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "84913b49-fd8c-45d0-d5ca-05b086828138"
      },
      "source": [
        "import time\n",
        "import re\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import requests\n",
        "import collections\n",
        "import pickle\n",
        "import copy, random\n",
        "np.random.seed(1234)\n",
        "import nltk as nl\n",
        "nl.download('punkt')\n",
        "\n",
        "from itertools import zip_longest\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Reshape, BatchNormalization, Dense, Dropout,       # General\n",
        "    Embedding, LSTM, Dense, GRU,                              # RNN\n",
        "    Conv2D, Conv2DTranspose, LeakyReLU, MaxPool2D, Flatten    # CNN\n",
        ")\n",
        "from tensorflow.keras.activations import elu, relu, softmax, sigmoid\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTpaQwHZsp2P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "d69767cc-1be2-499f-87d7-373a8f27e482"
      },
      "source": [
        "#########################################################################################################\n",
        "if False:\n",
        "    class LearningConfig:\n",
        "        def __init__(self, optimizer=tf.compat.v1.train.AdagradOptimizer, lr=0.001, norm_clip=100.0, batch_size=32,\n",
        "                    lr_scheduler=True, max_no_improve=5, n_epochs=10,\n",
        "                    trunc_norm_init_std=1e-4, is_test=False):\n",
        "            self.batch_size = batch_size\n",
        "            self.n_steps = 3500\n",
        "            self.n_epochs = n_epochs\n",
        "            self.lr = lr\n",
        "            # self.optimizer = tf.train.AdamOptimizer\n",
        "            self.optimizer = optimizer\n",
        "            self.norm_clip = norm_clip\n",
        "\n",
        "            self.lr_scheduler = lr_scheduler\n",
        "            self.max_no_improve = max_no_improve\n",
        "\n",
        "            self.trunc_norm_init_std = trunc_norm_init_std\n",
        "\n",
        "            self.is_test = is_test\n",
        "\n",
        "\n",
        "    class SyLMConfig:\n",
        "        def __init__(self, vocab_size,\n",
        "                    sentence_max_len, emb_size,\n",
        "                    rnn_size, cell_type, keep_prob,\n",
        "                    proj_size,\n",
        "                    optimizer, lr, norm_clip, batch_size, n_epochs, lr_scheduler, max_no_improve,\n",
        "                    restore_model,\n",
        "                    is_test=False):\n",
        "            \"\"\"\n",
        "            Language Model Configuration Class\n",
        "            :param vocab_size:\n",
        "            :param is_test:\n",
        "            \"\"\"\n",
        "\n",
        "            self.is_test = is_test\n",
        "            # General\n",
        "            self.input_vocab_size = self.output_vocab_size = vocab_size\n",
        "            self.sentence_max_len = sentence_max_len\n",
        "            self.input_emb_size = self.output_emb_size = emb_size\n",
        "\n",
        "            # Encoder\n",
        "            self.encoder_rnn_size = rnn_size\n",
        "            self.encoder_keep_prob = keep_prob if not is_test else 1\n",
        "            self.cell_type = cell_type\n",
        "            self.wrap_attention = False\n",
        "\n",
        "            self.proj_size = proj_size\n",
        "\n",
        "            # Learning\n",
        "            self.learning = LearningConfig(optimizer, lr, norm_clip, batch_size, lr_scheduler, max_no_improve, n_epochs=n_epochs, is_test=is_test)\n",
        "            self.restore_model = restore_model\n",
        "\n",
        "        def set_params(self, dict):\n",
        "            def _recursive_call(items, attr_id):\n",
        "                items = list(items)\n",
        "                attr_name = items[attr_id][0]\n",
        "                attr_values = items[attr_id][1]\n",
        "                for attr_value in attr_values:\n",
        "                    setattr(self, attr_name, attr_value)\n",
        "                    if attr_id == (len(items) - 1):  # base case[\n",
        "                        yield self\n",
        "                    else:\n",
        "                        for i in _recursive_call(items, attr_id + 1):\n",
        "                            yield i\n",
        "\n",
        "            items = dict.items()\n",
        "            for i in _recursive_call(items, 0):\n",
        "                yield i\n",
        "\n",
        "        def set_tied_params(self):\n",
        "            self.output_emb_size = self.input_emb_size\n",
        "\n",
        "\n",
        "    def setup_config(FLAGS):\n",
        "        \"\"\"\n",
        "        :param FLAGS:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "        config = SyLMConfig(vocab_size=FLAGS.vocab_size, sentence_max_len=FLAGS.sentence_max_len, emb_size=FLAGS.emb_size,\n",
        "                rnn_size=FLAGS.enc_size, cell_type=FLAGS.rnn_cell_type, keep_prob=FLAGS.enc_keep_prob, proj_size=FLAGS.proj_size,\n",
        "                optimizer=tf.train.AdamOptimizer, lr=FLAGS.lr, norm_clip=FLAGS.norm_clip, batch_size=FLAGS.batch_size, n_epochs=FLAGS.n_epochs,\n",
        "                lr_scheduler=FLAGS.lr_scheduler, max_no_improve=FLAGS.max_no_improve, restore_model=FLAGS.restore_model, is_test=False)\n",
        "\n",
        "        val_config = copy.deepcopy(config)\n",
        "        val_config.encoder_keep_prob = 1.0\n",
        "        val_config.is_test = True\n",
        "\n",
        "        gen_config = copy.deepcopy(val_config)\n",
        "\n",
        "        return config, val_config, gen_config\n",
        "\n",
        "\n",
        "\n",
        "    def get_vowels(w):\n",
        "        return [(c, i) for i, c in enumerate(w) if is_vowel(c)]\n",
        "\n",
        "    def get_next_vowel_pos(word, start_pos=0):\n",
        "        c = word[start_pos]\n",
        "        count = start_pos\n",
        "        while not is_vowel(c) or count == len(word):\n",
        "            count += 1\n",
        "            c = word[count]\n",
        "\n",
        "        return count + 1\n",
        "\n",
        "\n",
        "    def get_seq_hyphen_len(words):\n",
        "        return sum([len(hyphenation(w)) for w in words])\n",
        "\n",
        "    def hyp2word(hyphen, hyp_rev_vocabulary, special_tokens):\n",
        "        word = ''\n",
        "        for hyp in hyphen:\n",
        "            if hyp not in special_tokens and hyp in hyp_rev_vocabulary:\n",
        "                word += hyp_rev_vocabulary[hyp]\n",
        "            elif hyp not in special_tokens:\n",
        "                word += '<UNK>'\n",
        "\n",
        "        return word\n",
        "\n",
        "\n",
        "    def get_hyps(batch, hyp_rev_vocabulary, special_tokens):\n",
        "        hyps = []\n",
        "        for seq in batch:\n",
        "            hyps.append('')\n",
        "            for hyphen in seq:\n",
        "                hyps[-1] += hyp2word(hyphen, hyp_rev_vocabulary, special_tokens) + ' '\n",
        "\n",
        "        return hyps\n",
        "\n",
        "\n",
        "    def print_hyps(batch, hyp_rev_vocabulary, special_tokens):\n",
        "        for seq in batch:\n",
        "            to_print = ''\n",
        "            for hyphen in seq:\n",
        "                to_print.join(hyp2word(hyphen, hyp_rev_vocabulary, special_tokens) + ' ')\n",
        "            print(to_print)\n",
        "\n",
        "\n",
        "    def print_paired_hyps(file, batch_y, batch_z, hyp_rev_vocabulary, special_tokens):\n",
        "        hyps_y = get_hyps(batch_y, hyp_rev_vocabulary, special_tokens)\n",
        "        hyps_z = get_hyps(batch_z, hyp_rev_vocabulary, special_tokens)\n",
        "\n",
        "        for i in range(len(hyps_y)):\n",
        "            print_and_write(file, 'Ground Truth: ' + hyps_y[i])\n",
        "            print_and_write(file, 'Prediction: ' + hyps_z[i])\n",
        "\n",
        "\n",
        "    def print_paired_output(file, batch_y, batch_z, rev_vocabulary, special_tokens, end_of_tokens=None):\n",
        "\n",
        "        def output2string(batch, rev_vocabulary, special_tokens, end_of_tokens):\n",
        "            output_strings = []\n",
        "            for seq in batch:\n",
        "                to_print = ''\n",
        "                for token in seq:\n",
        "                    if token in special_tokens:\n",
        "                        to_print += ' '\n",
        "                    elif end_of_tokens and token in end_of_tokens:\n",
        "                        to_print += '\\n'\n",
        "                    elif token in rev_vocabulary:\n",
        "                        to_print += rev_vocabulary[token]\n",
        "                    else:\n",
        "                        to_print += '<UNK>'\n",
        "                output_strings.append(to_print)\n",
        "\n",
        "            return output_strings\n",
        "\n",
        "        hyps_y = output2string(batch_y, rev_vocabulary, special_tokens, end_of_tokens)\n",
        "        hyps_z = output2string(batch_z, rev_vocabulary, special_tokens, end_of_tokens)\n",
        "\n",
        "        for i in range(len(hyps_y)):\n",
        "            print_and_write(file, \"\\n================================================\")\n",
        "            print_and_write(file, 'Ground Truth: ' + hyps_y[i] + \"\\n\")\n",
        "            print_and_write(file, 'Prediction: ' + hyps_z[i] + \"\\n\")\n",
        "            print_and_write(file, \"================================================\\n\")\n",
        "\n",
        "\n",
        "    def hyps2words(ids, sep, pad=-1, with_sep=False, omit_pad=True):\n",
        "        \"\"\"\n",
        "        Splits the list of ids according to a separator.\n",
        "\n",
        "        :param ids: a list of hyphen' ids\n",
        "        :param sep: the separator token (INT value)\n",
        "        :param pad (optional): id of the pad token (INT value)\n",
        "        :param with_sep (optional): separators are omitted if True,\n",
        "        otherwise they are kept\n",
        "        :param omit_pad (optional): true or false to decide whether\n",
        "        to omit pad token or not\n",
        "        :return: a list of elements, where each element\n",
        "        is a list of tokens composing a word\n",
        "        \"\"\"\n",
        "\n",
        "        words = [[]]\n",
        "        for id in ids:\n",
        "            if id == sep:\n",
        "                if with_sep:\n",
        "                    words.append([sep])\n",
        "                words.append([])\n",
        "            elif id != pad or (id == pad and not omit_pad):\n",
        "                words[-1].append(id)\n",
        "\n",
        "        return words\n",
        "\n",
        "\n",
        "    def hyps2word(hyps):\n",
        "        \"\"\"\n",
        "        Converts a list of hyphens to a string.\n",
        "        :param hyps: a list of strings (hyphens)\n",
        "        :return: string of concatenated hyphens\n",
        "        \"\"\"\n",
        "\n",
        "        return ''.join(hyps)\n",
        "\n",
        "\n",
        "    def id2hyp(id, rev_dictionary):\n",
        "        \"\"\"\n",
        "        Converts an id to its respective hyphen in rev_dictionary.\n",
        "        :param id: an integer\n",
        "        :param rev_dictionary: a Python dictionary\n",
        "        with integer as keys and strings as values.\n",
        "        :return: a string\n",
        "        \"\"\"\n",
        "        return rev_dictionary[id] if id in rev_dictionary else '<UNK>'\n",
        "\n",
        "\n",
        "    def hyp2id(hyp, dictionary):\n",
        "        \"\"\"\n",
        "            Converts an hyphen to its respective id in dictionary.\n",
        "            :param hyp: a string\n",
        "            :param dictionary: a Python dictionary\n",
        "            with string as keys and integers as values.\n",
        "            :return: an integer\n",
        "            \"\"\"\n",
        "        return dictionary[hyp] if hyp in dictionary else 0\n",
        "\n",
        "\n",
        "    def ids2hyps(ids, rev_dictionary):\n",
        "        \"\"\"\n",
        "        Maps a list of ids into a list of hyphens.\n",
        "        :param ids: a list of ints\n",
        "        :param rev_dictionary:  Python dictionary\n",
        "        with string as keys and integers as values.\n",
        "        :return: a list of strings (hyphens)\n",
        "        \"\"\"\n",
        "        return [id2hyp(id, rev_dictionary) for id in ids]\n",
        "\n",
        "\n",
        "    def is_word(hyps, word_dictionary):\n",
        "        return hyps2word(hyps) in word_dictionary\n",
        "\n",
        "\n",
        "    def hyps2verses(ids, eos, eot):\n",
        "        \"\"\"\n",
        "        Split the list of hypens in different lists, separated\n",
        "        by the sep token.\n",
        "        :param ids: a list of hyphen' ids\n",
        "        :param eos: the separator token (INT) (id corresponding to <EOS>)\n",
        "        :return: a list of verses, each verse is a list of syllables\n",
        "        \"\"\"\n",
        "\n",
        "        verses = [[]]\n",
        "        for id in ids:\n",
        "            if id == eot:\n",
        "                break\n",
        "            elif id == eos:\n",
        "                verses.append([])\n",
        "            else:\n",
        "                verses[-1].append(id)\n",
        "\n",
        "        if len(verses[-1]) < 1:\n",
        "            verses = verses[:-1]\n",
        "\n",
        "        return verses\n",
        "\n",
        "\n",
        "    def hyphenize_list(l):\n",
        "        \"\"\"\n",
        "        Given a corpus, the function tokenizes it by dividing words into syllables\n",
        "        adding also a separator token between words.\n",
        "        :param l: a list of sequences, each sequence is a list of words (strings).\n",
        "        :return: a list of sequences, but each sequence is a list of syllables.\n",
        "        \"\"\"\n",
        "\n",
        "        sentences = [seq_hyphentation(s) for s in l]\n",
        "        sep_token = \"<SEP>\"\n",
        "        hyphenated_sentences = []\n",
        "        for s in sentences:\n",
        "            hyphenated_sentences.append([])\n",
        "            for w in s:\n",
        "                hyphenated_sentences[-1].extend(w)\n",
        "                hyphenated_sentences[-1].append(sep_token)\n",
        "            hyphenated_sentences[-1] = hyphenated_sentences[-1][:-1]  # removes last sep_token\n",
        "\n",
        "        return hyphenated_sentences\n",
        "\n",
        "    def read_words_from_folder(data_path):\n",
        "        try:\n",
        "            nl.data.find('tokenizers/punkt')\n",
        "        except LookupError:\n",
        "            nl.download('punkt')\n",
        "        list_files = [os.path.join(data_path, f)\n",
        "                    for f in os.listdir(data_path)\n",
        "                    if os.path.isfile(os.path.join(data_path, f))]\n",
        "        words = []\n",
        "        for filename in list_files:\n",
        "            with open(filename, \"r\") as f:\n",
        "                try:\n",
        "                    st = f.read()\n",
        "                except UnicodeDecodeError:\n",
        "                    print(\"File \"+filename+\" decode error: SKIPPED\")\n",
        "                    continue\n",
        "                st = st.translate(string.punctuation)\n",
        "                data = nl.word_tokenize(st)\n",
        "                del(st)\n",
        "                words.extend(data)\n",
        "        return words\n",
        "\n",
        "    def build_dataset_of_tokens(tokens, vocabulary_size, special_tokens=[]):\n",
        "        '''\n",
        "        Given a list of tokens, it creates a dictionary mapping each token to a unique id.\n",
        "        :param tokens: a list of strings.\n",
        "        E.g. [\"the\", \"cat\", \"is\", ... \".\", \"the\", \"house\" ,\"is\" ...].\n",
        "        NB: Here you should put all your token instances of the corpus.\n",
        "        :param vocabulary_size: The number of elements of your vocabulary. If there are more\n",
        "        than 'vocabulary_size' elements on tokens, it considers only the 'vocabulary_size'\n",
        "        most frequent ones.\n",
        "        :param special_tokens: Optional. Useful to add special tokens in vocabulary. I\n",
        "        f you don't have any, keep it empty.\n",
        "        :return: data: the mapped tokens list;\n",
        "        count: a dictionary containing the number of occurrences in 'tokens' for each\n",
        "        element on your dictionary.\n",
        "        dictionary: a python dictionary that associates a token with a unique integer identifier.\n",
        "        reverse_dictionary: a python dictionary mapping a unique integer identifier to its token.\n",
        "        E.g.\n",
        "        dictionary:{\"UNK\": 0, \"a\": 1, \"the\": 2, ....}\n",
        "        reverse_dictionary:{0:\"UNK\", 1:\"a\", 2:\"the\"}\n",
        "        '''\n",
        "        # counting occurrences of each token\n",
        "        count = [['UNK', -1]]\n",
        "        count.extend(collections.Counter(tokens).most_common(vocabulary_size - 1))  # takes only the most frequent ones\n",
        "        dictionary = dict()\n",
        "        for word, _ in count:\n",
        "            dictionary[word] = len(dictionary)\n",
        "\n",
        "        for token in special_tokens:\n",
        "            dictionary[token[0]] = token[1]\n",
        "\n",
        "        data = list()\n",
        "        unk_count = 0\n",
        "        for word in tokens:\n",
        "            if word in dictionary:\n",
        "                index = dictionary[word]\n",
        "            else:\n",
        "                index = 0  # dictionary['UNK']\n",
        "                unk_count += 1\n",
        "            data.append(index)\n",
        "        count[0][1] = unk_count\n",
        "        reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
        "        return data, count, dictionary, reverse_dictionary\n",
        "\n",
        "    def save_dictionary_tsv(filepath, dictionary, count):\n",
        "        keys, values = zip(*count)\n",
        "        with open(filepath, 'w') as f:\n",
        "            f.write('Word\\tFrequency\\n')\n",
        "            for k, v in dictionary.items():\n",
        "                if k in keys:\n",
        "                    f.write(k + '\\t' + str(values[keys.index(k)]) + '\\n')\n",
        "                else:\n",
        "                    f.write(k + '\\t' + '0' + '\\n')\n",
        "\n",
        "\n",
        "    def k_frequent(words_data, k):\n",
        "        counter = collections.Counter(words_data)\n",
        "        most = counter.most_common(k)\n",
        "        res = [most[i][0] for i in range(len(most))]\n",
        "        return res\n",
        "\n",
        "\n",
        "    def toWord(chars):\n",
        "        str =''\n",
        "        for c in chars:\n",
        "            if(c<2):\n",
        "                continue\n",
        "            elif(c==2):\n",
        "                break\n",
        "            str = str+chr(c)\n",
        "        return str\n",
        "\n",
        "\n",
        "    def grouper(iterable, n, fillvalue=None):\n",
        "        args = [iter(iterable)] * n\n",
        "        return zip_longest(*args, fillvalue=fillvalue)\n",
        "\n",
        "    def get_cantica(filename, encoding=None):\n",
        "        f_cantica = []\n",
        "        count = 1\n",
        "        with open(filename, \"r\", encoding=encoding) as f:\n",
        "            for line in f:\n",
        "                sentence = line.strip()\n",
        "                tokenized_sentence = nl.word_tokenize(sentence)\n",
        "\n",
        "                if len(tokenized_sentence) == 2:\n",
        "                    # setting feature cantica for each canto\n",
        "                    if count <= 34:\n",
        "                        f_cantica.append([1, 0, 0])\n",
        "                    elif count > 34 and count <= 67:\n",
        "                        f_cantica.append([0, 1, 0])\n",
        "                    else:\n",
        "                        f_cantica.append([0, 0, 1])\n",
        "\n",
        "                    count += 1\n",
        "\n",
        "        return f_cantica\n",
        "\n",
        "    def create_BAB_tercets(cantos):\n",
        "        tercets = []\n",
        "        for canto in cantos:\n",
        "            for v,verse in enumerate(canto):\n",
        "                if v%3 == 1:\n",
        "                    tercets.append([])\n",
        "                if v > 0: tercets[-1].append(verse)\n",
        "\n",
        "            tercets = tercets[:-1] # removes the last malformed tercets (only 2 verses)\n",
        "\n",
        "        return tercets\n",
        "\n",
        "\n",
        "    def get_poetry(filename, encoding=None):\n",
        "        # raw_data = read_words(filename=filename)\n",
        "\n",
        "        raw_data, words = [], []\n",
        "        # with open(filename, \"r\", encoding ='latin-1') as f:\n",
        "        with open(filename, \"r\", encoding=encoding) as f:\n",
        "            for line in f:\n",
        "                sentence = line.strip()\n",
        "                if len(sentence) > 1:\n",
        "                    sentence = sentence.translate(string.punctuation)\n",
        "                    tokenized_sentence = nl.word_tokenize(sentence)\n",
        "                    tokenized_sentence = [w.lower() for w in tokenized_sentence if len(w)>0]\n",
        "                    tokenized_sentence = [w for w in tokenized_sentence if \",\" not in w]\n",
        "                    tokenized_sentence = [w for w in tokenized_sentence if \".\" not in w]\n",
        "                    tokenized_sentence = [w for w in tokenized_sentence if \":\" not in w]\n",
        "                    tokenized_sentence = [w for w in tokenized_sentence if \";\" not in w]\n",
        "                    ts = []\n",
        "                    [ts.extend(re.split(\"(\\')\", e)) for e in tokenized_sentence]\n",
        "                    tokenized_sentence = [w for w in ts if len(w) > 0]\n",
        "\n",
        "                    if len(tokenized_sentence) > 2:\n",
        "                        raw_data.append(tokenized_sentence)\n",
        "                        words.extend(tokenized_sentence)\n",
        "\n",
        "        return raw_data, words\n",
        "\n",
        "    def get_decameron(filename):\n",
        "        raw_data, words = [], []\n",
        "        with open(filename, \"r\", encoding='latin-1') as f:\n",
        "            for line in f:\n",
        "                raw_sentences = line.strip()\n",
        "                if len(raw_sentences) > 1:\n",
        "                    sentences = raw_sentences.translate(string.punctuation)\n",
        "                    s_list = re.split(\"\\.\", sentences)\n",
        "                    for s in s_list:\n",
        "                        tokenized_sentences = nl.word_tokenize(s)\n",
        "                        tokenized_sentences = [w.lower() for w in tokenized_sentences if len(w)>0]\n",
        "                        tokenized_sentences = [w for w in tokenized_sentences if \",\" not in w]\n",
        "                        # tokenized_sentences = [w for w in tokenized_sentences if \".\" not in w]\n",
        "                        tokenized_sentences = [w for w in tokenized_sentences if \":\" not in w]\n",
        "                        ts = []\n",
        "                        [ts.extend(re.split(\"(\\')\", e)) for e in tokenized_sentences]\n",
        "                        tokenized_sentences = [w for w in ts if len(w) > 0]\n",
        "\n",
        "                        if len(tokenized_sentences) > 1 and len(tokenized_sentences) < 30:\n",
        "                            raw_data.append(tokenized_sentences)\n",
        "                            words.extend(tokenized_sentences)\n",
        "\n",
        "        return raw_data, words\n",
        "\n",
        "\n",
        "    def build_dataset_from_dict(raw_data, dictionary, config, shuffle=True):\n",
        "        '''\n",
        "        Converts all the tokens in raw_data by mapping each token with its corresponding\n",
        "        value in the dictionary. In case of token not in the dictionary, they are assigned to\n",
        "        a specific id. Each sequence is padded up to the sentence_max_len setup in the config.\n",
        "\n",
        "        :param raw_data: list of sequences, each sequences is a list of tokens (strings).\n",
        "        :param dictionary: a python dictionary having as keys strings\n",
        "        and int tokens as values.\n",
        "        :param config: config object from class Config.\n",
        "        :param shuffle: Optional. If True data are shuffled.\n",
        "        :return: A list of sequences where each token in each sequence is an int id.\n",
        "        '''\n",
        "        dataset = []\n",
        "        for sentence in raw_data:\n",
        "            sentence_ids = [config._GO]\n",
        "            sentence_ids.extend([dictionary[w] if w in dictionary else dictionary[\"UNK\"] for w in sentence])\n",
        "            sentence_ids.append(config._EOS)\n",
        "            sentence_ids = pad_list(sentence_ids, config._PAD, config.sentence_max_len)\n",
        "\n",
        "            dataset.append(sentence_ids)\n",
        "\n",
        "        if shuffle:\n",
        "            return random.sample(dataset, len(dataset))\n",
        "        else:\n",
        "            return dataset\n",
        "\n",
        "\n",
        "    def build_stanzas_dataset_from_dict(raw_data, dictionary, config, shuffle=True):\n",
        "        dataset = []\n",
        "        for stanza in raw_data:\n",
        "            stanza_ids = [config._GO]\n",
        "            for sentence in stanza:\n",
        "                sentence_ids = [dictionary[w] if w in dictionary else dictionary[\"UNK\"] for w in sentence]\n",
        "                sentence_ids.append(config._EOS)\n",
        "                stanza_ids.extend(sentence_ids)\n",
        "            # stanza_ids.append(config._EOT)\n",
        "            stanza_ids[-1] = config._EOT\n",
        "            stanza_ids = pad_list(stanza_ids, config._PAD, config.sentence_max_len)\n",
        "\n",
        "            dataset.append(stanza_ids)\n",
        "\n",
        "        if shuffle:\n",
        "            return random.sample(dataset, len(dataset))\n",
        "        else:\n",
        "            return dataset\n",
        "\n",
        "\n",
        "    def build_stanzas_dataset_from_subword_dict(raw_data, dictionary, config, shuffle=True):\n",
        "        dataset = []\n",
        "        for stanza in raw_data:\n",
        "            stanza_ids = [pad_list([config._GO], config._PAD, config.word_max_len)]\n",
        "            for sentence in stanza:\n",
        "                for word in sentence:\n",
        "                    hyp_ids = [dictionary[hyp] if hyp in dictionary else dictionary[\"UNK\"] for hyp in word]\n",
        "                    hyp_ids.append(config._EOW)\n",
        "                    hyp_ids = pad_list(hyp_ids, config._PAD, config.word_max_len)\n",
        "                    stanza_ids.extend([hyp_ids])\n",
        "\n",
        "                stanza_ids.append(pad_list([config._EOS], config._PAD, config.word_max_len))\n",
        "            # stanza_ids.append(config._EOT)\n",
        "            stanza_ids[-1] = pad_list([config._EOT], config._PAD, config.word_max_len)\n",
        "            stanza_ids = pad_list(stanza_ids, [config._PAD] * config.word_max_len, config.sentence_max_len)\n",
        "\n",
        "            dataset.append(stanza_ids)\n",
        "\n",
        "        if shuffle:\n",
        "            return random.sample(dataset, len(dataset))\n",
        "        else:\n",
        "            return dataset\n",
        "\n",
        "\n",
        "    def build_stanzas_dataset_from_chars(raw_data, config):\n",
        "        dataset = []\n",
        "        for stanza in raw_data:\n",
        "            stanza_ids = []\n",
        "            for sentence in stanza:\n",
        "                sentence_ids = [to_chars([w], config.word_max_len)[0] for w in sentence]\n",
        "                sentence_ids.append(to_chars([\"<EOS>\"], config.word_max_len)[0])\n",
        "                stanza_ids.extend(sentence_ids)\n",
        "            stanza_ids.append(to_chars([\"<EOT>\"], config.word_max_len)[0])\n",
        "            stanza_ids = pad_list(stanza_ids, to_chars([\"<pad>\"], config.word_max_len)[0], config.sentence_max_len)\n",
        "\n",
        "            dataset.append(stanza_ids)\n",
        "        return random.sample(dataset, len(dataset))\n",
        "\n",
        "    def create_lm_target(x, config):\n",
        "        return [e[1:] + [config._PAD] for e in x]\n",
        "\n",
        "\n",
        "    def create_hyp_lm_target(x, config):\n",
        "        return [e[1:] + [[config._PAD] * config.word_max_len] for e in x]\n",
        "\n",
        "\n",
        "    def batches(x, y, batch_size=128):\n",
        "\n",
        "        # Shuffle sentences\n",
        "        sentences_ids = random.sample(range(len(x)), len(x))\n",
        "\n",
        "        # Generator for batch\n",
        "        batch_x, batch_y = [], []\n",
        "        if batch_size is None:\n",
        "            batch_size = len(x)\n",
        "        for id in sentences_ids:\n",
        "            batch_x.append(x[id])\n",
        "            batch_y.append(y[id])\n",
        "            if len(batch_x) % batch_size == 0:\n",
        "                yield batch_x, batch_y\n",
        "                batch_x, batch_y = [], []\n",
        "\n",
        "\n",
        "    def _batches(iterable, batch_size=128):\n",
        "\n",
        "        # Shuffle sentences\n",
        "        x = list(zip(*iterable))\n",
        "        sentences_ids = random.sample(range(len(x)), len(x))\n",
        "\n",
        "        # Generator for batch\n",
        "        batch, batch_y = [], []\n",
        "        if batch_size is None:\n",
        "            batch_size = len(x)\n",
        "        for id in sentences_ids:\n",
        "            batch.append(x[id])\n",
        "            if len(batch) % batch_size == 0:\n",
        "                yield batch\n",
        "                batch = []\n",
        "\n",
        "\n",
        "    def batches3(chars, x, y, batch_size=128):\n",
        "\n",
        "        # Shuffle sentences\n",
        "        sentences_ids = random.sample(range(len(x)), len(x))\n",
        "\n",
        "        # Generator for batch\n",
        "        batch_chars, batch_x, batch_y = [], [], []\n",
        "        if batch_size is None:\n",
        "            batch_size = len(x)\n",
        "        for id in sentences_ids:\n",
        "            batch_chars.append(chars[id])\n",
        "            batch_x.append(x[id])\n",
        "            batch_y.append(y[id])\n",
        "            if len(batch_x) % batch_size == 0:\n",
        "                yield batch_chars, batch_x, batch_y\n",
        "                batch_chars, batch_x, batch_y = [], [], []\n",
        "\n",
        "\n",
        "    def are_in_rhyme(w1, w2):\n",
        "\n",
        "        def find_termination(w):\n",
        "            v_count = 0\n",
        "            for i in range(len(w)-1, -1, -1):\n",
        "                if is_vowel(w[i]):\n",
        "                    v_count += 1\n",
        "                    if v_count == 2 and i < len(w)-2:\n",
        "                        # se è la seconda vocale che trovo, è una vocale e non è nel penultimo carattere\n",
        "                        return w[i:]\n",
        "                    elif v_count == 3:\n",
        "                        # se è la terza vocale che trovo\n",
        "                        return w[i:]\n",
        "\n",
        "        t1 = find_termination(w1)\n",
        "        t2 = find_termination(w2)\n",
        "        if t1 and t2 and t1 == t2 and len(t1) > 1 and len(t2) > 1:\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def general_batches(iterables, full_size, batch_size=128):\n",
        "        if batch_size == None:\n",
        "            batch_size = full_size\n",
        "\n",
        "        batch = []\n",
        "        for i in range(len(iterables)):\n",
        "            batch.append([])\n",
        "\n",
        "        for id in range(full_size):\n",
        "            for i,it in enumerate(iterables):\n",
        "                batch[i].append(it[id])\n",
        "            if len(batch[0]) % batch_size == 0:\n",
        "                yield batch\n",
        "                batch = []\n",
        "                for i in range(len(iterables)):\n",
        "                    batch.append([])\n",
        "\n",
        "    def get_sonnets(verses, N=14):\n",
        "        sonnets, words = [], []\n",
        "        for v, verse in enumerate(verses):\n",
        "            if v % N == 0:\n",
        "                sonnets.append([])\n",
        "\n",
        "            tokenized_sentence = [w.lower().strip() for w in nl.word_tokenize(verse) if len(w) > 0]\n",
        "            tokenized_sentence = [w for w in tokenized_sentence if \",\" not in w]\n",
        "            tokenized_sentence = [w for w in tokenized_sentence if \".\" not in w]\n",
        "            tokenized_sentence = [w for w in tokenized_sentence if \":\" not in w]\n",
        "            tokenized_sentence = [w for w in tokenized_sentence if \";\" not in w]\n",
        "            tokenized_sentence = [w for w in tokenized_sentence if \"«\" not in w]\n",
        "            tokenized_sentence = [w for w in tokenized_sentence if \"»\" not in w]\n",
        "            tokenized_sentence = [w for w in tokenized_sentence if \"‘\" not in w]\n",
        "            tokenized_sentence = [w for w in tokenized_sentence if \"‘‘\" not in w]\n",
        "            tokenized_sentence = [w for w in tokenized_sentence if \"(\" not in w]\n",
        "            tokenized_sentence = [w for w in tokenized_sentence if \")\" not in w]\n",
        "            ts = []\n",
        "            [ts.extend(re.split(\"(\\')\", e)) for e in tokenized_sentence]\n",
        "            tokenized_sentence = [w for w in ts if len(w) > 0]\n",
        "            sonnets[-1].append(tokenized_sentence)\n",
        "            words.extend(tokenized_sentence)\n",
        "\n",
        "        return sonnets, words\n",
        "\n",
        "    def load_paisa_data(filename, n_docs=15000):\n",
        "        with open(filename, 'r', encoding=\"utf8\") as f:\n",
        "            it_data = []\n",
        "            c = 0\n",
        "            for line in f:\n",
        "                if line[0] not in ['#', '<'] and len(line) > 150:\n",
        "                    line = re.sub('\\d+', '0', line)\n",
        "                    line = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', 'url', line)\n",
        "                    line = re.sub(',', ' , ', line)\n",
        "                    line = re.sub(';', ' ; ', line)\n",
        "                    line = re.sub(':', ' : ', line)\n",
        "                    line = re.sub('\\(', ' ( ', line)\n",
        "                    line = re.sub('\\)', ' ) ', line)\n",
        "                    line = re.sub(\"\\'\", \" \\' \", line)\n",
        "                    line = line.lower()\n",
        "                    it_data.append(line)\n",
        "                    if c >= n_docs:\n",
        "                        break\n",
        "                    c += 1\n",
        "\n",
        "            return it_data\n",
        "\n",
        "\n",
        "    def build_dataset_with_context(raw_data, dictionary, config, pad_len):\n",
        "        dataset = []\n",
        "        for sentence in raw_data:\n",
        "            stanza_ids = [config._GO]\n",
        "            sentence_ids = [dictionary[w] if w in dictionary else dictionary[\"UNK\"] for w in sentence]\n",
        "            sentence_ids.append(config._EOS)\n",
        "            stanza_ids.extend(sentence_ids)\n",
        "            stanza_ids = pad_list(stanza_ids, config._PAD, pad_len)\n",
        "\n",
        "            dataset.append(stanza_ids)\n",
        "\n",
        "        return dataset\n",
        "\n",
        "    def split_in_ngrams(x, n=3, pad_token='_'):\n",
        "        '''\n",
        "        Arguments:\n",
        "        'x' a string of text, e.g. a word a sentence.\n",
        "        'pad_token' the token to add to unfinished trigrams.\n",
        "\n",
        "        Returns:\n",
        "        a list containing 'x' split in trigrams.'''\n",
        "\n",
        "        trigrams = []\n",
        "        for i, ch in enumerate(x):\n",
        "            if i % n == 0:\n",
        "                trigrams.append('')\n",
        "\n",
        "            trigrams[-1] += ch\n",
        "\n",
        "        for p in range(3 - len(trigrams[-1])):\n",
        "            trigrams[-1] += pad_token\n",
        "\n",
        "        return trigrams\n",
        "\n",
        "\n",
        "    def get_ngrams(l, n=3, pad_token='_'):\n",
        "        '''\n",
        "        Arguments:\n",
        "        'l' an input list of words.\n",
        "        'pad_token' the token to add to unfinished trigrams.\n",
        "\n",
        "        Returns:\n",
        "        a list of the trigrams of 'l'.'''\n",
        "        trigrams = []\n",
        "        for w in l:\n",
        "            t = split_in_ngrams(w, n, pad_token)\n",
        "            trigrams.extend(t)\n",
        "\n",
        "        return trigrams\n",
        "\n",
        "\n",
        "    def get_ngrams_from_tercets(tercets, n=3, pad_token='_'):\n",
        "        '''\n",
        "        Arguments:\n",
        "        'tercets' a list of tercets.\n",
        "        'pad_token' the token to add to unfinished trigrams.\n",
        "\n",
        "        Returns:\n",
        "        'tr_tercets' a list of tercets splitted in trigrams\n",
        "        'trigrams' a list of all the trigrams'''\n",
        "        tr_tercets, trigrams = [], []\n",
        "        for tercet in tercets:\n",
        "            tr_tercets.append([])\n",
        "            for verse in tercet:\n",
        "                # t = split_in_ngrams(verse, pad_token) # CHANGE IN get_ngrams IF tercets are not raw\n",
        "                t = get_ngrams(verse, n, pad_token)\n",
        "                tr_tercets[-1].append(t)\n",
        "                trigrams.extend(t)\n",
        "\n",
        "        return tr_tercets, trigrams\n",
        "\n",
        "\n",
        "    def get_ngrams_from_canzoniere(verses, n=3, pad_token='_'):\n",
        "        '''\n",
        "        Arguments:\n",
        "        'verses' a list of verses.\n",
        "        'pad_token' the token to add to unfinished trigrams.\n",
        "\n",
        "        Returns:\n",
        "        'tr_tercets' a list of tercets splitted in trigrams\n",
        "        'trigrams' a list of all the trigrams'''\n",
        "        tr_verses, trigrams = [], []\n",
        "        for verse in verses:\n",
        "            # t = split_in_ngrams(verse, pad_token) # CHANGE IN get_ngrams IF tercets are not raw\n",
        "            t = get_ngrams(verse, n, pad_token)\n",
        "            tr_verses.append(t)\n",
        "            trigrams.extend(t)\n",
        "\n",
        "        return tr_verses, trigrams\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def get_quatrains(shake_sonnets):\n",
        "        quatrains, words = [],[]\n",
        "        for sonnet in shake_sonnets:\n",
        "            quatrains.extend([sonnet[:4],sonnet[4:8], sonnet[:8:-2]])\n",
        "\n",
        "        return quatrains\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def get_context_dataset(sequences):\n",
        "        x_seq = []\n",
        "        for seq in sequences:\n",
        "            for i, c_verse in enumerate(seq):\n",
        "                context = []\n",
        "                [context.extend(v) for v in seq[:i]]\n",
        "                x_seq.append((context, c_verse))\n",
        "\n",
        "        return x_seq\n",
        "\n",
        "\n",
        "    def load_textual_corpus(filename, max_n_lines=-1):\n",
        "        '''\n",
        "        General function to load a textual file from corpus. A list of sentences\n",
        "        is given as return, data is also cleaned up to remove urls and numbers. Sentences are\n",
        "        split according to the dot '.' .\n",
        "        :param filename: the name of the textual file to load.\n",
        "        :param max_n_lines: Maximum number of lines to load from the file. Useful for huge corpora.\n",
        "        Set to -1 (default), to get all the lines.\n",
        "        :return: A list of sentences, where each sentence is a list of words.\n",
        "        '''\n",
        "\n",
        "        with open(filename, 'r', encoding=\"utf8\", errors='ignore') as f:\n",
        "            data = []\n",
        "            c = 0\n",
        "            for line in f:\n",
        "                line = re.sub('\\d+', '0', line)\n",
        "                line = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', 'url', line)\n",
        "                line = re.sub(',', ' , ', line)\n",
        "                line = re.sub(';', ' ; ', line)\n",
        "                line = re.sub(':', ' : ', line)\n",
        "                line = re.sub('\\(', ' ( ', line)\n",
        "                line = re.sub('\\)', ' ) ', line)\n",
        "                line = re.sub(\"\\'\", \" \\' \", line)\n",
        "                line = line.lower()\n",
        "                data.extend([s.strip().split() for s in line.split(\".\") if len(s) > 2])\n",
        "\n",
        "                if max_n_lines > 0 and c >= max_n_lines:\n",
        "                    break\n",
        "                c += 1\n",
        "\n",
        "            return data\n",
        "\n",
        "\n",
        "    def load_poetry_corpus(filename, scheme_n=3):\n",
        "        with open(filename, 'r', encoding=\"utf8\", errors='ignore') as f:\n",
        "            data = []\n",
        "            j = 0\n",
        "            for line in f:\n",
        "                if len(line) > 1:\n",
        "                    line = re.sub(',', ' , ', line)\n",
        "                    line = re.sub(';', ' ; ', line)\n",
        "                    line = re.sub(':', ' : ', line)\n",
        "                    line = re.sub('\\(', ' ( ', line)\n",
        "                    line = re.sub('\\)', ' ) ', line)\n",
        "                    line = re.sub(\"\\'\", \" \\' \", line)\n",
        "                    line = line.lower()\n",
        "                    tokenized_line = nl.word_tokenize(line)\n",
        "                    # tokenized_line = tokenized_line.split()\n",
        "                    tokenized_line = [w for w in tokenized_line if \",\" not in w]\n",
        "                    tokenized_line = [w for w in tokenized_line if \".\" not in w]\n",
        "                    tokenized_line = [w for w in tokenized_line if \":\" not in w]\n",
        "                    tokenized_line = [w for w in tokenized_line if \";\" not in w]\n",
        "                    tokenized_line = [w for w in tokenized_line if \"!\" not in w]\n",
        "                    tokenized_line = [w for w in tokenized_line if \"?\" not in w]\n",
        "                    tokenized_line = [w for w in tokenized_line if \"«\" not in w]\n",
        "                    tokenized_line = [w for w in tokenized_line if \"»\" not in w]\n",
        "\n",
        "                    if j % scheme_n == 0:\n",
        "                        data.append([])\n",
        "                    data[-1].append(tokenized_line)\n",
        "\n",
        "                    j += 1\n",
        "\n",
        "            return data\n",
        "\n",
        "\n",
        "    def load_dantes_poetry(filenames, stanza_size=3):\n",
        "        '''\n",
        "\n",
        "        :param filenames: A list of filenames containing raw textual Dante's data.\n",
        "        :param stanza_size: The number of verses to group together.\n",
        "        :return: a list of stanzas, each stanza is a list of verses\n",
        "        '''\n",
        "\n",
        "        poetries = []\n",
        "        for filename in filenames:\n",
        "            poetries.extend(load_poetry_corpus(filename, stanza_size))\n",
        "\n",
        "        return poetries\n",
        "\n",
        "\n",
        "    def load_dantes_prose(filenames, max_n_lines=-1):\n",
        "        '''\n",
        "        Function to retrieve Dante's prose.\n",
        "        :param filenames: A list of filenames containing raw textual Dante's data.\n",
        "        :param max_n_lines: Maximum number of lines to load from the files.\n",
        "        Default value -1, indicates no limit.\n",
        "        :return: A list of sentences.\n",
        "        '''\n",
        "\n",
        "        prose = []\n",
        "        for filename in filenames:\n",
        "            sentences = load_textual_corpus(filename, max_n_lines)\n",
        "            prose.extend(sentences)\n",
        "\n",
        "        return prose\n",
        "\n",
        "    \n",
        "\n",
        "    _PAD = 0\n",
        "    _GO = 1\n",
        "    _EOW = 2\n",
        "    _UNK = 3\n",
        "    def to_chars(words, word_max_size):\n",
        "        _PAD = 0\n",
        "        _GO = 1\n",
        "        _EOW = 2\n",
        "        _UNK = 3\n",
        "        chars = ['_PAD', '_GO', '_EOW', '_UNK', ' ', '!', '\"', '#', '$', '%', '&', '\\'', '(', ')',\n",
        "                '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
        "                ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I',\n",
        "                'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y',\n",
        "                'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i',\n",
        "                'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y',\n",
        "                'z', '{', '|', '}', 'ì', 'ò', 'ù', 'è', 'é', 'à']\n",
        "        char_dict = {}\n",
        "        for char in chars:\n",
        "            char_dict[char] = len(char_dict)\n",
        "        char_words = np.ndarray(shape=[len(words), word_max_size], dtype=np.int32)\n",
        "        for i in range(len(words)):\n",
        "            if words[i]==\"<pad>\":\n",
        "                char_words[i][:] = _PAD\n",
        "                continue\n",
        "            char_words[i][0]=_GO\n",
        "            for j in range(1,word_max_size):\n",
        "                if j < len(words[i])+1:\n",
        "                    if words[i][j-1] in char_dict:\n",
        "                        char_words[i][j] = char_dict[words[i][j-1]]\n",
        "                    else:\n",
        "                        char_words[i][j] = _UNK\n",
        "                elif j == len(words[i])+1:\n",
        "                    char_words[i][j] = _EOW\n",
        "                else:\n",
        "                    char_words[i][j] = _PAD\n",
        "            if char_words[i][word_max_size-1] != _PAD:\n",
        "                char_words[i][word_max_size-1] = _EOW\n",
        "        return char_words\n",
        "\n",
        "    '''\n",
        "    Read a file (filename) and return the textual content of the file in a vector of words\n",
        "    '''\n",
        "\n",
        "    def read_words(filename, max_len=None):\n",
        "        try:\n",
        "            nl.data.find('tokenizers/punkt')\n",
        "        except LookupError:\n",
        "            nl.download('punkt')\n",
        "        with open(filename, \"r\") as f:\n",
        "            st = f.read()\n",
        "            st = st.translate(string.punctuation)\n",
        "            data = nl.word_tokenize(st)\n",
        "            del(st)\n",
        "            if max_len:\n",
        "                return data[:max_len]\n",
        "            return data\n",
        "############################################################################\n",
        "def get_hyp_lm_tercets(tercets):\n",
        "    new_tercets = []\n",
        "    for tercet in tercets:\n",
        "        new_tercets.append([])\n",
        "        for verse in tercet:\n",
        "            new_tercets[-1].append([])\n",
        "            for hyp_w in verse:\n",
        "                new_tercets[-1][-1].extend(hyp_w)\n",
        "                new_tercets[-1][-1].append('<SEP>')\n",
        "            new_tercets[-1][-1] = new_tercets[-1][-1][:-1]\n",
        "\n",
        "    return new_tercets\n",
        "\n",
        "def is_vowel(c):\n",
        "    return c in 'aeiouAEIOUàìíèéùúüòï'\n",
        "\n",
        "def unsplittable_cons():\n",
        "    u_cons = []\n",
        "    for c1 in ('b', 'c', 'd', 'f', 'g', 'p', 't', 'v'):\n",
        "        for c2 in ('l', 'r'):\n",
        "            u_cons.append(c1 + c2)\n",
        "\n",
        "    others = ['gn', 'gh', 'ch']\n",
        "    u_cons.extend(others)\n",
        "    return u_cons\n",
        "\n",
        "\n",
        "def are_cons_to_split(c1, c2):\n",
        "    to_split = ('cq', 'cn', 'lm', 'rc', 'bd', 'mb', 'mn', 'ld', 'ng', 'nd', 'tm', 'nv', 'nc', 'ft', 'nf', 'gm', 'fm', 'rv', 'fp')\n",
        "    return (c1 + c2) in to_split or (not is_vowel(c1) and (c1 == c2)) or ((c1 + c2) not in unsplittable_cons()) and (\n",
        "        (not is_vowel(c1)) and (not is_vowel(c2)) and c1 != 's')\n",
        "\n",
        "\n",
        "def is_diphthong(c1, c2):\n",
        "    return (c1 + c2) in ('ia', 'ie', 'io', 'iu', 'ua', 'ue', 'uo', 'ui', 'ai', 'ei', 'oi', 'ui', 'au', 'eu', 'ïe', 'iú', 'iù')\n",
        "\n",
        "\n",
        "def is_triphthong(c1, c2, c3):\n",
        "    return (c1 + c2 + c3) in ('iai', 'iei', 'uoi', 'uai', 'uei', 'iuo')\n",
        "\n",
        "\n",
        "def is_toned_vowel(c):\n",
        "    return c in 'àìèéùòï'\n",
        "\n",
        "def has_vowels(sy):\n",
        "    for c in sy:\n",
        "        if is_vowel(c):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def hyphenation(word):\n",
        "    \"\"\"\n",
        "    Split word in syllables\n",
        "    :param word: input string\n",
        "    :return: a list containing syllables of the word\n",
        "    \"\"\"\n",
        "    if not word or word == '':\n",
        "        return []\n",
        "    # elif len(word) == 3 and (is_vowel(word[1]) and is_vowel(word[2]) and not is_toned_vowel(word[2]) and (\n",
        "    #     not is_diphthong(word[1], word[2]) or (word[1] == 'i'))):\n",
        "    elif len(word) == 3 and (is_vowel(word[1]) and is_vowel(word[2]) and not is_toned_vowel(word[2]) and (\n",
        "        not is_diphthong(word[1], word[2]))):\n",
        "        return [word[:2]] + [word[2]]\n",
        "    elif len(word) == 3 and is_vowel(word[0]) and not is_vowel(word[1]) and is_vowel(word[2]):\n",
        "        return [word[:2]] + [word[2]]\n",
        "    elif len(word) == 3:\n",
        "        return [word]\n",
        "\n",
        "    syllables = []\n",
        "    is_done = False\n",
        "    count = 0\n",
        "    while not is_done and count <= len(word) - 1:\n",
        "        syllables.append('')\n",
        "        c = word[count]\n",
        "        while not is_vowel(c) and count < len(word) - 1:\n",
        "            syllables[-1] = syllables[-1] + c\n",
        "            count += 1\n",
        "            c = word[count]\n",
        "\n",
        "        syllables[-1] = syllables[-1] + word[count]\n",
        "\n",
        "        if count == len(word) - 1:\n",
        "            is_done = True\n",
        "        else:\n",
        "            count += 1\n",
        "\n",
        "            if count < len(word) and not is_vowel(word[count]):\n",
        "                if count == len(word) - 1:\n",
        "                    syllables[-1] += word[count]\n",
        "                    count += 1\n",
        "                elif count + 1 < len(word) and are_cons_to_split(word[count], word[count + 1]):\n",
        "                    syllables[-1] += word[count]\n",
        "                    count += 1\n",
        "                elif count + 2 < len(word) and not is_vowel(word[count + 1]) and not is_vowel(word[count + 2]) and word[\n",
        "                    count] != 's':\n",
        "                    syllables[-1] += word[count]\n",
        "                    count += 1\n",
        "            elif count < len(word):\n",
        "                if count + 1 < len(word) and is_triphthong(word[count - 1], word[count], word[count + 1]):\n",
        "                    syllables[-1] += word[count] + word[count + 1]\n",
        "                    count += 2\n",
        "                elif is_diphthong(word[count - 1], word[count]):\n",
        "                    syllables[-1] += word[count]\n",
        "                    count += 1\n",
        "\n",
        "                if count + 1 < len(word) and are_cons_to_split(word[count], word[count + 1]):\n",
        "                    syllables[-1] += word[count]\n",
        "                    count += 1\n",
        "\n",
        "            else:\n",
        "                is_done = True\n",
        "\n",
        "    if not has_vowels(syllables[-1]) and len(syllables) > 1:\n",
        "        syllables[-2] = syllables[-2] + syllables[-1]\n",
        "        syllables = syllables[:-1]\n",
        "\n",
        "    return syllables\n",
        "\n",
        "\n",
        "\n",
        "def get_dc_hyphenation(canti):\n",
        "    hyp_canti, hyp_tokens = [], []\n",
        "    for canto in canti:\n",
        "        hyp_canti.append([])\n",
        "        for verso in canto:\n",
        "            syllables = seq_hyphentation(verso)\n",
        "            hyp_canti[-1].append(syllables)\n",
        "            for syllable in syllables:\n",
        "                hyp_tokens.extend(syllable)\n",
        "\n",
        "    return hyp_canti, hyp_tokens\n",
        "\n",
        "\n",
        "def seq_hyphentation(words):\n",
        "    \"\"\"\n",
        "    Converts words in a list of strings into lists of syllables\n",
        "    :param words: a list of words (strings)\n",
        "    :return: a list of lists containing word syllables\n",
        "    \"\"\"\n",
        "    return [hyphenation(w) for w in words]\n",
        "\n",
        "\n",
        "def get_dc_cantos(filename, encoding=None):\n",
        "    # raw_data = read_words(filename=filename)\n",
        "    cantos, words, raw = [], [], []\n",
        "    with open(filename, \"r\", encoding=encoding) as f:\n",
        "        for line in f:\n",
        "            sentence = line.strip()\n",
        "            sentence = str.replace(sentence, \"\\.\", \" \\. \")\n",
        "            sentence = str.replace(sentence, \"[\", '')\n",
        "            sentence = str.replace(sentence, \"]\", '')\n",
        "            sentence = str.replace(sentence, \"-\", '')\n",
        "            sentence = str.replace(sentence, \";\", \" ; \")\n",
        "            sentence = str.replace(sentence, \",\", \" , \")\n",
        "            # sentence = str.replace(sentence, \" \\'\", '')\n",
        "            sentence = str.replace(sentence, \"\\'\", ' \\' ')\n",
        "            if len(sentence) > 1:\n",
        "                # sentence = sentence.translate(string.punctuation)\n",
        "                tokenized_sentence = nl.word_tokenize(sentence)\n",
        "                # tokenized_sentence = sentence.split()\n",
        "                tokenized_sentence = [w.lower() for w in tokenized_sentence if len(w) > 0]\n",
        "                tokenized_sentence = [w for w in tokenized_sentence if \",\" not in w]\n",
        "                tokenized_sentence = [w for w in tokenized_sentence if \".\" not in w]\n",
        "                tokenized_sentence = [w for w in tokenized_sentence if \":\" not in w]\n",
        "                tokenized_sentence = [w for w in tokenized_sentence if \";\" not in w]\n",
        "                tokenized_sentence = [w for w in tokenized_sentence if \"«\" not in w]\n",
        "                tokenized_sentence = [w for w in tokenized_sentence if \"»\" not in w]\n",
        "                # ts = []\n",
        "                ts = tokenized_sentence\n",
        "                # [ts.extend(re.split(\"(\\')\", e)) for e in tokenized_sentence]\n",
        "                tokenized_sentence = [w for w in ts if len(w) > 0]\n",
        "\n",
        "                if len(tokenized_sentence) == 2:\n",
        "                    cantos.append([])\n",
        "                    raw.append([])\n",
        "                elif len(tokenized_sentence) > 2:\n",
        "                    raw[-1].append(sentence)\n",
        "                    cantos[-1].append(tokenized_sentence)\n",
        "                    words.extend(tokenized_sentence)\n",
        "\n",
        "    return cantos, words, raw\n",
        "\n",
        "\n",
        "def create_tercets(cantos):\n",
        "    tercets = []\n",
        "    for i,canto in enumerate(cantos):\n",
        "        for v,verse in enumerate(canto):\n",
        "            if v%3 == 0:\n",
        "                tercets.append([])\n",
        "\n",
        "            tercets[-1].append(verse)\n",
        "        tercets = tercets[:-1]  # removes the last malformed tercets (only 2 verses)\n",
        "\n",
        "    return tercets\n",
        "\n",
        "def pad_list(l, pad_token, max_l_size, keep_lasts=False, pad_right=True):\n",
        "    \"\"\"\n",
        "    Adds a padding token to a list\n",
        "    inputs:\n",
        "    :param l: input list to pad.\n",
        "    :param pad_token: value to add as padding.\n",
        "    :param max_l_size: length of the new padded list to return,\n",
        "    it truncates lists longer that 'max_l_size' without adding\n",
        "    padding values.\n",
        "    :param keep_lasts: If True, preserves the max_l_size last elements\n",
        "    of a sequence (by keeping the same order).  E.g.:\n",
        "    if keep_lasts is True and max_l_size=3 [1,2,3,4] becomes [2,3,4].\n",
        "\n",
        "\n",
        "    :return: the list padded or truncated.\n",
        "    \"\"\"\n",
        "    to_pad = []\n",
        "    max_l = min(max_l_size, len(l))  # maximum len\n",
        "    l_init = len(l) - max_l if len(l) > max_l and keep_lasts else 0  # initial position where to sample from the list\n",
        "    l_end = len(l) if len(l) > max_l and keep_lasts else max_l\n",
        "    for i in range(l_init, l_end):\n",
        "        to_pad.append(l[i])\n",
        "\n",
        "    # for j in range(len(l), max_l_size):\n",
        "    #     to_pad.append(pad_token)\n",
        "    pad_tokens = [pad_token] * (max_l_size-len(l))\n",
        "    padded_l = to_pad + pad_tokens if pad_right else pad_tokens + to_pad\n",
        "\n",
        "    return padded_l\n",
        "\n",
        "\n",
        "def save_data(data, file):\n",
        "    with open(file, 'wb') as output:\n",
        "        pickle.dump(data, output, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def load_data(file):\n",
        "    with open(file, 'rb') as obj:\n",
        "        return pickle.load(obj)\n",
        "\n",
        "def print_and_write(file, s):\n",
        "    print(s)\n",
        "    file.write(s)\n",
        "\n",
        "\n",
        "class Vocabulary(object):\n",
        "    def __init__(self, vocab_size=None):\n",
        "        self.dictionary = dict()\n",
        "        self.rev_dictionary = dict()\n",
        "        self.count = []\n",
        "        self.special_tokens = []\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    def build_vocabulary_from_counts(self, count, special_tokens=[]):\n",
        "        \"\"\"\n",
        "        Sets all the attributes of the Vocabulary object.\n",
        "        :param count: a list of lists as follows: [['token', number_of_occurrences],...]\n",
        "        :param special_tokens: a list of strings. E.g. ['<EOS>', '<PAD>',...]\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "\n",
        "        dictionary = dict()\n",
        "        for word, _ in count:\n",
        "            dictionary[word] = len(dictionary)\n",
        "\n",
        "        # adding eventual special tokens to the dictionary (e.g. <EOS>,<PAD> etc..)\n",
        "        d = len(dictionary)\n",
        "        for i, token in enumerate(special_tokens):\n",
        "            dictionary[token] = d + i\n",
        "\n",
        "        self.count = count\n",
        "        self.dictionary = dictionary\n",
        "        self.rev_dictionary = dict(zip(self.dictionary.values(), self.dictionary.keys()))\n",
        "        self.special_tokens = special_tokens\n",
        "        self.vocab_size = len(dictionary)\n",
        "\n",
        "    def build_vocabulary_from_tokens(self, tokens, vocabulary_size=None, special_tokens=[]):\n",
        "        \"\"\"\n",
        "        Given a list of tokens, it sets the Vocabulary object attributes by constructing\n",
        "        a dictionary mapping each token to a unique id.\n",
        "        :param tokens: a list of strings.\n",
        "         E.g. [\"the\", \"cat\", \"is\", ... \".\", \"the\", \"house\" ,\"is\" ...].\n",
        "         NB: Here you should put all your token instances of the corpus.\n",
        "        :param vocabulary_size: The number of elements of your vocabulary. If there are more\n",
        "        than 'vocabulary_size' elements on tokens, it considers only the 'vocabulary_size'\n",
        "        most frequent ones.\n",
        "        :param special_tokens: Optional. A list of strings. Useful to add special tokens in vocabulary.\n",
        "        If you don't have any, keep it empty.\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "\n",
        "        vocabulary_size = vocabulary_size if vocabulary_size is not None else self.vocab_size\n",
        "        vocabulary_size = vocabulary_size - (len(special_tokens) + 1) if vocabulary_size else None\n",
        "        # counts occurrences of each token\n",
        "        count = [['<UNK>', -1]]\n",
        "        count.extend(collections.Counter(tokens).most_common(vocabulary_size))  # takes only the most frequent ones, if size is None takes them all\n",
        "        self.build_vocabulary_from_counts(count, special_tokens)  # actually build the vocabulary\n",
        "        self._set_unk_count(tokens)  # set the number of OOV instances\n",
        "\n",
        "    @staticmethod\n",
        "    def merge_vocabulary(vocab0, vocab1, vocabulary_size=-1):\n",
        "        \"\"\"\n",
        "        Merge two Vocabulary objects into a new one.\n",
        "        :param vocab0: first Vocabulary object\n",
        "        :param vocab1: second Vocabulary object\n",
        "        :param vocabulary_size: parameter to decide the merged vocabulary size.\n",
        "        With default value -1, all the words of both vocabularies are preserved.\n",
        "        When set to 0, the size of the vocabulary is set to the size of vocab0,\n",
        "        when set to 1 it is kept the size of vocab1.\n",
        "        :return: a new vocabulary\n",
        "        \"\"\"\n",
        "        # get size of the new vocabulary\n",
        "        vocab_size = vocab0.vocab_size + vocab1.vocab_size if vocabulary_size == -1 else vocabulary_size\n",
        "        merged_special_tokens = list(set(vocab0.special_tokens) | set(vocab1.special_tokens))\n",
        "\n",
        "        # merge the counts from the two vocabularies and then selects the most_common tokens\n",
        "        merged_counts = collections.Counter(dict(vocab0.count)) + collections.Counter(dict(vocab1.count))\n",
        "        merged_counts = merged_counts.most_common(vocab_size)\n",
        "        count = [['<UNK>', -1]]\n",
        "        count.extend(merged_counts)\n",
        "\n",
        "        # create the new vocabulary\n",
        "        merged_vocab = Vocabulary(vocab_size)\n",
        "        merged_vocab.build_vocabulary_from_counts(count, merged_special_tokens)\n",
        "        return merged_vocab\n",
        "\n",
        "    @staticmethod\n",
        "    def merge_vocabularies(vocab_list, vocab_size=None):\n",
        "        \"\"\"\n",
        "        Join a list of vocabularies into a new one.\n",
        "        :param vocab_list: a list of Vocabulary objects\n",
        "        :param vocab_size: the maximum size of the merged vocabulary.\n",
        "        :return: a vocabulary merging them all.\n",
        "        \"\"\"\n",
        "        vocab_size = vocab_size if vocab_size else sum([v.vocab_size for v in vocab_list])\n",
        "        merged_vocab = Vocabulary(vocab_size)\n",
        "        for voc in vocab_list:\n",
        "            merged_vocab = Vocabulary.merge_vocabulary(merged_vocab, voc, vocab_size)\n",
        "        return merged_vocab\n",
        "\n",
        "    def string2id(self, dataset):\n",
        "        \"\"\"\n",
        "        Converts a dataset of strings into a dataset of ids according to the object dictionary.\n",
        "        :param dataset: any string-based dataset with any nested lists.\n",
        "        :return: a new dataset, with the same shape of dataset, where each string is mapped into its\n",
        "        corresponding id associated in the dictionary (0 for unknown tokens).\n",
        "        \"\"\"\n",
        "\n",
        "        def _recursive_call(items):\n",
        "            new_items = []\n",
        "            for item in items:\n",
        "                if isinstance(item, str) or isinstance(item, int) or isinstance(item, float):\n",
        "                    new_items.append(self.word2id(item))\n",
        "                else:\n",
        "                    new_items.append(_recursive_call(item))\n",
        "            return new_items\n",
        "\n",
        "        return _recursive_call(dataset)\n",
        "\n",
        "    def id2string(self, dataset):\n",
        "        \"\"\"\n",
        "        Converts a dataset of integer ids into a dataset of string according to the reverse dictionary.\n",
        "        :param dataset: any int-based dataset with any nested lists. Allowed types are int, np.int32, np.int64.\n",
        "        :return: a new dataset, with the same shape of dataset, where each token is mapped into its\n",
        "        corresponding string associated in the reverse dictionary.\n",
        "        \"\"\"\n",
        "        def _recursive_call(items):\n",
        "            new_items = []\n",
        "            for item in items:\n",
        "                if isinstance(item, int) or isinstance(item, np.int) or isinstance(item, np.int32) or isinstance(item, np.int64):\n",
        "                    new_items.append(self.id2word(item))\n",
        "                else:\n",
        "                    new_items.append(_recursive_call(item))\n",
        "            return new_items\n",
        "\n",
        "        return _recursive_call(dataset)\n",
        "\n",
        "    def word2id(self, item):\n",
        "        \"\"\"\n",
        "        Maps a string token to its corresponding id.\n",
        "        :param item: a string.\n",
        "        :return: If the token belongs to the vocabulary, it returns an integer id > 0, otherwise\n",
        "        it returns the value associated to the unknown symbol, that is typically 0.\n",
        "        \"\"\"\n",
        "        return self.dictionary[item] if item in self.dictionary else self.dictionary['<UNK>']\n",
        "\n",
        "    def id2word(self, token_id):\n",
        "        \"\"\"\n",
        "        Maps an integer token to its corresponding string.\n",
        "        :param token_id: an integer.\n",
        "        :return: If the id belongs to the vocabulary, it returns the string\n",
        "        associated to it, otherwise it returns the string associated\n",
        "        to the unknown symbol, that is '<UNK>'.\n",
        "        \"\"\"\n",
        "\n",
        "        return self.rev_dictionary[token_id] if token_id in self.rev_dictionary else self.rev_dictionary[self.dictionary['<UNK>']]\n",
        "\n",
        "    def get_unk_count(self):\n",
        "        return self.count[0][1]\n",
        "\n",
        "    def _set_unk_count(self, tokens):\n",
        "        \"\"\"\n",
        "        Sets the number of OOV instances in the tokens provided\n",
        "        :param tokens: a list of tokens\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "        data = list()\n",
        "        unk_count = 0\n",
        "        for word in tokens:\n",
        "            if word in self.dictionary:\n",
        "                index = self.dictionary[word]\n",
        "            else:\n",
        "                index = 0  # dictionary['<UNK>']\n",
        "                unk_count += 1\n",
        "            data.append(index)\n",
        "        self.count[0][1] = unk_count\n",
        "\n",
        "    def add_element(self, name, is_special_token=False):\n",
        "        if name not in self.dictionary:\n",
        "            self.vocab_size += 1\n",
        "            self.dictionary[name] = self.vocab_size\n",
        "            self.rev_dictionary[self.vocab_size] = name\n",
        "\n",
        "            if is_special_token:\n",
        "                self.special_tokens = list(self.special_tokens)\n",
        "                self.special_tokens.append(name)\n",
        "\n",
        "            self.count.append([name, 1])\n",
        "\n",
        "    def set_vocabulary(self, dictionary, rev_dictionary, special_tokens, vocab_size):\n",
        "        self.dictionary = dictionary,\n",
        "        self.rev_dictionary = rev_dictionary\n",
        "        self.special_tokens = special_tokens\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    @staticmethod\n",
        "    def load_vocabulary(filename):\n",
        "        return load_data(filename)\n",
        "\n",
        "    def save_vocabulary(self, filename):\n",
        "        save_data(self, filename)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class SyLMDataset(object):\n",
        "    def __init__(self, config, sy_vocab=None):\n",
        "        self.config = config\n",
        "        self.vocabulary = sy_vocab\n",
        "\n",
        "        self.raw_train_x = []\n",
        "        self.raw_val_x = []\n",
        "        self.raw_test_x = []\n",
        "        self.raw_x = []\n",
        "\n",
        "        self.train_x, self.train_y = [], []\n",
        "        self.val_x, self.val_y = [], []\n",
        "        self.test_x, self.test_y = [], []\n",
        "        self.x, self.y = [], []\n",
        "\n",
        "    def initialize(self, sess):\n",
        "        pass\n",
        "\n",
        "    def load(self, sources):\n",
        "        \"\"\"\n",
        "        Extract raw texts form sources and gather them all together.\n",
        "        :param sources: a string or an iterable of strings containing the file(s)\n",
        "        to process in order to build the dataset.\n",
        "        :return: a list of raw strings.\n",
        "        \"\"\"\n",
        "        return NotImplementedError\n",
        "\n",
        "    def build(self, sources, split_size=0.8):\n",
        "        \"\"\"\n",
        "        :param sources: a string or an iterable of strings containing the file(s)\n",
        "        to process in order to build the dataset.\n",
        "        :param split_size: the size to split the dataset, set >=1.0 to not split.\n",
        "        \"\"\"\n",
        "\n",
        "        raw_x = self.load(sources)\n",
        "        # raw_x = self.tokenize([self.preprocess(ex) for ex in raw_x])  # fixme\n",
        "        # splitting data\n",
        "        self.raw_x = raw_x\n",
        "        if split_size < 1.0:\n",
        "            self.raw_train_x, self.raw_test_x = self.split(self.raw_x, train_size=split_size)\n",
        "            self.raw_train_x, self.raw_val_x = self.split(self.raw_train_x, train_size=split_size)\n",
        "        else:\n",
        "            self.raw_train_x = self.raw_x\n",
        "\n",
        "        if self.vocabulary is None:\n",
        "            # creates vocabulary\n",
        "            tokens = [item for sublist in self.raw_train_x for item in sublist]  # get tokens\n",
        "            special_tokens = (\"<GO>\", \"<PAD>\", \"<SEP>\", \"<EOS>\", \"<EOV>\")\n",
        "            self._create_vocab(tokens, special_tokens=special_tokens)\n",
        "\n",
        "        # creates x,y for train\n",
        "        self.train_x = self._build_dataset(self.raw_train_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "        self.train_y = self._build_dataset(self.raw_train_x, insert_go=False, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "\n",
        "        # creates x,y for validation\n",
        "        self.val_x = self._build_dataset(self.raw_val_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "        self.val_y = self._build_dataset(self.raw_val_x, insert_go=False, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "\n",
        "        # creates x,y for validation\n",
        "        self.test_x = self._build_dataset(self.raw_test_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "        self.test_y = self._build_dataset(self.raw_test_x, insert_go=False, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "\n",
        "    def _create_vocab(self, tokens, special_tokens=(\"<PAD>\", \"<GO>\", \"<SEP>\", \"<EOV>\", \"<EOS>\")):\n",
        "        \"\"\"\n",
        "        Create the vocabulary. Special tokens can be added to the tokens obtained from\n",
        "        the corpus.\n",
        "        :param tokens: a list of all the tokens in the corpus. Each token is a string.\n",
        "        :param special_tokens: a list of strings.\n",
        "        \"\"\"\n",
        "        vocab = Vocabulary(vocab_size=self.config.input_vocab_size)\n",
        "        vocab.build_vocabulary_from_tokens(tokens, special_tokens=special_tokens)\n",
        "        self.vocabulary = vocab\n",
        "\n",
        "    @staticmethod\n",
        "    def split(raw_data, train_size=0.8):\n",
        "        size = math.floor(len(raw_data)*train_size)\n",
        "        return raw_data[:size], raw_data[size:]\n",
        "\n",
        "    @staticmethod\n",
        "    def preprocess(txt):\n",
        "        return txt\n",
        "\n",
        "    @staticmethod\n",
        "    def shuffle(x):\n",
        "        return random.sample(x, len(x))\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenize(txt):\n",
        "        return txt\n",
        "\n",
        "    def _build_dataset(self, raw_data, max_len=100, insert_go=True, keep_lasts=False, pad_right=True, shuffle=True):\n",
        "        \"\"\"\n",
        "        Converts all the tokens in e1_raw_data by mapping each token with its corresponding\n",
        "        value in the dictionary. In case of token not in the dictionary, they are assigned to\n",
        "        a specific id. Each sequence is padded up to the seq_max_len setup in the config.\n",
        "\n",
        "        :param raw_data: list of sequences, each sequence is a list of tokens (strings).\n",
        "        :param max_len: max length of a sequence, crop longer and pad smaller ones.\n",
        "        :param insert_go: True to insert <GO>, False otherwise.\n",
        "        :param keep_lasts: True to truncate initial elements of a sequence.\n",
        "        :param pad_right: pad to the right (default value True), otherwise pads to left.\n",
        "        :param shuffle: Optional. If True data are shuffled.\n",
        "        :return: A list of sequences where each token in each sequence is an int id.\n",
        "        \"\"\"\n",
        "        dataset = []\n",
        "        for sentence in raw_data:\n",
        "            sentence_ids = [self.vocabulary.word2id(\"<GO>\")] if insert_go else []\n",
        "            sentence_ids.extend([self.vocabulary.word2id(w) for w in sentence])\n",
        "            sentence_ids.append(self.vocabulary.word2id(\"<EOS>\"))\n",
        "            sentence_ids = pad_list(sentence_ids, self.vocabulary.word2id(\"<PAD>\"), max_len, keep_lasts=keep_lasts, pad_right=pad_right)\n",
        "\n",
        "            dataset.append(sentence_ids)\n",
        "\n",
        "        if shuffle:\n",
        "            return random.sample(dataset, len(dataset))\n",
        "        else:\n",
        "            return dataset\n",
        "\n",
        "    def get_batches(self, batch_size=32):\n",
        "        \"\"\"\n",
        "        Iterator over the training set. Useful method to run experiments.\n",
        "        :param batch_size: size of the mini_batch\n",
        "        :return: input and target.\n",
        "        \"\"\"\n",
        "        x, y = self.train_x, self.train_y\n",
        "        \n",
        "        i = random.randint(0, batch_size)\n",
        "        batches = []\n",
        "        eov = self.vocabulary.word2id(\"<EOV>\")\n",
        "        # prepare batches\n",
        "        while i < len(x):\n",
        "            j = 0\n",
        "            batch_x, batch_y = [], []\n",
        "            while j < batch_size and i+j<len(x):\n",
        "                for c in x[i+j]:\n",
        "                  batch_x.append(c)\n",
        "                batch_x.append(eov)\n",
        "                for c in y[i+j]:\n",
        "                  batch_y.append(c)\n",
        "                batch_y.append(eov)\n",
        "                j += 1\n",
        "            i += batch_size\n",
        "\n",
        "            batches.append((batch_x, batch_y))\n",
        "\n",
        "        # shuffle\n",
        "        random.shuffle(batches)\n",
        "\n",
        "        # supply\n",
        "        i = 0\n",
        "        while i < len(batches):\n",
        "            yield batches[i][0], batches[i][1]\n",
        "            i += 1\n",
        "\n",
        "class DanteSyLMDataset(SyLMDataset):\n",
        "    def __init__(self, config, sy_vocab=None):\n",
        "        \"\"\"\n",
        "        Class to create a dataset from Dante Alighieri's Divine Comedy.\n",
        "        :param config: a Config object\n",
        "        :param sy_vocab: (optional) a Vocabulary object where tokens of the dictionary\n",
        "        are syllables. If None, the vocabulary is create automatically from the source.\n",
        "        \"\"\"\n",
        "        super().__init__(config, sy_vocab)\n",
        "\n",
        "    def load(self, sources):\n",
        "        \"\"\"\n",
        "        Load examples from dataset\n",
        "        :param sources: data filepath.\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        canti, _, raw = get_dc_cantos(filename=sources)  # get raw data from file\n",
        "        canti, tokens = get_dc_hyphenation(canti)  # converts each\n",
        "\n",
        "        tercets = create_tercets(canti)\n",
        "        tercets = get_hyp_lm_tercets(tercets)\n",
        "\n",
        "        x = []\n",
        "        for tercet in tercets:\n",
        "            x.append([])\n",
        "            for verse in tercet:\n",
        "                x[-1].extend(verse)\n",
        "                x[-1].append(\"<EOV>\")\n",
        "\n",
        "        #x = self.shuffle(x)\n",
        "        return x\n",
        "\n",
        "def seq2str(seq):\n",
        "    def output2string(batch, rev_vocabulary, special_tokens, end_of_tokens):\n",
        "        to_print = ''\n",
        "        for token in batch:\n",
        "            if token in special_tokens:\n",
        "                to_print += ' '\n",
        "            elif end_of_tokens and token in end_of_tokens:\n",
        "                to_print += '\\n'\n",
        "            elif token in rev_vocabulary:\n",
        "                to_print += rev_vocabulary[token]\n",
        "            else:\n",
        "                to_print += '<UNK>'\n",
        "        return to_print\n",
        "\n",
        "    return output2string(seq, poetry_sy_lm_dataset.vocabulary.rev_dictionary,\n",
        "      special_tokens=[poetry_sy_lm_dataset.vocabulary.word2id(\"<PAD>\"), 0, poetry_sy_lm_dataset.vocabulary.word2id(\"<SEP>\"),\n",
        "                      poetry_sy_lm_dataset.vocabulary.word2id(\"<GO>\"), poetry_sy_lm_dataset.vocabulary.word2id(\"<EOS>\")],\n",
        "      end_of_tokens=[poetry_sy_lm_dataset.vocabulary.word2id(\"<EOV>\")])\n",
        "\n",
        "class cnfg:\n",
        "  vocab_size = 1884\n",
        "  input_vocab_size = 1884\n",
        "  emb_size = 300\n",
        "  sentence_max_len = 75\n",
        "\n",
        "config = cnfg()\n",
        "poetry_sy_lm_dataset = DanteSyLMDataset(config, sy_vocab=None)\n",
        "url = \"https://gitlab.com/zugo91/nlgpoetry/-/raw/release/data/la_divina_commedia.txt\"\n",
        "response = requests.get(url)\n",
        "response.encoding = 'ISO-8859-1'\n",
        "fi = open(\"divcom.txt\",\"w\")\n",
        "fi.write(response.text)\n",
        "fi.close()\n",
        "data_path = os.path.join(os.getcwd(), \"divcom.txt\")  # dataset location, here just the name of the source file\n",
        "poetry_sy_lm_dataset.build(data_path, split_size=0.9)  # actual creation of  vocabulary (if not provided) and dataset\n",
        "print(\"Train size: \" + str(len(poetry_sy_lm_dataset.train_y)))\n",
        "print(\"Val size: \" + str(len(poetry_sy_lm_dataset.val_y)))\n",
        "print(\"Test size: \" + str(len(poetry_sy_lm_dataset.test_y)))\n",
        "\n",
        "batches = [b for b in poetry_sy_lm_dataset.get_batches(32)]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train size: 3815\n",
            "Val size: 424\n",
            "Test size: 472\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPuIBZ2xu0lw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(init, generator, num_generate = 100, temperature = 1.0):\n",
        "    text_generated = []\n",
        "    generator.reset_states()\n",
        "    sym = init\n",
        "    \n",
        "    for i in range(num_generate):\n",
        "        predictions = generator(sym)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "        sym = tf.expand_dims([predicted_id], 0)\n",
        "        text_generated.append(predicted_id)\n",
        "    return (seq2str(text_generated))\n",
        "\n",
        "init = []\n",
        "for i in range(3):\n",
        "  init += poetry_sy_lm_dataset.train_x[i]\n",
        "init = np.asarray(init)\n",
        "init = np.expand_dims(init, axis=0)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owXvvnB8EJ_U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0dc78cc3-ed6b-43e8-f4df-c421548c823d"
      },
      "source": [
        "vocab_size = poetry_sy_lm_dataset.vocabulary.vocab_size + 2 # TODO: idk\n",
        "batch_size = 450 \n",
        "embedding_size = 256  #power of 2 to exploit cache\n",
        "len_input = 100\n",
        "hidden_size = 256\n",
        "n_epochs = 1000\n",
        "learning_rate = 1e-4\n",
        "\n",
        "char_input = Input(shape=(batch_size,))\n",
        "RNN = Sequential([\n",
        "    Embedding(vocab_size, embedding_size,\n",
        "              batch_input_shape=(batch_size, None)),\n",
        "    \n",
        "    LSTM(len_input, return_sequences = True, dropout=0.0),\n",
        "    \n",
        "    Dense(hidden_size, activation = relu),\n",
        "    #Dropout(0.2),\n",
        "    \n",
        "    Dense(vocab_size)\n",
        "])\n",
        "RNN.summary()\n",
        "gen = Sequential([\n",
        "    Embedding(vocab_size, embedding_size,\n",
        "              batch_input_shape=(1, None)),\n",
        "    \n",
        "    LSTM(len_input, return_sequences = True, stateful=True),\n",
        "    \n",
        "    Dense(hidden_size, activation = relu), \n",
        "    \n",
        "    Dense(vocab_size)\n",
        "])\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
        "\n",
        "# This is an Autograph function\n",
        "# its decorator makes it a TF op - i.e. much faster\n",
        "@tf.function\n",
        "def train_on_batch(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predicted = RNN(x)\n",
        "        cross_entropy = tf.keras.losses.sparse_categorical_crossentropy(y, predicted, from_logits = True)\n",
        "        current_loss = tf.reduce_mean(cross_entropy)\n",
        "\n",
        "    gradients = tape.gradient(current_loss, RNN.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, RNN.trainable_variables))\n",
        "    return current_loss\n",
        "\n",
        "loss_history = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    start = time.time()\n",
        "    \n",
        "    for b in batches:\n",
        "        x, y = b[0], b[1]\n",
        "        x = np.asarray(x)\n",
        "        y = np.asarray(y)\n",
        "        x = np.expand_dims(x[:batch_size], axis=1)\n",
        "        y = np.expand_dims(y[:batch_size], axis=1)\n",
        "\n",
        "        current_loss = train_on_batch(x, y)\n",
        "        loss_history.append(current_loss)\n",
        "    \n",
        "    print(\"=========================== Epoch: {}.  \\t  Loss: {}  \\t  Time: {}ss\".format(\n",
        "        epoch, current_loss.numpy(), round(time.time()-start, 2)))\n",
        "    \n",
        "    if epoch % 10 == 9:\n",
        "        gen.set_weights(RNN.get_weights())\n",
        "        print(generate_text(init, gen))\n",
        "\n",
        "\n",
        "plt.plot(loss_history)\n",
        "plt.title(\"Training Loss\")\n",
        "plt.show()\n",
        "\n",
        "RNN.save(\"/lstm_dense.h5\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_6 (Embedding)      (450, None, 256)          460032    \n",
            "_________________________________________________________________\n",
            "lstm_6 (LSTM)                (450, None, 100)          142800    \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (450, None, 256)          25856     \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (450, None, 1797)         461829    \n",
            "=================================================================\n",
            "Total params: 1,090,517\n",
            "Trainable params: 1,090,517\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "=========================== Epoch: 0.  \t  Loss: 7.26161003112793  \t  Time: 3.07ss\n",
            "=========================== Epoch: 1.  \t  Loss: 4.262088775634766  \t  Time: 1.37ss\n",
            "=========================== Epoch: 2.  \t  Loss: 3.7260982990264893  \t  Time: 1.36ss\n",
            "=========================== Epoch: 3.  \t  Loss: 3.6086361408233643  \t  Time: 1.29ss\n",
            "=========================== Epoch: 4.  \t  Loss: 3.4849700927734375  \t  Time: 1.35ss\n",
            "=========================== Epoch: 5.  \t  Loss: 3.353997230529785  \t  Time: 1.37ss\n",
            "=========================== Epoch: 6.  \t  Loss: 3.2633979320526123  \t  Time: 1.32ss\n",
            "=========================== Epoch: 7.  \t  Loss: 3.203918933868408  \t  Time: 1.31ss\n",
            "=========================== Epoch: 8.  \t  Loss: 3.1634521484375  \t  Time: 1.37ss\n",
            "=========================== Epoch: 9.  \t  Loss: 3.131615400314331  \t  Time: 1.34ss\n",
            "   '  '   '  ri a che  '  '    '  che  '   e'   '    il  e che  '  '   '  '  di  '   '  la che   a  la '   e '  '\n",
            "=========================== Epoch: 10.  \t  Loss: 3.103947401046753  \t  Time: 1.29ss\n",
            "=========================== Epoch: 11.  \t  Loss: 3.077183961868286  \t  Time: 1.35ss\n",
            "=========================== Epoch: 12.  \t  Loss: 3.048487901687622  \t  Time: 1.32ss\n",
            "=========================== Epoch: 13.  \t  Loss: 3.019322395324707  \t  Time: 1.3ss\n",
            "=========================== Epoch: 14.  \t  Loss: 2.994058609008789  \t  Time: 1.26ss\n",
            "=========================== Epoch: 15.  \t  Loss: 2.9730782508850098  \t  Time: 1.32ss\n",
            "=========================== Epoch: 16.  \t  Loss: 2.955562114715576  \t  Time: 1.35ss\n",
            "=========================== Epoch: 17.  \t  Loss: 2.9400925636291504  \t  Time: 1.26ss\n",
            "=========================== Epoch: 18.  \t  Loss: 2.9263668060302734  \t  Time: 1.29ss\n",
            "=========================== Epoch: 19.  \t  Loss: 2.9140987396240234  \t  Time: 1.33ss\n",
            "e di alo lela cona co conala o ' si la ' laaa  laverico se io ' a la lee mi saa inla coe ana laa ' e a a'  vila e coalo è la e lami a nel\n",
            "=========================== Epoch: 20.  \t  Loss: 2.9030861854553223  \t  Time: 1.34ss\n",
            "=========================== Epoch: 21.  \t  Loss: 2.8929147720336914  \t  Time: 1.38ss\n",
            "=========================== Epoch: 22.  \t  Loss: 2.883418321609497  \t  Time: 1.28ss\n",
            "=========================== Epoch: 23.  \t  Loss: 2.87446665763855  \t  Time: 1.34ss\n",
            "=========================== Epoch: 24.  \t  Loss: 2.8659074306488037  \t  Time: 1.34ss\n",
            "=========================== Epoch: 25.  \t  Loss: 2.8577945232391357  \t  Time: 1.33ss\n",
            "=========================== Epoch: 26.  \t  Loss: 2.8500661849975586  \t  Time: 1.27ss\n",
            "=========================== Epoch: 27.  \t  Loss: 2.842705011367798  \t  Time: 1.35ss\n",
            "=========================== Epoch: 28.  \t  Loss: 2.8357787132263184  \t  Time: 1.37ss\n",
            "=========================== Epoch: 29.  \t  Loss: 2.829021692276001  \t  Time: 1.27ss\n",
            "e coe ine e aa e ela sise lala lae la laa sia sí lia leli lae dire l e eaa la e cone e eaa aa die la aa la parame chea a ala la ela \n",
            "=========================== Epoch: 30.  \t  Loss: 2.8224449157714844  \t  Time: 1.33ss\n",
            "=========================== Epoch: 31.  \t  Loss: 2.8159470558166504  \t  Time: 1.33ss\n",
            "=========================== Epoch: 32.  \t  Loss: 2.809539556503296  \t  Time: 1.3ss\n",
            "=========================== Epoch: 33.  \t  Loss: 2.8032805919647217  \t  Time: 1.31ss\n",
            "=========================== Epoch: 34.  \t  Loss: 2.7972424030303955  \t  Time: 1.31ss\n",
            "=========================== Epoch: 35.  \t  Loss: 2.7912797927856445  \t  Time: 1.31ss\n",
            "=========================== Epoch: 36.  \t  Loss: 2.7855169773101807  \t  Time: 1.31ss\n",
            "=========================== Epoch: 37.  \t  Loss: 2.7801218032836914  \t  Time: 1.3ss\n",
            "=========================== Epoch: 38.  \t  Loss: 2.775002956390381  \t  Time: 1.3ss\n",
            "=========================== Epoch: 39.  \t  Loss: 2.770263195037842  \t  Time: 1.29ss\n",
            "la\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "che\n",
            " \n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "=========================== Epoch: 40.  \t  Loss: 2.7656750679016113  \t  Time: 1.3ss\n",
            "=========================== Epoch: 41.  \t  Loss: 2.7614083290100098  \t  Time: 1.29ss\n",
            "=========================== Epoch: 42.  \t  Loss: 2.7572624683380127  \t  Time: 1.28ss\n",
            "=========================== Epoch: 43.  \t  Loss: 2.753181219100952  \t  Time: 1.3ss\n",
            "=========================== Epoch: 44.  \t  Loss: 2.749140739440918  \t  Time: 1.31ss\n",
            "=========================== Epoch: 45.  \t  Loss: 2.7451066970825195  \t  Time: 1.3ss\n",
            "=========================== Epoch: 46.  \t  Loss: 2.7409937381744385  \t  Time: 1.29ss\n",
            "=========================== Epoch: 47.  \t  Loss: 2.73710560798645  \t  Time: 1.31ss\n",
            "=========================== Epoch: 48.  \t  Loss: 2.7334673404693604  \t  Time: 1.24ss\n",
            "=========================== Epoch: 49.  \t  Loss: 2.7300610542297363  \t  Time: 1.27ss\n",
            "e salipo poriala ole ola oa ola lole aao aae ala lalo ala litra aaa lipola ola e rila eo e aala ao ala ae loa laa lireo lala la ele eli\n",
            "=========================== Epoch: 50.  \t  Loss: 2.7269787788391113  \t  Time: 1.28ss\n",
            "=========================== Epoch: 51.  \t  Loss: 2.7240402698516846  \t  Time: 1.26ss\n",
            "=========================== Epoch: 52.  \t  Loss: 2.721313714981079  \t  Time: 1.27ss\n",
            "=========================== Epoch: 53.  \t  Loss: 2.7185919284820557  \t  Time: 1.31ss\n",
            "=========================== Epoch: 54.  \t  Loss: 2.716108560562134  \t  Time: 1.32ss\n",
            "=========================== Epoch: 55.  \t  Loss: 2.713698625564575  \t  Time: 1.31ss\n",
            "=========================== Epoch: 56.  \t  Loss: 2.7115118503570557  \t  Time: 1.3ss\n",
            "=========================== Epoch: 57.  \t  Loss: 2.7092459201812744  \t  Time: 1.28ss\n",
            "=========================== Epoch: 58.  \t  Loss: 2.7071475982666016  \t  Time: 1.3ss\n",
            "=========================== Epoch: 59.  \t  Loss: 2.705042839050293  \t  Time: 1.29ss\n",
            "vesí maama polele ila seolo le dala oaco lee e anela li laoo lapo aola o eo io lelo lii pala ao ele aoché aa lale ali ola aaa oola e abian\n",
            "=========================== Epoch: 60.  \t  Loss: 2.7028942108154297  \t  Time: 1.33ss\n",
            "=========================== Epoch: 61.  \t  Loss: 2.7007651329040527  \t  Time: 1.35ss\n",
            "=========================== Epoch: 62.  \t  Loss: 2.698606252670288  \t  Time: 1.37ss\n",
            "=========================== Epoch: 63.  \t  Loss: 2.696596145629883  \t  Time: 1.39ss\n",
            "=========================== Epoch: 64.  \t  Loss: 2.6944942474365234  \t  Time: 1.37ss\n",
            "=========================== Epoch: 65.  \t  Loss: 2.6925199031829834  \t  Time: 1.34ss\n",
            "=========================== Epoch: 66.  \t  Loss: 2.690584897994995  \t  Time: 1.34ss\n",
            "=========================== Epoch: 67.  \t  Loss: 2.6886677742004395  \t  Time: 1.32ss\n",
            "=========================== Epoch: 68.  \t  Loss: 2.6868255138397217  \t  Time: 1.29ss\n",
            "=========================== Epoch: 69.  \t  Loss: 2.6850287914276123  \t  Time: 1.33ss\n",
            "a ele lao chéa pela lao avilo laa oo cola omor mamo lao io iao io ale io aae oo aive lao oché ao aola io duli laoto la e eo ila di\n",
            "=========================== Epoch: 70.  \t  Loss: 2.683225393295288  \t  Time: 1.32ss\n",
            "=========================== Epoch: 71.  \t  Loss: 2.681504964828491  \t  Time: 1.29ss\n",
            "=========================== Epoch: 72.  \t  Loss: 2.67976975440979  \t  Time: 1.28ss\n",
            "=========================== Epoch: 73.  \t  Loss: 2.678053379058838  \t  Time: 1.29ss\n",
            "=========================== Epoch: 74.  \t  Loss: 2.676408529281616  \t  Time: 1.27ss\n",
            "=========================== Epoch: 75.  \t  Loss: 2.674741506576538  \t  Time: 1.31ss\n",
            "=========================== Epoch: 76.  \t  Loss: 2.673065423965454  \t  Time: 1.29ss\n",
            "=========================== Epoch: 77.  \t  Loss: 2.6713500022888184  \t  Time: 1.31ss\n",
            "=========================== Epoch: 78.  \t  Loss: 2.669576644897461  \t  Time: 1.29ss\n",
            "=========================== Epoch: 79.  \t  Loss: 2.667897939682007  \t  Time: 1.29ss\n",
            "malo lale io lao lao ivelo lepo lepela lelo lala ila oa cheopo oo ae pale oli apeli lea paa ao oaa lapole aché lo oao ebian lalela e iola  labian\n",
            "=========================== Epoch: 80.  \t  Loss: 2.6662237644195557  \t  Time: 1.26ss\n",
            "=========================== Epoch: 81.  \t  Loss: 2.664512872695923  \t  Time: 1.31ss\n",
            "=========================== Epoch: 82.  \t  Loss: 2.662757396697998  \t  Time: 1.3ss\n",
            "=========================== Epoch: 83.  \t  Loss: 2.6609137058258057  \t  Time: 1.29ss\n",
            "=========================== Epoch: 84.  \t  Loss: 2.659205198287964  \t  Time: 1.32ss\n",
            "=========================== Epoch: 85.  \t  Loss: 2.6572861671447754  \t  Time: 1.32ss\n",
            "=========================== Epoch: 86.  \t  Loss: 2.6553966999053955  \t  Time: 1.3ss\n",
            "=========================== Epoch: 87.  \t  Loss: 2.653716802597046  \t  Time: 1.29ss\n",
            "=========================== Epoch: 88.  \t  Loss: 2.651801824569702  \t  Time: 1.31ss\n",
            "=========================== Epoch: 89.  \t  Loss: 2.6500518321990967  \t  Time: 1.3ss\n",
            "síle quan se lao ala ae ao oo iane pressila e oo laa ialo ilo ao ia ilao e ao lao alo io lola aile pecoa aper opressa la e aio lale lala\n",
            "=========================== Epoch: 90.  \t  Loss: 2.648165225982666  \t  Time: 1.29ss\n",
            "=========================== Epoch: 91.  \t  Loss: 2.6463074684143066  \t  Time: 1.33ss\n",
            "=========================== Epoch: 92.  \t  Loss: 2.644385576248169  \t  Time: 1.29ss\n",
            "=========================== Epoch: 93.  \t  Loss: 2.6424782276153564  \t  Time: 1.31ss\n",
            "=========================== Epoch: 94.  \t  Loss: 2.6404991149902344  \t  Time: 1.33ss\n",
            "=========================== Epoch: 95.  \t  Loss: 2.6384007930755615  \t  Time: 1.32ss\n",
            "=========================== Epoch: 96.  \t  Loss: 2.6363065242767334  \t  Time: 1.33ss\n",
            "=========================== Epoch: 97.  \t  Loss: 2.6342687606811523  \t  Time: 1.34ss\n",
            "=========================== Epoch: 98.  \t  Loss: 2.632249116897583  \t  Time: 1.38ss\n",
            "=========================== Epoch: 99.  \t  Loss: 2.630192756652832  \t  Time: 1.39ss\n",
            "lolo van sili lala copopole dio aoo e oa ileo pape aole aoo oo loli laue lea fio ipoquan vol sili lebae apo verave ao lapra ola ila paa ole oa \n",
            "=========================== Epoch: 100.  \t  Loss: 2.6281893253326416  \t  Time: 1.39ss\n",
            "=========================== Epoch: 101.  \t  Loss: 2.626157522201538  \t  Time: 1.37ss\n",
            "=========================== Epoch: 102.  \t  Loss: 2.624056577682495  \t  Time: 1.42ss\n",
            "=========================== Epoch: 103.  \t  Loss: 2.6220295429229736  \t  Time: 1.42ss\n",
            "=========================== Epoch: 104.  \t  Loss: 2.620018482208252  \t  Time: 1.43ss\n",
            "=========================== Epoch: 105.  \t  Loss: 2.618077039718628  \t  Time: 1.43ss\n",
            "=========================== Epoch: 106.  \t  Loss: 2.616112470626831  \t  Time: 1.47ss\n",
            "=========================== Epoch: 107.  \t  Loss: 2.614093542098999  \t  Time: 1.42ss\n",
            "=========================== Epoch: 108.  \t  Loss: 2.612130880355835  \t  Time: 1.43ss\n",
            "=========================== Epoch: 109.  \t  Loss: 2.6102652549743652  \t  Time: 1.42ss\n",
            "papove poapoo filoo deen fapre e oon ana apove e e aie oo iché la que e opresli la lipe aoa popa aala poa ia ilo e oopa iqua iquale ailo corla \n",
            "=========================== Epoch: 110.  \t  Loss: 2.6082780361175537  \t  Time: 1.4ss\n",
            "=========================== Epoch: 111.  \t  Loss: 2.606259822845459  \t  Time: 1.39ss\n",
            "=========================== Epoch: 112.  \t  Loss: 2.604233980178833  \t  Time: 1.45ss\n",
            "=========================== Epoch: 113.  \t  Loss: 2.6023125648498535  \t  Time: 1.48ss\n",
            "=========================== Epoch: 114.  \t  Loss: 2.6003057956695557  \t  Time: 1.45ss\n",
            "=========================== Epoch: 115.  \t  Loss: 2.598369598388672  \t  Time: 1.44ss\n",
            "=========================== Epoch: 116.  \t  Loss: 2.596546173095703  \t  Time: 1.46ss\n",
            "=========================== Epoch: 117.  \t  Loss: 2.5945544242858887  \t  Time: 1.42ss\n",
            "=========================== Epoch: 118.  \t  Loss: 2.592822790145874  \t  Time: 1.39ss\n",
            "=========================== Epoch: 119.  \t  Loss: 2.591050863265991  \t  Time: 1.46ss\n",
            "lala ao trela veao mia ibuvríve que aoo aen ché tela encu babarsi pola leo copole ooa ope moo aopo ila apole oi amor lile poparcolo e poao obian ana la\n",
            "=========================== Epoch: 120.  \t  Loss: 2.588695764541626  \t  Time: 1.51ss\n",
            "=========================== Epoch: 121.  \t  Loss: 2.586815357208252  \t  Time: 1.53ss\n",
            "=========================== Epoch: 122.  \t  Loss: 2.584861993789673  \t  Time: 1.51ss\n",
            "=========================== Epoch: 123.  \t  Loss: 2.582716226577759  \t  Time: 1.49ss\n",
            "=========================== Epoch: 124.  \t  Loss: 2.5808722972869873  \t  Time: 1.52ss\n",
            "=========================== Epoch: 125.  \t  Loss: 2.5790514945983887  \t  Time: 1.49ss\n",
            "=========================== Epoch: 126.  \t  Loss: 2.577085256576538  \t  Time: 1.43ss\n",
            "=========================== Epoch: 127.  \t  Loss: 2.5753591060638428  \t  Time: 1.39ss\n",
            "=========================== Epoch: 128.  \t  Loss: 2.57342529296875  \t  Time: 1.4ss\n",
            "=========================== Epoch: 129.  \t  Loss: 2.5716006755828857  \t  Time: 1.36ss\n",
            "mepole dion di oo ao apoo aon loa ao oo oa ché ila ila lipa color la eo io oroa ala bauoro ao oo ape apola padu e apela en ila ila silo\n",
            "=========================== Epoch: 130.  \t  Loss: 2.5697171688079834  \t  Time: 1.32ss\n",
            "=========================== Epoch: 131.  \t  Loss: 2.5680108070373535  \t  Time: 1.28ss\n",
            "=========================== Epoch: 132.  \t  Loss: 2.5660579204559326  \t  Time: 1.29ss\n",
            "=========================== Epoch: 133.  \t  Loss: 2.5643692016601562  \t  Time: 1.31ss\n",
            "=========================== Epoch: 134.  \t  Loss: 2.5624425411224365  \t  Time: 1.3ss\n",
            "=========================== Epoch: 135.  \t  Loss: 2.560790538787842  \t  Time: 1.29ss\n",
            "=========================== Epoch: 136.  \t  Loss: 2.5592164993286133  \t  Time: 1.33ss\n",
            "=========================== Epoch: 137.  \t  Loss: 2.5569398403167725  \t  Time: 1.25ss\n",
            "=========================== Epoch: 138.  \t  Loss: 2.555248975753784  \t  Time: 1.37ss\n",
            "=========================== Epoch: 139.  \t  Loss: 2.5534777641296387  \t  Time: 1.37ss\n",
            "cosí en liale ente ae lao apee on a apoe ilepe oo apa oo alo aparca lida vea popea e loo uleo poae la que ope faan oa ipa io epe ape ali\n",
            "=========================== Epoch: 140.  \t  Loss: 2.551677942276001  \t  Time: 1.36ss\n",
            "=========================== Epoch: 141.  \t  Loss: 2.549440383911133  \t  Time: 1.37ss\n",
            "=========================== Epoch: 142.  \t  Loss: 2.548112154006958  \t  Time: 1.36ss\n",
            "=========================== Epoch: 143.  \t  Loss: 2.5461766719818115  \t  Time: 1.39ss\n",
            "=========================== Epoch: 144.  \t  Loss: 2.544623374938965  \t  Time: 1.38ss\n",
            "=========================== Epoch: 145.  \t  Loss: 2.5431535243988037  \t  Time: 1.36ss\n",
            "=========================== Epoch: 146.  \t  Loss: 2.5416767597198486  \t  Time: 1.38ss\n",
            "=========================== Epoch: 147.  \t  Loss: 2.5391385555267334  \t  Time: 1.36ss\n",
            "=========================== Epoch: 148.  \t  Loss: 2.5379409790039062  \t  Time: 1.38ss\n",
            "=========================== Epoch: 149.  \t  Loss: 2.5366289615631104  \t  Time: 1.38ss\n",
            "lele\n",
            "e\n",
            "\n",
            " zï\n",
            "\n",
            " tama\n",
            "\n",
            "che\n",
            "\n",
            "co\n",
            "\n",
            " tantoto\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " che\n",
            "\n",
            "\n",
            "ma\n",
            " scissemimi\n",
            "\n",
            "\n",
            "\n",
            "sílae\n",
            "\n",
            "\n",
            " scí\n",
            "\n",
            " scíma\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " dissesomamo lee en entre lunsusor soa ila olepe saon e\n",
            "=========================== Epoch: 150.  \t  Loss: 2.5347957611083984  \t  Time: 1.36ss\n",
            "=========================== Epoch: 151.  \t  Loss: 2.533503532409668  \t  Time: 1.31ss\n",
            "=========================== Epoch: 152.  \t  Loss: 2.531879186630249  \t  Time: 1.37ss\n",
            "=========================== Epoch: 153.  \t  Loss: 2.530263900756836  \t  Time: 1.37ss\n",
            "=========================== Epoch: 154.  \t  Loss: 2.5284690856933594  \t  Time: 1.36ss\n",
            "=========================== Epoch: 155.  \t  Loss: 2.527397394180298  \t  Time: 1.35ss\n",
            "=========================== Epoch: 156.  \t  Loss: 2.5263726711273193  \t  Time: 1.34ss\n",
            "=========================== Epoch: 157.  \t  Loss: 2.5241706371307373  \t  Time: 1.36ss\n",
            "=========================== Epoch: 158.  \t  Loss: 2.523205280303955  \t  Time: 1.38ss\n",
            "=========================== Epoch: 159.  \t  Loss: 2.5213804244995117  \t  Time: 1.36ss\n",
            "copo qualilisi lila ilo e aquan lo apa tee lalo la ooper biangi valae parla rale lila iquan lo lio a apia ipepe loa leole oo leon aa on au epe ila loo co\n",
            "=========================== Epoch: 160.  \t  Loss: 2.520052671432495  \t  Time: 1.35ss\n",
            "=========================== Epoch: 161.  \t  Loss: 2.5186643600463867  \t  Time: 1.36ss\n",
            "=========================== Epoch: 162.  \t  Loss: 2.517746925354004  \t  Time: 1.36ss\n",
            "=========================== Epoch: 163.  \t  Loss: 2.5160844326019287  \t  Time: 1.37ss\n",
            "=========================== Epoch: 164.  \t  Loss: 2.514887809753418  \t  Time: 1.38ss\n",
            "=========================== Epoch: 165.  \t  Loss: 2.5135927200317383  \t  Time: 1.36ss\n",
            "=========================== Epoch: 166.  \t  Loss: 2.5120296478271484  \t  Time: 1.35ss\n",
            "=========================== Epoch: 167.  \t  Loss: 2.5105996131896973  \t  Time: 1.34ss\n",
            "=========================== Epoch: 168.  \t  Loss: 2.509338140487671  \t  Time: 1.36ss\n",
            "=========================== Epoch: 169.  \t  Loss: 2.508451223373413  \t  Time: 1.36ss\n",
            "cose\n",
            "\n",
            " tata\n",
            "\n",
            "\n",
            " dissesanto mi lolo oa asselilo bape olo loa poao pape loa apa rii ae alee crecino paposlilo ao cole que ape pea chi lea ila opian loa laen\n",
            "=========================== Epoch: 170.  \t  Loss: 2.5067224502563477  \t  Time: 1.34ss\n",
            "=========================== Epoch: 171.  \t  Loss: 2.505733013153076  \t  Time: 1.4ss\n",
            "=========================== Epoch: 172.  \t  Loss: 2.503962993621826  \t  Time: 1.38ss\n",
            "=========================== Epoch: 173.  \t  Loss: 2.503103017807007  \t  Time: 1.37ss\n",
            "=========================== Epoch: 174.  \t  Loss: 2.5014100074768066  \t  Time: 1.36ss\n",
            "=========================== Epoch: 175.  \t  Loss: 2.5004608631134033  \t  Time: 1.39ss\n",
            "=========================== Epoch: 176.  \t  Loss: 2.499439239501953  \t  Time: 1.36ss\n",
            "=========================== Epoch: 177.  \t  Loss: 2.4979569911956787  \t  Time: 1.37ss\n",
            "=========================== Epoch: 178.  \t  Loss: 2.497096538543701  \t  Time: 1.34ss\n",
            "=========================== Epoch: 179.  \t  Loss: 2.4953725337982178  \t  Time: 1.32ss\n",
            "colimi loa ala e let lia iloa opoo en aa gioo loao a alae lee laor aa leon ao paen val colopa parla que\n",
            "co\n",
            "\n",
            "\n",
            " mamamama lae veli reo aopa ape aquan\n",
            "=========================== Epoch: 180.  \t  Loss: 2.494507074356079  \t  Time: 1.36ss\n",
            "=========================== Epoch: 181.  \t  Loss: 2.4934351444244385  \t  Time: 1.38ss\n",
            "=========================== Epoch: 182.  \t  Loss: 2.4918370246887207  \t  Time: 1.37ss\n",
            "=========================== Epoch: 183.  \t  Loss: 2.4907891750335693  \t  Time: 1.36ss\n",
            "=========================== Epoch: 184.  \t  Loss: 2.4900741577148438  \t  Time: 1.39ss\n",
            "=========================== Epoch: 185.  \t  Loss: 2.488311529159546  \t  Time: 1.39ss\n",
            "=========================== Epoch: 186.  \t  Loss: 2.4873571395874023  \t  Time: 1.36ss\n",
            "=========================== Epoch: 187.  \t  Loss: 2.486095666885376  \t  Time: 1.41ss\n",
            "=========================== Epoch: 188.  \t  Loss: 2.4851081371307373  \t  Time: 1.37ss\n",
            "=========================== Epoch: 189.  \t  Loss: 2.483731508255005  \t  Time: 1.37ss\n",
            "que\n",
            "\n",
            "\n",
            "con\n",
            "sle parla ula ae obarla ique\n",
            "o\n",
            "\n",
            " to\n",
            "\n",
            "\n",
            " dissemoto lo loae pi elet i sipe e mia pressiso lee ipo ae pera neo ibarapel aquan li lali oa ape vivi\n",
            "=========================== Epoch: 190.  \t  Loss: 2.482783794403076  \t  Time: 1.36ss\n",
            "=========================== Epoch: 191.  \t  Loss: 2.4815101623535156  \t  Time: 1.38ss\n",
            "=========================== Epoch: 192.  \t  Loss: 2.480374813079834  \t  Time: 1.36ss\n",
            "=========================== Epoch: 193.  \t  Loss: 2.479228973388672  \t  Time: 1.38ss\n",
            "=========================== Epoch: 194.  \t  Loss: 2.4782493114471436  \t  Time: 1.38ss\n",
            "=========================== Epoch: 195.  \t  Loss: 2.4768476486206055  \t  Time: 1.39ss\n",
            "=========================== Epoch: 196.  \t  Loss: 2.4759504795074463  \t  Time: 1.38ss\n",
            "=========================== Epoch: 197.  \t  Loss: 2.474674701690674  \t  Time: 1.38ss\n",
            "=========================== Epoch: 198.  \t  Loss: 2.473731279373169  \t  Time: 1.42ss\n",
            "=========================== Epoch: 199.  \t  Loss: 2.4726452827453613  \t  Time: 1.43ss\n",
            "dio pea pa opola la io asseperbu ae da poae or paen uoa ula dipren pare a laen ae quan re si opea apoe la lepa ao poao aor ene lu lape ae lata \n",
            "=========================== Epoch: 200.  \t  Loss: 2.471851110458374  \t  Time: 1.4ss\n",
            "=========================== Epoch: 201.  \t  Loss: 2.470552921295166  \t  Time: 1.37ss\n",
            "=========================== Epoch: 202.  \t  Loss: 2.469459295272827  \t  Time: 1.38ss\n",
            "=========================== Epoch: 203.  \t  Loss: 2.4688026905059814  \t  Time: 1.41ss\n",
            "=========================== Epoch: 204.  \t  Loss: 2.4674923419952393  \t  Time: 1.42ss\n",
            "=========================== Epoch: 205.  \t  Loss: 2.4662561416625977  \t  Time: 1.51ss\n",
            "=========================== Epoch: 206.  \t  Loss: 2.4657654762268066  \t  Time: 1.47ss\n",
            "=========================== Epoch: 207.  \t  Loss: 2.464432954788208  \t  Time: 1.39ss\n",
            "=========================== Epoch: 208.  \t  Loss: 2.463862657546997  \t  Time: 1.44ss\n",
            "=========================== Epoch: 209.  \t  Loss: 2.4626107215881348  \t  Time: 1.37ss\n",
            "semi lalo opo opar lae veo loo e apreli lila e que e pevea ola opo parlate aper e ipella qualo pera goorao barabarae let ilo paa lao quan tri ao e papi bran\n",
            "=========================== Epoch: 210.  \t  Loss: 2.461698293685913  \t  Time: 1.37ss\n",
            "=========================== Epoch: 211.  \t  Loss: 2.4603984355926514  \t  Time: 1.38ss\n",
            "=========================== Epoch: 212.  \t  Loss: 2.4596641063690186  \t  Time: 1.4ss\n",
            "=========================== Epoch: 213.  \t  Loss: 2.4586434364318848  \t  Time: 1.38ss\n",
            "=========================== Epoch: 214.  \t  Loss: 2.457611560821533  \t  Time: 1.4ss\n",
            "=========================== Epoch: 215.  \t  Loss: 2.45674991607666  \t  Time: 1.38ss\n",
            "=========================== Epoch: 216.  \t  Loss: 2.455524206161499  \t  Time: 1.42ss\n",
            "=========================== Epoch: 217.  \t  Loss: 2.4549474716186523  \t  Time: 1.42ss\n",
            "=========================== Epoch: 218.  \t  Loss: 2.4542160034179688  \t  Time: 1.37ss\n",
            "=========================== Epoch: 219.  \t  Loss: 2.453085422515869  \t  Time: 1.42ss\n",
            "pero bianla ape laae o baribali e pasi ila e leo mapera chi lila o sie quan te dipar lopa oa papa e apea lu parpar ila que\n",
            "\n",
            "e\n",
            "\n",
            "\n",
            "\n",
            " mamamama e quiloe o la\n",
            "=========================== Epoch: 220.  \t  Loss: 2.4523508548736572  \t  Time: 1.4ss\n",
            "=========================== Epoch: 221.  \t  Loss: 2.451380491256714  \t  Time: 1.39ss\n",
            "=========================== Epoch: 222.  \t  Loss: 2.4506757259368896  \t  Time: 1.37ss\n",
            "=========================== Epoch: 223.  \t  Loss: 2.4500389099121094  \t  Time: 1.38ss\n",
            "=========================== Epoch: 224.  \t  Loss: 2.4490599632263184  \t  Time: 1.37ss\n",
            "=========================== Epoch: 225.  \t  Loss: 2.4484188556671143  \t  Time: 1.4ss\n",
            "=========================== Epoch: 226.  \t  Loss: 2.447643995285034  \t  Time: 1.36ss\n",
            "=========================== Epoch: 227.  \t  Loss: 2.4469330310821533  \t  Time: 1.38ss\n",
            "=========================== Epoch: 228.  \t  Loss: 2.446096897125244  \t  Time: 1.38ss\n",
            "=========================== Epoch: 229.  \t  Loss: 2.4455885887145996  \t  Time: 1.36ss\n",
            "leca pape quan si loe opra popa ïoa quan la e lunsenra susa ila isi ope e nopo lin apar ila pela ala e cubalo aon a opel pepa lao lao aper pea que opa opo\n",
            "=========================== Epoch: 230.  \t  Loss: 2.444481134414673  \t  Time: 1.38ss\n",
            "=========================== Epoch: 231.  \t  Loss: 2.4440979957580566  \t  Time: 1.37ss\n",
            "=========================== Epoch: 232.  \t  Loss: 2.4431862831115723  \t  Time: 1.38ss\n",
            "=========================== Epoch: 233.  \t  Loss: 2.442493438720703  \t  Time: 1.39ss\n",
            "=========================== Epoch: 234.  \t  Loss: 2.441593647003174  \t  Time: 1.38ss\n",
            "=========================== Epoch: 235.  \t  Loss: 2.4410557746887207  \t  Time: 1.36ss\n",
            "=========================== Epoch: 236.  \t  Loss: 2.440169095993042  \t  Time: 1.35ss\n",
            "=========================== Epoch: 237.  \t  Loss: 2.4395883083343506  \t  Time: 1.37ss\n",
            "=========================== Epoch: 238.  \t  Loss: 2.4389805793762207  \t  Time: 1.35ss\n",
            "=========================== Epoch: 239.  \t  Loss: 2.4383137226104736  \t  Time: 1.36ss\n",
            "mi\n",
            "lo l afi papar e ipe lequan si copopo orpea ola a ripaval lao oprelet baropa opresa o prelape copoo e papra apo quan ta imor mi e lile ano oa parla ae per \n",
            "=========================== Epoch: 240.  \t  Loss: 2.4376749992370605  \t  Time: 1.38ss\n",
            "=========================== Epoch: 241.  \t  Loss: 2.43705153465271  \t  Time: 1.39ss\n",
            "=========================== Epoch: 242.  \t  Loss: 2.436443567276001  \t  Time: 1.38ss\n",
            "=========================== Epoch: 243.  \t  Loss: 2.4357831478118896  \t  Time: 1.36ss\n",
            "=========================== Epoch: 244.  \t  Loss: 2.4351394176483154  \t  Time: 1.4ss\n",
            "=========================== Epoch: 245.  \t  Loss: 2.434732437133789  \t  Time: 1.39ss\n",
            "=========================== Epoch: 246.  \t  Loss: 2.433638572692871  \t  Time: 1.39ss\n",
            "=========================== Epoch: 247.  \t  Loss: 2.4329614639282227  \t  Time: 1.4ss\n",
            "=========================== Epoch: 248.  \t  Loss: 2.4324440956115723  \t  Time: 1.41ss\n",
            "=========================== Epoch: 249.  \t  Loss: 2.43215012550354  \t  Time: 1.37ss\n",
            "poloo apar lao poolo pia quape trapa pape o paiaope pape ape pela ago ae leo brepo e onin pressisi pola lao luno ono pa papoo oroca lipe lao lobre ché i oraa\n",
            "=========================== Epoch: 250.  \t  Loss: 2.4313056468963623  \t  Time: 1.39ss\n",
            "=========================== Epoch: 251.  \t  Loss: 2.4306981563568115  \t  Time: 1.38ss\n",
            "=========================== Epoch: 252.  \t  Loss: 2.4302406311035156  \t  Time: 1.44ss\n",
            "=========================== Epoch: 253.  \t  Loss: 2.4299049377441406  \t  Time: 1.43ss\n",
            "=========================== Epoch: 254.  \t  Loss: 2.4288036823272705  \t  Time: 1.41ss\n",
            "=========================== Epoch: 255.  \t  Loss: 2.4284493923187256  \t  Time: 1.39ss\n",
            "=========================== Epoch: 256.  \t  Loss: 2.427851676940918  \t  Time: 1.38ss\n",
            "=========================== Epoch: 257.  \t  Loss: 2.4274041652679443  \t  Time: 1.3ss\n",
            "=========================== Epoch: 258.  \t  Loss: 2.42681622505188  \t  Time: 1.27ss\n",
            "=========================== Epoch: 259.  \t  Loss: 2.4262747764587402  \t  Time: 1.28ss\n",
            "dio o apope que ope que la opepra lapra le opoo laca an oa dipressa lapren ae or opa prala en aquan  gila valse oala ae pela prela e a ché ae ve assisimi la \n",
            "=========================== Epoch: 260.  \t  Loss: 2.425915241241455  \t  Time: 1.32ss\n",
            "=========================== Epoch: 261.  \t  Loss: 2.425344228744507  \t  Time: 1.29ss\n",
            "=========================== Epoch: 262.  \t  Loss: 2.4248738288879395  \t  Time: 1.34ss\n",
            "=========================== Epoch: 263.  \t  Loss: 2.424452543258667  \t  Time: 1.29ss\n",
            "=========================== Epoch: 264.  \t  Loss: 2.423621416091919  \t  Time: 1.29ss\n",
            "=========================== Epoch: 265.  \t  Loss: 2.423572063446045  \t  Time: 1.3ss\n",
            "=========================== Epoch: 266.  \t  Loss: 2.4226627349853516  \t  Time: 1.37ss\n",
            "=========================== Epoch: 267.  \t  Loss: 2.4220356941223145  \t  Time: 1.3ss\n",
            "=========================== Epoch: 268.  \t  Loss: 2.4217476844787598  \t  Time: 1.27ss\n",
            "=========================== Epoch: 269.  \t  Loss: 2.421347141265869  \t  Time: 1.38ss\n",
            "mimin lo ascosí fia quan loa e pape ali an deoa pala la pao ipe ae la pian seli ola pressa bala podan la opea e so pera qualedibre quan leo opra que o bianpa pa a\n",
            "=========================== Epoch: 270.  \t  Loss: 2.420722007751465  \t  Time: 1.34ss\n",
            "=========================== Epoch: 271.  \t  Loss: 2.4207475185394287  \t  Time: 1.37ss\n",
            "=========================== Epoch: 272.  \t  Loss: 2.419799327850342  \t  Time: 1.45ss\n",
            "=========================== Epoch: 273.  \t  Loss: 2.4198038578033447  \t  Time: 1.48ss\n",
            "=========================== Epoch: 274.  \t  Loss: 2.4190449714660645  \t  Time: 1.38ss\n",
            "=========================== Epoch: 275.  \t  Loss: 2.4181158542633057  \t  Time: 1.41ss\n",
            "=========================== Epoch: 276.  \t  Loss: 2.4180233478546143  \t  Time: 1.39ss\n",
            "=========================== Epoch: 277.  \t  Loss: 2.4178266525268555  \t  Time: 1.4ss\n",
            "=========================== Epoch: 278.  \t  Loss: 2.417144298553467  \t  Time: 1.39ss\n",
            "=========================== Epoch: 279.  \t  Loss: 2.416767120361328  \t  Time: 1.39ss\n",
            "\n",
            "maperlo o papepe aque le parle ae la opa brepren o apa ipe\n",
            "nova posa coperpo aprenve aari are traper aché  ala quila oole poo dipoo lor loa o biana vallae o pre\n",
            "=========================== Epoch: 280.  \t  Loss: 2.4162702560424805  \t  Time: 1.32ss\n",
            "=========================== Epoch: 281.  \t  Loss: 2.4158310890197754  \t  Time: 1.37ss\n",
            "=========================== Epoch: 282.  \t  Loss: 2.4157068729400635  \t  Time: 1.36ss\n",
            "=========================== Epoch: 283.  \t  Loss: 2.4148693084716797  \t  Time: 1.38ss\n",
            "=========================== Epoch: 284.  \t  Loss: 2.4146721363067627  \t  Time: 1.34ss\n",
            "=========================== Epoch: 285.  \t  Loss: 2.4139797687530518  \t  Time: 1.35ss\n",
            "=========================== Epoch: 286.  \t  Loss: 2.413651466369629  \t  Time: 1.4ss\n",
            "=========================== Epoch: 287.  \t  Loss: 2.413461685180664  \t  Time: 1.38ss\n",
            "=========================== Epoch: 288.  \t  Loss: 2.4125313758850098  \t  Time: 1.41ss\n",
            "=========================== Epoch: 289.  \t  Loss: 2.4123435020446777  \t  Time: 1.4ss\n",
            "se que\n",
            "\n",
            "a\n",
            "\n",
            "\n",
            "\n",
            " dissesetimo sima e laquan e gale ae la acaquan  an ae loo ape opea o ope dipar ipe lume pocopola selo ao pape pepa pape apa bare ipo poe\n",
            "=========================== Epoch: 290.  \t  Loss: 2.4119436740875244  \t  Time: 1.34ss\n",
            "=========================== Epoch: 291.  \t  Loss: 2.4114716053009033  \t  Time: 1.4ss\n",
            "=========================== Epoch: 292.  \t  Loss: 2.4111697673797607  \t  Time: 1.4ss\n",
            "=========================== Epoch: 293.  \t  Loss: 2.4105937480926514  \t  Time: 1.37ss\n",
            "=========================== Epoch: 294.  \t  Loss: 2.410244941711426  \t  Time: 1.4ss\n",
            "=========================== Epoch: 295.  \t  Loss: 2.409757137298584  \t  Time: 1.37ss\n",
            "=========================== Epoch: 296.  \t  Loss: 2.409560441970825  \t  Time: 1.36ss\n",
            "=========================== Epoch: 297.  \t  Loss: 2.4089064598083496  \t  Time: 1.31ss\n",
            "=========================== Epoch: 298.  \t  Loss: 2.40876841545105  \t  Time: 1.34ss\n",
            "=========================== Epoch: 299.  \t  Loss: 2.408421277999878  \t  Time: 1.37ss\n",
            "sta an  coloripoo opoo leo ovresso po sopeno pa pape que\n",
            "nell lao pa fibree la pela ora ope peca que ope lupo leparla e mi pera osi tale oraa peae  que lapren con\n",
            "=========================== Epoch: 300.  \t  Loss: 2.4077539443969727  \t  Time: 1.37ss\n",
            "=========================== Epoch: 301.  \t  Loss: 2.407424211502075  \t  Time: 1.37ss\n",
            "=========================== Epoch: 302.  \t  Loss: 2.4073500633239746  \t  Time: 1.39ss\n",
            "=========================== Epoch: 303.  \t  Loss: 2.406813859939575  \t  Time: 1.36ss\n",
            "=========================== Epoch: 304.  \t  Loss: 2.4066107273101807  \t  Time: 1.34ss\n",
            "=========================== Epoch: 305.  \t  Loss: 2.405931234359741  \t  Time: 1.32ss\n",
            "=========================== Epoch: 306.  \t  Loss: 2.405974864959717  \t  Time: 1.35ss\n",
            "=========================== Epoch: 307.  \t  Loss: 2.405621290206909  \t  Time: 1.34ss\n",
            "=========================== Epoch: 308.  \t  Loss: 2.404858350753784  \t  Time: 1.35ss\n",
            "=========================== Epoch: 309.  \t  Loss: 2.404900550842285  \t  Time: 1.33ss\n",
            "mi cobu\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " matise\n",
            "\n",
            "\n",
            "\n",
            " dissesemi mi copola poe la biane quali gioa mo e api o dipeprenro an ae la liquali ala quan a dio apren aaper apren sitri o dio pra poo\n",
            "=========================== Epoch: 310.  \t  Loss: 2.4041736125946045  \t  Time: 1.39ss\n",
            "=========================== Epoch: 311.  \t  Loss: 2.4038171768188477  \t  Time: 1.36ss\n",
            "=========================== Epoch: 312.  \t  Loss: 2.4033186435699463  \t  Time: 1.36ss\n",
            "=========================== Epoch: 313.  \t  Loss: 2.403339147567749  \t  Time: 1.36ss\n",
            "=========================== Epoch: 314.  \t  Loss: 2.4026145935058594  \t  Time: 1.34ss\n",
            "=========================== Epoch: 315.  \t  Loss: 2.402719020843506  \t  Time: 1.35ss\n",
            "=========================== Epoch: 316.  \t  Loss: 2.4026200771331787  \t  Time: 1.34ss\n",
            "=========================== Epoch: 317.  \t  Loss: 2.401933431625366  \t  Time: 1.39ss\n",
            "=========================== Epoch: 318.  \t  Loss: 2.4014108180999756  \t  Time: 1.36ss\n",
            "=========================== Epoch: 319.  \t  Loss: 2.4012980461120605  \t  Time: 1.35ss\n",
            "mi pocola riloo lao popa asmise lapren e vin poora ate oraquan  alu pocopopola laon  oo lapressa la ae e la papra no opoopra popola buo ape orapo quan balo an a\n",
            "=========================== Epoch: 320.  \t  Loss: 2.4010281562805176  \t  Time: 1.34ss\n",
            "=========================== Epoch: 321.  \t  Loss: 2.4018421173095703  \t  Time: 1.38ss\n",
            "=========================== Epoch: 322.  \t  Loss: 2.4001245498657227  \t  Time: 1.37ss\n",
            "=========================== Epoch: 323.  \t  Loss: 2.399986982345581  \t  Time: 1.39ss\n",
            "=========================== Epoch: 324.  \t  Loss: 2.399569511413574  \t  Time: 1.4ss\n",
            "=========================== Epoch: 325.  \t  Loss: 2.3994672298431396  \t  Time: 1.36ss\n",
            "=========================== Epoch: 326.  \t  Loss: 2.399845600128174  \t  Time: 1.35ss\n",
            "=========================== Epoch: 327.  \t  Loss: 2.398566961288452  \t  Time: 1.37ss\n",
            "=========================== Epoch: 328.  \t  Loss: 2.3980157375335693  \t  Time: 1.37ss\n",
            "=========================== Epoch: 329.  \t  Loss: 2.3978590965270996  \t  Time: 1.34ss\n",
            "colo nipolo o aprepo supoan pao ora oa parpepe paquan e pe agi pa apela orlien ata orpa an poo opra pape oa ilarin u fa rivol la que\n",
            "laò  luprepo opo e vio\n",
            "=========================== Epoch: 330.  \t  Loss: 2.398561716079712  \t  Time: 1.36ss\n",
            "=========================== Epoch: 331.  \t  Loss: 2.3975322246551514  \t  Time: 1.35ss\n",
            "=========================== Epoch: 332.  \t  Loss: 2.39801287651062  \t  Time: 1.39ss\n",
            "=========================== Epoch: 333.  \t  Loss: 2.39656138420105  \t  Time: 1.48ss\n",
            "=========================== Epoch: 334.  \t  Loss: 2.3970632553100586  \t  Time: 1.45ss\n",
            "=========================== Epoch: 335.  \t  Loss: 2.396009683609009  \t  Time: 1.5ss\n",
            "=========================== Epoch: 336.  \t  Loss: 2.396570920944214  \t  Time: 1.47ss\n",
            "=========================== Epoch: 337.  \t  Loss: 2.3953726291656494  \t  Time: 1.42ss\n",
            "=========================== Epoch: 338.  \t  Loss: 2.3950600624084473  \t  Time: 1.46ss\n",
            "=========================== Epoch: 339.  \t  Loss: 2.3949668407440186  \t  Time: 1.4ss\n",
            "di\n",
            "\n",
            "cheo quan lao poo coposta cheo ao apo opola ché a ato gio ata opa pepeo sapra poo cala ola en ae pi lio prena fa lape lao quan  e ae a quan pocopolo \n",
            "=========================== Epoch: 340.  \t  Loss: 2.3944284915924072  \t  Time: 1.29ss\n",
            "=========================== Epoch: 341.  \t  Loss: 2.3942267894744873  \t  Time: 1.33ss\n",
            "=========================== Epoch: 342.  \t  Loss: 2.395233392715454  \t  Time: 1.3ss\n",
            "=========================== Epoch: 343.  \t  Loss: 2.393692970275879  \t  Time: 1.34ss\n",
            "=========================== Epoch: 344.  \t  Loss: 2.3933496475219727  \t  Time: 1.34ss\n",
            "=========================== Epoch: 345.  \t  Loss: 2.3928630352020264  \t  Time: 1.36ss\n",
            "=========================== Epoch: 346.  \t  Loss: 2.392850399017334  \t  Time: 1.36ss\n",
            "=========================== Epoch: 347.  \t  Loss: 2.3926336765289307  \t  Time: 1.34ss\n",
            "=========================== Epoch: 348.  \t  Loss: 2.3918955326080322  \t  Time: 1.34ss\n",
            "=========================== Epoch: 349.  \t  Loss: 2.3920364379882812  \t  Time: 1.35ss\n",
            "tal sao peopo peo lapa on o aque quan co aci opo preorbu oprape la pape opea oprapo leo colpola quan oo que o popabustia  e pedi fantro vin pola obre papa gipe pever \n",
            "=========================== Epoch: 350.  \t  Loss: 2.3927342891693115  \t  Time: 1.39ss\n",
            "=========================== Epoch: 351.  \t  Loss: 2.391465663909912  \t  Time: 1.35ss\n",
            "=========================== Epoch: 352.  \t  Loss: 2.392448902130127  \t  Time: 1.36ss\n",
            "=========================== Epoch: 353.  \t  Loss: 2.3902807235717773  \t  Time: 1.38ss\n",
            "=========================== Epoch: 354.  \t  Loss: 2.3907113075256348  \t  Time: 1.41ss\n",
            "=========================== Epoch: 355.  \t  Loss: 2.3904213905334473  \t  Time: 1.41ss\n",
            "=========================== Epoch: 356.  \t  Loss: 2.3904008865356445  \t  Time: 1.42ss\n",
            "=========================== Epoch: 357.  \t  Loss: 2.389681339263916  \t  Time: 1.38ss\n",
            "=========================== Epoch: 358.  \t  Loss: 2.3894436359405518  \t  Time: 1.41ss\n",
            "=========================== Epoch: 359.  \t  Loss: 2.389177083969116  \t  Time: 1.41ss\n",
            "mi\n",
            "di\n",
            "\n",
            "\n",
            "\n",
            " che\n",
            "\n",
            " totata\n",
            "\n",
            " tota\n",
            "\n",
            "\n",
            " totato\n",
            "\n",
            "\n",
            "\n",
            " chema\n",
            "\n",
            "ch lala quan  e alupo mica dipa orlo apa opepa ra poe opra  parla ifi lichi opo ora opape upo lila \n",
            "=========================== Epoch: 360.  \t  Loss: 2.3903439044952393  \t  Time: 1.44ss\n",
            "=========================== Epoch: 361.  \t  Loss: 2.3885498046875  \t  Time: 1.42ss\n",
            "=========================== Epoch: 362.  \t  Loss: 2.3895580768585205  \t  Time: 1.43ss\n",
            "=========================== Epoch: 363.  \t  Loss: 2.387819766998291  \t  Time: 1.4ss\n",
            "=========================== Epoch: 364.  \t  Loss: 2.3879871368408203  \t  Time: 1.38ss\n",
            "=========================== Epoch: 365.  \t  Loss: 2.387855291366577  \t  Time: 1.44ss\n",
            "=========================== Epoch: 366.  \t  Loss: 2.387507915496826  \t  Time: 1.42ss\n",
            "=========================== Epoch: 367.  \t  Loss: 2.3871209621429443  \t  Time: 1.41ss\n",
            "=========================== Epoch: 368.  \t  Loss: 2.386569023132324  \t  Time: 1.41ss\n",
            "=========================== Epoch: 369.  \t  Loss: 2.3868696689605713  \t  Time: 1.44ss\n",
            "se\n",
            "ase paopra pipra pela lapren vera e fipe ala morla die peo a pao dipape pa que gu\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " tache\n",
            "si\n",
            "\n",
            "\n",
            " tatorto  eca pala en aquan a lor e lor opo lasto pa te\n",
            "=========================== Epoch: 370.  \t  Loss: 2.3880598545074463  \t  Time: 1.4ss\n",
            "=========================== Epoch: 371.  \t  Loss: 2.3861420154571533  \t  Time: 1.37ss\n",
            "=========================== Epoch: 372.  \t  Loss: 2.386176824569702  \t  Time: 1.41ss\n",
            "=========================== Epoch: 373.  \t  Loss: 2.385643482208252  \t  Time: 1.43ss\n",
            "=========================== Epoch: 374.  \t  Loss: 2.38680100440979  \t  Time: 1.4ss\n",
            "=========================== Epoch: 375.  \t  Loss: 2.3849599361419678  \t  Time: 1.41ss\n",
            "=========================== Epoch: 376.  \t  Loss: 2.385009765625  \t  Time: 1.36ss\n",
            "=========================== Epoch: 377.  \t  Loss: 2.385175943374634  \t  Time: 1.38ss\n",
            "=========================== Epoch: 378.  \t  Loss: 2.3845465183258057  \t  Time: 1.38ss\n",
            "=========================== Epoch: 379.  \t  Loss: 2.38430118560791  \t  Time: 1.38ss\n",
            "se merlo aoro orpre prela ora ala opren colo ari pape opape o dipre paprape opra poo opapra pe apepe la dion  opa pabre par laquan men la aquan to e sa e lapren paa \n",
            "=========================== Epoch: 380.  \t  Loss: 2.383841037750244  \t  Time: 1.35ss\n",
            "=========================== Epoch: 381.  \t  Loss: 2.3850173950195312  \t  Time: 1.3ss\n",
            "=========================== Epoch: 382.  \t  Loss: 2.383535623550415  \t  Time: 1.36ss\n",
            "=========================== Epoch: 383.  \t  Loss: 2.3832521438598633  \t  Time: 1.38ss\n",
            "=========================== Epoch: 384.  \t  Loss: 2.383183240890503  \t  Time: 1.37ss\n",
            "=========================== Epoch: 385.  \t  Loss: 2.384162664413452  \t  Time: 1.3ss\n",
            "=========================== Epoch: 386.  \t  Loss: 2.382652521133423  \t  Time: 1.29ss\n",
            "=========================== Epoch: 387.  \t  Loss: 2.382948160171509  \t  Time: 1.32ss\n",
            "=========================== Epoch: 388.  \t  Loss: 2.3820300102233887  \t  Time: 1.31ss\n",
            "=========================== Epoch: 389.  \t  Loss: 2.3818583488464355  \t  Time: 1.36ss\n",
            "leo lapa quan a en e opo sepra apola quan e mi dibre loopra orpra e tepose men lao ala chi peren ato di vieno peorpa o papra perpepo opa bu\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " che\n",
            "monche leo o \n",
            "=========================== Epoch: 390.  \t  Loss: 2.382180690765381  \t  Time: 1.35ss\n",
            "=========================== Epoch: 391.  \t  Loss: 2.3815500736236572  \t  Time: 1.39ss\n",
            "=========================== Epoch: 392.  \t  Loss: 2.380772352218628  \t  Time: 1.37ss\n",
            "=========================== Epoch: 393.  \t  Loss: 2.381337881088257  \t  Time: 1.35ss\n",
            "=========================== Epoch: 394.  \t  Loss: 2.380924940109253  \t  Time: 1.37ss\n",
            "=========================== Epoch: 395.  \t  Loss: 2.380765676498413  \t  Time: 1.36ss\n",
            "=========================== Epoch: 396.  \t  Loss: 2.380711793899536  \t  Time: 1.35ss\n",
            "=========================== Epoch: 397.  \t  Loss: 2.380061149597168  \t  Time: 1.34ss\n",
            "=========================== Epoch: 398.  \t  Loss: 2.3802897930145264  \t  Time: 1.36ss\n",
            "=========================== Epoch: 399.  \t  Loss: 2.3800792694091797  \t  Time: 1.38ss\n",
            "mi cimi la opa orpa oro ape pooro peo ninpe aquan a lia mi que pero pipo quella opra pala lez volsi so rimi o oraquan e o pobarcalo saapo ae pa ope poola poopra\n",
            "=========================== Epoch: 400.  \t  Loss: 2.380016565322876  \t  Time: 1.37ss\n",
            "=========================== Epoch: 401.  \t  Loss: 2.379636764526367  \t  Time: 1.34ss\n",
            "=========================== Epoch: 402.  \t  Loss: 2.3793952465057373  \t  Time: 1.37ss\n",
            "=========================== Epoch: 403.  \t  Loss: 2.3792366981506348  \t  Time: 1.39ss\n",
            "=========================== Epoch: 404.  \t  Loss: 2.3792757987976074  \t  Time: 1.4ss\n",
            "=========================== Epoch: 405.  \t  Loss: 2.378661870956421  \t  Time: 1.43ss\n",
            "=========================== Epoch: 406.  \t  Loss: 2.37890887260437  \t  Time: 1.41ss\n",
            "=========================== Epoch: 407.  \t  Loss: 2.3788070678710938  \t  Time: 1.35ss\n",
            "=========================== Epoch: 408.  \t  Loss: 2.3785650730133057  \t  Time: 1.36ss\n",
            "=========================== Epoch: 409.  \t  Loss: 2.378713846206665  \t  Time: 1.32ss\n",
            "se leo dipa ape lao on pa o alo oo poopra orpe pae o que vista e la piume lava e que\n",
            "\n",
            "\n",
            "di\n",
            "\n",
            "\n",
            " che\n",
            "\n",
            "\n",
            " tota\n",
            "\n",
            " tito\n",
            "nel cala mo pee per raa e pipo opope\n",
            "=========================== Epoch: 410.  \t  Loss: 2.3782198429107666  \t  Time: 1.33ss\n",
            "=========================== Epoch: 411.  \t  Loss: 2.3780860900878906  \t  Time: 1.33ss\n",
            "=========================== Epoch: 412.  \t  Loss: 2.3781981468200684  \t  Time: 1.38ss\n",
            "=========================== Epoch: 413.  \t  Loss: 2.3775599002838135  \t  Time: 1.41ss\n",
            "=========================== Epoch: 414.  \t  Loss: 2.3776614665985107  \t  Time: 1.35ss\n",
            "=========================== Epoch: 415.  \t  Loss: 2.3774333000183105  \t  Time: 1.36ss\n",
            "=========================== Epoch: 416.  \t  Loss: 2.3769376277923584  \t  Time: 1.39ss\n",
            "=========================== Epoch: 417.  \t  Loss: 2.378558397293091  \t  Time: 1.38ss\n",
            "=========================== Epoch: 418.  \t  Loss: 2.3764381408691406  \t  Time: 1.37ss\n",
            "=========================== Epoch: 419.  \t  Loss: 2.3768749237060547  \t  Time: 1.38ss\n",
            "se lio perprabarbarbala loo opra que ope opra pape pape pea que  opoala no e pianse dipar e velo ferlo oo ape poopo leopa ape laquan a pae rin ave pe ea opo e\n",
            "=========================== Epoch: 420.  \t  Loss: 2.37701678276062  \t  Time: 1.42ss\n",
            "=========================== Epoch: 421.  \t  Loss: 2.3765852451324463  \t  Time: 1.35ss\n",
            "=========================== Epoch: 422.  \t  Loss: 2.3760623931884766  \t  Time: 1.39ss\n",
            "=========================== Epoch: 423.  \t  Loss: 2.3764801025390625  \t  Time: 1.38ss\n",
            "=========================== Epoch: 424.  \t  Loss: 2.3762006759643555  \t  Time: 1.36ss\n",
            "=========================== Epoch: 425.  \t  Loss: 2.3757781982421875  \t  Time: 1.38ss\n",
            "=========================== Epoch: 426.  \t  Loss: 2.3756277561187744  \t  Time: 1.38ss\n",
            "=========================== Epoch: 427.  \t  Loss: 2.3759653568267822  \t  Time: 1.4ss\n",
            "=========================== Epoch: 428.  \t  Loss: 2.3753957748413086  \t  Time: 1.35ss\n",
            "=========================== Epoch: 429.  \t  Loss: 2.3753225803375244  \t  Time: 1.35ss\n",
            "sta pola e lala a poe e lapren o ola que asnomi pola opoa copola asch sila poe va poopa ope lipe quan ae e pola fie pe la ora cela e prebarbali la vera laquan\n",
            "=========================== Epoch: 430.  \t  Loss: 2.3749637603759766  \t  Time: 1.39ss\n",
            "=========================== Epoch: 431.  \t  Loss: 2.3747217655181885  \t  Time: 1.42ss\n",
            "=========================== Epoch: 432.  \t  Loss: 2.3750243186950684  \t  Time: 1.4ss\n",
            "=========================== Epoch: 433.  \t  Loss: 2.3750247955322266  \t  Time: 1.44ss\n",
            "=========================== Epoch: 434.  \t  Loss: 2.374224901199341  \t  Time: 1.46ss\n",
            "=========================== Epoch: 435.  \t  Loss: 2.374457359313965  \t  Time: 1.44ss\n",
            "=========================== Epoch: 436.  \t  Loss: 2.374223470687866  \t  Time: 1.43ss\n",
            "=========================== Epoch: 437.  \t  Loss: 2.3742592334747314  \t  Time: 1.43ss\n",
            "=========================== Epoch: 438.  \t  Loss: 2.3742563724517822  \t  Time: 1.42ss\n",
            "=========================== Epoch: 439.  \t  Loss: 2.3739001750946045  \t  Time: 1.43ss\n",
            "si lao e lumepo looan la ïoopra  peleo papra poopra pola oo poleo lo poo pape ae lume popa laquan a e dali poe la ao pape que e pepoe pe que vi oo sa\n",
            "=========================== Epoch: 440.  \t  Loss: 2.3732008934020996  \t  Time: 1.41ss\n",
            "=========================== Epoch: 441.  \t  Loss: 2.3735084533691406  \t  Time: 1.41ss\n",
            "=========================== Epoch: 442.  \t  Loss: 2.374911308288574  \t  Time: 1.42ss\n",
            "=========================== Epoch: 443.  \t  Loss: 2.3729164600372314  \t  Time: 1.41ss\n",
            "=========================== Epoch: 444.  \t  Loss: 2.373014211654663  \t  Time: 1.43ss\n",
            "=========================== Epoch: 445.  \t  Loss: 2.373155355453491  \t  Time: 1.39ss\n",
            "=========================== Epoch: 446.  \t  Loss: 2.3729453086853027  \t  Time: 1.41ss\n",
            "=========================== Epoch: 447.  \t  Loss: 2.372403860092163  \t  Time: 1.43ss\n",
            "=========================== Epoch: 448.  \t  Loss: 2.3731706142425537  \t  Time: 1.44ss\n",
            "=========================== Epoch: 449.  \t  Loss: 2.3724939823150635  \t  Time: 1.41ss\n",
            "si di\n",
            "quan corce oro lapa pape teo ape opa que riprenpe ae a la verla pa parli e laquan  cheo ape mia pape diquala opra oror papa pellepe loo peropra pa\n",
            "la\n",
            "pe\n",
            "\n",
            "quanche\n",
            "\n",
            "=========================== Epoch: 450.  \t  Loss: 2.3723580837249756  \t  Time: 1.42ss\n",
            "=========================== Epoch: 451.  \t  Loss: 2.3720383644104004  \t  Time: 1.41ss\n",
            "=========================== Epoch: 452.  \t  Loss: 2.372302770614624  \t  Time: 1.4ss\n",
            "=========================== Epoch: 453.  \t  Loss: 2.372173309326172  \t  Time: 1.42ss\n",
            "=========================== Epoch: 454.  \t  Loss: 2.373338222503662  \t  Time: 1.39ss\n",
            "=========================== Epoch: 455.  \t  Loss: 2.371830940246582  \t  Time: 1.42ss\n",
            "=========================== Epoch: 456.  \t  Loss: 2.371786594390869  \t  Time: 1.45ss\n",
            "=========================== Epoch: 457.  \t  Loss: 2.3731675148010254  \t  Time: 1.41ss\n",
            "=========================== Epoch: 458.  \t  Loss: 2.37104868888855  \t  Time: 1.4ss\n",
            "=========================== Epoch: 459.  \t  Loss: 2.372420310974121  \t  Time: 1.34ss\n",
            "mo la asmino millela  ala aquan te veo alo obu\n",
            "\n",
            "\n",
            " che\n",
            "e\n",
            "\n",
            "\n",
            " tota\n",
            "\n",
            "\n",
            " tantato \n",
            "mama queper la opoopra o lepra pape ape la pae quan li e pelli lala e e la aquan\n",
            "=========================== Epoch: 460.  \t  Loss: 2.3709349632263184  \t  Time: 1.39ss\n",
            "=========================== Epoch: 461.  \t  Loss: 2.3721940517425537  \t  Time: 1.43ss\n",
            "=========================== Epoch: 462.  \t  Loss: 2.3707118034362793  \t  Time: 1.45ss\n",
            "=========================== Epoch: 463.  \t  Loss: 2.371464252471924  \t  Time: 1.36ss\n",
            "=========================== Epoch: 464.  \t  Loss: 2.3704614639282227  \t  Time: 1.37ss\n",
            "=========================== Epoch: 465.  \t  Loss: 2.370546817779541  \t  Time: 1.4ss\n",
            "=========================== Epoch: 466.  \t  Loss: 2.3707492351531982  \t  Time: 1.41ss\n",
            "=========================== Epoch: 467.  \t  Loss: 2.370636463165283  \t  Time: 1.42ss\n",
            "=========================== Epoch: 468.  \t  Loss: 2.3703339099884033  \t  Time: 1.42ss\n",
            "=========================== Epoch: 469.  \t  Loss: 2.3717377185821533  \t  Time: 1.45ss\n",
            "mi meloo o peo lio apo opoo ripe oo lape que\n",
            "\n",
            "a\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " che\n",
            "\n",
            " tutote ma pola opo dipa obre pepolo ae la opa o brepoobre popo pao lape ven i e sapoloo\n",
            "=========================== Epoch: 470.  \t  Loss: 2.37023663520813  \t  Time: 1.39ss\n",
            "=========================== Epoch: 471.  \t  Loss: 2.3715360164642334  \t  Time: 1.37ss\n",
            "=========================== Epoch: 472.  \t  Loss: 2.3700156211853027  \t  Time: 1.36ss\n",
            "=========================== Epoch: 473.  \t  Loss: 2.3709373474121094  \t  Time: 1.43ss\n",
            "=========================== Epoch: 474.  \t  Loss: 2.3697147369384766  \t  Time: 1.38ss\n",
            "=========================== Epoch: 475.  \t  Loss: 2.371363401412964  \t  Time: 1.37ss\n",
            "=========================== Epoch: 476.  \t  Loss: 2.370572090148926  \t  Time: 1.48ss\n",
            "=========================== Epoch: 477.  \t  Loss: 2.3706727027893066  \t  Time: 1.41ss\n",
            "=========================== Epoch: 478.  \t  Loss: 2.370433807373047  \t  Time: 1.39ss\n",
            "=========================== Epoch: 479.  \t  Loss: 2.370267152786255  \t  Time: 1.42ss\n",
            "misa opole la pa opape a que perpa pa ditepren vio papra\n",
            "\n",
            "\n",
            "e\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " cheta\n",
            " totota le la rae\n",
            "e\n",
            "sí la loopra que apo e opabre lo o leopo olo apra pepe lio an\n",
            "=========================== Epoch: 480.  \t  Loss: 2.3700637817382812  \t  Time: 1.49ss\n",
            "=========================== Epoch: 481.  \t  Loss: 2.368569850921631  \t  Time: 1.48ss\n",
            "=========================== Epoch: 482.  \t  Loss: 2.3688156604766846  \t  Time: 1.5ss\n",
            "=========================== Epoch: 483.  \t  Loss: 2.369251012802124  \t  Time: 1.48ss\n",
            "=========================== Epoch: 484.  \t  Loss: 2.3688416481018066  \t  Time: 1.4ss\n",
            "=========================== Epoch: 485.  \t  Loss: 2.3697357177734375  \t  Time: 1.43ss\n",
            "=========================== Epoch: 486.  \t  Loss: 2.368691921234131  \t  Time: 1.45ss\n",
            "=========================== Epoch: 487.  \t  Loss: 2.3697307109832764  \t  Time: 1.45ss\n",
            "=========================== Epoch: 488.  \t  Loss: 2.368375062942505  \t  Time: 1.43ss\n",
            "=========================== Epoch: 489.  \t  Loss: 2.369380474090576  \t  Time: 1.42ss\n",
            "mi poci peque\n",
            "\n",
            "\n",
            "\n",
            " che\n",
            "ba\n",
            "\n",
            " totata\n",
            "\n",
            "\n",
            " vetata\n",
            "\n",
            " tutota  mali verma bala e ti fa laque a veo oro pape opopopo an ae pocopo perpa ae peo pela acopo pola epi\n",
            "=========================== Epoch: 490.  \t  Loss: 2.369969367980957  \t  Time: 1.39ss\n",
            "=========================== Epoch: 491.  \t  Loss: 2.3692526817321777  \t  Time: 1.4ss\n",
            "=========================== Epoch: 492.  \t  Loss: 2.3696420192718506  \t  Time: 1.33ss\n",
            "=========================== Epoch: 493.  \t  Loss: 2.369182825088501  \t  Time: 1.42ss\n",
            "=========================== Epoch: 494.  \t  Loss: 2.3696799278259277  \t  Time: 1.41ss\n",
            "=========================== Epoch: 495.  \t  Loss: 2.3694238662719727  \t  Time: 1.32ss\n",
            "=========================== Epoch: 496.  \t  Loss: 2.369203805923462  \t  Time: 1.32ss\n",
            "=========================== Epoch: 497.  \t  Loss: 2.368640184402466  \t  Time: 1.41ss\n",
            "=========================== Epoch: 498.  \t  Loss: 2.369530200958252  \t  Time: 1.37ss\n",
            "=========================== Epoch: 499.  \t  Loss: 2.368604898452759  \t  Time: 1.39ss\n",
            "mi sepo copopolo bu\n",
            "\n",
            "\n",
            "\n",
            " tatorto\n",
            "\n",
            "\n",
            " tamata\n",
            "\n",
            "\n",
            " timati\n",
            "chele  laen aquan dumen atra vate pea opo ari fa opope ape que\n",
            "\n",
            "\n",
            "mae\n",
            "\n",
            "\n",
            "\n",
            " tamala\n",
            "ma\n",
            "e\n",
            "\n",
            "chelola pa prapee\n",
            "=========================== Epoch: 500.  \t  Loss: 2.36916184425354  \t  Time: 1.39ss\n",
            "=========================== Epoch: 501.  \t  Loss: 2.368488073348999  \t  Time: 1.37ss\n",
            "=========================== Epoch: 502.  \t  Loss: 2.368664503097534  \t  Time: 1.39ss\n",
            "=========================== Epoch: 503.  \t  Loss: 2.3686017990112305  \t  Time: 1.36ss\n",
            "=========================== Epoch: 504.  \t  Loss: 2.368253469467163  \t  Time: 1.39ss\n",
            "=========================== Epoch: 505.  \t  Loss: 2.3682126998901367  \t  Time: 1.35ss\n",
            "=========================== Epoch: 506.  \t  Loss: 2.368182420730591  \t  Time: 1.37ss\n",
            "=========================== Epoch: 507.  \t  Loss: 2.3678994178771973  \t  Time: 1.37ss\n",
            "=========================== Epoch: 508.  \t  Loss: 2.367757797241211  \t  Time: 1.38ss\n",
            "=========================== Epoch: 509.  \t  Loss: 2.367605447769165  \t  Time: 1.42ss\n",
            "mi\n",
            "là leo ape peae li e meper o perpepe oo apola quan cono orquan o pa laquan di o dipi parver man atro ae a prentene an o que\n",
            "ri\n",
            "\n",
            " dive\n",
            "per aché si poe lifi lu\n",
            "=========================== Epoch: 510.  \t  Loss: 2.3674826622009277  \t  Time: 1.4ss\n",
            "=========================== Epoch: 511.  \t  Loss: 2.3676998615264893  \t  Time: 1.41ss\n",
            "=========================== Epoch: 512.  \t  Loss: 2.3678150177001953  \t  Time: 1.36ss\n",
            "=========================== Epoch: 513.  \t  Loss: 2.367435932159424  \t  Time: 1.4ss\n",
            "=========================== Epoch: 514.  \t  Loss: 2.3672714233398438  \t  Time: 1.39ss\n",
            "=========================== Epoch: 515.  \t  Loss: 2.367526054382324  \t  Time: 1.36ss\n",
            "=========================== Epoch: 516.  \t  Loss: 2.3674399852752686  \t  Time: 1.37ss\n",
            "=========================== Epoch: 517.  \t  Loss: 2.3676812648773193  \t  Time: 1.35ss\n",
            "=========================== Epoch: 518.  \t  Loss: 2.3671715259552  \t  Time: 1.35ss\n",
            "=========================== Epoch: 519.  \t  Loss: 2.3669686317443848  \t  Time: 1.39ss\n",
            "se fie la parla la ae la pee la chi e pela opra\n",
            "\n",
            "\n",
            "\n",
            " tito\n",
            "\n",
            " trasveto\n",
            "\n",
            "\n",
            " tota\n",
            "se\n",
            "\n",
            " tato mi\n",
            "maglio\n",
            "\n",
            " to\n",
            "\n",
            " tutato li la quan  e la ipren\n",
            "loan se ae a efiper\n",
            "=========================== Epoch: 520.  \t  Loss: 2.3672232627868652  \t  Time: 1.37ss\n",
            "=========================== Epoch: 521.  \t  Loss: 2.3667545318603516  \t  Time: 1.36ss\n",
            "=========================== Epoch: 522.  \t  Loss: 2.3668015003204346  \t  Time: 1.39ss\n",
            "=========================== Epoch: 523.  \t  Loss: 2.3671040534973145  \t  Time: 1.38ss\n",
            "=========================== Epoch: 524.  \t  Loss: 2.3670284748077393  \t  Time: 1.37ss\n",
            "=========================== Epoch: 525.  \t  Loss: 2.3667263984680176  \t  Time: 1.39ss\n",
            "=========================== Epoch: 526.  \t  Loss: 2.3666083812713623  \t  Time: 1.39ss\n",
            "=========================== Epoch: 527.  \t  Loss: 2.366905927658081  \t  Time: 1.36ss\n",
            "=========================== Epoch: 528.  \t  Loss: 2.3661935329437256  \t  Time: 1.37ss\n",
            "=========================== Epoch: 529.  \t  Loss: 2.3664634227752686  \t  Time: 1.37ss\n",
            "parmo que\n",
            "\n",
            "\n",
            " tata\n",
            "\n",
            " tota ta simi chela o lor la ae que ti que\n",
            "ò copola lu apere que chi opo dipeo prepo aopopo ora poe pa pae perbu poe apo osto bu\n",
            "\n",
            "\n",
            "\n",
            " tu\n",
            "=========================== Epoch: 530.  \t  Loss: 2.366314172744751  \t  Time: 1.37ss\n",
            "=========================== Epoch: 531.  \t  Loss: 2.366852045059204  \t  Time: 1.36ss\n",
            "=========================== Epoch: 532.  \t  Loss: 2.3662302494049072  \t  Time: 1.38ss\n",
            "=========================== Epoch: 533.  \t  Loss: 2.366128444671631  \t  Time: 1.37ss\n",
            "=========================== Epoch: 534.  \t  Loss: 2.366259813308716  \t  Time: 1.4ss\n",
            "=========================== Epoch: 535.  \t  Loss: 2.36570143699646  \t  Time: 1.36ss\n",
            "=========================== Epoch: 536.  \t  Loss: 2.3656115531921387  \t  Time: 1.38ss\n",
            "=========================== Epoch: 537.  \t  Loss: 2.3661446571350098  \t  Time: 1.4ss\n",
            "=========================== Epoch: 538.  \t  Loss: 2.365736246109009  \t  Time: 1.39ss\n",
            "=========================== Epoch: 539.  \t  Loss: 2.365891695022583  \t  Time: 1.36ss\n",
            "fipa\n",
            "\n",
            " gen                                                                                              \n",
            "=========================== Epoch: 540.  \t  Loss: 2.365447521209717  \t  Time: 1.38ss\n",
            "=========================== Epoch: 541.  \t  Loss: 2.3654298782348633  \t  Time: 1.37ss\n",
            "=========================== Epoch: 542.  \t  Loss: 2.365795135498047  \t  Time: 1.34ss\n",
            "=========================== Epoch: 543.  \t  Loss: 2.3653156757354736  \t  Time: 1.43ss\n",
            "=========================== Epoch: 544.  \t  Loss: 2.365309715270996  \t  Time: 1.42ss\n",
            "=========================== Epoch: 545.  \t  Loss: 2.365203380584717  \t  Time: 1.45ss\n",
            "=========================== Epoch: 546.  \t  Loss: 2.3651628494262695  \t  Time: 1.46ss\n",
            "=========================== Epoch: 547.  \t  Loss: 2.365211248397827  \t  Time: 1.63ss\n",
            "=========================== Epoch: 548.  \t  Loss: 2.3652536869049072  \t  Time: 1.49ss\n",
            "=========================== Epoch: 549.  \t  Loss: 2.364935874938965  \t  Time: 1.48ss\n",
            "mi mi sala o poe o apela pape quan o laa on di parte per achi  opape pea la la quan li liqua fi eper oo ria rola oro peo vope\n",
            "\n",
            "dissesele\n",
            "o\n",
            "poi\n",
            "a\n",
            "pero paprape \n",
            "=========================== Epoch: 550.  \t  Loss: 2.3650155067443848  \t  Time: 1.37ss\n",
            "=========================== Epoch: 551.  \t  Loss: 2.3649139404296875  \t  Time: 1.38ss\n",
            "=========================== Epoch: 552.  \t  Loss: 2.3648386001586914  \t  Time: 1.36ss\n",
            "=========================== Epoch: 553.  \t  Loss: 2.3645477294921875  \t  Time: 1.38ss\n",
            "=========================== Epoch: 554.  \t  Loss: 2.3644514083862305  \t  Time: 1.37ss\n",
            "=========================== Epoch: 555.  \t  Loss: 2.3643763065338135  \t  Time: 1.39ss\n",
            "=========================== Epoch: 556.  \t  Loss: 2.364607810974121  \t  Time: 1.39ss\n",
            "=========================== Epoch: 557.  \t  Loss: 2.3642637729644775  \t  Time: 1.4ss\n",
            "=========================== Epoch: 558.  \t  Loss: 2.364370822906494  \t  Time: 1.38ss\n",
            "=========================== Epoch: 559.  \t  Loss: 2.3641905784606934  \t  Time: 1.41ss\n",
            "le\n",
            "di\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " che\n",
            "\n",
            " titrastoto\n",
            "\n",
            "\n",
            "\n",
            " che\n",
            "mo\n",
            "\n",
            " titota\n",
            "\n",
            " tota\n",
            "\n",
            "\n",
            " toma\n",
            "che le opoo obu\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " che\n",
            "\n",
            " tito\n",
            "\n",
            " tatatato\n",
            "\n",
            "\n",
            " tamata\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " che\n",
            "perlen no mi\n",
            "se daper \n",
            "=========================== Epoch: 560.  \t  Loss: 2.364279270172119  \t  Time: 1.4ss\n",
            "=========================== Epoch: 561.  \t  Loss: 2.3643765449523926  \t  Time: 1.38ss\n",
            "=========================== Epoch: 562.  \t  Loss: 2.3641998767852783  \t  Time: 1.41ss\n",
            "=========================== Epoch: 563.  \t  Loss: 2.363987684249878  \t  Time: 1.38ss\n",
            "=========================== Epoch: 564.  \t  Loss: 2.3638789653778076  \t  Time: 1.36ss\n",
            "=========================== Epoch: 565.  \t  Loss: 2.3642983436584473  \t  Time: 1.38ss\n",
            "=========================== Epoch: 566.  \t  Loss: 2.3641197681427  \t  Time: 1.38ss\n",
            "=========================== Epoch: 567.  \t  Loss: 2.3638863563537598  \t  Time: 1.39ss\n",
            "=========================== Epoch: 568.  \t  Loss: 2.363563299179077  \t  Time: 1.42ss\n",
            "=========================== Epoch: 569.  \t  Loss: 2.3640248775482178  \t  Time: 1.39ss\n",
            "parmo vela opa ora pedi ala man laen o rila vilie a chi ala opra pela peo poo poo peo parla iper me opopoopo opa oo peo ope pra ripola\n",
            "\n",
            "\n",
            " tutita man se \n",
            "=========================== Epoch: 570.  \t  Loss: 2.363983631134033  \t  Time: 1.38ss\n",
            "=========================== Epoch: 571.  \t  Loss: 2.3638200759887695  \t  Time: 1.37ss\n",
            "=========================== Epoch: 572.  \t  Loss: 2.3635730743408203  \t  Time: 1.37ss\n",
            "=========================== Epoch: 573.  \t  Loss: 2.3635308742523193  \t  Time: 1.38ss\n",
            "=========================== Epoch: 574.  \t  Loss: 2.36372971534729  \t  Time: 1.37ss\n",
            "=========================== Epoch: 575.  \t  Loss: 2.3635876178741455  \t  Time: 1.37ss\n",
            "=========================== Epoch: 576.  \t  Loss: 2.3635733127593994  \t  Time: 1.37ss\n",
            "=========================== Epoch: 577.  \t  Loss: 2.363246440887451  \t  Time: 1.39ss\n",
            "=========================== Epoch: 578.  \t  Loss: 2.3631765842437744  \t  Time: 1.36ss\n",
            "=========================== Epoch: 579.  \t  Loss: 2.3632876873016357  \t  Time: 1.38ss\n",
            "simi lali pa pa oro cipe opopopoe ci ata rique\n",
            "\n",
            "\n",
            "\n",
            " toto\n",
            "ver\n",
            "diò  orala e no pocila e palu tape lapren la pao poo opopra la barcapa loo\n",
            "que\n",
            "\n",
            "di\n",
            "\n",
            "\n",
            "\n",
            " che\n",
            "\n",
            " \n",
            "=========================== Epoch: 580.  \t  Loss: 2.3633086681365967  \t  Time: 1.39ss\n",
            "=========================== Epoch: 581.  \t  Loss: 2.363629102706909  \t  Time: 1.39ss\n",
            "=========================== Epoch: 582.  \t  Loss: 2.363318920135498  \t  Time: 1.42ss\n",
            "=========================== Epoch: 583.  \t  Loss: 2.3630447387695312  \t  Time: 1.39ss\n",
            "=========================== Epoch: 584.  \t  Loss: 2.3629403114318848  \t  Time: 1.4ss\n",
            "=========================== Epoch: 585.  \t  Loss: 2.3632307052612305  \t  Time: 1.39ss\n",
            "=========================== Epoch: 586.  \t  Loss: 2.363262176513672  \t  Time: 1.38ss\n",
            "=========================== Epoch: 587.  \t  Loss: 2.363123655319214  \t  Time: 1.38ss\n",
            "=========================== Epoch: 588.  \t  Loss: 2.3629863262176514  \t  Time: 1.36ss\n",
            "=========================== Epoch: 589.  \t  Loss: 2.3630287647247314  \t  Time: 1.41ss\n",
            "te sta na poloo ao peala che gioo pe quespet \n",
            "\n",
            "\n",
            "\n",
            " cheta\n",
            "\n",
            "\n",
            " tutate\n",
            "ta\n",
            "\n",
            "\n",
            "\n",
            " tantoto  beglio ami leo lopoo copoorpra ala pocopocapoo parlae  pola cer cape vala ver pa\n",
            "=========================== Epoch: 590.  \t  Loss: 2.362452745437622  \t  Time: 1.33ss\n",
            "=========================== Epoch: 591.  \t  Loss: 2.36258602142334  \t  Time: 1.38ss\n",
            "=========================== Epoch: 592.  \t  Loss: 2.3629989624023438  \t  Time: 1.37ss\n",
            "=========================== Epoch: 593.  \t  Loss: 2.362593650817871  \t  Time: 1.38ss\n",
            "=========================== Epoch: 594.  \t  Loss: 2.3624932765960693  \t  Time: 1.39ss\n",
            "=========================== Epoch: 595.  \t  Loss: 2.3625986576080322  \t  Time: 1.39ss\n",
            "=========================== Epoch: 596.  \t  Loss: 2.362621784210205  \t  Time: 1.39ss\n",
            "=========================== Epoch: 597.  \t  Loss: 2.3624327182769775  \t  Time: 1.36ss\n",
            "=========================== Epoch: 598.  \t  Loss: 2.362215995788574  \t  Time: 1.38ss\n",
            "=========================== Epoch: 599.  \t  Loss: 2.3621997833251953  \t  Time: 1.36ss\n",
            "sepo dimes siper meo dipar la operbre apren vio aspopala la ope rio opa\n",
            "\n",
            "e\n",
            "\n",
            "\n",
            "di\n",
            "\n",
            " veto\n",
            "\n",
            "\n",
            "\n",
            " tatota\n",
            "\n",
            "\n",
            " tota\n",
            "\n",
            "\n",
            " titota\n",
            "\n",
            " toto ve\n",
            "conveo pra pola la e prapola la\n",
            "=========================== Epoch: 600.  \t  Loss: 2.3625121116638184  \t  Time: 1.38ss\n",
            "=========================== Epoch: 601.  \t  Loss: 2.3621716499328613  \t  Time: 1.32ss\n",
            "=========================== Epoch: 602.  \t  Loss: 2.3622429370880127  \t  Time: 1.3ss\n",
            "=========================== Epoch: 603.  \t  Loss: 2.3621668815612793  \t  Time: 1.4ss\n",
            "=========================== Epoch: 604.  \t  Loss: 2.362086057662964  \t  Time: 1.36ss\n",
            "=========================== Epoch: 605.  \t  Loss: 2.3620574474334717  \t  Time: 1.38ss\n",
            "=========================== Epoch: 606.  \t  Loss: 2.361564874649048  \t  Time: 1.34ss\n",
            "=========================== Epoch: 607.  \t  Loss: 2.3617444038391113  \t  Time: 1.3ss\n",
            "=========================== Epoch: 608.  \t  Loss: 2.3621280193328857  \t  Time: 1.33ss\n",
            "=========================== Epoch: 609.  \t  Loss: 2.3618814945220947  \t  Time: 1.34ss\n",
            "si dipo e potepopo olabrepren pe pi\n",
            "e\n",
            "\n",
            " titato\n",
            "\n",
            "\n",
            " bito\n",
            "\n",
            " tota\n",
            "\n",
            "\n",
            "\n",
            " tatace me leoo poo nipola laen a pape o apeque  assopala pe veopra o paa pola pi loo \n",
            "=========================== Epoch: 610.  \t  Loss: 2.361696243286133  \t  Time: 1.37ss\n",
            "=========================== Epoch: 611.  \t  Loss: 2.3620431423187256  \t  Time: 1.37ss\n",
            "=========================== Epoch: 612.  \t  Loss: 2.361593723297119  \t  Time: 1.37ss\n",
            "=========================== Epoch: 613.  \t  Loss: 2.361851215362549  \t  Time: 1.36ss\n",
            "=========================== Epoch: 614.  \t  Loss: 2.3610992431640625  \t  Time: 1.36ss\n",
            "=========================== Epoch: 615.  \t  Loss: 2.3612730503082275  \t  Time: 1.38ss\n",
            "=========================== Epoch: 616.  \t  Loss: 2.3611488342285156  \t  Time: 1.37ss\n",
            "=========================== Epoch: 617.  \t  Loss: 2.3619258403778076  \t  Time: 1.37ss\n",
            "=========================== Epoch: 618.  \t  Loss: 2.361544370651245  \t  Time: 1.39ss\n",
            "=========================== Epoch: 619.  \t  Loss: 2.3613712787628174  \t  Time: 1.37ss\n",
            "se loo poo pelo opoo lagno  lape dien der a poe poe serpo opa apo ladi ente loo biangna\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " tiche\n",
            "e\n",
            "\n",
            "chela gli pao e perder opopo lape opopra pocopopo gola o\n",
            "=========================== Epoch: 620.  \t  Loss: 2.361534357070923  \t  Time: 1.38ss\n",
            "=========================== Epoch: 621.  \t  Loss: 2.3611648082733154  \t  Time: 1.4ss\n",
            "=========================== Epoch: 622.  \t  Loss: 2.3607425689697266  \t  Time: 1.4ss\n",
            "=========================== Epoch: 623.  \t  Loss: 2.36106276512146  \t  Time: 1.39ss\n",
            "=========================== Epoch: 624.  \t  Loss: 2.3606011867523193  \t  Time: 1.43ss\n",
            "=========================== Epoch: 625.  \t  Loss: 2.3610148429870605  \t  Time: 1.39ss\n",
            "=========================== Epoch: 626.  \t  Loss: 2.3611037731170654  \t  Time: 1.4ss\n",
            "=========================== Epoch: 627.  \t  Loss: 2.3606927394866943  \t  Time: 1.38ss\n",
            "=========================== Epoch: 628.  \t  Loss: 2.3610119819641113  \t  Time: 1.36ss\n",
            "=========================== Epoch: 629.  \t  Loss: 2.3604793548583984  \t  Time: 1.25ss\n",
            "mise o o one apola caopra popoquan co parala\n",
            "\n",
            "quand opope epola copopa orpa pra o lioro orpa ape ae perte poo o apepe poe lupo pocala parla en tro ae la lia \n",
            "=========================== Epoch: 630.  \t  Loss: 2.360989809036255  \t  Time: 1.29ss\n",
            "=========================== Epoch: 631.  \t  Loss: 2.360456705093384  \t  Time: 1.2ss\n",
            "=========================== Epoch: 632.  \t  Loss: 2.360518217086792  \t  Time: 1.35ss\n",
            "=========================== Epoch: 633.  \t  Loss: 2.3606348037719727  \t  Time: 1.3ss\n",
            "=========================== Epoch: 634.  \t  Loss: 2.360501766204834  \t  Time: 1.29ss\n",
            "=========================== Epoch: 635.  \t  Loss: 2.3609163761138916  \t  Time: 1.17ss\n",
            "=========================== Epoch: 636.  \t  Loss: 2.360563039779663  \t  Time: 1.29ss\n",
            "=========================== Epoch: 637.  \t  Loss: 2.3603475093841553  \t  Time: 1.19ss\n",
            "=========================== Epoch: 638.  \t  Loss: 2.3604016304016113  \t  Time: 1.24ss\n",
            "=========================== Epoch: 639.  \t  Loss: 2.3604953289031982  \t  Time: 1.25ss\n",
            "dio la\n",
            "e\n",
            "que\n",
            "\n",
            "\n",
            " che\n",
            "perla  eti\n",
            "\n",
            "\n",
            "di\n",
            "ti\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " che\n",
            "mala\n",
            "\n",
            "\n",
            " che\n",
            "\n",
            "\n",
            " toma\n",
            "\n",
            "\n",
            " tatorto\n",
            "co\n",
            "\n",
            "\n",
            "disseseso apola opra que mensto lor la la en atri on  ae pa co leorga\n",
            "=========================== Epoch: 640.  \t  Loss: 2.3603336811065674  \t  Time: 1.21ss\n",
            "=========================== Epoch: 641.  \t  Loss: 2.3603196144104004  \t  Time: 1.24ss\n",
            "=========================== Epoch: 642.  \t  Loss: 2.360492467880249  \t  Time: 1.27ss\n",
            "=========================== Epoch: 643.  \t  Loss: 2.360180377960205  \t  Time: 1.28ss\n",
            "=========================== Epoch: 644.  \t  Loss: 2.3598830699920654  \t  Time: 1.27ss\n",
            "=========================== Epoch: 645.  \t  Loss: 2.3600573539733887  \t  Time: 1.26ss\n",
            "=========================== Epoch: 646.  \t  Loss: 2.360297203063965  \t  Time: 1.29ss\n",
            "=========================== Epoch: 647.  \t  Loss: 2.360046148300171  \t  Time: 1.29ss\n",
            "=========================== Epoch: 648.  \t  Loss: 2.360010862350464  \t  Time: 1.3ss\n",
            "=========================== Epoch: 649.  \t  Loss: 2.35970401763916  \t  Time: 1.3ss\n",
            "mi sea e popola ae pee le la opra que\n",
            "\n",
            "conposta la tri la opra riopra\n",
            "la\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " di\n",
            "\n",
            "\n",
            " tutito \n",
            "\n",
            " temato\n",
            "\n",
            "\n",
            " trasseto\n",
            "\n",
            "\n",
            " bito\n",
            "che\n",
            "\n",
            "síla la dien te peo alo or\n",
            "=========================== Epoch: 650.  \t  Loss: 2.3594939708709717  \t  Time: 1.32ss\n",
            "=========================== Epoch: 651.  \t  Loss: 2.359847068786621  \t  Time: 1.34ss\n",
            "=========================== Epoch: 652.  \t  Loss: 2.3596959114074707  \t  Time: 1.32ss\n",
            "=========================== Epoch: 653.  \t  Loss: 2.3595404624938965  \t  Time: 1.36ss\n",
            "=========================== Epoch: 654.  \t  Loss: 2.359825849533081  \t  Time: 1.37ss\n",
            "=========================== Epoch: 655.  \t  Loss: 2.359163522720337  \t  Time: 1.42ss\n",
            "=========================== Epoch: 656.  \t  Loss: 2.3590447902679443  \t  Time: 1.36ss\n",
            "=========================== Epoch: 657.  \t  Loss: 2.359445810317993  \t  Time: 1.34ss\n",
            "=========================== Epoch: 658.  \t  Loss: 2.3594768047332764  \t  Time: 1.34ss\n",
            "=========================== Epoch: 659.  \t  Loss: 2.3597285747528076  \t  Time: 1.32ss\n",
            "mise o\n",
            "se\n",
            "\n",
            "\n",
            " tito\n",
            "\n",
            " gíma\n",
            "chela\n",
            "ri\n",
            "però  quan li colla per pape sie te popoe la e pela o pipa opra lo lio nomepoe le que lo an ae fi\n",
            "e  diparla e mela\n",
            "e \n",
            "=========================== Epoch: 660.  \t  Loss: 2.3594613075256348  \t  Time: 1.32ss\n",
            "=========================== Epoch: 661.  \t  Loss: 2.3592398166656494  \t  Time: 1.33ss\n",
            "=========================== Epoch: 662.  \t  Loss: 2.359227418899536  \t  Time: 1.36ss\n",
            "=========================== Epoch: 663.  \t  Loss: 2.3590948581695557  \t  Time: 1.31ss\n",
            "=========================== Epoch: 664.  \t  Loss: 2.359255075454712  \t  Time: 1.33ss\n",
            "=========================== Epoch: 665.  \t  Loss: 2.3590192794799805  \t  Time: 1.36ss\n",
            "=========================== Epoch: 666.  \t  Loss: 2.359010934829712  \t  Time: 1.4ss\n",
            "=========================== Epoch: 667.  \t  Loss: 2.358829975128174  \t  Time: 1.35ss\n",
            "=========================== Epoch: 668.  \t  Loss: 2.359006404876709  \t  Time: 1.37ss\n",
            "=========================== Epoch: 669.  \t  Loss: 2.3588056564331055  \t  Time: 1.4ss\n",
            "mi rimi riqui e siper poe fipopa la tebre di e leo poo pella prela ope peae que le e perta pila men a ipote copopola capeo bu\n",
            "\n",
            "\n",
            "\n",
            " timata\n",
            "\n",
            "perpolo oo olo ape\n",
            "=========================== Epoch: 670.  \t  Loss: 2.3587934970855713  \t  Time: 1.21ss\n",
            "=========================== Epoch: 671.  \t  Loss: 2.3591647148132324  \t  Time: 1.22ss\n",
            "=========================== Epoch: 672.  \t  Loss: 2.3588004112243652  \t  Time: 1.23ss\n",
            "=========================== Epoch: 673.  \t  Loss: 2.3587489128112793  \t  Time: 1.2ss\n",
            "=========================== Epoch: 674.  \t  Loss: 2.358471393585205  \t  Time: 1.18ss\n",
            "=========================== Epoch: 675.  \t  Loss: 2.358475923538208  \t  Time: 1.22ss\n",
            "=========================== Epoch: 676.  \t  Loss: 2.358548402786255  \t  Time: 1.3ss\n",
            "=========================== Epoch: 677.  \t  Loss: 2.358449935913086  \t  Time: 1.31ss\n",
            "=========================== Epoch: 678.  \t  Loss: 2.35845947265625  \t  Time: 1.28ss\n",
            "=========================== Epoch: 679.  \t  Loss: 2.358449697494507  \t  Time: 1.28ss\n",
            "se omi o leo nopola pe e dipa bran\n",
            "\n",
            " titi\n",
            "che\n",
            "\n",
            "\n",
            " tato\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " tutota\n",
            "\n",
            "\n",
            " tima\n",
            "perrita lae pola copi oro ape opope dela o ape la opea ritraquan  la ete pee\n",
            "=========================== Epoch: 680.  \t  Loss: 2.3585422039031982  \t  Time: 1.36ss\n",
            "=========================== Epoch: 681.  \t  Loss: 2.358365535736084  \t  Time: 1.37ss\n",
            "=========================== Epoch: 682.  \t  Loss: 2.3585386276245117  \t  Time: 1.27ss\n",
            "=========================== Epoch: 683.  \t  Loss: 2.3581526279449463  \t  Time: 1.28ss\n",
            "=========================== Epoch: 684.  \t  Loss: 2.35843563079834  \t  Time: 1.32ss\n",
            "=========================== Epoch: 685.  \t  Loss: 2.358400344848633  \t  Time: 1.33ss\n",
            "=========================== Epoch: 686.  \t  Loss: 2.3581292629241943  \t  Time: 1.3ss\n",
            "=========================== Epoch: 687.  \t  Loss: 2.3580403327941895  \t  Time: 1.31ss\n",
            "=========================== Epoch: 688.  \t  Loss: 2.358361005783081  \t  Time: 1.37ss\n",
            "=========================== Epoch: 689.  \t  Loss: 2.3581466674804688  \t  Time: 1.35ss\n",
            "parla\n",
            "\n",
            "\n",
            "\n",
            " tantoto \n",
            "ma di quan volser mo vivisoa veo ape la volla\n",
            "\n",
            "l pee e tre vapo laopra que ora quan vol tu sili la i barcali la igi vi e perpe orala quan re copo\n",
            "=========================== Epoch: 690.  \t  Loss: 2.358093500137329  \t  Time: 1.34ss\n",
            "=========================== Epoch: 691.  \t  Loss: 2.358031988143921  \t  Time: 1.37ss\n",
            "=========================== Epoch: 692.  \t  Loss: 2.358124017715454  \t  Time: 1.44ss\n",
            "=========================== Epoch: 693.  \t  Loss: 2.358351945877075  \t  Time: 1.41ss\n",
            "=========================== Epoch: 694.  \t  Loss: 2.3577880859375  \t  Time: 1.39ss\n",
            "=========================== Epoch: 695.  \t  Loss: 2.3578133583068848  \t  Time: 1.38ss\n",
            "=========================== Epoch: 696.  \t  Loss: 2.3575401306152344  \t  Time: 1.43ss\n",
            "=========================== Epoch: 697.  \t  Loss: 2.357516288757324  \t  Time: 1.38ss\n",
            "=========================== Epoch: 698.  \t  Loss: 2.3579211235046387  \t  Time: 1.39ss\n",
            "=========================== Epoch: 699.  \t  Loss: 2.3576202392578125  \t  Time: 1.4ss\n",
            "di e e permu o peape la e pe ae per opobre gioco melo veo ovra e a que condo capola o fa orpra pa lao brepola pa e pi que e la e pola ora eper bi e o \n",
            "=========================== Epoch: 700.  \t  Loss: 2.3579907417297363  \t  Time: 1.32ss\n",
            "=========================== Epoch: 701.  \t  Loss: 2.357442855834961  \t  Time: 1.32ss\n",
            "=========================== Epoch: 702.  \t  Loss: 2.357719659805298  \t  Time: 1.34ss\n",
            "=========================== Epoch: 703.  \t  Loss: 2.3575515747070312  \t  Time: 1.34ss\n",
            "=========================== Epoch: 704.  \t  Loss: 2.357469320297241  \t  Time: 1.36ss\n",
            "=========================== Epoch: 705.  \t  Loss: 2.357593297958374  \t  Time: 1.4ss\n",
            "=========================== Epoch: 706.  \t  Loss: 2.3573505878448486  \t  Time: 1.4ss\n",
            "=========================== Epoch: 707.  \t  Loss: 2.3575503826141357  \t  Time: 1.39ss\n",
            "=========================== Epoch: 708.  \t  Loss: 2.3569865226745605  \t  Time: 1.42ss\n",
            "=========================== Epoch: 709.  \t  Loss: 2.3571133613586426  \t  Time: 1.36ss\n",
            "mi su ae per dea e poe pape la e veo dipa lupi verprennigna\n",
            "\n",
            "\n",
            "\n",
            "di\n",
            "\n",
            "\n",
            "perlaò  aen e liva la asmila ae pa pola loo e pella la pa tre opra la la cala pe neo\n",
            "=========================== Epoch: 710.  \t  Loss: 2.3573157787323  \t  Time: 1.36ss\n",
            "=========================== Epoch: 711.  \t  Loss: 2.3571183681488037  \t  Time: 1.4ss\n",
            "=========================== Epoch: 712.  \t  Loss: 2.357208728790283  \t  Time: 1.35ss\n",
            "=========================== Epoch: 713.  \t  Loss: 2.357130289077759  \t  Time: 1.37ss\n",
            "=========================== Epoch: 714.  \t  Loss: 2.3568530082702637  \t  Time: 1.36ss\n",
            "=========================== Epoch: 715.  \t  Loss: 2.3570868968963623  \t  Time: 1.35ss\n",
            "=========================== Epoch: 716.  \t  Loss: 2.3569045066833496  \t  Time: 1.38ss\n",
            "=========================== Epoch: 717.  \t  Loss: 2.3568289279937744  \t  Time: 1.37ss\n",
            "=========================== Epoch: 718.  \t  Loss: 2.3569655418395996  \t  Time: 1.38ss\n",
            "=========================== Epoch: 719.  \t  Loss: 2.3568313121795654  \t  Time: 1.35ss\n",
            "si coperpopo ricopopa lape bare pate lepe co pipoposa lao lupepola opra lo bian di\n",
            "lo\n",
            "quancoce\n",
            "\n",
            " teta\n",
            "se\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " bitrassesole a diprenta opra\n",
            "perpo aché li aque e te mi meo\n",
            "=========================== Epoch: 720.  \t  Loss: 2.3567745685577393  \t  Time: 1.43ss\n",
            "=========================== Epoch: 721.  \t  Loss: 2.356572151184082  \t  Time: 1.4ss\n",
            "=========================== Epoch: 722.  \t  Loss: 2.3565785884857178  \t  Time: 1.43ss\n",
            "=========================== Epoch: 723.  \t  Loss: 2.3565659523010254  \t  Time: 1.39ss\n",
            "=========================== Epoch: 724.  \t  Loss: 2.3566970825195312  \t  Time: 1.38ss\n",
            "=========================== Epoch: 725.  \t  Loss: 2.3565337657928467  \t  Time: 1.44ss\n",
            "=========================== Epoch: 726.  \t  Loss: 2.3568131923675537  \t  Time: 1.39ss\n",
            "=========================== Epoch: 727.  \t  Loss: 2.3562188148498535  \t  Time: 1.38ss\n",
            "=========================== Epoch: 728.  \t  Loss: 2.3563036918640137  \t  Time: 1.41ss\n",
            "=========================== Epoch: 729.  \t  Loss: 2.3568484783172607  \t  Time: 1.33ss\n",
            "an colui la la gio e peae pola quan vea ope bu\n",
            "\n",
            "\n",
            "\n",
            " tota\n",
            "\n",
            "\n",
            " totaso\n",
            "ver\n",
            "ma\n",
            "\n",
            " tota\n",
            "ta\n",
            "perpopo loo popocopoparla leo o lape oro biangna\n",
            "\n",
            "\n",
            "chela perpe e peli oo a\n",
            "=========================== Epoch: 730.  \t  Loss: 2.3567655086517334  \t  Time: 1.41ss\n",
            "=========================== Epoch: 731.  \t  Loss: 2.356705665588379  \t  Time: 1.39ss\n",
            "=========================== Epoch: 732.  \t  Loss: 2.3564095497131348  \t  Time: 1.41ss\n",
            "=========================== Epoch: 733.  \t  Loss: 2.3565218448638916  \t  Time: 1.37ss\n",
            "=========================== Epoch: 734.  \t  Loss: 2.3564648628234863  \t  Time: 1.38ss\n",
            "=========================== Epoch: 735.  \t  Loss: 2.3563661575317383  \t  Time: 1.4ss\n",
            "=========================== Epoch: 736.  \t  Loss: 2.3560233116149902  \t  Time: 1.35ss\n",
            "=========================== Epoch: 737.  \t  Loss: 2.356449604034424  \t  Time: 1.4ss\n",
            "=========================== Epoch: 738.  \t  Loss: 2.3563663959503174  \t  Time: 1.37ss\n",
            "=========================== Epoch: 739.  \t  Loss: 2.3563573360443115  \t  Time: 1.41ss\n",
            "la mila  lapa quanrí ae lor  poae  e lor la poe  lapren\n",
            "e pe mila quanco\n",
            "\n",
            "\n",
            "\n",
            " tibito\n",
            "\n",
            "san\n",
            "\n",
            "\n",
            " giato ti\n",
            "\n",
            "\n",
            "\n",
            " totama\n",
            "ver le la la quan e fa per ento a ae  to \n",
            "=========================== Epoch: 740.  \t  Loss: 2.3562188148498535  \t  Time: 1.35ss\n",
            "=========================== Epoch: 741.  \t  Loss: 2.3560431003570557  \t  Time: 1.4ss\n",
            "=========================== Epoch: 742.  \t  Loss: 2.3558385372161865  \t  Time: 1.4ss\n",
            "=========================== Epoch: 743.  \t  Loss: 2.3563804626464844  \t  Time: 1.36ss\n",
            "=========================== Epoch: 744.  \t  Loss: 2.355844259262085  \t  Time: 1.36ss\n",
            "=========================== Epoch: 745.  \t  Loss: 2.3559987545013428  \t  Time: 1.36ss\n",
            "=========================== Epoch: 746.  \t  Loss: 2.3559134006500244  \t  Time: 1.4ss\n",
            "=========================== Epoch: 747.  \t  Loss: 2.35575270652771  \t  Time: 1.39ss\n",
            "=========================== Epoch: 748.  \t  Loss: 2.3557562828063965  \t  Time: 1.42ss\n",
            "=========================== Epoch: 749.  \t  Loss: 2.355929374694824  \t  Time: 1.43ss\n",
            "mi an dila qua pea e vila\n",
            "prima ali li volmale li volsi la e le e pi lo pocopopola o apa parla e tebu\n",
            "\n",
            "\n",
            " bitito\n",
            "me\n",
            "\n",
            "\n",
            "\n",
            " totita maper tala  parla quala ver pao \n",
            "=========================== Epoch: 750.  \t  Loss: 2.355884313583374  \t  Time: 1.37ss\n",
            "=========================== Epoch: 751.  \t  Loss: 2.3557779788970947  \t  Time: 1.39ss\n",
            "=========================== Epoch: 752.  \t  Loss: 2.3557803630828857  \t  Time: 1.3ss\n",
            "=========================== Epoch: 753.  \t  Loss: 2.3558757305145264  \t  Time: 1.35ss\n",
            "=========================== Epoch: 754.  \t  Loss: 2.3561038970947266  \t  Time: 1.34ss\n",
            "=========================== Epoch: 755.  \t  Loss: 2.3558363914489746  \t  Time: 1.34ss\n",
            "=========================== Epoch: 756.  \t  Loss: 2.3558146953582764  \t  Time: 1.43ss\n",
            "=========================== Epoch: 757.  \t  Loss: 2.355656147003174  \t  Time: 1.41ss\n",
            "=========================== Epoch: 758.  \t  Loss: 2.355865955352783  \t  Time: 1.42ss\n",
            "=========================== Epoch: 759.  \t  Loss: 2.3558521270751953  \t  Time: 1.48ss\n",
            "san\n",
            "\n",
            "\n",
            " toto ve col loo ape e pergio a bu\n",
            "e\n",
            "\n",
            "\n",
            "\n",
            " sebita\n",
            "\n",
            "di\n",
            "\n",
            "\n",
            " tata\n",
            "perper pate bu\n",
            "\n",
            "\n",
            " tato\n",
            "\n",
            "ver\n",
            "\n",
            " timato pin\n",
            "mama la e poe perpa obu le ceveo lao pe cori\n",
            "=========================== Epoch: 760.  \t  Loss: 2.3556835651397705  \t  Time: 1.46ss\n",
            "=========================== Epoch: 761.  \t  Loss: 2.3555195331573486  \t  Time: 1.42ss\n",
            "=========================== Epoch: 762.  \t  Loss: 2.355332374572754  \t  Time: 1.48ss\n",
            "=========================== Epoch: 763.  \t  Loss: 2.3554346561431885  \t  Time: 1.48ss\n",
            "=========================== Epoch: 764.  \t  Loss: 2.355327606201172  \t  Time: 1.54ss\n",
            "=========================== Epoch: 765.  \t  Loss: 2.355327844619751  \t  Time: 1.39ss\n",
            "=========================== Epoch: 766.  \t  Loss: 2.355372905731201  \t  Time: 1.35ss\n",
            "=========================== Epoch: 767.  \t  Loss: 2.3554208278656006  \t  Time: 1.39ss\n",
            "=========================== Epoch: 768.  \t  Loss: 2.3553848266601562  \t  Time: 1.39ss\n",
            "=========================== Epoch: 769.  \t  Loss: 2.3553757667541504  \t  Time: 1.41ss\n",
            "se mi la dion  o assopapra\n",
            "perpela o pee la po pee peri lio o a que ola peo o o sapo apo e lepoopo opa poripopocoposta apoe capola  capa pella gi poe ta\n",
            "=========================== Epoch: 770.  \t  Loss: 2.35522723197937  \t  Time: 1.4ss\n",
            "=========================== Epoch: 771.  \t  Loss: 2.3551909923553467  \t  Time: 1.42ss\n",
            "=========================== Epoch: 772.  \t  Loss: 2.355201482772827  \t  Time: 1.44ss\n",
            "=========================== Epoch: 773.  \t  Loss: 2.3549249172210693  \t  Time: 1.43ss\n",
            "=========================== Epoch: 774.  \t  Loss: 2.3553037643432617  \t  Time: 1.39ss\n",
            "=========================== Epoch: 775.  \t  Loss: 2.355350971221924  \t  Time: 1.4ss\n",
            "=========================== Epoch: 776.  \t  Loss: 2.3552706241607666  \t  Time: 1.4ss\n",
            "=========================== Epoch: 777.  \t  Loss: 2.355196475982666  \t  Time: 1.46ss\n",
            "=========================== Epoch: 778.  \t  Loss: 2.3548953533172607  \t  Time: 1.41ss\n",
            "=========================== Epoch: 779.  \t  Loss: 2.355114459991455  \t  Time: 1.41ss\n",
            "si mala e quan copoposto poe pa loo peo nopopo e o pearique\n",
            "men gili ipo popola pa opeo loo orape meo pa pea e pee rique\n",
            "\n",
            "\n",
            "\n",
            " che\n",
            "perlen pa\n",
            "pe\n",
            "\n",
            " titota\n",
            "\n",
            "\n",
            "\n",
            "=========================== Epoch: 780.  \t  Loss: 2.3550455570220947  \t  Time: 1.31ss\n",
            "=========================== Epoch: 781.  \t  Loss: 2.355313301086426  \t  Time: 1.35ss\n",
            "=========================== Epoch: 782.  \t  Loss: 2.355001449584961  \t  Time: 1.31ss\n",
            "=========================== Epoch: 783.  \t  Loss: 2.3548789024353027  \t  Time: 1.3ss\n",
            "=========================== Epoch: 784.  \t  Loss: 2.3549466133117676  \t  Time: 1.39ss\n",
            "=========================== Epoch: 785.  \t  Loss: 2.354889154434204  \t  Time: 1.39ss\n",
            "=========================== Epoch: 786.  \t  Loss: 2.3547239303588867  \t  Time: 1.41ss\n",
            "=========================== Epoch: 787.  \t  Loss: 2.354506015777588  \t  Time: 1.38ss\n",
            "=========================== Epoch: 788.  \t  Loss: 2.3547041416168213  \t  Time: 1.39ss\n",
            "=========================== Epoch: 789.  \t  Loss: 2.3549938201904297  \t  Time: 1.35ss\n",
            "me lio dibre poposeo meo o\n",
            "apolobre lo an dolo anrar a lie fila fi e da perder lipa la pequan  e paga ado dopopopocopoposto sto pi aprena menver pa lo laon  aen a \n",
            "=========================== Epoch: 790.  \t  Loss: 2.355069160461426  \t  Time: 1.36ss\n",
            "=========================== Epoch: 791.  \t  Loss: 2.3545544147491455  \t  Time: 1.36ss\n",
            "=========================== Epoch: 792.  \t  Loss: 2.3545806407928467  \t  Time: 1.32ss\n",
            "=========================== Epoch: 793.  \t  Loss: 2.354917049407959  \t  Time: 1.32ss\n",
            "=========================== Epoch: 794.  \t  Loss: 2.3547534942626953  \t  Time: 1.29ss\n",
            "=========================== Epoch: 795.  \t  Loss: 2.3547096252441406  \t  Time: 1.24ss\n",
            "=========================== Epoch: 796.  \t  Loss: 2.3547258377075195  \t  Time: 1.35ss\n",
            "=========================== Epoch: 797.  \t  Loss: 2.3544673919677734  \t  Time: 1.32ss\n",
            "=========================== Epoch: 798.  \t  Loss: 2.354501247406006  \t  Time: 1.37ss\n",
            "=========================== Epoch: 799.  \t  Loss: 2.3544206619262695  \t  Time: 1.3ss\n",
            "sa\n",
            "\n",
            "di\n",
            "\n",
            "\n",
            "\n",
            " chele gola\n",
            "quie si lae  polobu o meo aapren ninfe  apa laen ga pee e la pepee per pee fi parla quan e ae e  ché lifi pape dipe die let\n",
            "\n",
            "\n",
            " tras\n",
            "=========================== Epoch: 800.  \t  Loss: 2.3545663356781006  \t  Time: 1.28ss\n",
            "=========================== Epoch: 801.  \t  Loss: 2.3542027473449707  \t  Time: 1.32ss\n",
            "=========================== Epoch: 802.  \t  Loss: 2.3544740676879883  \t  Time: 1.3ss\n",
            "=========================== Epoch: 803.  \t  Loss: 2.3541340827941895  \t  Time: 1.37ss\n",
            "=========================== Epoch: 804.  \t  Loss: 2.353904962539673  \t  Time: 1.31ss\n",
            "=========================== Epoch: 805.  \t  Loss: 2.3540632724761963  \t  Time: 1.3ss\n",
            "=========================== Epoch: 806.  \t  Loss: 2.354130268096924  \t  Time: 1.27ss\n",
            "=========================== Epoch: 807.  \t  Loss: 2.3539860248565674  \t  Time: 1.22ss\n",
            "=========================== Epoch: 808.  \t  Loss: 2.3541338443756104  \t  Time: 1.26ss\n",
            "=========================== Epoch: 809.  \t  Loss: 2.3540635108947754  \t  Time: 1.27ss\n",
            "cose se quan e fiper aprenduro\n",
            "\n",
            " bitito\n",
            "\n",
            " tato remo\n",
            "\n",
            "\n",
            " che\n",
            "perlo prenloandentine gno lo o la\n",
            "o que\n",
            "\n",
            "\n",
            "si\n",
            "e\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " che\n",
            "\n",
            " trasseto bi le loo oro apopoco mela on e gi\n",
            "=========================== Epoch: 810.  \t  Loss: 2.354311466217041  \t  Time: 1.26ss\n",
            "=========================== Epoch: 811.  \t  Loss: 2.354050636291504  \t  Time: 1.22ss\n",
            "=========================== Epoch: 812.  \t  Loss: 2.354188919067383  \t  Time: 1.27ss\n",
            "=========================== Epoch: 813.  \t  Loss: 2.3541319370269775  \t  Time: 1.28ss\n",
            "=========================== Epoch: 814.  \t  Loss: 2.3541951179504395  \t  Time: 1.25ss\n",
            "=========================== Epoch: 815.  \t  Loss: 2.3542985916137695  \t  Time: 1.31ss\n",
            "=========================== Epoch: 816.  \t  Loss: 2.3540151119232178  \t  Time: 1.32ss\n",
            "=========================== Epoch: 817.  \t  Loss: 2.3540356159210205  \t  Time: 1.24ss\n",
            "=========================== Epoch: 818.  \t  Loss: 2.353728771209717  \t  Time: 1.24ss\n",
            "=========================== Epoch: 819.  \t  Loss: 2.353891611099243  \t  Time: 1.3ss\n",
            "si di menla o pressiso mi que\n",
            "la\n",
            "\n",
            "\n",
            " che sto ver mi mi ora baraper opa que pola elet  quan tella ae len  lio lebre li colpolaquan  sta ipo loobu\n",
            "\n",
            "\n",
            "\n",
            " tato\n",
            "\n",
            " toto\n",
            "to\n",
            "\n",
            "\n",
            "=========================== Epoch: 820.  \t  Loss: 2.353745222091675  \t  Time: 1.34ss\n",
            "=========================== Epoch: 821.  \t  Loss: 2.353844165802002  \t  Time: 1.38ss\n",
            "=========================== Epoch: 822.  \t  Loss: 2.3539156913757324  \t  Time: 1.32ss\n",
            "=========================== Epoch: 823.  \t  Loss: 2.353851556777954  \t  Time: 1.33ss\n",
            "=========================== Epoch: 824.  \t  Loss: 2.3539905548095703  \t  Time: 1.3ss\n",
            "=========================== Epoch: 825.  \t  Loss: 2.353698253631592  \t  Time: 1.31ss\n",
            "=========================== Epoch: 826.  \t  Loss: 2.353999137878418  \t  Time: 1.31ss\n",
            "=========================== Epoch: 827.  \t  Loss: 2.3540689945220947  \t  Time: 1.37ss\n",
            "=========================== Epoch: 828.  \t  Loss: 2.3537633419036865  \t  Time: 1.34ss\n",
            "=========================== Epoch: 829.  \t  Loss: 2.3533477783203125  \t  Time: 1.37ss\n",
            "se o apa cape eposto o apoe pe opo assepola sie a cer let e pin mi mi o mi dipe ae poe pa creobu\n",
            "\n",
            "\n",
            "\n",
            " tatorto\n",
            "\n",
            "\n",
            "che\n",
            "\n",
            "\n",
            " torceta po lae pa eletre\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "=========================== Epoch: 830.  \t  Loss: 2.3536529541015625  \t  Time: 1.28ss\n",
            "=========================== Epoch: 831.  \t  Loss: 2.3532261848449707  \t  Time: 1.3ss\n",
            "=========================== Epoch: 832.  \t  Loss: 2.3538172245025635  \t  Time: 1.3ss\n",
            "=========================== Epoch: 833.  \t  Loss: 2.3533854484558105  \t  Time: 1.28ss\n",
            "=========================== Epoch: 834.  \t  Loss: 2.3538076877593994  \t  Time: 1.29ss\n",
            "=========================== Epoch: 835.  \t  Loss: 2.3535077571868896  \t  Time: 1.3ss\n",
            "=========================== Epoch: 836.  \t  Loss: 2.3536410331726074  \t  Time: 1.4ss\n",
            "=========================== Epoch: 837.  \t  Loss: 2.3536458015441895  \t  Time: 1.38ss\n",
            "=========================== Epoch: 838.  \t  Loss: 2.3534061908721924  \t  Time: 1.29ss\n",
            "=========================== Epoch: 839.  \t  Loss: 2.3534905910491943  \t  Time: 1.23ss\n",
            "pa\n",
            "dipar\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " velata\n",
            "per mi leo aprenra\n",
            "\n",
            "\n",
            " tuteto va pola stape la  lae di pi\n",
            "veo\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " totata\n",
            "per poe pola sape opopola olo lo confa pa ape que\n",
            "\n",
            "e\n",
            "\n",
            "\n",
            "se\n",
            "\n",
            "=========================== Epoch: 840.  \t  Loss: 2.3535354137420654  \t  Time: 1.38ss\n",
            "=========================== Epoch: 841.  \t  Loss: 2.352985143661499  \t  Time: 1.44ss\n",
            "=========================== Epoch: 842.  \t  Loss: 2.353296995162964  \t  Time: 1.37ss\n",
            "=========================== Epoch: 843.  \t  Loss: 2.353362798690796  \t  Time: 1.37ss\n",
            "=========================== Epoch: 844.  \t  Loss: 2.3533239364624023  \t  Time: 1.35ss\n",
            "=========================== Epoch: 845.  \t  Loss: 2.3535423278808594  \t  Time: 1.33ss\n",
            "=========================== Epoch: 846.  \t  Loss: 2.353222131729126  \t  Time: 1.28ss\n",
            "=========================== Epoch: 847.  \t  Loss: 2.353131055831909  \t  Time: 1.36ss\n",
            "=========================== Epoch: 848.  \t  Loss: 2.3531103134155273  \t  Time: 1.39ss\n",
            "=========================== Epoch: 849.  \t  Loss: 2.3532352447509766  \t  Time: 1.35ss\n",
            "simi mi sa mi o lopa sta ape o peorabi pe ono nopo dive e ae peper pee per barapa la brane fi\n",
            "là la la aquan di te mensi coperpa alo o sipo ane mu\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " bi\n",
            "=========================== Epoch: 850.  \t  Loss: 2.3529295921325684  \t  Time: 1.25ss\n",
            "=========================== Epoch: 851.  \t  Loss: 2.352632999420166  \t  Time: 1.27ss\n",
            "=========================== Epoch: 852.  \t  Loss: 2.353782892227173  \t  Time: 1.33ss\n",
            "=========================== Epoch: 853.  \t  Loss: 2.3526737689971924  \t  Time: 1.31ss\n",
            "=========================== Epoch: 854.  \t  Loss: 2.3532466888427734  \t  Time: 1.33ss\n",
            "=========================== Epoch: 855.  \t  Loss: 2.3532297611236572  \t  Time: 1.28ss\n",
            "=========================== Epoch: 856.  \t  Loss: 2.353238344192505  \t  Time: 1.3ss\n",
            "=========================== Epoch: 857.  \t  Loss: 2.3530032634735107  \t  Time: 1.3ss\n",
            "=========================== Epoch: 858.  \t  Loss: 2.352947235107422  \t  Time: 1.34ss\n",
            "=========================== Epoch: 859.  \t  Loss: 2.3527915477752686  \t  Time: 1.29ss\n",
            "mi rico lape lie  ipopo ola e popopoe riperder apren nosta popo dapo oola possipa pe e peli pae la pe la qua pera obu\n",
            "\n",
            "e\n",
            "\n",
            "\n",
            "che\n",
            "\n",
            "\n",
            " to\n",
            " tota\n",
            "\n",
            "maper olo apo \n",
            "=========================== Epoch: 860.  \t  Loss: 2.3530688285827637  \t  Time: 1.31ss\n",
            "=========================== Epoch: 861.  \t  Loss: 2.3528881072998047  \t  Time: 1.35ss\n",
            "=========================== Epoch: 862.  \t  Loss: 2.353111505508423  \t  Time: 1.33ss\n",
            "=========================== Epoch: 863.  \t  Loss: 2.352924346923828  \t  Time: 1.34ss\n",
            "=========================== Epoch: 864.  \t  Loss: 2.3528411388397217  \t  Time: 1.35ss\n",
            "=========================== Epoch: 865.  \t  Loss: 2.353102207183838  \t  Time: 1.38ss\n",
            "=========================== Epoch: 866.  \t  Loss: 2.352987289428711  \t  Time: 1.39ss\n",
            "=========================== Epoch: 867.  \t  Loss: 2.352858066558838  \t  Time: 1.34ss\n",
            "=========================== Epoch: 868.  \t  Loss: 2.352612257003784  \t  Time: 1.3ss\n",
            "=========================== Epoch: 869.  \t  Loss: 2.352768659591675  \t  Time: 1.31ss\n",
            "se lape la e pola poopo oco popolobu\n",
            "\n",
            "\n",
            " timata chele oo pe dipee pra le peo ae mepolo o apa peae  o opa guao oro mi o e dipi\n",
            "lo\n",
            "\n",
            "\n",
            " che\n",
            "\n",
            "\n",
            " trasnelse\n",
            "=========================== Epoch: 870.  \t  Loss: 2.352914571762085  \t  Time: 1.33ss\n",
            "=========================== Epoch: 871.  \t  Loss: 2.3528237342834473  \t  Time: 1.42ss\n",
            "=========================== Epoch: 872.  \t  Loss: 2.3527777194976807  \t  Time: 1.36ss\n",
            "=========================== Epoch: 873.  \t  Loss: 2.352923631668091  \t  Time: 1.34ss\n",
            "=========================== Epoch: 874.  \t  Loss: 2.3529281616210938  \t  Time: 1.32ss\n",
            "=========================== Epoch: 875.  \t  Loss: 2.3528828620910645  \t  Time: 1.33ss\n",
            "=========================== Epoch: 876.  \t  Loss: 2.353243589401245  \t  Time: 1.27ss\n",
            "=========================== Epoch: 877.  \t  Loss: 2.3529982566833496  \t  Time: 1.31ss\n",
            "=========================== Epoch: 878.  \t  Loss: 2.352717399597168  \t  Time: 1.29ss\n",
            "=========================== Epoch: 879.  \t  Loss: 2.3528645038604736  \t  Time: 1.34ss\n",
            "ma moe mi polo ae barapa e fi pate e gio la aquan a e  laen pa usa ora per pepe eper prela pe veo o o opa lupopa que pola colla ope quan e a fie pa e \n",
            "=========================== Epoch: 880.  \t  Loss: 2.3526830673217773  \t  Time: 1.38ss\n",
            "=========================== Epoch: 881.  \t  Loss: 2.352548360824585  \t  Time: 1.39ss\n",
            "=========================== Epoch: 882.  \t  Loss: 2.352311134338379  \t  Time: 1.39ss\n",
            "=========================== Epoch: 883.  \t  Loss: 2.3526690006256104  \t  Time: 1.42ss\n",
            "=========================== Epoch: 884.  \t  Loss: 2.3524746894836426  \t  Time: 1.36ss\n",
            "=========================== Epoch: 885.  \t  Loss: 2.3527770042419434  \t  Time: 1.37ss\n",
            "=========================== Epoch: 886.  \t  Loss: 2.352485179901123  \t  Time: 1.37ss\n",
            "=========================== Epoch: 887.  \t  Loss: 2.3527729511260986  \t  Time: 1.38ss\n",
            "=========================== Epoch: 888.  \t  Loss: 2.3524038791656494  \t  Time: 1.34ss\n",
            "=========================== Epoch: 889.  \t  Loss: 2.3524346351623535  \t  Time: 1.37ss\n",
            "si mi ora mi rique\n",
            "sta\n",
            "\n",
            "\n",
            "\n",
            " trassetato  e pola e bu\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " di\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " bitota\n",
            "per poe loopra  poo bu\n",
            "\n",
            "\n",
            "\n",
            " vetortate re opoo veo lape o assisoper pe pae\n",
            "pe\n",
            "men\n",
            "=========================== Epoch: 890.  \t  Loss: 2.352574110031128  \t  Time: 1.35ss\n",
            "=========================== Epoch: 891.  \t  Loss: 2.352583885192871  \t  Time: 1.39ss\n",
            "=========================== Epoch: 892.  \t  Loss: 2.3526175022125244  \t  Time: 1.38ss\n",
            "=========================== Epoch: 893.  \t  Loss: 2.3525338172912598  \t  Time: 1.39ss\n",
            "=========================== Epoch: 894.  \t  Loss: 2.352452039718628  \t  Time: 1.4ss\n",
            "=========================== Epoch: 895.  \t  Loss: 2.3523592948913574  \t  Time: 1.39ss\n",
            "=========================== Epoch: 896.  \t  Loss: 2.3526322841644287  \t  Time: 1.39ss\n",
            "=========================== Epoch: 897.  \t  Loss: 2.3524105548858643  \t  Time: 1.38ss\n",
            "=========================== Epoch: 898.  \t  Loss: 2.3523733615875244  \t  Time: 1.4ss\n",
            "=========================== Epoch: 899.  \t  Loss: 2.352571725845337  \t  Time: 1.4ss\n",
            "se o mi cono pape a ipocapodee di pa la peo ogni la quan nar o meo o ape di\n",
            "e presmi le ora poe rela e popo ae ven laen  meoo bu\n",
            "\n",
            "\n",
            " bito\n",
            "\n",
            "\n",
            "\n",
            " tota\n",
            "\n",
            "=========================== Epoch: 900.  \t  Loss: 2.352327585220337  \t  Time: 1.41ss\n",
            "=========================== Epoch: 901.  \t  Loss: 2.3519186973571777  \t  Time: 1.41ss\n",
            "=========================== Epoch: 902.  \t  Loss: 2.3519608974456787  \t  Time: 1.38ss\n",
            "=========================== Epoch: 903.  \t  Loss: 2.3521323204040527  \t  Time: 1.47ss\n",
            "=========================== Epoch: 904.  \t  Loss: 2.352426052093506  \t  Time: 1.45ss\n",
            "=========================== Epoch: 905.  \t  Loss: 2.3522098064422607  \t  Time: 1.44ss\n",
            "=========================== Epoch: 906.  \t  Loss: 2.3524765968322754  \t  Time: 1.4ss\n",
            "=========================== Epoch: 907.  \t  Loss: 2.3522493839263916  \t  Time: 1.42ss\n",
            "=========================== Epoch: 908.  \t  Loss: 2.352323055267334  \t  Time: 1.41ss\n",
            "=========================== Epoch: 909.  \t  Loss: 2.3523762226104736  \t  Time: 1.38ss\n",
            "mi ala e fispet  mi que\n",
            "\n",
            "\n",
            "\n",
            " che\n",
            "la\n",
            "\n",
            "\n",
            " totata\n",
            "\n",
            "\n",
            " tota\n",
            "\n",
            "\n",
            " tota\n",
            "\n",
            "\n",
            " trasseto\n",
            "\n",
            "perglio\n",
            "\n",
            "\n",
            "che\n",
            " tima\n",
            "cose\n",
            "perpolo lobu\n",
            "\n",
            "\n",
            "\n",
            " che\n",
            "perlobre pa veo oan pra o\n",
            "pe\n",
            "lo\n",
            "\n",
            "sío\n",
            "=========================== Epoch: 910.  \t  Loss: 2.3519692420959473  \t  Time: 1.41ss\n",
            "=========================== Epoch: 911.  \t  Loss: 2.352015733718872  \t  Time: 1.38ss\n",
            "=========================== Epoch: 912.  \t  Loss: 2.352351665496826  \t  Time: 1.35ss\n",
            "=========================== Epoch: 913.  \t  Loss: 2.3519439697265625  \t  Time: 1.37ss\n",
            "=========================== Epoch: 914.  \t  Loss: 2.3521907329559326  \t  Time: 1.39ss\n",
            "=========================== Epoch: 915.  \t  Loss: 2.3522539138793945  \t  Time: 1.4ss\n",
            "=========================== Epoch: 916.  \t  Loss: 2.3519773483276367  \t  Time: 1.34ss\n",
            "=========================== Epoch: 917.  \t  Loss: 2.3519561290740967  \t  Time: 1.37ss\n",
            "=========================== Epoch: 918.  \t  Loss: 2.3521575927734375  \t  Time: 1.34ss\n",
            "=========================== Epoch: 919.  \t  Loss: 2.3520572185516357  \t  Time: 1.43ss\n",
            "mi mi mi ala la ae e me rimino orpa quan cuba to de aquan mo di messe di essisa perpa peo o dipee popo opape lie tapola pe eper pel olepra pe la pe pee fira e\n",
            "=========================== Epoch: 920.  \t  Loss: 2.3523268699645996  \t  Time: 1.38ss\n",
            "=========================== Epoch: 921.  \t  Loss: 2.3521761894226074  \t  Time: 1.38ss\n",
            "=========================== Epoch: 922.  \t  Loss: 2.3518238067626953  \t  Time: 1.39ss\n",
            "=========================== Epoch: 923.  \t  Loss: 2.3516435623168945  \t  Time: 1.33ss\n",
            "=========================== Epoch: 924.  \t  Loss: 2.3522098064422607  \t  Time: 1.38ss\n",
            "=========================== Epoch: 925.  \t  Loss: 2.3520195484161377  \t  Time: 1.35ss\n",
            "=========================== Epoch: 926.  \t  Loss: 2.351836681365967  \t  Time: 1.39ss\n",
            "=========================== Epoch: 927.  \t  Loss: 2.352104425430298  \t  Time: 1.39ss\n",
            "=========================== Epoch: 928.  \t  Loss: 2.3519880771636963  \t  Time: 1.35ss\n",
            "=========================== Epoch: 929.  \t  Loss: 2.3520543575286865  \t  Time: 1.41ss\n",
            "mi a e cia\n",
            "la\n",
            "\n",
            "síla e la pae te pa an e caper pa le meopra  e peli e pefiri e orpea e mi chesto   quela perpe ae  e mi lamo capoe len la teo ripoo la\n",
            "=========================== Epoch: 930.  \t  Loss: 2.351836919784546  \t  Time: 1.41ss\n",
            "=========================== Epoch: 931.  \t  Loss: 2.3519575595855713  \t  Time: 1.39ss\n",
            "=========================== Epoch: 932.  \t  Loss: 2.352064847946167  \t  Time: 1.41ss\n",
            "=========================== Epoch: 933.  \t  Loss: 2.3519556522369385  \t  Time: 1.41ss\n",
            "=========================== Epoch: 934.  \t  Loss: 2.3517794609069824  \t  Time: 1.39ss\n",
            "=========================== Epoch: 935.  \t  Loss: 2.352126359939575  \t  Time: 1.38ss\n",
            "=========================== Epoch: 936.  \t  Loss: 2.3518378734588623  \t  Time: 1.43ss\n",
            "=========================== Epoch: 937.  \t  Loss: 2.3518800735473633  \t  Time: 1.38ss\n",
            "=========================== Epoch: 938.  \t  Loss: 2.351919412612915  \t  Time: 1.38ss\n",
            "=========================== Epoch: 939.  \t  Loss: 2.3516879081726074  \t  Time: 1.36ss\n",
            "si somi lo e diparspet \n",
            "per pe loa e sta pe ae e no dami pa a vala opra lo rio pa quan\n",
            "lume\n",
            "o\n",
            "\n",
            "\n",
            "\n",
            " tama\n",
            "\n",
            "\n",
            " toto\n",
            "\n",
            " bitoto\n",
            "\n",
            "\n",
            " toto\n",
            "\n",
            "\n",
            " tota  somi la\n",
            "\n",
            "\n",
            "\n",
            "=========================== Epoch: 940.  \t  Loss: 2.35189151763916  \t  Time: 1.38ss\n",
            "=========================== Epoch: 941.  \t  Loss: 2.3514530658721924  \t  Time: 1.37ss\n",
            "=========================== Epoch: 942.  \t  Loss: 2.3514578342437744  \t  Time: 1.36ss\n",
            "=========================== Epoch: 943.  \t  Loss: 2.351468563079834  \t  Time: 1.39ss\n",
            "=========================== Epoch: 944.  \t  Loss: 2.351806879043579  \t  Time: 1.37ss\n",
            "=========================== Epoch: 945.  \t  Loss: 2.351674795150757  \t  Time: 1.39ss\n",
            "=========================== Epoch: 946.  \t  Loss: 2.3520138263702393  \t  Time: 1.38ss\n",
            "=========================== Epoch: 947.  \t  Loss: 2.351783514022827  \t  Time: 1.38ss\n",
            "=========================== Epoch: 948.  \t  Loss: 2.3518741130828857  \t  Time: 1.35ss\n",
            "=========================== Epoch: 949.  \t  Loss: 2.351679801940918  \t  Time: 1.4ss\n",
            "se cipopocosto cer parla passerba sifino\n",
            "\n",
            "non e pola opra\n",
            "perpopoco dopopopoteco anme apoe gi vila vapoe  copola loo a prebarbasi rifique\n",
            "\n",
            "che\n",
            "\n",
            "chele la o pee ala e pola\n",
            "=========================== Epoch: 950.  \t  Loss: 2.3512325286865234  \t  Time: 1.4ss\n",
            "=========================== Epoch: 951.  \t  Loss: 2.3516077995300293  \t  Time: 1.36ss\n",
            "=========================== Epoch: 952.  \t  Loss: 2.351696491241455  \t  Time: 1.31ss\n",
            "=========================== Epoch: 953.  \t  Loss: 2.3519580364227295  \t  Time: 1.38ss\n",
            "=========================== Epoch: 954.  \t  Loss: 2.3515896797180176  \t  Time: 1.37ss\n",
            "=========================== Epoch: 955.  \t  Loss: 2.351592779159546  \t  Time: 1.36ss\n",
            "=========================== Epoch: 956.  \t  Loss: 2.3516690731048584  \t  Time: 1.39ss\n",
            "=========================== Epoch: 957.  \t  Loss: 2.351463556289673  \t  Time: 1.37ss\n",
            "=========================== Epoch: 958.  \t  Loss: 2.351614236831665  \t  Time: 1.38ss\n",
            "=========================== Epoch: 959.  \t  Loss: 2.351487636566162  \t  Time: 1.33ss\n",
            "se\n",
            "perpo lae pe per aope la men ala  teo rea cola pe lie pa pee que sto a pee a poe e tepo la e meopopo apoe ate apren quando veo veo  dipeo apee gi\n",
            "=========================== Epoch: 960.  \t  Loss: 2.3515658378601074  \t  Time: 1.38ss\n",
            "=========================== Epoch: 961.  \t  Loss: 2.351536750793457  \t  Time: 1.36ss\n",
            "=========================== Epoch: 962.  \t  Loss: 2.3514411449432373  \t  Time: 1.35ss\n",
            "=========================== Epoch: 963.  \t  Loss: 2.3516714572906494  \t  Time: 1.38ss\n",
            "=========================== Epoch: 964.  \t  Loss: 2.3514578342437744  \t  Time: 1.31ss\n",
            "=========================== Epoch: 965.  \t  Loss: 2.3513927459716797  \t  Time: 1.34ss\n",
            "=========================== Epoch: 966.  \t  Loss: 2.3513991832733154  \t  Time: 1.37ss\n",
            "=========================== Epoch: 967.  \t  Loss: 2.3513965606689453  \t  Time: 1.39ss\n",
            "=========================== Epoch: 968.  \t  Loss: 2.3512587547302246  \t  Time: 1.38ss\n",
            "=========================== Epoch: 969.  \t  Loss: 2.3513360023498535  \t  Time: 1.37ss\n",
            "la o melo apoo cipocote\n",
            "sta\n",
            "\n",
            "\n",
            "\n",
            " bito\n",
            "\n",
            "\n",
            " to\n",
            "nelle efi per per deo ormeo apoe len  meo le deo poo o assisa que\n",
            "e\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " che\n",
            "\n",
            " tate\n",
            "\n",
            "\n",
            " tota\n",
            "man\n",
            "lo\n",
            "\n",
            "\n",
            "\n",
            "=========================== Epoch: 970.  \t  Loss: 2.3514556884765625  \t  Time: 1.36ss\n",
            "=========================== Epoch: 971.  \t  Loss: 2.351322889328003  \t  Time: 1.34ss\n",
            "=========================== Epoch: 972.  \t  Loss: 2.3514153957366943  \t  Time: 1.39ss\n",
            "=========================== Epoch: 973.  \t  Loss: 2.351120948791504  \t  Time: 1.38ss\n",
            "=========================== Epoch: 974.  \t  Loss: 2.3510379791259766  \t  Time: 1.44ss\n",
            "=========================== Epoch: 975.  \t  Loss: 2.351135492324829  \t  Time: 1.45ss\n",
            "=========================== Epoch: 976.  \t  Loss: 2.3512396812438965  \t  Time: 1.46ss\n",
            "=========================== Epoch: 977.  \t  Loss: 2.3511931896209717  \t  Time: 1.43ss\n",
            "=========================== Epoch: 978.  \t  Loss: 2.3514161109924316  \t  Time: 1.44ss\n",
            "=========================== Epoch: 979.  \t  Loss: 2.3513169288635254  \t  Time: 1.47ss\n",
            "si quere ola lo duco\n",
            "\n",
            "perpopopocopomelo oo o branla que\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " totata\n",
            "tal pola pee  e prelet civelen  ate la e te copopola e popo dipe\n",
            "là opa pe e per prebarliba si\n",
            "=========================== Epoch: 980.  \t  Loss: 2.3511643409729004  \t  Time: 1.39ss\n",
            "=========================== Epoch: 981.  \t  Loss: 2.3512356281280518  \t  Time: 1.37ss\n",
            "=========================== Epoch: 982.  \t  Loss: 2.351221799850464  \t  Time: 1.37ss\n",
            "=========================== Epoch: 983.  \t  Loss: 2.3512911796569824  \t  Time: 1.37ss\n",
            "=========================== Epoch: 984.  \t  Loss: 2.3512418270111084  \t  Time: 1.39ss\n",
            "=========================== Epoch: 985.  \t  Loss: 2.3513832092285156  \t  Time: 1.37ss\n",
            "=========================== Epoch: 986.  \t  Loss: 2.350872039794922  \t  Time: 1.41ss\n",
            "=========================== Epoch: 987.  \t  Loss: 2.3512814044952393  \t  Time: 1.37ss\n",
            "=========================== Epoch: 988.  \t  Loss: 2.3508613109588623  \t  Time: 1.39ss\n",
            "=========================== Epoch: 989.  \t  Loss: 2.350919485092163  \t  Time: 1.4ss\n",
            "man copopolo o lapa dipe ore la deope orape e pee per quan ver o afino pa poera pee pola e la scor cala e poe posto a fie mi no inte viste  opoan ae ci mos\n",
            "=========================== Epoch: 990.  \t  Loss: 2.351097583770752  \t  Time: 1.37ss\n",
            "=========================== Epoch: 991.  \t  Loss: 2.3507790565490723  \t  Time: 1.37ss\n",
            "=========================== Epoch: 992.  \t  Loss: 2.3510475158691406  \t  Time: 1.36ss\n",
            "=========================== Epoch: 993.  \t  Loss: 2.351107597351074  \t  Time: 1.38ss\n",
            "=========================== Epoch: 994.  \t  Loss: 2.3512039184570312  \t  Time: 1.4ss\n",
            "=========================== Epoch: 995.  \t  Loss: 2.3509910106658936  \t  Time: 1.38ss\n",
            "=========================== Epoch: 996.  \t  Loss: 2.3510682582855225  \t  Time: 1.33ss\n",
            "=========================== Epoch: 997.  \t  Loss: 2.3510851860046387  \t  Time: 1.32ss\n",
            "=========================== Epoch: 998.  \t  Loss: 2.350985288619995  \t  Time: 1.38ss\n",
            "=========================== Epoch: 999.  \t  Loss: 2.3509714603424072  \t  Time: 1.38ss\n",
            "se o e lepo opra copopolo obu\n",
            "\n",
            "\n",
            "\n",
            " che\n",
            "\n",
            " totate\n",
            "\n",
            "\n",
            " toto\n",
            "\n",
            "\n",
            " tata\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " che\n",
            "perlen \n",
            "veole opra na loo o piansi ra go dipisi\n",
            "\n",
            "\n",
            "\n",
            " che\n",
            "\n",
            " titota  sali pero pe\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAEICAYAAABGaK+TAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAY5klEQVR4nO3dfZRcdZ3n8fenqrrz/ARpICSBgOEgiBJig7K4iDAoEIQzzpwzuI4DzrrsOo7jzLjH5eHoOO7DgLqzMiMqGRV1REEZYF1mREAUdXTBDqBAEiBAskmEpHkISQhJP9R3/6ibWIndubebvl2/6nxe59TJrV/dW/d769f9ye1f3QdFBGZm1n4qrS7AzMxGxwFuZtamHOBmZm3KAW5m1qYc4GZmbcoBbmbWphzgljRJ35N08VjPazYRyMeB21iTtL3p6VRgFzCYPf+PEXHD+Fc1epLOAL4REQtaXYtZs1qrC7CJJyKm756WtBZ4f0Tcve98kmoRMTCetZlNJB5CsXEj6QxJGyT9F0nPAtdLmiPpdkm9kl7Mphc0LfMjSe/Ppi+R9FNJn8nmfVrSuaOc9yhJP5a0TdLdkq6V9I1RbNNx2Xq3SHpU0gVNr50naWW2jo2S/nPWPjfbzi2SXpD0E0n+XbQR8w+NjbfDgIOAI4FLafwMXp89PwJ4BfjcfpZ/E/AYMBf4FPBlSRrFvN8E7gcOBj4BvHekGyKpA/g/wJ3AIcCHgBskHZvN8mUaQ0YzgBOAe7L2jwAbgC7gUOAKwGOZNmIOcBtvdeCvImJXRLwSEc9HxD9FxI6I2Ab8d+Ct+1l+XUT8Q0QMAl8D5tEIwcLzSjoCOBn4eET0RcRPge+OYlveDEwHrsre5x7gduDd2ev9wPGSZkbEixHxQFP7PODIiOiPiJ+Ev4yyUXCA23jrjYidu59ImirpOknrJG0FfgzMllQdZvlnd09ExI5scvoI5z0ceKGpDWD9CLeD7H3WR0S9qW0dMD+b/j3gPGCdpHslnZq1fxpYA9wp6SlJl41i3WYOcBt3++5pfgQ4FnhTRMwETs/ahxsWGQvPAAdJmtrUtnAU7/NrYOE+49dHABsBIuIXEXEhjeGV24BvZ+3bIuIjEXE0cAHwl5LOGsX67QDnALdWm0Fj3HuLpIOAvyp7hRGxDugBPiGpM9szfmfecpImNz9ojKHvAD4qqSM73PCdwI3Z+75H0qyI6Ae20hg+QtL5khZn4/Ev0TjEsj7kSs32wwFurfZZYArwHPB/gTvGab3vAU4Fngf+G3ATjePVhzOfxn80zY+FNAL7XBr1fx74o4hYnS3zXmBtNjT0n7J1AhwD3A1sB34OfD4ifjhmW2YHDJ/IYwZIuglYHRGl/wVgNla8B24HJEknS3qNpIqkc4ALaYxTm7UNn4lpB6rDgFtoHAe+AfhARDzY2pLMRsZDKGZmbcpDKGZmbaqUIZS5c+fGokWLynhrM7MJacWKFc9FRNdIliklwBctWkRPT08Zb21mNiFJWjfSZTyEYmbWphzgZmZtygFuZtamHOBmZm3KAW5m1qYc4GZmbcoBbmbWppIJ8Ijg73/wBPc+3tvqUszM2kIyAS6J5T9+ih+u3tzqUszM2kIyAQ6wbdcAX/3Z2laXYWbWFpIKcDMzKy6p64EvWTibGZOTKsnMLFlJ7YGrzPuQm5lNMEkFeEXC95cwMysmqQAXUHeCm5kVklaAC++Bm5kVlFiAi8AJbmZWRFoBDtSd32ZmhaQV4ALvgJuZFZNUgFc8hGJmVlhSAS55CMXMrKi0AhwRPgzFzKyQtAJcHgI3MysqN8AlHSvpoabHVkl/XkYx8pmYZmaF5V45KiIeA5YASKoCG4FbyyhGjfWV8dZmZhPOSIdQzgKejIh1ZRTjIRQzs+JGGuAXAd8a6gVJl0rqkdTT2zu626L5YlZmZsUVDnBJncAFwHeGej0ilkdEd0R0d3V1jaoYX8zKzKy4keyBnws8EBGbyirGF7MyMytuJAH+boYZPhkrjYtZmZlZEYUCXNI04GzgljKL8VEoZmbFFboBZUS8DBxcci0eQjEzG4G0zsTEF7MyMysqqQCvVLwHbmZWVFIBLuTDCM3MCkoqwPGZmGZmhSUV4BWfS29mVlhSAe4zMc3MiksrwL0DbmZWWFIB7otZmZkVl1SAewjFzKy4pAIcn4lpZlZYUgEu1OoSzMzaRlIBXpEvZmVmVlRSAS5B3fltZlZIWgHui1mZmRWWVID7YlZmZsUlFeAgD6GYmRWUVIBL4HMxzcyKSSrAKz4O3MyssKQC3NcDNzMrLq0A98WszMwKSyvA8RCKmVlRaQW45DMxzcwKSizAvQduZlZUWgGOPAZuZlZQUgHui1mZmRWXVID7YlZmZsUlFuC+mJWZWVGJBbi/xDQzK6pQgEuaLelmSaslrZJ0ahnFCN/U2MysqFrB+a4B7oiI35fUCUwto5jGmZhOcDOzInIDXNIs4HTgEoCI6AP6yijGZ2KamRVXZAjlKKAXuF7Sg5K+JGnavjNJulRSj6Se3t7e0RUjHwduZlZUkQCvAUuBL0TEScDLwGX7zhQRyyOiOyK6u7q6RlVM4zBCR7iZWRFFAnwDsCEi7sue30wj0Mech1DMzIrLDfCIeBZYL+nYrOksYGUZxWzdObB7nWW8vZnZhFL0KJQPATdkR6A8BbyvjGK++rO1ALzcN8j0SUVLMzM7MBVKyYh4COguuZY9dvQNOMDNzHIkdSbmbtXG3Y3NzGw/kgxwOcDNzHIlGeD+EtPMLF+SAe5LypqZ5UsywH09FDOzfGkGuPPbzCyXA9zMrE0lGeC+HoqZWb4kA9zxbWaWL8kAr/swFDOzXEkGuJmZ5UsywD0GbmaWL8kAd36bmeVLM8BbXYCZWRtIMsA9hGJmli/JAHd+m5nlSzTAneBmZnnSDPBWF2Bm1gaSDHCPgZuZ5UsywJ3fZmb5HOBmZm0qyQD3EIqZWb4kA9zMzPIlGeDeAzczy5dkgDu/zczyJRng3gM3M8uXZIA7vs3M8qUZ4E5wM7NctSIzSVoLbAMGgYGI6C6zKF8LxcwsX6EAz7wtIp4rrZImjm8zs3xJDqH4psZmZvmKBngAd0paIenSoWaQdKmkHkk9vb29r6oox7eZWb6iAf6WiFgKnAt8UNLp+84QEcsjojsiuru6ul5VUT6M0MwsX6EAj4iN2b+bgVuBU8osyrvgZmb5cgNc0jRJM3ZPA28HHimzKA+Bm5nlK3IUyqHArZJ2z//NiLijzKLCu+BmZrlyAzwingJOHIdamtY5nmszM2tPSR5G6Pw2M8uXZID7KBQzs3xJBrh3wc3M8iUZ4N4DNzPLl2SAO7/NzPIlGeDeAzczy5dogLe6AjOz9CUZ4P4W08wsX5IB7hEUM7N8SQa4h1DMzPIlGeC+FoqZWb4kA9x74GZm+ZIMcN/U2MwsX5IBbmZm+ZIMcJ/IY2aWL8kAd36bmeVLMsD7BuqtLsHMLHlJBvgdjz7b6hLMzJKXZIAPDHoMxcwsT5oBXvcQiplZniQDfNBn8piZ5UoywPs9hGJmlivJAPcQiplZvjQD3HvgZma50gxwj4GbmeVKMsBfd/jMVpdgZpa8pAL8sJmTAZhcq7a4EjOz9CUV4NWKAA+hmJkVUTjAJVUlPSjp9rKK2R3ggz4Kxcws10j2wD8MrCqrEIANL+4AfC0UM7MiCgW4pAXAMuBLZRaze+RkZ7/3wM3M8hTdA/8s8FFg2GSVdKmkHkk9vb29Y1KcmZkNLzfAJZ0PbI6IFfubLyKWR0R3RHR3dXWNWYFmZja0InvgpwEXSFoL3AicKekbpVZlZma5cgM8Ii6PiAURsQi4CLgnIv6w9MrMzGy/kjoO3MzMihtRgEfEjyLi/LKKWbJwdllvbWY24SS1B/7Epm2tLsHMrG0kFeB/+OYjW12CmVnbSCrAp0+qtboEM7O2kVSAV7JroZiZWb6kArzaFOA7+gZaWImZWfqSCvDm/e/tOx3gZmb7k1aAewTFzKywpAL8bcce0uoSzMzaRlIBPrXpKJRVz/qYcDOz/UkqwJtHUN53/f0tq8PMrB0kFeCVpkFw3xbTzGz/EgvwVldgZtY+kgrwaT4T08ysMAe4mVmbSirAzcysOAe4mVmbcoCbmbWppAP8unufbHUJZmbJSjrA/+Z7q1tdgplZspIOcDMzG17yAf7iy32tLsHMLEnJB/hJ//WuVpdgZpak5APczMyGllyAD3U9lJd29I9/IWZmiUswwH87wU/85J0tqMTMLG3JBfhRc6cN2b5i3QvjXImZWdqSC/Arlx03ZPvvfeHn7OwfHOdqzMzSlRvgkiZLul/SLyU9Kumvyyzo9GO6hn3ttR+7g/7BepmrNzNrG0X2wHcBZ0bEicAS4BxJby6toJy7Ohxz5ff45fotZa3ezKxt5F6AOyIC2J497cgeLb3h2YXX/isA71o6n7951+uZVKu2shwzs5YodAcFSVVgBbAYuDYi7iu1qoJueWAjtzywEYArznst73/L0bl78GZmE0WhLzEjYjAilgALgFMknbDvPJIuldQjqae3t3es68z1P/5lNUdf8S8suuyf+dQdqz1WbmYTnhojJCNYQPo4sCMiPjPcPN3d3dHT0zPqohZd9s+jXnYo3/3T03jDgtlj+p5mZmNJ0oqI6B7JMrlDKJK6gP6I2CJpCnA2cPUoayzk8nNfO6aXkr3gc/+6Z3rxIdP5ysUnc8TBU8fs/c3MWiF3D1zSG4CvAVUaQy7fjohP7m+ZV7sHDmO/Fz6cxYdM58sXd3PkwUOfQGRmNh5K2QOPiF8BJ426qsSt2bydt376R3ueHz9vJsv/6I0smOM9dDNLW6GjUA4kK5/Zyluu/uGe5+9aOp9PXngC0yf5ozKztIz4S8wixmII5ddbXuHfXHXPGFU0dv6geyFXLDuOWVM6Wl2KmU0gpQyhtMrhs6e0uoQh3dSznpt61u/Vds1FS1j2+nnUqsldWsbMJrBkA7ydfPjGh/jwjQ/t1bb4kOl86MzFnH38oUzt9MdsZmMv2SEUgIc3vMQ7P/fTMagoPScvmsMFS+bzjuMPpWvGJDTEddDN7MAxoYZQAF6/YFarSyjNL9a+yC/WvsjHbntk2HlmTK7xuyfN550nHs6ShbPp8BCNmTVJeg8cYEffAMd//Ptj8l4T1e8cdyhve20X/3ZxFwvmTPH1YMza0ITbAweY2lnj2n+3lA9+84FWl5Ksu1dt4u5Vm0a0zCEzJnHcvJkcc8h0ju6azqK5U1l08DQOnt7pqzuatYnkAxxg2Rvmcc0PpvP4pu35M1shm7ftYvO2Xu59fPwuPDZzco2DpnUyY3IHs6d2MGNyjamdNQ6e1snMKR3MnFxj+uQaUzqqdFQrzJrSwZTOKtMn1ZhUq9JRFdMm1eisVvxXhhltEuAAd/7FW8ft9Horx9adA2zdOdDqMmwUJJjWWaNvoE5HVUzprFKRCKBWEZ21CgLqAVM6qtSqoiIxqVYhgGr2H25VYlJHhWr2pX2lIiKCzlqFgcEgYM8yndUK9QiqFSFERWSH6gb9g0FHtUJntVFDRWJyR5VaRQzUgyCIgI6sjlqlQketsc6BwcawcQTUqqJaEVWJWlVEQD1iz7Z1VkWlImpZ/VJj3r7BOlU1lh2M2LP8m446mMNmTR63fmmbAAdYe9Uyh7hZC0TA9l2N/3z7BuHlPt+fdjhrr1o2butqu8Ma1l61jDOOHf6+mWZmB4q2C3CAr77vFO6/4qxWl2Fm1lJtGeAAh8yczNqrlrHyk+9g3jiOOZmZpaKtxsCHMrWzxs8vb+yNP9m7nbP+570trsjMbHy0fYA3e03X9D1fINy1chP/4etjczKRmVmKJlSANzv7+ENZe9UyIoIH12/hXZ//WatLMjMbUxM2wHeTxNIj5ux1aM/GLa/wsdse4Z7Vm1tYmZnZqzPhA3wo82dP4SuXnLzn+WA9+E7Pei675eEWVmVmNjIHZIDvq1oRF51yBBedcgQAEcFdKzdx6T+uaHFlZmbDc4APQRJvf91hew277Ogb4Es/eZq/vevxFlZmZvYbDvCCpnbW+LOzjuHPzjpmT1u9Hjy0YQvL732KOx59toXVmdmByAH+KlQqjS9Iv/jeNw75+jMvvcJtD/6a6378JFt29I9zdWY20TnASzRv1hQ+cMZr+MAZrxny9Z39g6xY9yI3r9jArQ9uHOfqzKzdOcBbaHJHldMWz+W0xXP5X3+wZNj5IoJNW3dx39PP87M1z/PdX/6aV/p9NTizA50DvA1I4rBZk7lwyXwuXDKfq3//DYWW275rgLXPvcyqZ7by+KZtPLzxJVY9s42XXvFwjtlE4ACfwKZPqnHC/FmcMH/sbg69a2CQ57b38dy2XTy7dSfPvrST3m27eGFHH89v38Xz2/vYvG0X23b286LH/c1K5QC3EZlUqzJ/9hTmz57Cia0uZoxFBPWAvoE6fQN1Bup1dg7UGRis0z8YDNaDgXpjun+wzsBgsKNvgAjYNVBnMIK+gTr1eqO9fzDor9f3tA1G404yA4NB3+AgA9l7Dkbj7jF9A3W27xpgsB5IjeedtQp9A3VeeLmPAAYGG+8XgIAd/YPZnWkqvPRKPzv7B5Ea5zbs7K+3+BM98Pzp2xaP6/pyA1zSQuDrwKFAAMsj4pqyCzMbb43bZcGUzipTOn1jZ0tfkT3wAeAjEfGApBnACkl3RcTKkmszM7P9yL2hQ0Q8ExEPZNPbgFXA/LILMzOz/RvRHXkkLQJOAu4b4rVLJfVI6unt7R2b6szMbFiFA1zSdOCfgD+PiK37vh4RyyOiOyK6u7p802Ezs7IVCnBJHTTC+4aIuKXckszMrIjcAJck4MvAqoj42/JLMjOzIorsgZ8GvBc4U9JD2eO8kusyM7McuYcRRsRPaZwzYGZmCVFEjP2bSr3AulEuPhd4bgzLaSVvS5q8LemaSNsz0m05MiJGdARIKQH+akjqiYjuVtcxFrwtafK2pGsibc94bMuIjgM3M7N0OMDNzNpUigG+vNUFjCFvS5q8LemaSNtT+rYkNwZuZmbFpLgHbmZmBTjAzczaVDIBLukcSY9JWiPpslbXs5ukhZJ+KGmlpEclfThrP0jSXZKeyP6dk7VL0t9l2/ErSUub3uvibP4nJF3c1P5GSQ9ny/xddvmCMrepKulBSbdnz4+SdF+2/pskdWbtk7Lna7LXFzW9x+VZ+2OS3tHUPm79KGm2pJslrZa0StKp7dovkv4i+/l6RNK3JE1up36R9BVJmyU90tRWel8Mt44StuXT2c/ZryTdKml202sj+sxH06/DioiWP4Aq8CRwNNAJ/BI4vtV1ZbXNA5Zm0zOAx4HjgU8Bl2XtlwFXZ9PnAd+jcfbqm4H7svaDgKeyf+dk03Oy1+7P5lW27Lklb9NfAt8Ebs+efxu4KJv+IvCBbPpPgC9m0xcBN2XTx2d9NAk4Kuu76nj3I/A14P3ZdCcwux37hcb19Z8GpjT1xyXt1C/A6cBS4JGmttL7Yrh1lLAtbwdq2fTVTdsy4s98pP2631rL+uUa4Qd2KvD9pueXA5e3uq5hav3fwNnAY8C8rG0e8Fg2fR3w7qb5H8tefzdwXVP7dVnbPGB1U/te85VQ/wLgB8CZwO3ZL8RzTT+ce/oC+D5wajZdy+bTvv2ze77x7EdgFo3Q0z7tbdcvNAJ8PY3gqmX98o526xdgEXuHXul9Mdw6xnpb9nntd2lcmfW3Psu8z3w0v2/7qzOVIZTdP8C7bSDBu/5o7xtaHBoRz2QvPUvjnqEw/Lbsr33DEO1l+SzwUWD3HW8PBrZExMAQ699Tc/b6S9n8I93GMhwF9ALXqzEc9CVJ02jDfomIjcBngP8HPEPjc15Be/ZLs/Hoi+HWUaY/pvFXAIx8W0bz+zasVAI8edrPDS2i8V9m8sdjSjof2BwRK1pdyxio0fgz9wsRcRLwMo0/ofdoo36ZA1xI4z+lw4FpwDktLWqMjUdfjMc6JF1J4z7BN5S5nqJSCfCNwMKm5wuytiRo6BtabJI0L3t9HrA5ax9uW/bXvmCI9jKcBlwgaS1wI41hlGuA2ZJ2X5myef17as5enwU8z8i3sQwbgA0Rsfv2fjfTCPR27JffAZ6OiN6I6AduodFX7dgvzcajL4Zbx5iTdAlwPvCe7D8Lcmoeqv15Rt6vwytjTG8U4001Gl9YHMVvBvxf1+q6stoEfB347D7tn2bvL08+lU0vY+8vaO7P2g+iMWY7J3s8DRyUvbbvFzTnjcN2ncFvvsT8Dnt/qfIn2fQH2ftLlW9n069j7y9unqLxpc249iPwE+DYbPoTWZ+0Xb8AbwIeBaZm6/oa8KF26xd+ewy89L4Ybh0lbMs5wEqga5/5RvyZj7Rf91tnWb9co/jAzqNxhMeTwJWtrqeprrfQ+LPsV8BD2eM8GmNTPwCeAO5u+kETcG22HQ8D3U3v9cfAmuzxvqb2buCRbJnPkfPFxRht1xn8JsCPzn5B1mQ/XJOy9snZ8zXZ60c3LX9lVu9jNB2dMZ79CCwBerK+uS37pW/LfgH+Glidre8fs0Bom34BvkVj/L6fxl9H/348+mK4dZSwLWtojE/vzoAvjvYzH02/DvfwqfRmZm0qlTFwMzMbIQe4mVmbcoCbmbUpB7iZWZtygJuZtSkHuJlZm3KAm5m1qf8Pj7KegZGbSWMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}