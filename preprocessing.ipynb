{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "provenance": [],
      "authorship_tag": "ABX9TyNp3hF3zF+wniOsiKMQ1+TD"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0qfnRB53CwQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "7791231a-9ce5-4ea1-f4cd-a4d7ff01ec11"
      },
      "source": [
        "import time\n",
        "import re\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import requests\n",
        "import collections\n",
        "import pickle\n",
        "import copy, random\n",
        "np.random.seed(1234)\n",
        "import nltk as nl\n",
        "nl.download('punkt')\n",
        "\n",
        "from itertools import zip_longest\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Reshape, BatchNormalization, Dense, Dropout,       # General\n",
        "    Embedding, LSTM, Dense, GRU,                              # RNN\n",
        "    Conv2D, Conv2DTranspose, LeakyReLU, MaxPool2D, Flatten    # CNN\n",
        ")\n",
        "from tensorflow.keras.activations import elu, relu, softmax, sigmoid\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTpaQwHZsp2P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "e7000227-39f6-436b-e42c-4be70263f742"
      },
      "source": [
        "#########################################################################################################\n",
        "if False:\n",
        "    class LearningConfig:\n",
        "        def __init__(self, optimizer=tf.compat.v1.train.AdagradOptimizer, lr=0.001, norm_clip=100.0, batch_size=32,\n",
        "                    lr_scheduler=True, max_no_improve=5, n_epochs=10,\n",
        "                    trunc_norm_init_std=1e-4, is_test=False):\n",
        "            self.batch_size = batch_size\n",
        "            self.n_steps = 3500\n",
        "            self.n_epochs = n_epochs\n",
        "            self.lr = lr\n",
        "            # self.optimizer = tf.train.AdamOptimizer\n",
        "            self.optimizer = optimizer\n",
        "            self.norm_clip = norm_clip\n",
        "\n",
        "            self.lr_scheduler = lr_scheduler\n",
        "            self.max_no_improve = max_no_improve\n",
        "\n",
        "            self.trunc_norm_init_std = trunc_norm_init_std\n",
        "\n",
        "            self.is_test = is_test\n",
        "\n",
        "\n",
        "    class SyLMConfig:\n",
        "        def __init__(self, vocab_size,\n",
        "                    sentence_max_len, emb_size,\n",
        "                    rnn_size, cell_type, keep_prob,\n",
        "                    proj_size,\n",
        "                    optimizer, lr, norm_clip, batch_size, n_epochs, lr_scheduler, max_no_improve,\n",
        "                    restore_model,\n",
        "                    is_test=False):\n",
        "            \"\"\"\n",
        "            Language Model Configuration Class\n",
        "            :param vocab_size:\n",
        "            :param is_test:\n",
        "            \"\"\"\n",
        "\n",
        "            self.is_test = is_test\n",
        "            # General\n",
        "            self.input_vocab_size = self.output_vocab_size = vocab_size\n",
        "            self.sentence_max_len = sentence_max_len\n",
        "            self.input_emb_size = self.output_emb_size = emb_size\n",
        "\n",
        "            # Encoder\n",
        "            self.encoder_rnn_size = rnn_size\n",
        "            self.encoder_keep_prob = keep_prob if not is_test else 1\n",
        "            self.cell_type = cell_type\n",
        "            self.wrap_attention = False\n",
        "\n",
        "            self.proj_size = proj_size\n",
        "\n",
        "            # Learning\n",
        "            self.learning = LearningConfig(optimizer, lr, norm_clip, batch_size, lr_scheduler, max_no_improve, n_epochs=n_epochs, is_test=is_test)\n",
        "            self.restore_model = restore_model\n",
        "\n",
        "        def set_params(self, dict):\n",
        "            def _recursive_call(items, attr_id):\n",
        "                items = list(items)\n",
        "                attr_name = items[attr_id][0]\n",
        "                attr_values = items[attr_id][1]\n",
        "                for attr_value in attr_values:\n",
        "                    setattr(self, attr_name, attr_value)\n",
        "                    if attr_id == (len(items) - 1):  # base case[\n",
        "                        yield self\n",
        "                    else:\n",
        "                        for i in _recursive_call(items, attr_id + 1):\n",
        "                            yield i\n",
        "\n",
        "            items = dict.items()\n",
        "            for i in _recursive_call(items, 0):\n",
        "                yield i\n",
        "\n",
        "        def set_tied_params(self):\n",
        "            self.output_emb_size = self.input_emb_size\n",
        "\n",
        "\n",
        "    def setup_config(FLAGS):\n",
        "        \"\"\"\n",
        "        :param FLAGS:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "        config = SyLMConfig(vocab_size=FLAGS.vocab_size, sentence_max_len=FLAGS.sentence_max_len, emb_size=FLAGS.emb_size,\n",
        "                rnn_size=FLAGS.enc_size, cell_type=FLAGS.rnn_cell_type, keep_prob=FLAGS.enc_keep_prob, proj_size=FLAGS.proj_size,\n",
        "                optimizer=tf.train.AdamOptimizer, lr=FLAGS.lr, norm_clip=FLAGS.norm_clip, batch_size=FLAGS.batch_size, n_epochs=FLAGS.n_epochs,\n",
        "                lr_scheduler=FLAGS.lr_scheduler, max_no_improve=FLAGS.max_no_improve, restore_model=FLAGS.restore_model, is_test=False)\n",
        "\n",
        "        val_config = copy.deepcopy(config)\n",
        "        val_config.encoder_keep_prob = 1.0\n",
        "        val_config.is_test = True\n",
        "\n",
        "        gen_config = copy.deepcopy(val_config)\n",
        "\n",
        "        return config, val_config, gen_config\n",
        "\n",
        "\n",
        "\n",
        "    def get_vowels(w):\n",
        "        return [(c, i) for i, c in enumerate(w) if is_vowel(c)]\n",
        "\n",
        "    def get_next_vowel_pos(word, start_pos=0):\n",
        "        c = word[start_pos]\n",
        "        count = start_pos\n",
        "        while not is_vowel(c) or count == len(word):\n",
        "            count += 1\n",
        "            c = word[count]\n",
        "\n",
        "        return count + 1\n",
        "\n",
        "\n",
        "    def get_seq_hyphen_len(words):\n",
        "        return sum([len(hyphenation(w)) for w in words])\n",
        "\n",
        "    def hyp2word(hyphen, hyp_rev_vocabulary, special_tokens):\n",
        "        word = ''\n",
        "        for hyp in hyphen:\n",
        "            if hyp not in special_tokens and hyp in hyp_rev_vocabulary:\n",
        "                word += hyp_rev_vocabulary[hyp]\n",
        "            elif hyp not in special_tokens:\n",
        "                word += '<UNK>'\n",
        "\n",
        "        return word\n",
        "\n",
        "\n",
        "    def get_hyps(batch, hyp_rev_vocabulary, special_tokens):\n",
        "        hyps = []\n",
        "        for seq in batch:\n",
        "            hyps.append('')\n",
        "            for hyphen in seq:\n",
        "                hyps[-1] += hyp2word(hyphen, hyp_rev_vocabulary, special_tokens) + ' '\n",
        "\n",
        "        return hyps\n",
        "\n",
        "\n",
        "    def print_hyps(batch, hyp_rev_vocabulary, special_tokens):\n",
        "        for seq in batch:\n",
        "            to_print = ''\n",
        "            for hyphen in seq:\n",
        "                to_print.join(hyp2word(hyphen, hyp_rev_vocabulary, special_tokens) + ' ')\n",
        "            print(to_print)\n",
        "\n",
        "\n",
        "    def print_paired_hyps(file, batch_y, batch_z, hyp_rev_vocabulary, special_tokens):\n",
        "        hyps_y = get_hyps(batch_y, hyp_rev_vocabulary, special_tokens)\n",
        "        hyps_z = get_hyps(batch_z, hyp_rev_vocabulary, special_tokens)\n",
        "\n",
        "        for i in range(len(hyps_y)):\n",
        "            print_and_write(file, 'Ground Truth: ' + hyps_y[i])\n",
        "            print_and_write(file, 'Prediction: ' + hyps_z[i])\n",
        "\n",
        "\n",
        "    def print_paired_output(file, batch_y, batch_z, rev_vocabulary, special_tokens, end_of_tokens=None):\n",
        "\n",
        "        def output2string(batch, rev_vocabulary, special_tokens, end_of_tokens):\n",
        "            output_strings = []\n",
        "            for seq in batch:\n",
        "                to_print = ''\n",
        "                for token in seq:\n",
        "                    if token in special_tokens:\n",
        "                        to_print += ' '\n",
        "                    elif end_of_tokens and token in end_of_tokens:\n",
        "                        to_print += '\\n'\n",
        "                    elif token in rev_vocabulary:\n",
        "                        to_print += rev_vocabulary[token]\n",
        "                    else:\n",
        "                        to_print += '<UNK>'\n",
        "                output_strings.append(to_print)\n",
        "\n",
        "            return output_strings\n",
        "\n",
        "        hyps_y = output2string(batch_y, rev_vocabulary, special_tokens, end_of_tokens)\n",
        "        hyps_z = output2string(batch_z, rev_vocabulary, special_tokens, end_of_tokens)\n",
        "\n",
        "        for i in range(len(hyps_y)):\n",
        "            print_and_write(file, \"\\n================================================\")\n",
        "            print_and_write(file, 'Ground Truth: ' + hyps_y[i] + \"\\n\")\n",
        "            print_and_write(file, 'Prediction: ' + hyps_z[i] + \"\\n\")\n",
        "            print_and_write(file, \"================================================\\n\")\n",
        "\n",
        "\n",
        "    def hyps2words(ids, sep, pad=-1, with_sep=False, omit_pad=True):\n",
        "        \"\"\"\n",
        "        Splits the list of ids according to a separator.\n",
        "\n",
        "        :param ids: a list of hyphen' ids\n",
        "        :param sep: the separator token (INT value)\n",
        "        :param pad (optional): id of the pad token (INT value)\n",
        "        :param with_sep (optional): separators are omitted if True,\n",
        "        otherwise they are kept\n",
        "        :param omit_pad (optional): true or false to decide whether\n",
        "        to omit pad token or not\n",
        "        :return: a list of elements, where each element\n",
        "        is a list of tokens composing a word\n",
        "        \"\"\"\n",
        "\n",
        "        words = [[]]\n",
        "        for id in ids:\n",
        "            if id == sep:\n",
        "                if with_sep:\n",
        "                    words.append([sep])\n",
        "                words.append([])\n",
        "            elif id != pad or (id == pad and not omit_pad):\n",
        "                words[-1].append(id)\n",
        "\n",
        "        return words\n",
        "\n",
        "\n",
        "    def hyps2word(hyps):\n",
        "        \"\"\"\n",
        "        Converts a list of hyphens to a string.\n",
        "        :param hyps: a list of strings (hyphens)\n",
        "        :return: string of concatenated hyphens\n",
        "        \"\"\"\n",
        "\n",
        "        return ''.join(hyps)\n",
        "\n",
        "\n",
        "    def id2hyp(id, rev_dictionary):\n",
        "        \"\"\"\n",
        "        Converts an id to its respective hyphen in rev_dictionary.\n",
        "        :param id: an integer\n",
        "        :param rev_dictionary: a Python dictionary\n",
        "        with integer as keys and strings as values.\n",
        "        :return: a string\n",
        "        \"\"\"\n",
        "        return rev_dictionary[id] if id in rev_dictionary else '<UNK>'\n",
        "\n",
        "\n",
        "    def hyp2id(hyp, dictionary):\n",
        "        \"\"\"\n",
        "            Converts an hyphen to its respective id in dictionary.\n",
        "            :param hyp: a string\n",
        "            :param dictionary: a Python dictionary\n",
        "            with string as keys and integers as values.\n",
        "            :return: an integer\n",
        "            \"\"\"\n",
        "        return dictionary[hyp] if hyp in dictionary else 0\n",
        "\n",
        "\n",
        "    def ids2hyps(ids, rev_dictionary):\n",
        "        \"\"\"\n",
        "        Maps a list of ids into a list of hyphens.\n",
        "        :param ids: a list of ints\n",
        "        :param rev_dictionary:  Python dictionary\n",
        "        with string as keys and integers as values.\n",
        "        :return: a list of strings (hyphens)\n",
        "        \"\"\"\n",
        "        return [id2hyp(id, rev_dictionary) for id in ids]\n",
        "\n",
        "\n",
        "    def is_word(hyps, word_dictionary):\n",
        "        return hyps2word(hyps) in word_dictionary\n",
        "\n",
        "\n",
        "    def hyps2verses(ids, eos, eot):\n",
        "        \"\"\"\n",
        "        Split the list of hypens in different lists, separated\n",
        "        by the sep token.\n",
        "        :param ids: a list of hyphen' ids\n",
        "        :param eos: the separator token (INT) (id corresponding to <EOS>)\n",
        "        :return: a list of verses, each verse is a list of syllables\n",
        "        \"\"\"\n",
        "\n",
        "        verses = [[]]\n",
        "        for id in ids:\n",
        "            if id == eot:\n",
        "                break\n",
        "            elif id == eos:\n",
        "                verses.append([])\n",
        "            else:\n",
        "                verses[-1].append(id)\n",
        "\n",
        "        if len(verses[-1]) < 1:\n",
        "            verses = verses[:-1]\n",
        "\n",
        "        return verses\n",
        "\n",
        "\n",
        "    def hyphenize_list(l):\n",
        "        \"\"\"\n",
        "        Given a corpus, the function tokenizes it by dividing words into syllables\n",
        "        adding also a separator token between words.\n",
        "        :param l: a list of sequences, each sequence is a list of words (strings).\n",
        "        :return: a list of sequences, but each sequence is a list of syllables.\n",
        "        \"\"\"\n",
        "\n",
        "        sentences = [seq_hyphentation(s) for s in l]\n",
        "        sep_token = \"<SEP>\"\n",
        "        hyphenated_sentences = []\n",
        "        for s in sentences:\n",
        "            hyphenated_sentences.append([])\n",
        "            for w in s:\n",
        "                hyphenated_sentences[-1].extend(w)\n",
        "                hyphenated_sentences[-1].append(sep_token)\n",
        "            hyphenated_sentences[-1] = hyphenated_sentences[-1][:-1]  # removes last sep_token\n",
        "\n",
        "        return hyphenated_sentences\n",
        "\n",
        "    def read_words_from_folder(data_path):\n",
        "        try:\n",
        "            nl.data.find('tokenizers/punkt')\n",
        "        except LookupError:\n",
        "            nl.download('punkt')\n",
        "        list_files = [os.path.join(data_path, f)\n",
        "                    for f in os.listdir(data_path)\n",
        "                    if os.path.isfile(os.path.join(data_path, f))]\n",
        "        words = []\n",
        "        for filename in list_files:\n",
        "            with open(filename, \"r\") as f:\n",
        "                try:\n",
        "                    st = f.read()\n",
        "                except UnicodeDecodeError:\n",
        "                    print(\"File \"+filename+\" decode error: SKIPPED\")\n",
        "                    continue\n",
        "                st = st.translate(string.punctuation)\n",
        "                data = nl.word_tokenize(st)\n",
        "                del(st)\n",
        "                words.extend(data)\n",
        "        return words\n",
        "\n",
        "    def build_dataset_of_tokens(tokens, vocabulary_size, special_tokens=[]):\n",
        "        '''\n",
        "        Given a list of tokens, it creates a dictionary mapping each token to a unique id.\n",
        "        :param tokens: a list of strings.\n",
        "        E.g. [\"the\", \"cat\", \"is\", ... \".\", \"the\", \"house\" ,\"is\" ...].\n",
        "        NB: Here you should put all your token instances of the corpus.\n",
        "        :param vocabulary_size: The number of elements of your vocabulary. If there are more\n",
        "        than 'vocabulary_size' elements on tokens, it considers only the 'vocabulary_size'\n",
        "        most frequent ones.\n",
        "        :param special_tokens: Optional. Useful to add special tokens in vocabulary. I\n",
        "        f you don't have any, keep it empty.\n",
        "        :return: data: the mapped tokens list;\n",
        "        count: a dictionary containing the number of occurrences in 'tokens' for each\n",
        "        element on your dictionary.\n",
        "        dictionary: a python dictionary that associates a token with a unique integer identifier.\n",
        "        reverse_dictionary: a python dictionary mapping a unique integer identifier to its token.\n",
        "        E.g.\n",
        "        dictionary:{\"UNK\": 0, \"a\": 1, \"the\": 2, ....}\n",
        "        reverse_dictionary:{0:\"UNK\", 1:\"a\", 2:\"the\"}\n",
        "        '''\n",
        "        # counting occurrences of each token\n",
        "        count = [['UNK', -1]]\n",
        "        count.extend(collections.Counter(tokens).most_common(vocabulary_size - 1))  # takes only the most frequent ones\n",
        "        dictionary = dict()\n",
        "        for word, _ in count:\n",
        "            dictionary[word] = len(dictionary)\n",
        "\n",
        "        for token in special_tokens:\n",
        "            dictionary[token[0]] = token[1]\n",
        "\n",
        "        data = list()\n",
        "        unk_count = 0\n",
        "        for word in tokens:\n",
        "            if word in dictionary:\n",
        "                index = dictionary[word]\n",
        "            else:\n",
        "                index = 0  # dictionary['UNK']\n",
        "                unk_count += 1\n",
        "            data.append(index)\n",
        "        count[0][1] = unk_count\n",
        "        reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
        "        return data, count, dictionary, reverse_dictionary\n",
        "\n",
        "    def save_dictionary_tsv(filepath, dictionary, count):\n",
        "        keys, values = zip(*count)\n",
        "        with open(filepath, 'w') as f:\n",
        "            f.write('Word\\tFrequency\\n')\n",
        "            for k, v in dictionary.items():\n",
        "                if k in keys:\n",
        "                    f.write(k + '\\t' + str(values[keys.index(k)]) + '\\n')\n",
        "                else:\n",
        "                    f.write(k + '\\t' + '0' + '\\n')\n",
        "\n",
        "\n",
        "    def k_frequent(words_data, k):\n",
        "        counter = collections.Counter(words_data)\n",
        "        most = counter.most_common(k)\n",
        "        res = [most[i][0] for i in range(len(most))]\n",
        "        return res\n",
        "\n",
        "\n",
        "    def toWord(chars):\n",
        "        str =''\n",
        "        for c in chars:\n",
        "            if(c<2):\n",
        "                continue\n",
        "            elif(c==2):\n",
        "                break\n",
        "            str = str+chr(c)\n",
        "        return str\n",
        "\n",
        "\n",
        "    def grouper(iterable, n, fillvalue=None):\n",
        "        args = [iter(iterable)] * n\n",
        "        return zip_longest(*args, fillvalue=fillvalue)\n",
        "\n",
        "    def get_cantica(filename, encoding=None):\n",
        "        f_cantica = []\n",
        "        count = 1\n",
        "        with open(filename, \"r\", encoding=encoding) as f:\n",
        "            for line in f:\n",
        "                sentence = line.strip()\n",
        "                tokenized_sentence = nl.word_tokenize(sentence)\n",
        "\n",
        "                if len(tokenized_sentence) == 2:\n",
        "                    # setting feature cantica for each canto\n",
        "                    if count <= 34:\n",
        "                        f_cantica.append([1, 0, 0])\n",
        "                    elif count > 34 and count <= 67:\n",
        "                        f_cantica.append([0, 1, 0])\n",
        "                    else:\n",
        "                        f_cantica.append([0, 0, 1])\n",
        "\n",
        "                    count += 1\n",
        "\n",
        "        return f_cantica\n",
        "\n",
        "    def create_BAB_tercets(cantos):\n",
        "        tercets = []\n",
        "        for canto in cantos:\n",
        "            for v,verse in enumerate(canto):\n",
        "                if v%3 == 1:\n",
        "                    tercets.append([])\n",
        "                if v > 0: tercets[-1].append(verse)\n",
        "\n",
        "            tercets = tercets[:-1] # removes the last malformed tercets (only 2 verses)\n",
        "\n",
        "        return tercets\n",
        "\n",
        "\n",
        "    def get_poetry(filename, encoding=None):\n",
        "        # raw_data = read_words(filename=filename)\n",
        "\n",
        "        raw_data, words = [], []\n",
        "        # with open(filename, \"r\", encoding ='latin-1') as f:\n",
        "        with open(filename, \"r\", encoding=encoding) as f:\n",
        "            for line in f:\n",
        "                sentence = line.strip()\n",
        "                if len(sentence) > 1:\n",
        "                    sentence = sentence.translate(string.punctuation)\n",
        "                    tokenized_sentence = nl.word_tokenize(sentence)\n",
        "                    tokenized_sentence = [w.lower() for w in tokenized_sentence if len(w)>0]\n",
        "                    tokenized_sentence = [w for w in tokenized_sentence if \",\" not in w]\n",
        "                    tokenized_sentence = [w for w in tokenized_sentence if \".\" not in w]\n",
        "                    tokenized_sentence = [w for w in tokenized_sentence if \":\" not in w]\n",
        "                    tokenized_sentence = [w for w in tokenized_sentence if \";\" not in w]\n",
        "                    ts = []\n",
        "                    [ts.extend(re.split(\"(\\')\", e)) for e in tokenized_sentence]\n",
        "                    tokenized_sentence = [w for w in ts if len(w) > 0]\n",
        "\n",
        "                    if len(tokenized_sentence) > 2:\n",
        "                        raw_data.append(tokenized_sentence)\n",
        "                        words.extend(tokenized_sentence)\n",
        "\n",
        "        return raw_data, words\n",
        "\n",
        "    def get_decameron(filename):\n",
        "        raw_data, words = [], []\n",
        "        with open(filename, \"r\", encoding='latin-1') as f:\n",
        "            for line in f:\n",
        "                raw_sentences = line.strip()\n",
        "                if len(raw_sentences) > 1:\n",
        "                    sentences = raw_sentences.translate(string.punctuation)\n",
        "                    s_list = re.split(\"\\.\", sentences)\n",
        "                    for s in s_list:\n",
        "                        tokenized_sentences = nl.word_tokenize(s)\n",
        "                        tokenized_sentences = [w.lower() for w in tokenized_sentences if len(w)>0]\n",
        "                        tokenized_sentences = [w for w in tokenized_sentences if \",\" not in w]\n",
        "                        # tokenized_sentences = [w for w in tokenized_sentences if \".\" not in w]\n",
        "                        tokenized_sentences = [w for w in tokenized_sentences if \":\" not in w]\n",
        "                        ts = []\n",
        "                        [ts.extend(re.split(\"(\\')\", e)) for e in tokenized_sentences]\n",
        "                        tokenized_sentences = [w for w in ts if len(w) > 0]\n",
        "\n",
        "                        if len(tokenized_sentences) > 1 and len(tokenized_sentences) < 30:\n",
        "                            raw_data.append(tokenized_sentences)\n",
        "                            words.extend(tokenized_sentences)\n",
        "\n",
        "        return raw_data, words\n",
        "\n",
        "\n",
        "    def build_dataset_from_dict(raw_data, dictionary, config, shuffle=True):\n",
        "        '''\n",
        "        Converts all the tokens in raw_data by mapping each token with its corresponding\n",
        "        value in the dictionary. In case of token not in the dictionary, they are assigned to\n",
        "        a specific id. Each sequence is padded up to the sentence_max_len setup in the config.\n",
        "\n",
        "        :param raw_data: list of sequences, each sequences is a list of tokens (strings).\n",
        "        :param dictionary: a python dictionary having as keys strings\n",
        "        and int tokens as values.\n",
        "        :param config: config object from class Config.\n",
        "        :param shuffle: Optional. If True data are shuffled.\n",
        "        :return: A list of sequences where each token in each sequence is an int id.\n",
        "        '''\n",
        "        dataset = []\n",
        "        for sentence in raw_data:\n",
        "            sentence_ids = [config._GO]\n",
        "            sentence_ids.extend([dictionary[w] if w in dictionary else dictionary[\"UNK\"] for w in sentence])\n",
        "            sentence_ids.append(config._EOS)\n",
        "            sentence_ids = pad_list(sentence_ids, config._PAD, config.sentence_max_len)\n",
        "\n",
        "            dataset.append(sentence_ids)\n",
        "\n",
        "        if shuffle:\n",
        "            return random.sample(dataset, len(dataset))\n",
        "        else:\n",
        "            return dataset\n",
        "\n",
        "\n",
        "    def build_stanzas_dataset_from_dict(raw_data, dictionary, config, shuffle=True):\n",
        "        dataset = []\n",
        "        for stanza in raw_data:\n",
        "            stanza_ids = [config._GO]\n",
        "            for sentence in stanza:\n",
        "                sentence_ids = [dictionary[w] if w in dictionary else dictionary[\"UNK\"] for w in sentence]\n",
        "                sentence_ids.append(config._EOS)\n",
        "                stanza_ids.extend(sentence_ids)\n",
        "            # stanza_ids.append(config._EOT)\n",
        "            stanza_ids[-1] = config._EOT\n",
        "            stanza_ids = pad_list(stanza_ids, config._PAD, config.sentence_max_len)\n",
        "\n",
        "            dataset.append(stanza_ids)\n",
        "\n",
        "        if shuffle:\n",
        "            return random.sample(dataset, len(dataset))\n",
        "        else:\n",
        "            return dataset\n",
        "\n",
        "\n",
        "    def build_stanzas_dataset_from_subword_dict(raw_data, dictionary, config, shuffle=True):\n",
        "        dataset = []\n",
        "        for stanza in raw_data:\n",
        "            stanza_ids = [pad_list([config._GO], config._PAD, config.word_max_len)]\n",
        "            for sentence in stanza:\n",
        "                for word in sentence:\n",
        "                    hyp_ids = [dictionary[hyp] if hyp in dictionary else dictionary[\"UNK\"] for hyp in word]\n",
        "                    hyp_ids.append(config._EOW)\n",
        "                    hyp_ids = pad_list(hyp_ids, config._PAD, config.word_max_len)\n",
        "                    stanza_ids.extend([hyp_ids])\n",
        "\n",
        "                stanza_ids.append(pad_list([config._EOS], config._PAD, config.word_max_len))\n",
        "            # stanza_ids.append(config._EOT)\n",
        "            stanza_ids[-1] = pad_list([config._EOT], config._PAD, config.word_max_len)\n",
        "            stanza_ids = pad_list(stanza_ids, [config._PAD] * config.word_max_len, config.sentence_max_len)\n",
        "\n",
        "            dataset.append(stanza_ids)\n",
        "\n",
        "        if shuffle:\n",
        "            return random.sample(dataset, len(dataset))\n",
        "        else:\n",
        "            return dataset\n",
        "\n",
        "\n",
        "    def build_stanzas_dataset_from_chars(raw_data, config):\n",
        "        dataset = []\n",
        "        for stanza in raw_data:\n",
        "            stanza_ids = []\n",
        "            for sentence in stanza:\n",
        "                sentence_ids = [to_chars([w], config.word_max_len)[0] for w in sentence]\n",
        "                sentence_ids.append(to_chars([\"<EOS>\"], config.word_max_len)[0])\n",
        "                stanza_ids.extend(sentence_ids)\n",
        "            stanza_ids.append(to_chars([\"<EOT>\"], config.word_max_len)[0])\n",
        "            stanza_ids = pad_list(stanza_ids, to_chars([\"<pad>\"], config.word_max_len)[0], config.sentence_max_len)\n",
        "\n",
        "            dataset.append(stanza_ids)\n",
        "        return random.sample(dataset, len(dataset))\n",
        "\n",
        "    def create_lm_target(x, config):\n",
        "        return [e[1:] + [config._PAD] for e in x]\n",
        "\n",
        "\n",
        "    def create_hyp_lm_target(x, config):\n",
        "        return [e[1:] + [[config._PAD] * config.word_max_len] for e in x]\n",
        "\n",
        "\n",
        "    def batches(x, y, batch_size=128):\n",
        "\n",
        "        # Shuffle sentences\n",
        "        sentences_ids = random.sample(range(len(x)), len(x))\n",
        "\n",
        "        # Generator for batch\n",
        "        batch_x, batch_y = [], []\n",
        "        if batch_size is None:\n",
        "            batch_size = len(x)\n",
        "        for id in sentences_ids:\n",
        "            batch_x.append(x[id])\n",
        "            batch_y.append(y[id])\n",
        "            if len(batch_x) % batch_size == 0:\n",
        "                yield batch_x, batch_y\n",
        "                batch_x, batch_y = [], []\n",
        "\n",
        "\n",
        "    def _batches(iterable, batch_size=128):\n",
        "\n",
        "        # Shuffle sentences\n",
        "        x = list(zip(*iterable))\n",
        "        sentences_ids = random.sample(range(len(x)), len(x))\n",
        "\n",
        "        # Generator for batch\n",
        "        batch, batch_y = [], []\n",
        "        if batch_size is None:\n",
        "            batch_size = len(x)\n",
        "        for id in sentences_ids:\n",
        "            batch.append(x[id])\n",
        "            if len(batch) % batch_size == 0:\n",
        "                yield batch\n",
        "                batch = []\n",
        "\n",
        "\n",
        "    def batches3(chars, x, y, batch_size=128):\n",
        "\n",
        "        # Shuffle sentences\n",
        "        sentences_ids = random.sample(range(len(x)), len(x))\n",
        "\n",
        "        # Generator for batch\n",
        "        batch_chars, batch_x, batch_y = [], [], []\n",
        "        if batch_size is None:\n",
        "            batch_size = len(x)\n",
        "        for id in sentences_ids:\n",
        "            batch_chars.append(chars[id])\n",
        "            batch_x.append(x[id])\n",
        "            batch_y.append(y[id])\n",
        "            if len(batch_x) % batch_size == 0:\n",
        "                yield batch_chars, batch_x, batch_y\n",
        "                batch_chars, batch_x, batch_y = [], [], []\n",
        "\n",
        "\n",
        "    def are_in_rhyme(w1, w2):\n",
        "\n",
        "        def find_termination(w):\n",
        "            v_count = 0\n",
        "            for i in range(len(w)-1, -1, -1):\n",
        "                if is_vowel(w[i]):\n",
        "                    v_count += 1\n",
        "                    if v_count == 2 and i < len(w)-2:\n",
        "                        # se è la seconda vocale che trovo, è una vocale e non è nel penultimo carattere\n",
        "                        return w[i:]\n",
        "                    elif v_count == 3:\n",
        "                        # se è la terza vocale che trovo\n",
        "                        return w[i:]\n",
        "\n",
        "        t1 = find_termination(w1)\n",
        "        t2 = find_termination(w2)\n",
        "        if t1 and t2 and t1 == t2 and len(t1) > 1 and len(t2) > 1:\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def general_batches(iterables, full_size, batch_size=128):\n",
        "        if batch_size == None:\n",
        "            batch_size = full_size\n",
        "\n",
        "        batch = []\n",
        "        for i in range(len(iterables)):\n",
        "            batch.append([])\n",
        "\n",
        "        for id in range(full_size):\n",
        "            for i,it in enumerate(iterables):\n",
        "                batch[i].append(it[id])\n",
        "            if len(batch[0]) % batch_size == 0:\n",
        "                yield batch\n",
        "                batch = []\n",
        "                for i in range(len(iterables)):\n",
        "                    batch.append([])\n",
        "\n",
        "    def get_sonnets(verses, N=14):\n",
        "        sonnets, words = [], []\n",
        "        for v, verse in enumerate(verses):\n",
        "            if v % N == 0:\n",
        "                sonnets.append([])\n",
        "\n",
        "            tokenized_sentence = [w.lower().strip() for w in nl.word_tokenize(verse) if len(w) > 0]\n",
        "            tokenized_sentence = [w for w in tokenized_sentence if \",\" not in w]\n",
        "            tokenized_sentence = [w for w in tokenized_sentence if \".\" not in w]\n",
        "            tokenized_sentence = [w for w in tokenized_sentence if \":\" not in w]\n",
        "            tokenized_sentence = [w for w in tokenized_sentence if \";\" not in w]\n",
        "            tokenized_sentence = [w for w in tokenized_sentence if \"«\" not in w]\n",
        "            tokenized_sentence = [w for w in tokenized_sentence if \"»\" not in w]\n",
        "            tokenized_sentence = [w for w in tokenized_sentence if \"‘\" not in w]\n",
        "            tokenized_sentence = [w for w in tokenized_sentence if \"‘‘\" not in w]\n",
        "            tokenized_sentence = [w for w in tokenized_sentence if \"(\" not in w]\n",
        "            tokenized_sentence = [w for w in tokenized_sentence if \")\" not in w]\n",
        "            ts = []\n",
        "            [ts.extend(re.split(\"(\\')\", e)) for e in tokenized_sentence]\n",
        "            tokenized_sentence = [w for w in ts if len(w) > 0]\n",
        "            sonnets[-1].append(tokenized_sentence)\n",
        "            words.extend(tokenized_sentence)\n",
        "\n",
        "        return sonnets, words\n",
        "\n",
        "    def load_paisa_data(filename, n_docs=15000):\n",
        "        with open(filename, 'r', encoding=\"utf8\") as f:\n",
        "            it_data = []\n",
        "            c = 0\n",
        "            for line in f:\n",
        "                if line[0] not in ['#', '<'] and len(line) > 150:\n",
        "                    line = re.sub('\\d+', '0', line)\n",
        "                    line = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', 'url', line)\n",
        "                    line = re.sub(',', ' , ', line)\n",
        "                    line = re.sub(';', ' ; ', line)\n",
        "                    line = re.sub(':', ' : ', line)\n",
        "                    line = re.sub('\\(', ' ( ', line)\n",
        "                    line = re.sub('\\)', ' ) ', line)\n",
        "                    line = re.sub(\"\\'\", \" \\' \", line)\n",
        "                    line = line.lower()\n",
        "                    it_data.append(line)\n",
        "                    if c >= n_docs:\n",
        "                        break\n",
        "                    c += 1\n",
        "\n",
        "            return it_data\n",
        "\n",
        "\n",
        "    def build_dataset_with_context(raw_data, dictionary, config, pad_len):\n",
        "        dataset = []\n",
        "        for sentence in raw_data:\n",
        "            stanza_ids = [config._GO]\n",
        "            sentence_ids = [dictionary[w] if w in dictionary else dictionary[\"UNK\"] for w in sentence]\n",
        "            sentence_ids.append(config._EOS)\n",
        "            stanza_ids.extend(sentence_ids)\n",
        "            stanza_ids = pad_list(stanza_ids, config._PAD, pad_len)\n",
        "\n",
        "            dataset.append(stanza_ids)\n",
        "\n",
        "        return dataset\n",
        "\n",
        "    def split_in_ngrams(x, n=3, pad_token='_'):\n",
        "        '''\n",
        "        Arguments:\n",
        "        'x' a string of text, e.g. a word a sentence.\n",
        "        'pad_token' the token to add to unfinished trigrams.\n",
        "\n",
        "        Returns:\n",
        "        a list containing 'x' split in trigrams.'''\n",
        "\n",
        "        trigrams = []\n",
        "        for i, ch in enumerate(x):\n",
        "            if i % n == 0:\n",
        "                trigrams.append('')\n",
        "\n",
        "            trigrams[-1] += ch\n",
        "\n",
        "        for p in range(3 - len(trigrams[-1])):\n",
        "            trigrams[-1] += pad_token\n",
        "\n",
        "        return trigrams\n",
        "\n",
        "\n",
        "    def get_ngrams(l, n=3, pad_token='_'):\n",
        "        '''\n",
        "        Arguments:\n",
        "        'l' an input list of words.\n",
        "        'pad_token' the token to add to unfinished trigrams.\n",
        "\n",
        "        Returns:\n",
        "        a list of the trigrams of 'l'.'''\n",
        "        trigrams = []\n",
        "        for w in l:\n",
        "            t = split_in_ngrams(w, n, pad_token)\n",
        "            trigrams.extend(t)\n",
        "\n",
        "        return trigrams\n",
        "\n",
        "\n",
        "    def get_ngrams_from_tercets(tercets, n=3, pad_token='_'):\n",
        "        '''\n",
        "        Arguments:\n",
        "        'tercets' a list of tercets.\n",
        "        'pad_token' the token to add to unfinished trigrams.\n",
        "\n",
        "        Returns:\n",
        "        'tr_tercets' a list of tercets splitted in trigrams\n",
        "        'trigrams' a list of all the trigrams'''\n",
        "        tr_tercets, trigrams = [], []\n",
        "        for tercet in tercets:\n",
        "            tr_tercets.append([])\n",
        "            for verse in tercet:\n",
        "                # t = split_in_ngrams(verse, pad_token) # CHANGE IN get_ngrams IF tercets are not raw\n",
        "                t = get_ngrams(verse, n, pad_token)\n",
        "                tr_tercets[-1].append(t)\n",
        "                trigrams.extend(t)\n",
        "\n",
        "        return tr_tercets, trigrams\n",
        "\n",
        "\n",
        "    def get_ngrams_from_canzoniere(verses, n=3, pad_token='_'):\n",
        "        '''\n",
        "        Arguments:\n",
        "        'verses' a list of verses.\n",
        "        'pad_token' the token to add to unfinished trigrams.\n",
        "\n",
        "        Returns:\n",
        "        'tr_tercets' a list of tercets splitted in trigrams\n",
        "        'trigrams' a list of all the trigrams'''\n",
        "        tr_verses, trigrams = [], []\n",
        "        for verse in verses:\n",
        "            # t = split_in_ngrams(verse, pad_token) # CHANGE IN get_ngrams IF tercets are not raw\n",
        "            t = get_ngrams(verse, n, pad_token)\n",
        "            tr_verses.append(t)\n",
        "            trigrams.extend(t)\n",
        "\n",
        "        return tr_verses, trigrams\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def get_quatrains(shake_sonnets):\n",
        "        quatrains, words = [],[]\n",
        "        for sonnet in shake_sonnets:\n",
        "            quatrains.extend([sonnet[:4],sonnet[4:8], sonnet[:8:-2]])\n",
        "\n",
        "        return quatrains\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def get_context_dataset(sequences):\n",
        "        x_seq = []\n",
        "        for seq in sequences:\n",
        "            for i, c_verse in enumerate(seq):\n",
        "                context = []\n",
        "                [context.extend(v) for v in seq[:i]]\n",
        "                x_seq.append((context, c_verse))\n",
        "\n",
        "        return x_seq\n",
        "\n",
        "\n",
        "    def load_textual_corpus(filename, max_n_lines=-1):\n",
        "        '''\n",
        "        General function to load a textual file from corpus. A list of sentences\n",
        "        is given as return, data is also cleaned up to remove urls and numbers. Sentences are\n",
        "        split according to the dot '.' .\n",
        "        :param filename: the name of the textual file to load.\n",
        "        :param max_n_lines: Maximum number of lines to load from the file. Useful for huge corpora.\n",
        "        Set to -1 (default), to get all the lines.\n",
        "        :return: A list of sentences, where each sentence is a list of words.\n",
        "        '''\n",
        "\n",
        "        with open(filename, 'r', encoding=\"utf8\", errors='ignore') as f:\n",
        "            data = []\n",
        "            c = 0\n",
        "            for line in f:\n",
        "                line = re.sub('\\d+', '0', line)\n",
        "                line = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', 'url', line)\n",
        "                line = re.sub(',', ' , ', line)\n",
        "                line = re.sub(';', ' ; ', line)\n",
        "                line = re.sub(':', ' : ', line)\n",
        "                line = re.sub('\\(', ' ( ', line)\n",
        "                line = re.sub('\\)', ' ) ', line)\n",
        "                line = re.sub(\"\\'\", \" \\' \", line)\n",
        "                line = line.lower()\n",
        "                data.extend([s.strip().split() for s in line.split(\".\") if len(s) > 2])\n",
        "\n",
        "                if max_n_lines > 0 and c >= max_n_lines:\n",
        "                    break\n",
        "                c += 1\n",
        "\n",
        "            return data\n",
        "\n",
        "\n",
        "    def load_poetry_corpus(filename, scheme_n=3):\n",
        "        with open(filename, 'r', encoding=\"utf8\", errors='ignore') as f:\n",
        "            data = []\n",
        "            j = 0\n",
        "            for line in f:\n",
        "                if len(line) > 1:\n",
        "                    line = re.sub(',', ' , ', line)\n",
        "                    line = re.sub(';', ' ; ', line)\n",
        "                    line = re.sub(':', ' : ', line)\n",
        "                    line = re.sub('\\(', ' ( ', line)\n",
        "                    line = re.sub('\\)', ' ) ', line)\n",
        "                    line = re.sub(\"\\'\", \" \\' \", line)\n",
        "                    line = line.lower()\n",
        "                    tokenized_line = nl.word_tokenize(line)\n",
        "                    # tokenized_line = tokenized_line.split()\n",
        "                    tokenized_line = [w for w in tokenized_line if \",\" not in w]\n",
        "                    tokenized_line = [w for w in tokenized_line if \".\" not in w]\n",
        "                    tokenized_line = [w for w in tokenized_line if \":\" not in w]\n",
        "                    tokenized_line = [w for w in tokenized_line if \";\" not in w]\n",
        "                    tokenized_line = [w for w in tokenized_line if \"!\" not in w]\n",
        "                    tokenized_line = [w for w in tokenized_line if \"?\" not in w]\n",
        "                    tokenized_line = [w for w in tokenized_line if \"«\" not in w]\n",
        "                    tokenized_line = [w for w in tokenized_line if \"»\" not in w]\n",
        "\n",
        "                    if j % scheme_n == 0:\n",
        "                        data.append([])\n",
        "                    data[-1].append(tokenized_line)\n",
        "\n",
        "                    j += 1\n",
        "\n",
        "            return data\n",
        "\n",
        "\n",
        "    def load_dantes_poetry(filenames, stanza_size=3):\n",
        "        '''\n",
        "\n",
        "        :param filenames: A list of filenames containing raw textual Dante's data.\n",
        "        :param stanza_size: The number of verses to group together.\n",
        "        :return: a list of stanzas, each stanza is a list of verses\n",
        "        '''\n",
        "\n",
        "        poetries = []\n",
        "        for filename in filenames:\n",
        "            poetries.extend(load_poetry_corpus(filename, stanza_size))\n",
        "\n",
        "        return poetries\n",
        "\n",
        "\n",
        "    def load_dantes_prose(filenames, max_n_lines=-1):\n",
        "        '''\n",
        "        Function to retrieve Dante's prose.\n",
        "        :param filenames: A list of filenames containing raw textual Dante's data.\n",
        "        :param max_n_lines: Maximum number of lines to load from the files.\n",
        "        Default value -1, indicates no limit.\n",
        "        :return: A list of sentences.\n",
        "        '''\n",
        "\n",
        "        prose = []\n",
        "        for filename in filenames:\n",
        "            sentences = load_textual_corpus(filename, max_n_lines)\n",
        "            prose.extend(sentences)\n",
        "\n",
        "        return prose\n",
        "\n",
        "    \n",
        "\n",
        "    _PAD = 0\n",
        "    _GO = 1\n",
        "    _EOW = 2\n",
        "    _UNK = 3\n",
        "    def to_chars(words, word_max_size):\n",
        "        _PAD = 0\n",
        "        _GO = 1\n",
        "        _EOW = 2\n",
        "        _UNK = 3\n",
        "        chars = ['_PAD', '_GO', '_EOW', '_UNK', ' ', '!', '\"', '#', '$', '%', '&', '\\'', '(', ')',\n",
        "                '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
        "                ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I',\n",
        "                'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y',\n",
        "                'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i',\n",
        "                'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y',\n",
        "                'z', '{', '|', '}', 'ì', 'ò', 'ù', 'è', 'é', 'à']\n",
        "        char_dict = {}\n",
        "        for char in chars:\n",
        "            char_dict[char] = len(char_dict)\n",
        "        char_words = np.ndarray(shape=[len(words), word_max_size], dtype=np.int32)\n",
        "        for i in range(len(words)):\n",
        "            if words[i]==\"<pad>\":\n",
        "                char_words[i][:] = _PAD\n",
        "                continue\n",
        "            char_words[i][0]=_GO\n",
        "            for j in range(1,word_max_size):\n",
        "                if j < len(words[i])+1:\n",
        "                    if words[i][j-1] in char_dict:\n",
        "                        char_words[i][j] = char_dict[words[i][j-1]]\n",
        "                    else:\n",
        "                        char_words[i][j] = _UNK\n",
        "                elif j == len(words[i])+1:\n",
        "                    char_words[i][j] = _EOW\n",
        "                else:\n",
        "                    char_words[i][j] = _PAD\n",
        "            if char_words[i][word_max_size-1] != _PAD:\n",
        "                char_words[i][word_max_size-1] = _EOW\n",
        "        return char_words\n",
        "\n",
        "    '''\n",
        "    Read a file (filename) and return the textual content of the file in a vector of words\n",
        "    '''\n",
        "\n",
        "    def read_words(filename, max_len=None):\n",
        "        try:\n",
        "            nl.data.find('tokenizers/punkt')\n",
        "        except LookupError:\n",
        "            nl.download('punkt')\n",
        "        with open(filename, \"r\") as f:\n",
        "            st = f.read()\n",
        "            st = st.translate(string.punctuation)\n",
        "            data = nl.word_tokenize(st)\n",
        "            del(st)\n",
        "            if max_len:\n",
        "                return data[:max_len]\n",
        "            return data\n",
        "############################################################################\n",
        "def get_hyp_lm_tercets(tercets):\n",
        "    new_tercets = []\n",
        "    for tercet in tercets:\n",
        "        new_tercets.append([])\n",
        "        for verse in tercet:\n",
        "            new_tercets[-1].append([])\n",
        "            for hyp_w in verse:\n",
        "                new_tercets[-1][-1].extend(hyp_w)\n",
        "                new_tercets[-1][-1].append('<SEP>')\n",
        "            new_tercets[-1][-1] = new_tercets[-1][-1][:-1]\n",
        "\n",
        "    return new_tercets\n",
        "\n",
        "def is_vowel(c):\n",
        "    return c in 'aeiouAEIOUàìíèéùúüòï'\n",
        "\n",
        "def unsplittable_cons():\n",
        "    u_cons = []\n",
        "    for c1 in ('b', 'c', 'd', 'f', 'g', 'p', 't', 'v'):\n",
        "        for c2 in ('l', 'r'):\n",
        "            u_cons.append(c1 + c2)\n",
        "\n",
        "    others = ['gn', 'gh', 'ch']\n",
        "    u_cons.extend(others)\n",
        "    return u_cons\n",
        "\n",
        "\n",
        "def are_cons_to_split(c1, c2):\n",
        "    to_split = ('cq', 'cn', 'lm', 'rc', 'bd', 'mb', 'mn', 'ld', 'ng', 'nd', 'tm', 'nv', 'nc', 'ft', 'nf', 'gm', 'fm', 'rv', 'fp')\n",
        "    return (c1 + c2) in to_split or (not is_vowel(c1) and (c1 == c2)) or ((c1 + c2) not in unsplittable_cons()) and (\n",
        "        (not is_vowel(c1)) and (not is_vowel(c2)) and c1 != 's')\n",
        "\n",
        "\n",
        "def is_diphthong(c1, c2):\n",
        "    return (c1 + c2) in ('ia', 'ie', 'io', 'iu', 'ua', 'ue', 'uo', 'ui', 'ai', 'ei', 'oi', 'ui', 'au', 'eu', 'ïe', 'iú', 'iù')\n",
        "\n",
        "\n",
        "def is_triphthong(c1, c2, c3):\n",
        "    return (c1 + c2 + c3) in ('iai', 'iei', 'uoi', 'uai', 'uei', 'iuo')\n",
        "\n",
        "\n",
        "def is_toned_vowel(c):\n",
        "    return c in 'àìèéùòï'\n",
        "\n",
        "def has_vowels(sy):\n",
        "    for c in sy:\n",
        "        if is_vowel(c):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def hyphenation(word):\n",
        "    \"\"\"\n",
        "    Split word in syllables\n",
        "    :param word: input string\n",
        "    :return: a list containing syllables of the word\n",
        "    \"\"\"\n",
        "    if not word or word == '':\n",
        "        return []\n",
        "    # elif len(word) == 3 and (is_vowel(word[1]) and is_vowel(word[2]) and not is_toned_vowel(word[2]) and (\n",
        "    #     not is_diphthong(word[1], word[2]) or (word[1] == 'i'))):\n",
        "    elif len(word) == 3 and (is_vowel(word[1]) and is_vowel(word[2]) and not is_toned_vowel(word[2]) and (\n",
        "        not is_diphthong(word[1], word[2]))):\n",
        "        return [word[:2]] + [word[2]]\n",
        "    elif len(word) == 3 and is_vowel(word[0]) and not is_vowel(word[1]) and is_vowel(word[2]):\n",
        "        return [word[:2]] + [word[2]]\n",
        "    elif len(word) == 3:\n",
        "        return [word]\n",
        "\n",
        "    syllables = []\n",
        "    is_done = False\n",
        "    count = 0\n",
        "    while not is_done and count <= len(word) - 1:\n",
        "        syllables.append('')\n",
        "        c = word[count]\n",
        "        while not is_vowel(c) and count < len(word) - 1:\n",
        "            syllables[-1] = syllables[-1] + c\n",
        "            count += 1\n",
        "            c = word[count]\n",
        "\n",
        "        syllables[-1] = syllables[-1] + word[count]\n",
        "\n",
        "        if count == len(word) - 1:\n",
        "            is_done = True\n",
        "        else:\n",
        "            count += 1\n",
        "\n",
        "            if count < len(word) and not is_vowel(word[count]):\n",
        "                if count == len(word) - 1:\n",
        "                    syllables[-1] += word[count]\n",
        "                    count += 1\n",
        "                elif count + 1 < len(word) and are_cons_to_split(word[count], word[count + 1]):\n",
        "                    syllables[-1] += word[count]\n",
        "                    count += 1\n",
        "                elif count + 2 < len(word) and not is_vowel(word[count + 1]) and not is_vowel(word[count + 2]) and word[\n",
        "                    count] != 's':\n",
        "                    syllables[-1] += word[count]\n",
        "                    count += 1\n",
        "            elif count < len(word):\n",
        "                if count + 1 < len(word) and is_triphthong(word[count - 1], word[count], word[count + 1]):\n",
        "                    syllables[-1] += word[count] + word[count + 1]\n",
        "                    count += 2\n",
        "                elif is_diphthong(word[count - 1], word[count]):\n",
        "                    syllables[-1] += word[count]\n",
        "                    count += 1\n",
        "\n",
        "                if count + 1 < len(word) and are_cons_to_split(word[count], word[count + 1]):\n",
        "                    syllables[-1] += word[count]\n",
        "                    count += 1\n",
        "\n",
        "            else:\n",
        "                is_done = True\n",
        "\n",
        "    if not has_vowels(syllables[-1]) and len(syllables) > 1:\n",
        "        syllables[-2] = syllables[-2] + syllables[-1]\n",
        "        syllables = syllables[:-1]\n",
        "\n",
        "    return syllables\n",
        "\n",
        "\n",
        "\n",
        "def get_dc_hyphenation(canti):\n",
        "    hyp_canti, hyp_tokens = [], []\n",
        "    for canto in canti:\n",
        "        hyp_canti.append([])\n",
        "        for verso in canto:\n",
        "            syllables = seq_hyphentation(verso)\n",
        "            hyp_canti[-1].append(syllables)\n",
        "            for syllable in syllables:\n",
        "                hyp_tokens.extend(syllable)\n",
        "\n",
        "    return hyp_canti, hyp_tokens\n",
        "\n",
        "\n",
        "def seq_hyphentation(words):\n",
        "    \"\"\"\n",
        "    Converts words in a list of strings into lists of syllables\n",
        "    :param words: a list of words (strings)\n",
        "    :return: a list of lists containing word syllables\n",
        "    \"\"\"\n",
        "    return [hyphenation(w) for w in words]\n",
        "\n",
        "\n",
        "def get_dc_cantos(filename, encoding=None):\n",
        "    # raw_data = read_words(filename=filename)\n",
        "    cantos, words, raw = [], [], []\n",
        "    with open(filename, \"r\", encoding=encoding) as f:\n",
        "        for line in f:\n",
        "            sentence = line.strip()\n",
        "            sentence = str.replace(sentence, \"\\.\", \" \\. \")\n",
        "            sentence = str.replace(sentence, \"[\", '')\n",
        "            sentence = str.replace(sentence, \"]\", '')\n",
        "            sentence = str.replace(sentence, \"-\", '')\n",
        "            sentence = str.replace(sentence, \";\", \" ; \")\n",
        "            sentence = str.replace(sentence, \",\", \" , \")\n",
        "            # sentence = str.replace(sentence, \" \\'\", '')\n",
        "            sentence = str.replace(sentence, \"\\'\", ' \\' ')\n",
        "            if len(sentence) > 1:\n",
        "                # sentence = sentence.translate(string.punctuation)\n",
        "                tokenized_sentence = nl.word_tokenize(sentence)\n",
        "                # tokenized_sentence = sentence.split()\n",
        "                tokenized_sentence = [w.lower() for w in tokenized_sentence if len(w) > 0]\n",
        "                tokenized_sentence = [w for w in tokenized_sentence if \",\" not in w]\n",
        "                tokenized_sentence = [w for w in tokenized_sentence if \".\" not in w]\n",
        "                tokenized_sentence = [w for w in tokenized_sentence if \":\" not in w]\n",
        "                tokenized_sentence = [w for w in tokenized_sentence if \";\" not in w]\n",
        "                tokenized_sentence = [w for w in tokenized_sentence if \"«\" not in w]\n",
        "                tokenized_sentence = [w for w in tokenized_sentence if \"»\" not in w]\n",
        "                # ts = []\n",
        "                ts = tokenized_sentence\n",
        "                # [ts.extend(re.split(\"(\\')\", e)) for e in tokenized_sentence]\n",
        "                tokenized_sentence = [w for w in ts if len(w) > 0]\n",
        "\n",
        "                if len(tokenized_sentence) == 2:\n",
        "                    cantos.append([])\n",
        "                    raw.append([])\n",
        "                elif len(tokenized_sentence) > 2:\n",
        "                    raw[-1].append(sentence)\n",
        "                    cantos[-1].append(tokenized_sentence)\n",
        "                    words.extend(tokenized_sentence)\n",
        "\n",
        "    return cantos, words, raw\n",
        "\n",
        "\n",
        "def create_tercets(cantos):\n",
        "    tercets = []\n",
        "    for i,canto in enumerate(cantos):\n",
        "        for v,verse in enumerate(canto):\n",
        "            if v%3 == 0:\n",
        "                tercets.append([])\n",
        "\n",
        "            tercets[-1].append(verse)\n",
        "        tercets = tercets[:-1]  # removes the last malformed tercets (only 2 verses)\n",
        "\n",
        "    return tercets\n",
        "\n",
        "def pad_list(l, pad_token, max_l_size, keep_lasts=False, pad_right=True):\n",
        "    \"\"\"\n",
        "    Adds a padding token to a list\n",
        "    inputs:\n",
        "    :param l: input list to pad.\n",
        "    :param pad_token: value to add as padding.\n",
        "    :param max_l_size: length of the new padded list to return,\n",
        "    it truncates lists longer that 'max_l_size' without adding\n",
        "    padding values.\n",
        "    :param keep_lasts: If True, preserves the max_l_size last elements\n",
        "    of a sequence (by keeping the same order).  E.g.:\n",
        "    if keep_lasts is True and max_l_size=3 [1,2,3,4] becomes [2,3,4].\n",
        "\n",
        "\n",
        "    :return: the list padded or truncated.\n",
        "    \"\"\"\n",
        "    to_pad = []\n",
        "    max_l = min(max_l_size, len(l))  # maximum len\n",
        "    l_init = len(l) - max_l if len(l) > max_l and keep_lasts else 0  # initial position where to sample from the list\n",
        "    l_end = len(l) if len(l) > max_l and keep_lasts else max_l\n",
        "    for i in range(l_init, l_end):\n",
        "        to_pad.append(l[i])\n",
        "\n",
        "    # for j in range(len(l), max_l_size):\n",
        "    #     to_pad.append(pad_token)\n",
        "    pad_tokens = [pad_token] * (max_l_size-len(l))\n",
        "    padded_l = to_pad + pad_tokens if pad_right else pad_tokens + to_pad\n",
        "\n",
        "    return padded_l\n",
        "\n",
        "\n",
        "def save_data(data, file):\n",
        "    with open(file, 'wb') as output:\n",
        "        pickle.dump(data, output, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def load_data(file):\n",
        "    with open(file, 'rb') as obj:\n",
        "        return pickle.load(obj)\n",
        "\n",
        "def print_and_write(file, s):\n",
        "    print(s)\n",
        "    file.write(s)\n",
        "\n",
        "\n",
        "class Vocabulary(object):\n",
        "    def __init__(self, vocab_size=None):\n",
        "        self.dictionary = dict()\n",
        "        self.rev_dictionary = dict()\n",
        "        self.count = []\n",
        "        self.special_tokens = []\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    def build_vocabulary_from_counts(self, count, special_tokens=[]):\n",
        "        \"\"\"\n",
        "        Sets all the attributes of the Vocabulary object.\n",
        "        :param count: a list of lists as follows: [['token', number_of_occurrences],...]\n",
        "        :param special_tokens: a list of strings. E.g. ['<EOS>', '<PAD>',...]\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "\n",
        "        dictionary = dict()\n",
        "        for word, _ in count:\n",
        "            dictionary[word] = len(dictionary)\n",
        "\n",
        "        # adding eventual special tokens to the dictionary (e.g. <EOS>,<PAD> etc..)\n",
        "        d = len(dictionary)\n",
        "        for i, token in enumerate(special_tokens):\n",
        "            dictionary[token] = d + i\n",
        "\n",
        "        self.count = count\n",
        "        self.dictionary = dictionary\n",
        "        self.rev_dictionary = dict(zip(self.dictionary.values(), self.dictionary.keys()))\n",
        "        self.special_tokens = special_tokens\n",
        "        self.vocab_size = len(dictionary)\n",
        "\n",
        "    def build_vocabulary_from_tokens(self, tokens, vocabulary_size=None, special_tokens=[]):\n",
        "        \"\"\"\n",
        "        Given a list of tokens, it sets the Vocabulary object attributes by constructing\n",
        "        a dictionary mapping each token to a unique id.\n",
        "        :param tokens: a list of strings.\n",
        "         E.g. [\"the\", \"cat\", \"is\", ... \".\", \"the\", \"house\" ,\"is\" ...].\n",
        "         NB: Here you should put all your token instances of the corpus.\n",
        "        :param vocabulary_size: The number of elements of your vocabulary. If there are more\n",
        "        than 'vocabulary_size' elements on tokens, it considers only the 'vocabulary_size'\n",
        "        most frequent ones.\n",
        "        :param special_tokens: Optional. A list of strings. Useful to add special tokens in vocabulary.\n",
        "        If you don't have any, keep it empty.\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "\n",
        "        vocabulary_size = vocabulary_size if vocabulary_size is not None else self.vocab_size\n",
        "        vocabulary_size = vocabulary_size - (len(special_tokens) + 1) if vocabulary_size else None\n",
        "        # counts occurrences of each token\n",
        "        count = [['<UNK>', -1]]\n",
        "        count.extend(collections.Counter(tokens).most_common(vocabulary_size))  # takes only the most frequent ones, if size is None takes them all\n",
        "        self.build_vocabulary_from_counts(count, special_tokens)  # actually build the vocabulary\n",
        "        self._set_unk_count(tokens)  # set the number of OOV instances\n",
        "\n",
        "    @staticmethod\n",
        "    def merge_vocabulary(vocab0, vocab1, vocabulary_size=-1):\n",
        "        \"\"\"\n",
        "        Merge two Vocabulary objects into a new one.\n",
        "        :param vocab0: first Vocabulary object\n",
        "        :param vocab1: second Vocabulary object\n",
        "        :param vocabulary_size: parameter to decide the merged vocabulary size.\n",
        "        With default value -1, all the words of both vocabularies are preserved.\n",
        "        When set to 0, the size of the vocabulary is set to the size of vocab0,\n",
        "        when set to 1 it is kept the size of vocab1.\n",
        "        :return: a new vocabulary\n",
        "        \"\"\"\n",
        "        # get size of the new vocabulary\n",
        "        vocab_size = vocab0.vocab_size + vocab1.vocab_size if vocabulary_size == -1 else vocabulary_size\n",
        "        merged_special_tokens = list(set(vocab0.special_tokens) | set(vocab1.special_tokens))\n",
        "\n",
        "        # merge the counts from the two vocabularies and then selects the most_common tokens\n",
        "        merged_counts = collections.Counter(dict(vocab0.count)) + collections.Counter(dict(vocab1.count))\n",
        "        merged_counts = merged_counts.most_common(vocab_size)\n",
        "        count = [['<UNK>', -1]]\n",
        "        count.extend(merged_counts)\n",
        "\n",
        "        # create the new vocabulary\n",
        "        merged_vocab = Vocabulary(vocab_size)\n",
        "        merged_vocab.build_vocabulary_from_counts(count, merged_special_tokens)\n",
        "        return merged_vocab\n",
        "\n",
        "    @staticmethod\n",
        "    def merge_vocabularies(vocab_list, vocab_size=None):\n",
        "        \"\"\"\n",
        "        Join a list of vocabularies into a new one.\n",
        "        :param vocab_list: a list of Vocabulary objects\n",
        "        :param vocab_size: the maximum size of the merged vocabulary.\n",
        "        :return: a vocabulary merging them all.\n",
        "        \"\"\"\n",
        "        vocab_size = vocab_size if vocab_size else sum([v.vocab_size for v in vocab_list])\n",
        "        merged_vocab = Vocabulary(vocab_size)\n",
        "        for voc in vocab_list:\n",
        "            merged_vocab = Vocabulary.merge_vocabulary(merged_vocab, voc, vocab_size)\n",
        "        return merged_vocab\n",
        "\n",
        "    def string2id(self, dataset):\n",
        "        \"\"\"\n",
        "        Converts a dataset of strings into a dataset of ids according to the object dictionary.\n",
        "        :param dataset: any string-based dataset with any nested lists.\n",
        "        :return: a new dataset, with the same shape of dataset, where each string is mapped into its\n",
        "        corresponding id associated in the dictionary (0 for unknown tokens).\n",
        "        \"\"\"\n",
        "\n",
        "        def _recursive_call(items):\n",
        "            new_items = []\n",
        "            for item in items:\n",
        "                if isinstance(item, str) or isinstance(item, int) or isinstance(item, float):\n",
        "                    new_items.append(self.word2id(item))\n",
        "                else:\n",
        "                    new_items.append(_recursive_call(item))\n",
        "            return new_items\n",
        "\n",
        "        return _recursive_call(dataset)\n",
        "\n",
        "    def id2string(self, dataset):\n",
        "        \"\"\"\n",
        "        Converts a dataset of integer ids into a dataset of string according to the reverse dictionary.\n",
        "        :param dataset: any int-based dataset with any nested lists. Allowed types are int, np.int32, np.int64.\n",
        "        :return: a new dataset, with the same shape of dataset, where each token is mapped into its\n",
        "        corresponding string associated in the reverse dictionary.\n",
        "        \"\"\"\n",
        "        def _recursive_call(items):\n",
        "            new_items = []\n",
        "            for item in items:\n",
        "                if isinstance(item, int) or isinstance(item, np.int) or isinstance(item, np.int32) or isinstance(item, np.int64):\n",
        "                    new_items.append(self.id2word(item))\n",
        "                else:\n",
        "                    new_items.append(_recursive_call(item))\n",
        "            return new_items\n",
        "\n",
        "        return _recursive_call(dataset)\n",
        "\n",
        "    def word2id(self, item):\n",
        "        \"\"\"\n",
        "        Maps a string token to its corresponding id.\n",
        "        :param item: a string.\n",
        "        :return: If the token belongs to the vocabulary, it returns an integer id > 0, otherwise\n",
        "        it returns the value associated to the unknown symbol, that is typically 0.\n",
        "        \"\"\"\n",
        "        return self.dictionary[item] if item in self.dictionary else self.dictionary['<UNK>']\n",
        "\n",
        "    def id2word(self, token_id):\n",
        "        \"\"\"\n",
        "        Maps an integer token to its corresponding string.\n",
        "        :param token_id: an integer.\n",
        "        :return: If the id belongs to the vocabulary, it returns the string\n",
        "        associated to it, otherwise it returns the string associated\n",
        "        to the unknown symbol, that is '<UNK>'.\n",
        "        \"\"\"\n",
        "\n",
        "        return self.rev_dictionary[token_id] if token_id in self.rev_dictionary else self.rev_dictionary[self.dictionary['<UNK>']]\n",
        "\n",
        "    def get_unk_count(self):\n",
        "        return self.count[0][1]\n",
        "\n",
        "    def _set_unk_count(self, tokens):\n",
        "        \"\"\"\n",
        "        Sets the number of OOV instances in the tokens provided\n",
        "        :param tokens: a list of tokens\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "        data = list()\n",
        "        unk_count = 0\n",
        "        for word in tokens:\n",
        "            if word in self.dictionary:\n",
        "                index = self.dictionary[word]\n",
        "            else:\n",
        "                index = 0  # dictionary['<UNK>']\n",
        "                unk_count += 1\n",
        "            data.append(index)\n",
        "        self.count[0][1] = unk_count\n",
        "\n",
        "    def add_element(self, name, is_special_token=False):\n",
        "        if name not in self.dictionary:\n",
        "            self.vocab_size += 1\n",
        "            self.dictionary[name] = self.vocab_size\n",
        "            self.rev_dictionary[self.vocab_size] = name\n",
        "\n",
        "            if is_special_token:\n",
        "                self.special_tokens = list(self.special_tokens)\n",
        "                self.special_tokens.append(name)\n",
        "\n",
        "            self.count.append([name, 1])\n",
        "\n",
        "    def set_vocabulary(self, dictionary, rev_dictionary, special_tokens, vocab_size):\n",
        "        self.dictionary = dictionary,\n",
        "        self.rev_dictionary = rev_dictionary\n",
        "        self.special_tokens = special_tokens\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    @staticmethod\n",
        "    def load_vocabulary(filename):\n",
        "        return load_data(filename)\n",
        "\n",
        "    def save_vocabulary(self, filename):\n",
        "        save_data(self, filename)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class SyLMDataset(object):\n",
        "    def __init__(self, config, sy_vocab=None):\n",
        "        self.config = config\n",
        "        self.vocabulary = sy_vocab\n",
        "\n",
        "        self.raw_train_x = []\n",
        "        self.raw_val_x = []\n",
        "        self.raw_test_x = []\n",
        "        self.raw_x = []\n",
        "\n",
        "        self.train_x, self.train_y = [], []\n",
        "        self.val_x, self.val_y = [], []\n",
        "        self.test_x, self.test_y = [], []\n",
        "        self.x, self.y = [], []\n",
        "\n",
        "    def initialize(self, sess):\n",
        "        pass\n",
        "\n",
        "    def load(self, sources):\n",
        "        \"\"\"\n",
        "        Extract raw texts form sources and gather them all together.\n",
        "        :param sources: a string or an iterable of strings containing the file(s)\n",
        "        to process in order to build the dataset.\n",
        "        :return: a list of raw strings.\n",
        "        \"\"\"\n",
        "        return NotImplementedError\n",
        "\n",
        "    def build(self, sources, split_size=0.8):\n",
        "        \"\"\"\n",
        "        :param sources: a string or an iterable of strings containing the file(s)\n",
        "        to process in order to build the dataset.\n",
        "        :param split_size: the size to split the dataset, set >=1.0 to not split.\n",
        "        \"\"\"\n",
        "\n",
        "        raw_x = self.load(sources)\n",
        "        # raw_x = self.tokenize([self.preprocess(ex) for ex in raw_x])  # fixme\n",
        "        # splitting data\n",
        "        self.raw_x = raw_x\n",
        "        if split_size < 1.0:\n",
        "            self.raw_train_x, self.raw_test_x = self.split(self.raw_x, train_size=split_size)\n",
        "            self.raw_train_x, self.raw_val_x = self.split(self.raw_train_x, train_size=split_size)\n",
        "        else:\n",
        "            self.raw_train_x = self.raw_x\n",
        "\n",
        "        if self.vocabulary is None:\n",
        "            # creates vocabulary\n",
        "            tokens = [item for sublist in self.raw_train_x for item in sublist]  # get tokens\n",
        "            special_tokens = (\"<GO>\", \"<PAD>\", \"<SEP>\", \"<EOS>\", \"<EOV>\")\n",
        "            self._create_vocab(tokens, special_tokens=special_tokens)\n",
        "\n",
        "        # creates x,y for train\n",
        "        self.train_x = self._build_dataset(self.raw_train_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "        self.train_y = self._build_dataset(self.raw_train_x, insert_go=False, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "\n",
        "        # creates x,y for validation\n",
        "        self.val_x = self._build_dataset(self.raw_val_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "        self.val_y = self._build_dataset(self.raw_val_x, insert_go=False, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "\n",
        "        # creates x,y for validation\n",
        "        self.test_x = self._build_dataset(self.raw_test_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "        self.test_y = self._build_dataset(self.raw_test_x, insert_go=False, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "\n",
        "    def _create_vocab(self, tokens, special_tokens=(\"<PAD>\", \"<GO>\", \"<SEP>\", \"<EOV>\", \"<EOS>\")):\n",
        "        \"\"\"\n",
        "        Create the vocabulary. Special tokens can be added to the tokens obtained from\n",
        "        the corpus.\n",
        "        :param tokens: a list of all the tokens in the corpus. Each token is a string.\n",
        "        :param special_tokens: a list of strings.\n",
        "        \"\"\"\n",
        "        vocab = Vocabulary(vocab_size=self.config.input_vocab_size)\n",
        "        vocab.build_vocabulary_from_tokens(tokens, special_tokens=special_tokens)\n",
        "        self.vocabulary = vocab\n",
        "\n",
        "    @staticmethod\n",
        "    def split(raw_data, train_size=0.8):\n",
        "        size = math.floor(len(raw_data)*train_size)\n",
        "        return raw_data[:size], raw_data[size:]\n",
        "\n",
        "    @staticmethod\n",
        "    def preprocess(txt):\n",
        "        return txt\n",
        "\n",
        "    @staticmethod\n",
        "    def shuffle(x):\n",
        "        return random.sample(x, len(x))\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenize(txt):\n",
        "        return txt\n",
        "\n",
        "    def _build_dataset(self, raw_data, max_len=100, insert_go=True, keep_lasts=False, pad_right=True, shuffle=True):\n",
        "        \"\"\"\n",
        "        Converts all the tokens in e1_raw_data by mapping each token with its corresponding\n",
        "        value in the dictionary. In case of token not in the dictionary, they are assigned to\n",
        "        a specific id. Each sequence is padded up to the seq_max_len setup in the config.\n",
        "\n",
        "        :param raw_data: list of sequences, each sequence is a list of tokens (strings).\n",
        "        :param max_len: max length of a sequence, crop longer and pad smaller ones.\n",
        "        :param insert_go: True to insert <GO>, False otherwise.\n",
        "        :param keep_lasts: True to truncate initial elements of a sequence.\n",
        "        :param pad_right: pad to the right (default value True), otherwise pads to left.\n",
        "        :param shuffle: Optional. If True data are shuffled.\n",
        "        :return: A list of sequences where each token in each sequence is an int id.\n",
        "        \"\"\"\n",
        "        dataset = []\n",
        "        for sentence in raw_data:\n",
        "            sentence_ids = [self.vocabulary.word2id(\"<GO>\")] if insert_go else []\n",
        "            sentence_ids.extend([self.vocabulary.word2id(w) for w in sentence])\n",
        "            sentence_ids.append(self.vocabulary.word2id(\"<EOS>\"))\n",
        "            sentence_ids = pad_list(sentence_ids, self.vocabulary.word2id(\"<PAD>\"), max_len, keep_lasts=keep_lasts, pad_right=pad_right)\n",
        "\n",
        "            dataset.append(sentence_ids)\n",
        "\n",
        "        if shuffle:\n",
        "            return random.sample(dataset, len(dataset))\n",
        "        else:\n",
        "            return dataset\n",
        "\n",
        "    def get_batches(self, batch_size=32):\n",
        "        \"\"\"\n",
        "        Iterator over the training set. Useful method to run experiments.\n",
        "        :param batch_size: size of the mini_batch\n",
        "        :return: input and target.\n",
        "        \"\"\"\n",
        "        x, y = self.train_x, self.train_y\n",
        "        \n",
        "        i = random.randint(0, batch_size)\n",
        "        batches = []\n",
        "        eov = self.vocabulary.word2id(\"<EOV>\")\n",
        "        # prepare batches\n",
        "        while i < len(x):\n",
        "            j = 0\n",
        "            batch_x, batch_y = [], []\n",
        "            while j < batch_size and i+j<len(x):\n",
        "                for c in x[i+j]:\n",
        "                  batch_x.append(c)\n",
        "                batch_x.append(eov)\n",
        "                for c in y[i+j]:\n",
        "                  batch_y.append(c)\n",
        "                batch_y.append(eov)\n",
        "                j += 1\n",
        "            i += batch_size\n",
        "\n",
        "            batches.append((batch_x, batch_y))\n",
        "\n",
        "        # shuffle\n",
        "        random.shuffle(batches)\n",
        "\n",
        "        # supply\n",
        "        i = 0\n",
        "        while i < len(batches):\n",
        "            yield batches[i][0], batches[i][1]\n",
        "            i += 1\n",
        "\n",
        "class DanteSyLMDataset(SyLMDataset):\n",
        "    def __init__(self, config, sy_vocab=None):\n",
        "        \"\"\"\n",
        "        Class to create a dataset from Dante Alighieri's Divine Comedy.\n",
        "        :param config: a Config object\n",
        "        :param sy_vocab: (optional) a Vocabulary object where tokens of the dictionary\n",
        "        are syllables. If None, the vocabulary is create automatically from the source.\n",
        "        \"\"\"\n",
        "        super().__init__(config, sy_vocab)\n",
        "\n",
        "    def load(self, sources):\n",
        "        \"\"\"\n",
        "        Load examples from dataset\n",
        "        :param sources: data filepath.\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        canti, _, raw = get_dc_cantos(filename=sources)  # get raw data from file\n",
        "        canti, tokens = get_dc_hyphenation(canti)  # converts each\n",
        "\n",
        "        tercets = create_tercets(canti)\n",
        "        tercets = get_hyp_lm_tercets(tercets)\n",
        "\n",
        "        x = []\n",
        "        for tercet in tercets:\n",
        "            x.append([])\n",
        "            for verse in tercet:\n",
        "                x[-1].extend(verse)\n",
        "                x[-1].append(\"<EOV>\")\n",
        "\n",
        "        #x = self.shuffle(x)\n",
        "        return x\n",
        "\n",
        "def seq2str(seq):\n",
        "    def output2string(batch, rev_vocabulary, special_tokens, end_of_tokens):\n",
        "        to_print = ''\n",
        "        for token in batch:\n",
        "            if token in special_tokens:\n",
        "                to_print += ' '\n",
        "            elif end_of_tokens and token in end_of_tokens:\n",
        "                to_print += '\\n'\n",
        "            elif token in rev_vocabulary:\n",
        "                to_print += rev_vocabulary[token]\n",
        "            else:\n",
        "                to_print += '<UNK>'\n",
        "        return to_print\n",
        "\n",
        "    return output2string(seq, poetry_sy_lm_dataset.vocabulary.rev_dictionary,\n",
        "      special_tokens=[poetry_sy_lm_dataset.vocabulary.word2id(\"<PAD>\"), 0, poetry_sy_lm_dataset.vocabulary.word2id(\"<SEP>\"),\n",
        "                      poetry_sy_lm_dataset.vocabulary.word2id(\"<GO>\"), poetry_sy_lm_dataset.vocabulary.word2id(\"<EOS>\")],\n",
        "      end_of_tokens=[poetry_sy_lm_dataset.vocabulary.word2id(\"<EOV>\")])\n",
        "\n",
        "class cnfg:\n",
        "  vocab_size = 1884\n",
        "  input_vocab_size = 1884\n",
        "  emb_size = 300\n",
        "  sentence_max_len = 75\n",
        "\n",
        "config = cnfg()\n",
        "poetry_sy_lm_dataset = DanteSyLMDataset(config, sy_vocab=None)\n",
        "url = \"https://gitlab.com/zugo91/nlgpoetry/-/raw/release/data/la_divina_commedia.txt\"\n",
        "response = requests.get(url)\n",
        "response.encoding = 'ISO-8859-1'\n",
        "fi = open(\"divcom.txt\",\"w\")\n",
        "fi.write(response.text)\n",
        "fi.close()\n",
        "data_path = os.path.join(os.getcwd(), \"divcom.txt\")  # dataset location, here just the name of the source file\n",
        "poetry_sy_lm_dataset.build(data_path, split_size=0.9)  # actual creation of  vocabulary (if not provided) and dataset\n",
        "print(\"Train size: \" + str(len(poetry_sy_lm_dataset.train_y)))\n",
        "print(\"Val size: \" + str(len(poetry_sy_lm_dataset.val_y)))\n",
        "print(\"Test size: \" + str(len(poetry_sy_lm_dataset.test_y)))\n",
        "\n",
        "batches = [b for b in poetry_sy_lm_dataset.get_batches(3)]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train size: 3815\n",
            "Val size: 424\n",
            "Test size: 472\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPuIBZ2xu0lw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(init, generator, num_generate = 225, temperature = 1.0):\n",
        "    text_generated = []\n",
        "    generator.reset_states()\n",
        "    sym = init\n",
        "    \n",
        "    for i in range(num_generate):\n",
        "        predictions = generator(sym)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "        sym = tf.expand_dims([predicted_id], 0)\n",
        "        text_generated.append(predicted_id)\n",
        "    return (seq2str(text_generated))\n",
        "\n",
        "init = [poetry_sy_lm_dataset.vocabulary.word2id(\"<GO>\")]\n",
        "init = np.asarray(init)\n",
        "init = np.expand_dims(init, axis=0)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owXvvnB8EJ_U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3cbdfe06-4fd1-4b1f-dac2-df9b6c664013"
      },
      "source": [
        "vocab_size = poetry_sy_lm_dataset.vocabulary.vocab_size + 2 # TODO: idk\n",
        "batch_size = 225 \n",
        "embedding_size = 8  #power of 2 to exploit cache\n",
        "len_input = 100\n",
        "hidden_size = 100\n",
        "n_epochs = 1000\n",
        "learning_rate = 1e-3\n",
        "\n",
        "char_input = Input(shape=(batch_size,))\n",
        "RNN = Sequential([\n",
        "    Embedding(vocab_size, embedding_size,\n",
        "              batch_input_shape=(batch_size, None)),\n",
        "    \n",
        "    LSTM(len_input, return_sequences = True),\n",
        "    \n",
        "    Dense(hidden_size, activation = relu),\n",
        "    #Dropout(0.2),\n",
        "    \n",
        "    Dense(vocab_size)\n",
        "])\n",
        "RNN.summary()\n",
        "gen = Sequential([\n",
        "    Embedding(vocab_size, embedding_size,\n",
        "              batch_input_shape=(1, None)),\n",
        "    \n",
        "    LSTM(len_input, return_sequences = True, stateful=True),\n",
        "    \n",
        "    Dense(hidden_size, activation = relu), \n",
        "    \n",
        "    Dense(vocab_size)\n",
        "])\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
        "\n",
        "# This is an Autograph function\n",
        "# its decorator makes it a TF op - i.e. much faster\n",
        "@tf.function\n",
        "def train_on_batch(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predicted = RNN(x)\n",
        "        cross_entropy = tf.keras.losses.sparse_categorical_crossentropy(y, predicted, from_logits = True)\n",
        "        current_loss = tf.reduce_mean(cross_entropy)\n",
        "\n",
        "    gradients = tape.gradient(current_loss, RNN.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, RNN.trainable_variables))\n",
        "    return current_loss\n",
        "\n",
        "loss_history = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    start = time.time()\n",
        "    \n",
        "    for b in batches:\n",
        "        x, y = b[0], b[1]\n",
        "        x = np.asarray(x)\n",
        "        y = np.asarray(y)\n",
        "        x = np.expand_dims(x[:batch_size], axis=1)\n",
        "        y = np.expand_dims(y[:batch_size], axis=1)\n",
        "\n",
        "        current_loss = train_on_batch(x, y)\n",
        "        loss_history.append(current_loss)\n",
        "    \n",
        "    print(\"=========================== Epoch: {}.  \\t  Loss: {}  \\t  Time: {}ss\".format(\n",
        "        epoch, current_loss.numpy(), round(time.time()-start, 2)))\n",
        "    \n",
        "    if epoch % 10 == 9:\n",
        "        gen.set_weights(RNN.get_weights())\n",
        "        print(generate_text(init, gen))\n",
        "\n",
        "\n",
        "plt.plot(loss_history)\n",
        "plt.title(\"Training Loss\")\n",
        "plt.show()\n",
        "\n",
        "RNN.save(\"/lstm_dense.h5\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (225, None, 8)            14376     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (225, None, 100)          43600     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (225, None, 100)          10100     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (225, None, 1797)         181497    \n",
            "=================================================================\n",
            "Total params: 249,573\n",
            "Trainable params: 249,573\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "=========================== Epoch: 0.  \t  Loss: 3.0843076705932617  \t  Time: 11.1ss\n",
            "=========================== Epoch: 1.  \t  Loss: 2.914513349533081  \t  Time: 3.05ss\n",
            "=========================== Epoch: 2.  \t  Loss: 2.824671506881714  \t  Time: 2.98ss\n",
            "=========================== Epoch: 3.  \t  Loss: 2.7668709754943848  \t  Time: 2.97ss\n",
            "=========================== Epoch: 4.  \t  Loss: 2.7280209064483643  \t  Time: 3.14ss\n",
            "=========================== Epoch: 5.  \t  Loss: 2.6927919387817383  \t  Time: 3.01ss\n",
            "=========================== Epoch: 6.  \t  Loss: 2.660008192062378  \t  Time: 3.04ss\n",
            "=========================== Epoch: 7.  \t  Loss: 2.6400797367095947  \t  Time: 3.23ss\n",
            "=========================== Epoch: 8.  \t  Loss: 2.613805055618286  \t  Time: 3.13ss\n",
            "=========================== Epoch: 9.  \t  Loss: 2.5960333347320557  \t  Time: 3.08ss\n",
            "lo\n",
            "\n",
            " la  ghie  mia to la ma ma gna o riri\n",
            "\n",
            "\n",
            "che\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "   do so \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  ta \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "=========================== Epoch: 10.  \t  Loss: 2.57989764213562  \t  Time: 3.09ss\n",
            "=========================== Epoch: 11.  \t  Loss: 2.5669326782226562  \t  Time: 3.11ss\n",
            "=========================== Epoch: 12.  \t  Loss: 2.559751033782959  \t  Time: 3.09ss\n",
            "=========================== Epoch: 13.  \t  Loss: 2.5556373596191406  \t  Time: 2.92ss\n",
            "=========================== Epoch: 14.  \t  Loss: 2.5452778339385986  \t  Time: 2.96ss\n",
            "=========================== Epoch: 15.  \t  Loss: 2.5434086322784424  \t  Time: 3.09ss\n",
            "=========================== Epoch: 16.  \t  Loss: 2.541691541671753  \t  Time: 3.05ss\n",
            "=========================== Epoch: 17.  \t  Loss: 2.5391721725463867  \t  Time: 3.17ss\n",
            "=========================== Epoch: 18.  \t  Loss: 2.5421414375305176  \t  Time: 3.04ss\n",
            "=========================== Epoch: 19.  \t  Loss: 2.5425546169281006  \t  Time: 2.98ss\n",
            "pensegger  sea  nona cuca tà  en gresa a dimmia ma ma to meggei mer ode mo\n",
            "non\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "   pinrueïe scritto to  to\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  to \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  \n",
            "=========================== Epoch: 20.  \t  Loss: 2.542839527130127  \t  Time: 3.07ss\n",
            "=========================== Epoch: 21.  \t  Loss: 2.5414106845855713  \t  Time: 3.24ss\n",
            "=========================== Epoch: 22.  \t  Loss: 2.5339200496673584  \t  Time: 3.14ss\n",
            "=========================== Epoch: 23.  \t  Loss: 2.535457134246826  \t  Time: 3.08ss\n",
            "=========================== Epoch: 24.  \t  Loss: 2.532045841217041  \t  Time: 3.26ss\n",
            "=========================== Epoch: 25.  \t  Loss: 2.5281786918640137  \t  Time: 3.14ss\n",
            "=========================== Epoch: 26.  \t  Loss: 2.527005434036255  \t  Time: 3.0ss\n",
            "=========================== Epoch: 27.  \t  Loss: 2.5232393741607666  \t  Time: 3.18ss\n",
            "=========================== Epoch: 28.  \t  Loss: 2.520233392715454  \t  Time: 3.01ss\n",
            "=========================== Epoch: 29.  \t  Loss: 2.51751971244812  \t  Time: 2.97ss\n",
            "eta\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  smen gia quattu baviam to  antrovai \n",
            "na o\n",
            "va\n",
            "\n",
            "\n",
            "\n",
            "  fisagsegtritor gòn brio  ina ro\n",
            "\n",
            "se\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  ritii  pa \n",
            "=========================== Epoch: 30.  \t  Loss: 2.5142171382904053  \t  Time: 3.0ss\n",
            "=========================== Epoch: 31.  \t  Loss: 2.5106706619262695  \t  Time: 3.06ss\n",
            "=========================== Epoch: 32.  \t  Loss: 2.505542755126953  \t  Time: 3.02ss\n",
            "=========================== Epoch: 33.  \t  Loss: 2.50658917427063  \t  Time: 3.0ss\n",
            "=========================== Epoch: 34.  \t  Loss: 2.5040128231048584  \t  Time: 3.04ss\n",
            "=========================== Epoch: 35.  \t  Loss: 2.5050623416900635  \t  Time: 3.19ss\n",
            "=========================== Epoch: 36.  \t  Loss: 2.4972546100616455  \t  Time: 3.04ss\n",
            "=========================== Epoch: 37.  \t  Loss: 2.4997589588165283  \t  Time: 3.05ss\n",
            "=========================== Epoch: 38.  \t  Loss: 2.499483346939087  \t  Time: 3.16ss\n",
            "=========================== Epoch: 39.  \t  Loss: 2.4987313747406006  \t  Time: 3.08ss\n",
            "divoci\n",
            "\n",
            "\n",
            "\n",
            "per\n",
            "\n",
            "\n",
            "\n",
            "  regeva  no do \n",
            "dimo nandughenïanefva\n",
            "e\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                      \n",
            "=========================== Epoch: 40.  \t  Loss: 2.4922544956207275  \t  Time: 3.24ss\n",
            "=========================== Epoch: 41.  \t  Loss: 2.495131254196167  \t  Time: 3.31ss\n",
            "=========================== Epoch: 42.  \t  Loss: 2.4855234622955322  \t  Time: 3.37ss\n",
            "=========================== Epoch: 43.  \t  Loss: 2.4867985248565674  \t  Time: 3.15ss\n",
            "=========================== Epoch: 44.  \t  Loss: 2.483100414276123  \t  Time: 3.42ss\n",
            "=========================== Epoch: 45.  \t  Loss: 2.476627826690674  \t  Time: 3.12ss\n",
            "=========================== Epoch: 46.  \t  Loss: 2.4844720363616943  \t  Time: 3.14ss\n",
            "=========================== Epoch: 47.  \t  Loss: 2.478565216064453  \t  Time: 3.16ss\n",
            "=========================== Epoch: 48.  \t  Loss: 2.4742271900177  \t  Time: 3.31ss\n",
            "=========================== Epoch: 49.  \t  Loss: 2.4711837768554688  \t  Time: 3.26ss\n",
            "so\n",
            "\n",
            "                                                                                                 \n",
            "=========================== Epoch: 50.  \t  Loss: 2.4684395790100098  \t  Time: 3.17ss\n",
            "=========================== Epoch: 51.  \t  Loss: 2.478778600692749  \t  Time: 3.28ss\n",
            "=========================== Epoch: 52.  \t  Loss: 2.4669837951660156  \t  Time: 3.17ss\n",
            "=========================== Epoch: 53.  \t  Loss: 2.4679627418518066  \t  Time: 3.08ss\n",
            "=========================== Epoch: 54.  \t  Loss: 2.4613170623779297  \t  Time: 3.04ss\n",
            "=========================== Epoch: 55.  \t  Loss: 2.4596714973449707  \t  Time: 3.17ss\n",
            "=========================== Epoch: 56.  \t  Loss: 2.461620330810547  \t  Time: 3.26ss\n",
            "=========================== Epoch: 57.  \t  Loss: 2.4603567123413086  \t  Time: 3.32ss\n",
            "=========================== Epoch: 58.  \t  Loss: 2.4554200172424316  \t  Time: 3.12ss\n",
            "=========================== Epoch: 59.  \t  Loss: 2.4569664001464844  \t  Time: 3.03ss\n",
            "trasta re\n",
            "\n",
            "eta ne rito \n",
            "mamai men mal digrinanno so vò buccescai tromi no\n",
            " revivam mo mibrio  men lissea  \n",
            "po\n",
            "tro\n",
            "\n",
            "\n",
            "                                   \n",
            "=========================== Epoch: 60.  \t  Loss: 2.455092668533325  \t  Time: 3.34ss\n",
            "=========================== Epoch: 61.  \t  Loss: 2.4510574340820312  \t  Time: 3.09ss\n",
            "=========================== Epoch: 62.  \t  Loss: 2.4489288330078125  \t  Time: 3.14ss\n",
            "=========================== Epoch: 63.  \t  Loss: 2.447132110595703  \t  Time: 3.1ss\n",
            "=========================== Epoch: 64.  \t  Loss: 2.4474925994873047  \t  Time: 3.2ss\n",
            "=========================== Epoch: 65.  \t  Loss: 2.447385311126709  \t  Time: 3.14ss\n",
            "=========================== Epoch: 66.  \t  Loss: 2.444610834121704  \t  Time: 3.5ss\n",
            "=========================== Epoch: 67.  \t  Loss: 2.445202112197876  \t  Time: 3.56ss\n",
            "=========================== Epoch: 68.  \t  Loss: 2.4396350383758545  \t  Time: 3.44ss\n",
            "=========================== Epoch: 69.  \t  Loss: 2.441157341003418  \t  Time: 3.46ss\n",
            "e \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  brei\n",
            "\n",
            "\n",
            "\n",
            "                                                                                      \n",
            "=========================== Epoch: 70.  \t  Loss: 2.4420721530914307  \t  Time: 3.08ss\n",
            "=========================== Epoch: 71.  \t  Loss: 2.4468650817871094  \t  Time: 3.16ss\n",
            "=========================== Epoch: 72.  \t  Loss: 2.440308094024658  \t  Time: 3.38ss\n",
            "=========================== Epoch: 73.  \t  Loss: 2.4398298263549805  \t  Time: 3.11ss\n",
            "=========================== Epoch: 74.  \t  Loss: 2.440530300140381  \t  Time: 3.02ss\n",
            "=========================== Epoch: 75.  \t  Loss: 2.437167167663574  \t  Time: 3.19ss\n",
            "=========================== Epoch: 76.  \t  Loss: 2.4404587745666504  \t  Time: 3.24ss\n",
            "=========================== Epoch: 77.  \t  Loss: 2.4357755184173584  \t  Time: 3.0ss\n",
            "=========================== Epoch: 78.  \t  Loss: 2.440330743789673  \t  Time: 2.98ss\n",
            "=========================== Epoch: 79.  \t  Loss: 2.444028854370117  \t  Time: 3.07ss\n",
            "pe dissezion to bra no bra ra \n",
            "\n",
            "\n",
            "\n",
            "                                                                                \n",
            "=========================== Epoch: 80.  \t  Loss: 2.445382595062256  \t  Time: 3.42ss\n",
            "=========================== Epoch: 81.  \t  Loss: 2.4378933906555176  \t  Time: 2.99ss\n",
            "=========================== Epoch: 82.  \t  Loss: 2.440992593765259  \t  Time: 3.21ss\n",
            "=========================== Epoch: 83.  \t  Loss: 2.4406347274780273  \t  Time: 3.25ss\n",
            "=========================== Epoch: 84.  \t  Loss: 2.4417803287506104  \t  Time: 3.37ss\n",
            "=========================== Epoch: 85.  \t  Loss: 2.4403111934661865  \t  Time: 3.29ss\n",
            "=========================== Epoch: 86.  \t  Loss: 2.440011501312256  \t  Time: 3.24ss\n",
            "=========================== Epoch: 87.  \t  Loss: 2.4519708156585693  \t  Time: 2.99ss\n",
            "=========================== Epoch: 88.  \t  Loss: 2.4397504329681396  \t  Time: 3.27ss\n",
            "=========================== Epoch: 89.  \t  Loss: 2.4402365684509277  \t  Time: 3.21ss\n",
            "dipuo cu poe\n",
            "se polchègni còb e venli do ral drantir ce dïsogli volgigliai to se\n",
            "renni menstamíao \n",
            "cancusla stallipoda \n",
            "\n",
            "                                     \n",
            "=========================== Epoch: 90.  \t  Loss: 2.4357736110687256  \t  Time: 3.14ss\n",
            "=========================== Epoch: 91.  \t  Loss: 2.4398908615112305  \t  Time: 3.13ss\n",
            "=========================== Epoch: 92.  \t  Loss: 2.433716058731079  \t  Time: 3.09ss\n",
            "=========================== Epoch: 93.  \t  Loss: 2.4345648288726807  \t  Time: 3.16ss\n",
            "=========================== Epoch: 94.  \t  Loss: 2.4394309520721436  \t  Time: 3.36ss\n",
            "=========================== Epoch: 95.  \t  Loss: 2.441077470779419  \t  Time: 3.28ss\n",
            "=========================== Epoch: 96.  \t  Loss: 2.4344286918640137  \t  Time: 3.05ss\n",
            "=========================== Epoch: 97.  \t  Loss: 2.4423816204071045  \t  Time: 3.02ss\n",
            "=========================== Epoch: 98.  \t  Loss: 2.434905529022217  \t  Time: 3.09ss\n",
            "=========================== Epoch: 99.  \t  Loss: 2.4364352226257324  \t  Time: 3.28ss\n",
            "quindia pescendïa fer betorà a me ypto carva pri men mempiaò sragliai \n",
            "dito ado\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                      \n",
            "=========================== Epoch: 100.  \t  Loss: 2.4379587173461914  \t  Time: 3.1ss\n",
            "=========================== Epoch: 101.  \t  Loss: 2.4345543384552  \t  Time: 3.23ss\n",
            "=========================== Epoch: 102.  \t  Loss: 2.433358669281006  \t  Time: 3.15ss\n",
            "=========================== Epoch: 103.  \t  Loss: 2.4310433864593506  \t  Time: 3.17ss\n",
            "=========================== Epoch: 104.  \t  Loss: 2.4298057556152344  \t  Time: 3.11ss\n",
            "=========================== Epoch: 105.  \t  Loss: 2.435224771499634  \t  Time: 3.21ss\n",
            "=========================== Epoch: 106.  \t  Loss: 2.4284591674804688  \t  Time: 3.02ss\n",
            "=========================== Epoch: 107.  \t  Loss: 2.4278900623321533  \t  Time: 3.27ss\n",
            "=========================== Epoch: 108.  \t  Loss: 2.4262900352478027  \t  Time: 3.3ss\n",
            "=========================== Epoch: 109.  \t  Loss: 2.4294586181640625  \t  Time: 3.03ss\n",
            "e voredri ma\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                                        \n",
            "=========================== Epoch: 110.  \t  Loss: 2.4308316707611084  \t  Time: 3.08ss\n",
            "=========================== Epoch: 111.  \t  Loss: 2.426872730255127  \t  Time: 3.12ss\n",
            "=========================== Epoch: 112.  \t  Loss: 2.429861545562744  \t  Time: 3.32ss\n",
            "=========================== Epoch: 113.  \t  Loss: 2.4266161918640137  \t  Time: 3.12ss\n",
            "=========================== Epoch: 114.  \t  Loss: 2.4318087100982666  \t  Time: 3.24ss\n",
            "=========================== Epoch: 115.  \t  Loss: 2.4319007396698  \t  Time: 3.01ss\n",
            "=========================== Epoch: 116.  \t  Loss: 2.422994613647461  \t  Time: 2.98ss\n",
            "=========================== Epoch: 117.  \t  Loss: 2.424182891845703  \t  Time: 3.19ss\n",
            "=========================== Epoch: 118.  \t  Loss: 2.429150104522705  \t  Time: 3.17ss\n",
            "=========================== Epoch: 119.  \t  Loss: 2.432237148284912  \t  Time: 3.1ss\n",
            "e pargnuoli landodo darlo\n",
            " \n",
            "                                                                                     \n",
            "=========================== Epoch: 120.  \t  Loss: 2.4267988204956055  \t  Time: 2.98ss\n",
            "=========================== Epoch: 121.  \t  Loss: 2.432455539703369  \t  Time: 3.0ss\n",
            "=========================== Epoch: 122.  \t  Loss: 2.4267020225524902  \t  Time: 3.02ss\n",
            "=========================== Epoch: 123.  \t  Loss: 2.4246952533721924  \t  Time: 3.0ss\n",
            "=========================== Epoch: 124.  \t  Loss: 2.428483009338379  \t  Time: 3.27ss\n",
            "=========================== Epoch: 125.  \t  Loss: 2.4306554794311523  \t  Time: 3.06ss\n",
            "=========================== Epoch: 126.  \t  Loss: 2.426630973815918  \t  Time: 3.15ss\n",
            "=========================== Epoch: 127.  \t  Loss: 2.4186716079711914  \t  Time: 3.05ss\n",
            "=========================== Epoch: 128.  \t  Loss: 2.4228298664093018  \t  Time: 2.91ss\n",
            "=========================== Epoch: 129.  \t  Loss: 2.4372239112854004  \t  Time: 3.01ss\n",
            "logno\n",
            "le\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                                    \n",
            "=========================== Epoch: 130.  \t  Loss: 2.425384759902954  \t  Time: 2.93ss\n",
            "=========================== Epoch: 131.  \t  Loss: 2.429771661758423  \t  Time: 3.06ss\n",
            "=========================== Epoch: 132.  \t  Loss: 2.4276633262634277  \t  Time: 3.01ss\n",
            "=========================== Epoch: 133.  \t  Loss: 2.4316437244415283  \t  Time: 3.15ss\n",
            "=========================== Epoch: 134.  \t  Loss: 2.432955741882324  \t  Time: 3.16ss\n",
            "=========================== Epoch: 135.  \t  Loss: 2.4285688400268555  \t  Time: 2.89ss\n",
            "=========================== Epoch: 136.  \t  Loss: 2.4309275150299072  \t  Time: 3.1ss\n",
            "=========================== Epoch: 137.  \t  Loss: 2.435757637023926  \t  Time: 3.24ss\n",
            "=========================== Epoch: 138.  \t  Loss: 2.428196668624878  \t  Time: 3.19ss\n",
            "=========================== Epoch: 139.  \t  Loss: 2.440581798553467  \t  Time: 3.39ss\n",
            "non mil con scossozie \n",
            "\n",
            "\n",
            "che \n",
            "                                                                                    \n",
            "=========================== Epoch: 140.  \t  Loss: 2.4307730197906494  \t  Time: 3.24ss\n",
            "=========================== Epoch: 141.  \t  Loss: 2.428562641143799  \t  Time: 3.12ss\n",
            "=========================== Epoch: 142.  \t  Loss: 2.4334967136383057  \t  Time: 3.38ss\n",
            "=========================== Epoch: 143.  \t  Loss: 2.4416146278381348  \t  Time: 3.27ss\n",
            "=========================== Epoch: 144.  \t  Loss: 2.428659439086914  \t  Time: 3.31ss\n",
            "=========================== Epoch: 145.  \t  Loss: 2.4265565872192383  \t  Time: 3.3ss\n",
            "=========================== Epoch: 146.  \t  Loss: 2.4297544956207275  \t  Time: 3.12ss\n",
            "=========================== Epoch: 147.  \t  Loss: 2.4307785034179688  \t  Time: 3.25ss\n",
            "=========================== Epoch: 148.  \t  Loss: 2.4298248291015625  \t  Time: 3.17ss\n",
            "=========================== Epoch: 149.  \t  Loss: 2.4292349815368652  \t  Time: 3.16ss\n",
            "di \n",
            "\n",
            "\n",
            "                                                                                               \n",
            "=========================== Epoch: 150.  \t  Loss: 2.429166316986084  \t  Time: 3.15ss\n",
            "=========================== Epoch: 151.  \t  Loss: 2.431729316711426  \t  Time: 3.18ss\n",
            "=========================== Epoch: 152.  \t  Loss: 2.439295768737793  \t  Time: 3.13ss\n",
            "=========================== Epoch: 153.  \t  Loss: 2.4324793815612793  \t  Time: 3.07ss\n",
            "=========================== Epoch: 154.  \t  Loss: 2.4306721687316895  \t  Time: 3.18ss\n",
            "=========================== Epoch: 155.  \t  Loss: 2.4346208572387695  \t  Time: 3.26ss\n",
            "=========================== Epoch: 156.  \t  Loss: 2.425922393798828  \t  Time: 3.11ss\n",
            "=========================== Epoch: 157.  \t  Loss: 2.4332683086395264  \t  Time: 2.97ss\n",
            "=========================== Epoch: 158.  \t  Loss: 2.44118332862854  \t  Time: 3.06ss\n",
            "=========================== Epoch: 159.  \t  Loss: 2.4373087882995605  \t  Time: 3.05ss\n",
            "e\n",
            "vera\n",
            "\n",
            "\n",
            "\n",
            "co\n",
            "\n",
            "\n",
            "per che \n",
            "per che \n",
            "\n",
            "eïei \n",
            "\n",
            "\n",
            "\n",
            "e \n",
            "                                                                  \n",
            "=========================== Epoch: 160.  \t  Loss: 2.428378105163574  \t  Time: 3.17ss\n",
            "=========================== Epoch: 161.  \t  Loss: 2.4296228885650635  \t  Time: 3.26ss\n",
            "=========================== Epoch: 162.  \t  Loss: 2.444528341293335  \t  Time: 3.18ss\n",
            "=========================== Epoch: 163.  \t  Loss: 2.4243245124816895  \t  Time: 3.19ss\n",
            "=========================== Epoch: 164.  \t  Loss: 2.422588348388672  \t  Time: 3.25ss\n",
            "=========================== Epoch: 165.  \t  Loss: 2.437426805496216  \t  Time: 3.19ss\n",
            "=========================== Epoch: 166.  \t  Loss: 2.4290456771850586  \t  Time: 3.29ss\n",
            "=========================== Epoch: 167.  \t  Loss: 2.4252963066101074  \t  Time: 3.4ss\n",
            "=========================== Epoch: 168.  \t  Loss: 2.4309043884277344  \t  Time: 3.15ss\n",
            "=========================== Epoch: 169.  \t  Loss: 2.4313669204711914  \t  Time: 3.08ss\n",
            "felo\n",
            "di \n",
            "tal ma da \n",
            "\n",
            "\n",
            "che \n",
            "e \n",
            "cosí si chi \n",
            "\n",
            "tu \n",
            "                                                                   \n",
            "=========================== Epoch: 170.  \t  Loss: 2.444002628326416  \t  Time: 3.05ss\n",
            "=========================== Epoch: 171.  \t  Loss: 2.4292619228363037  \t  Time: 3.07ss\n",
            "=========================== Epoch: 172.  \t  Loss: 2.4347729682922363  \t  Time: 3.19ss\n",
            "=========================== Epoch: 173.  \t  Loss: 2.428720235824585  \t  Time: 3.02ss\n",
            "=========================== Epoch: 174.  \t  Loss: 2.4292776584625244  \t  Time: 2.94ss\n",
            "=========================== Epoch: 175.  \t  Loss: 2.425703763961792  \t  Time: 2.92ss\n",
            "=========================== Epoch: 176.  \t  Loss: 2.439622402191162  \t  Time: 3.06ss\n",
            "=========================== Epoch: 177.  \t  Loss: 2.426715850830078  \t  Time: 3.06ss\n",
            "=========================== Epoch: 178.  \t  Loss: 2.4355623722076416  \t  Time: 2.94ss\n",
            "=========================== Epoch: 179.  \t  Loss: 2.4349231719970703  \t  Time: 3.14ss\n",
            "talta dicra \n",
            "quanivepibent bebent smi zia\n",
            "\n",
            "che stra to \n",
            "                                                                        \n",
            "=========================== Epoch: 180.  \t  Loss: 2.43072772026062  \t  Time: 3.2ss\n",
            "=========================== Epoch: 181.  \t  Loss: 2.430185317993164  \t  Time: 2.94ss\n",
            "=========================== Epoch: 182.  \t  Loss: 2.428382158279419  \t  Time: 2.93ss\n",
            "=========================== Epoch: 183.  \t  Loss: 2.431065797805786  \t  Time: 3.15ss\n",
            "=========================== Epoch: 184.  \t  Loss: 2.4282703399658203  \t  Time: 2.94ss\n",
            "=========================== Epoch: 185.  \t  Loss: 2.429663896560669  \t  Time: 2.94ss\n",
            "=========================== Epoch: 186.  \t  Loss: 2.4321937561035156  \t  Time: 3.2ss\n",
            "=========================== Epoch: 187.  \t  Loss: 2.4281563758850098  \t  Time: 3.07ss\n",
            "=========================== Epoch: 188.  \t  Loss: 2.4351654052734375  \t  Time: 2.93ss\n",
            "=========================== Epoch: 189.  \t  Loss: 2.429840326309204  \t  Time: 2.92ss\n",
            "na che no \n",
            "\n",
            "per purgar \n",
            "\n",
            "                                                                                     \n",
            "=========================== Epoch: 190.  \t  Loss: 2.4366135597229004  \t  Time: 2.97ss\n",
            "=========================== Epoch: 191.  \t  Loss: 2.433955669403076  \t  Time: 3.07ss\n",
            "=========================== Epoch: 192.  \t  Loss: 2.4373679161071777  \t  Time: 3.16ss\n",
            "=========================== Epoch: 193.  \t  Loss: 2.428154230117798  \t  Time: 3.14ss\n",
            "=========================== Epoch: 194.  \t  Loss: 2.4266419410705566  \t  Time: 3.0ss\n",
            "=========================== Epoch: 195.  \t  Loss: 2.432345390319824  \t  Time: 3.06ss\n",
            "=========================== Epoch: 196.  \t  Loss: 2.4324774742126465  \t  Time: 2.96ss\n",
            "=========================== Epoch: 197.  \t  Loss: 2.4357783794403076  \t  Time: 2.9ss\n",
            "=========================== Epoch: 198.  \t  Loss: 2.431752920150757  \t  Time: 2.95ss\n",
            "=========================== Epoch: 199.  \t  Loss: 2.4311306476593018  \t  Time: 2.95ss\n",
            "quandel lauda denpon forzia mesmargua glia le \n",
            "di\n",
            "\n",
            "  mi flegettissi ror nusi lenzie rezla \n",
            "                                                       \n",
            "=========================== Epoch: 200.  \t  Loss: 2.432943820953369  \t  Time: 3.12ss\n",
            "=========================== Epoch: 201.  \t  Loss: 2.4322564601898193  \t  Time: 3.26ss\n",
            "=========================== Epoch: 202.  \t  Loss: 2.440375328063965  \t  Time: 3.04ss\n",
            "=========================== Epoch: 203.  \t  Loss: 2.4337780475616455  \t  Time: 3.13ss\n",
            "=========================== Epoch: 204.  \t  Loss: 2.431861162185669  \t  Time: 3.08ss\n",
            "=========================== Epoch: 205.  \t  Loss: 2.4296817779541016  \t  Time: 3.15ss\n",
            "=========================== Epoch: 206.  \t  Loss: 2.427027463912964  \t  Time: 3.0ss\n",
            "=========================== Epoch: 207.  \t  Loss: 2.433953285217285  \t  Time: 3.19ss\n",
            "=========================== Epoch: 208.  \t  Loss: 2.4337165355682373  \t  Time: 3.11ss\n",
            "=========================== Epoch: 209.  \t  Loss: 2.428557872772217  \t  Time: 2.93ss\n",
            "que che che le\n",
            "per nete nuirio nà ni trabent so \n",
            "\n",
            "e                                                                       \n",
            "=========================== Epoch: 210.  \t  Loss: 2.4367058277130127  \t  Time: 3.02ss\n",
            "=========================== Epoch: 211.  \t  Loss: 2.4288294315338135  \t  Time: 2.92ss\n",
            "=========================== Epoch: 212.  \t  Loss: 2.430293560028076  \t  Time: 3.07ss\n",
            "=========================== Epoch: 213.  \t  Loss: 2.430655002593994  \t  Time: 3.03ss\n",
            "=========================== Epoch: 214.  \t  Loss: 2.4335687160491943  \t  Time: 2.95ss\n",
            "=========================== Epoch: 215.  \t  Loss: 2.4266884326934814  \t  Time: 3.17ss\n",
            "=========================== Epoch: 216.  \t  Loss: 2.4286179542541504  \t  Time: 3.12ss\n",
            "=========================== Epoch: 217.  \t  Loss: 2.4265806674957275  \t  Time: 2.91ss\n",
            "=========================== Epoch: 218.  \t  Loss: 2.4325194358825684  \t  Time: 2.98ss\n",
            "=========================== Epoch: 219.  \t  Loss: 2.4294002056121826  \t  Time: 3.12ss\n",
            "noi ma lor dissiepevilribi\n",
            "cau\n",
            "                                                                                     \n",
            "=========================== Epoch: 220.  \t  Loss: 2.424083948135376  \t  Time: 3.11ss\n",
            "=========================== Epoch: 221.  \t  Loss: 2.4277000427246094  \t  Time: 2.91ss\n",
            "=========================== Epoch: 222.  \t  Loss: 2.427565336227417  \t  Time: 3.02ss\n",
            "=========================== Epoch: 223.  \t  Loss: 2.4319469928741455  \t  Time: 2.87ss\n",
            "=========================== Epoch: 224.  \t  Loss: 2.434891939163208  \t  Time: 3.13ss\n",
            "=========================== Epoch: 225.  \t  Loss: 2.429877281188965  \t  Time: 2.96ss\n",
            "=========================== Epoch: 226.  \t  Loss: 2.4315598011016846  \t  Time: 3.09ss\n",
            "=========================== Epoch: 227.  \t  Loss: 2.4345579147338867  \t  Time: 3.15ss\n",
            "=========================== Epoch: 228.  \t  Loss: 2.430121898651123  \t  Time: 3.03ss\n",
            "=========================== Epoch: 229.  \t  Loss: 2.43509578704834  \t  Time: 2.93ss\n",
            "comanve che lobe to nisranrò relcarío gno anita ti lo quangidaci di\n",
            "\n",
            "\n",
            "                                                             \n",
            "=========================== Epoch: 230.  \t  Loss: 2.430858850479126  \t  Time: 3.04ss\n",
            "=========================== Epoch: 231.  \t  Loss: 2.4308359622955322  \t  Time: 2.96ss\n",
            "=========================== Epoch: 232.  \t  Loss: 2.428713321685791  \t  Time: 2.89ss\n",
            "=========================== Epoch: 233.  \t  Loss: 2.431083917617798  \t  Time: 2.94ss\n",
            "=========================== Epoch: 234.  \t  Loss: 2.4332003593444824  \t  Time: 2.93ss\n",
            "=========================== Epoch: 235.  \t  Loss: 2.4342358112335205  \t  Time: 3.11ss\n",
            "=========================== Epoch: 236.  \t  Loss: 2.4293479919433594  \t  Time: 2.9ss\n",
            "=========================== Epoch: 237.  \t  Loss: 2.4320695400238037  \t  Time: 2.95ss\n",
            "=========================== Epoch: 238.  \t  Loss: 2.4332499504089355  \t  Time: 2.9ss\n",
            "=========================== Epoch: 239.  \t  Loss: 2.433058977127075  \t  Time: 3.02ss\n",
            "qual giee naran asmonli cade \n",
            "\n",
            "\n",
            "                                                                                  \n",
            "=========================== Epoch: 240.  \t  Loss: 2.4328529834747314  \t  Time: 3.16ss\n",
            "=========================== Epoch: 241.  \t  Loss: 2.4330039024353027  \t  Time: 2.95ss\n",
            "=========================== Epoch: 242.  \t  Loss: 2.4330265522003174  \t  Time: 2.91ss\n",
            "=========================== Epoch: 243.  \t  Loss: 2.4339377880096436  \t  Time: 2.89ss\n",
            "=========================== Epoch: 244.  \t  Loss: 2.4349136352539062  \t  Time: 3.02ss\n",
            "=========================== Epoch: 245.  \t  Loss: 2.431471347808838  \t  Time: 2.93ss\n",
            "=========================== Epoch: 246.  \t  Loss: 2.431403636932373  \t  Time: 3.04ss\n",
            "=========================== Epoch: 247.  \t  Loss: 2.4297099113464355  \t  Time: 3.19ss\n",
            "=========================== Epoch: 248.  \t  Loss: 2.4304282665252686  \t  Time: 3.15ss\n",
            "=========================== Epoch: 249.  \t  Loss: 2.428596019744873  \t  Time: 3.08ss\n",
            "voi pe c zo me tro veberebcet fo vòlto\n",
            "re\n",
            "me ch se\n",
            "togi scoglio \n",
            "per sonloda bil che per poi poscia \n",
            "cose \n",
            "\n",
            "\n",
            "\n",
            "che cre che \n",
            "\n",
            "\n",
            "e po\n",
            "\n",
            "\n",
            "\n",
            "dito e e le \n",
            "\n",
            "che ta\n",
            "\n",
            "sí canlar spe\n",
            "=========================== Epoch: 250.  \t  Loss: 2.4307096004486084  \t  Time: 2.92ss\n",
            "=========================== Epoch: 251.  \t  Loss: 2.4300851821899414  \t  Time: 3.2ss\n",
            "=========================== Epoch: 252.  \t  Loss: 2.431363344192505  \t  Time: 3.02ss\n",
            "=========================== Epoch: 253.  \t  Loss: 2.4343974590301514  \t  Time: 2.91ss\n",
            "=========================== Epoch: 254.  \t  Loss: 2.4283902645111084  \t  Time: 3.08ss\n",
            "=========================== Epoch: 255.  \t  Loss: 2.431915044784546  \t  Time: 3.21ss\n",
            "=========================== Epoch: 256.  \t  Loss: 2.425480604171753  \t  Time: 2.89ss\n",
            "=========================== Epoch: 257.  \t  Loss: 2.4253597259521484  \t  Time: 2.97ss\n",
            "=========================== Epoch: 258.  \t  Loss: 2.4332470893859863  \t  Time: 2.93ss\n",
            "=========================== Epoch: 259.  \t  Loss: 2.427579641342163  \t  Time: 2.95ss\n",
            "o ragprese ed  dell mís   valorgne\n",
            "spondal sarcis ma disno si lo non beabiet liooco dïse \n",
            "\n",
            "\n",
            "perta per dell ra bro \n",
            "\n",
            "\n",
            "che i eïe oravani \n",
            "\n",
            "\n",
            "che \n",
            "che e lo pulgiabini\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "e  \n",
            "=========================== Epoch: 260.  \t  Loss: 2.429316997528076  \t  Time: 2.95ss\n",
            "=========================== Epoch: 261.  \t  Loss: 2.4278757572174072  \t  Time: 2.94ss\n",
            "=========================== Epoch: 262.  \t  Loss: 2.4282805919647217  \t  Time: 2.99ss\n",
            "=========================== Epoch: 263.  \t  Loss: 2.4267261028289795  \t  Time: 3.27ss\n",
            "=========================== Epoch: 264.  \t  Loss: 2.427126169204712  \t  Time: 3.21ss\n",
            "=========================== Epoch: 265.  \t  Loss: 2.4268109798431396  \t  Time: 3.24ss\n",
            "=========================== Epoch: 266.  \t  Loss: 2.4266793727874756  \t  Time: 3.32ss\n",
            "=========================== Epoch: 267.  \t  Loss: 2.424027681350708  \t  Time: 3.07ss\n",
            "=========================== Epoch: 268.  \t  Loss: 2.4220597743988037  \t  Time: 2.97ss\n",
            "=========================== Epoch: 269.  \t  Loss: 2.42099928855896  \t  Time: 3.33ss\n",
            "ro poi disfatvedici poltra \n",
            "\n",
            "\n",
            "\n",
            "e per \n",
            "che \n",
            "lava \n",
            "to\n",
            "sí mi \n",
            "co\n",
            "\n",
            "che se\n",
            "peche \n",
            "sí che beran vo a primò gne gasqueé notmo én qua gni le se vecrerar che lesto\n",
            "\n",
            "\n",
            "per poi\n",
            "\n",
            "\n",
            "li che\n",
            "=========================== Epoch: 270.  \t  Loss: 2.4244744777679443  \t  Time: 3.17ss\n",
            "=========================== Epoch: 271.  \t  Loss: 2.420311689376831  \t  Time: 3.34ss\n",
            "=========================== Epoch: 272.  \t  Loss: 2.4199061393737793  \t  Time: 2.99ss\n",
            "=========================== Epoch: 273.  \t  Loss: 2.413724660873413  \t  Time: 3.05ss\n",
            "=========================== Epoch: 274.  \t  Loss: 2.4193406105041504  \t  Time: 3.29ss\n",
            "=========================== Epoch: 275.  \t  Loss: 2.416533946990967  \t  Time: 3.36ss\n",
            "=========================== Epoch: 276.  \t  Loss: 2.4139416217803955  \t  Time: 2.95ss\n",
            "=========================== Epoch: 277.  \t  Loss: 2.4171018600463867  \t  Time: 3.19ss\n",
            "=========================== Epoch: 278.  \t  Loss: 2.4143054485321045  \t  Time: 3.17ss\n",
            "=========================== Epoch: 279.  \t  Loss: 2.4154491424560547  \t  Time: 2.98ss\n",
            "io creè sò scíondi fia smaciglio\n",
            "\n",
            "sí co che se\n",
            "\n",
            "\n",
            "lato\n",
            "\n",
            "\n",
            "che per che ranstro bro vrar rot cese  \n",
            "ch che zïolo lo da dí se co chi se credí  panet tanto stravavan grestaro grazie gio di \n",
            "=========================== Epoch: 280.  \t  Loss: 2.411578416824341  \t  Time: 3.07ss\n",
            "=========================== Epoch: 281.  \t  Loss: 2.4156737327575684  \t  Time: 3.05ss\n",
            "=========================== Epoch: 282.  \t  Loss: 2.4122650623321533  \t  Time: 2.96ss\n",
            "=========================== Epoch: 283.  \t  Loss: 2.4080281257629395  \t  Time: 3.32ss\n",
            "=========================== Epoch: 284.  \t  Loss: 2.412205934524536  \t  Time: 3.08ss\n",
            "=========================== Epoch: 285.  \t  Loss: 2.409806966781616  \t  Time: 3.2ss\n",
            "=========================== Epoch: 286.  \t  Loss: 2.4132871627807617  \t  Time: 3.26ss\n",
            "=========================== Epoch: 287.  \t  Loss: 2.4111263751983643  \t  Time: 3.08ss\n",
            "=========================== Epoch: 288.  \t  Loss: 2.4176454544067383  \t  Time: 3.22ss\n",
            "=========================== Epoch: 289.  \t  Loss: 2.4092843532562256  \t  Time: 3.09ss\n",
            "io pur all dimi quando èl io giabiet sei mettaringroscacoasti sí sraella le qui ralramila geglie nosche\n",
            "tanzi sme tri refigda\n",
            "davan nimmo ralvicidinio lo\n",
            "\n",
            "\n",
            "\n",
            "mo \n",
            "\n",
            "e \n",
            "avesí rà zïse a  galnot\n",
            "=========================== Epoch: 290.  \t  Loss: 2.41268253326416  \t  Time: 3.24ss\n",
            "=========================== Epoch: 291.  \t  Loss: 2.4117746353149414  \t  Time: 3.04ss\n",
            "=========================== Epoch: 292.  \t  Loss: 2.4072301387786865  \t  Time: 3.13ss\n",
            "=========================== Epoch: 293.  \t  Loss: 2.404939889907837  \t  Time: 3.11ss\n",
            "=========================== Epoch: 294.  \t  Loss: 2.408099412918091  \t  Time: 3.07ss\n",
            "=========================== Epoch: 295.  \t  Loss: 2.407844305038452  \t  Time: 3.07ss\n",
            "=========================== Epoch: 296.  \t  Loss: 2.410540819168091  \t  Time: 3.12ss\n",
            "=========================== Epoch: 297.  \t  Loss: 2.4076414108276367  \t  Time: 3.05ss\n",
            "=========================== Epoch: 298.  \t  Loss: 2.4096322059631348  \t  Time: 3.04ss\n",
            "=========================== Epoch: 299.  \t  Loss: 2.404479742050171  \t  Time: 3.19ss\n",
            "mentr sciutesi che le \n",
            "e con \n",
            "\n",
            "pobròt che pepem giula stïpriranne giungue drà vò zie qua \n",
            "da didete dimen mum giú svelodïdiose í  acpassassozïo\n",
            "nelïetarvi dí di né tin cilla pepem pularrio fabpio \n",
            "=========================== Epoch: 300.  \t  Loss: 2.4079647064208984  \t  Time: 3.35ss\n",
            "=========================== Epoch: 301.  \t  Loss: 2.407623767852783  \t  Time: 3.29ss\n",
            "=========================== Epoch: 302.  \t  Loss: 2.411410331726074  \t  Time: 3.09ss\n",
            "=========================== Epoch: 303.  \t  Loss: 2.4132137298583984  \t  Time: 3.29ss\n",
            "=========================== Epoch: 304.  \t  Loss: 2.415923595428467  \t  Time: 3.52ss\n",
            "=========================== Epoch: 305.  \t  Loss: 2.4133081436157227  \t  Time: 3.15ss\n",
            "=========================== Epoch: 306.  \t  Loss: 2.413813591003418  \t  Time: 3.3ss\n",
            "=========================== Epoch: 307.  \t  Loss: 2.411574125289917  \t  Time: 3.26ss\n",
            "=========================== Epoch: 308.  \t  Loss: 2.4144468307495117  \t  Time: 3.04ss\n",
            "=========================== Epoch: 309.  \t  Loss: 2.405970335006714  \t  Time: 3.0ss\n",
            "pore ma \n",
            "\n",
            "e lognestre che sion tanvinal gozzo prezzatïran men il dí gar la a                                                            \n",
            "=========================== Epoch: 310.  \t  Loss: 2.414437770843506  \t  Time: 3.3ss\n",
            "=========================== Epoch: 311.  \t  Loss: 2.4131250381469727  \t  Time: 3.23ss\n",
            "=========================== Epoch: 312.  \t  Loss: 2.4146220684051514  \t  Time: 3.2ss\n",
            "=========================== Epoch: 313.  \t  Loss: 2.4107160568237305  \t  Time: 3.2ss\n",
            "=========================== Epoch: 314.  \t  Loss: 2.4085097312927246  \t  Time: 3.25ss\n",
            "=========================== Epoch: 315.  \t  Loss: 2.4149725437164307  \t  Time: 3.09ss\n",
            "=========================== Epoch: 316.  \t  Loss: 2.410587787628174  \t  Time: 3.18ss\n",
            "=========================== Epoch: 317.  \t  Loss: 2.411273717880249  \t  Time: 3.05ss\n",
            "=========================== Epoch: 318.  \t  Loss: 2.4127631187438965  \t  Time: 3.21ss\n",
            "=========================== Epoch: 319.  \t  Loss: 2.4130795001983643  \t  Time: 3.27ss\n",
            "e fientro dígili costragiavera\n",
            "cheva ral  da\n",
            "\n",
            "per amonigri                                                                     \n",
            "=========================== Epoch: 320.  \t  Loss: 2.4146828651428223  \t  Time: 3.25ss\n",
            "=========================== Epoch: 321.  \t  Loss: 2.4116861820220947  \t  Time: 3.32ss\n",
            "=========================== Epoch: 322.  \t  Loss: 2.4118895530700684  \t  Time: 3.21ss\n",
            "=========================== Epoch: 323.  \t  Loss: 2.4161767959594727  \t  Time: 3.21ss\n",
            "=========================== Epoch: 324.  \t  Loss: 2.4174234867095947  \t  Time: 3.17ss\n",
            "=========================== Epoch: 325.  \t  Loss: 2.4177770614624023  \t  Time: 3.19ss\n",
            "=========================== Epoch: 326.  \t  Loss: 2.4176666736602783  \t  Time: 3.05ss\n",
            "=========================== Epoch: 327.  \t  Loss: 2.41654896736145  \t  Time: 3.07ss\n",
            "=========================== Epoch: 328.  \t  Loss: 2.419100522994995  \t  Time: 3.06ss\n",
            "=========================== Epoch: 329.  \t  Loss: 2.41892147064209  \t  Time: 3.19ss\n",
            "itperché ti ò ov gelolo ramo ner cingozsegnoran vrebb \n",
            "sí per par fui schio cleralvadio nisranle tier rata\n",
            "\n",
            "\n",
            "lo\n",
            "che po se perdauiral toreba robbile gïunt nule pristri ma va\n",
            "lovi\n",
            "colo  \n",
            "\n",
            "   \n",
            "=========================== Epoch: 330.  \t  Loss: 2.4154231548309326  \t  Time: 3.1ss\n",
            "=========================== Epoch: 331.  \t  Loss: 2.4162814617156982  \t  Time: 3.06ss\n",
            "=========================== Epoch: 332.  \t  Loss: 2.4126930236816406  \t  Time: 3.09ss\n",
            "=========================== Epoch: 333.  \t  Loss: 2.4233193397521973  \t  Time: 3.28ss\n",
            "=========================== Epoch: 334.  \t  Loss: 2.4164421558380127  \t  Time: 3.24ss\n",
            "=========================== Epoch: 335.  \t  Loss: 2.4173953533172607  \t  Time: 3.02ss\n",
            "=========================== Epoch: 336.  \t  Loss: 2.4232068061828613  \t  Time: 3.19ss\n",
            "=========================== Epoch: 337.  \t  Loss: 2.422147274017334  \t  Time: 3.02ss\n",
            "=========================== Epoch: 338.  \t  Loss: 2.4200847148895264  \t  Time: 3.35ss\n",
            "=========================== Epoch: 339.  \t  Loss: 2.425495147705078  \t  Time: 3.25ss\n",
            "e e bollor  tra èl querum genniadio rar volt nocchio nimmo qua àr vu ghiavantï no\n",
            "talava otla cogni reztrimentole scicciroma vesfichi\n",
            "cerguatacerqua sun quel pille\n",
            "\n",
            "per vomen sé stammodio to vo partu nacchi\n",
            "=========================== Epoch: 340.  \t  Loss: 2.4278674125671387  \t  Time: 3.22ss\n",
            "=========================== Epoch: 341.  \t  Loss: 2.41921067237854  \t  Time: 3.06ss\n",
            "=========================== Epoch: 342.  \t  Loss: 2.4207746982574463  \t  Time: 3.1ss\n",
            "=========================== Epoch: 343.  \t  Loss: 2.422194004058838  \t  Time: 3.11ss\n",
            "=========================== Epoch: 344.  \t  Loss: 2.4229328632354736  \t  Time: 3.16ss\n",
            "=========================== Epoch: 345.  \t  Loss: 2.42691707611084  \t  Time: 3.1ss\n",
            "=========================== Epoch: 346.  \t  Loss: 2.4195706844329834  \t  Time: 3.15ss\n",
            "=========================== Epoch: 347.  \t  Loss: 2.42736554145813  \t  Time: 3.2ss\n",
            "=========================== Epoch: 348.  \t  Loss: 2.425827741622925  \t  Time: 3.19ss\n",
            "=========================== Epoch: 349.  \t  Loss: 2.4208006858825684  \t  Time: 3.12ss\n",
            "oh precigna\n",
            "\n",
            "sí le torstosche so\n",
            "sí ond oconscrizïatar go è ri \n",
            "                                                                 \n",
            "=========================== Epoch: 350.  \t  Loss: 2.4243810176849365  \t  Time: 3.2ss\n",
            "=========================== Epoch: 351.  \t  Loss: 2.4240803718566895  \t  Time: 3.18ss\n",
            "=========================== Epoch: 352.  \t  Loss: 2.4203569889068604  \t  Time: 3.18ss\n",
            "=========================== Epoch: 353.  \t  Loss: 2.417111873626709  \t  Time: 3.11ss\n",
            "=========================== Epoch: 354.  \t  Loss: 2.4232094287872314  \t  Time: 3.07ss\n",
            "=========================== Epoch: 355.  \t  Loss: 2.420408248901367  \t  Time: 3.13ss\n",
            "=========================== Epoch: 356.  \t  Loss: 2.417675495147705  \t  Time: 3.04ss\n",
            "=========================== Epoch: 357.  \t  Loss: 2.4236438274383545  \t  Time: 3.06ss\n",
            "=========================== Epoch: 358.  \t  Loss: 2.4190168380737305  \t  Time: 3.0ss\n",
            "=========================== Epoch: 359.  \t  Loss: 2.4191324710845947  \t  Time: 3.18ss\n",
            "ed roat dellodicarlo giuminorlenni dischespecivatà glia barnazïstoran desen lisde tra íoraldo bilvabietve cis lovien ma loglio at pocchiodi\n",
            "ocondaui re via a all ma mian rïa sai rinstar di mai forgni gnemmo\n",
            "=========================== Epoch: 360.  \t  Loss: 2.4269003868103027  \t  Time: 3.56ss\n",
            "=========================== Epoch: 361.  \t  Loss: 2.419325351715088  \t  Time: 3.48ss\n",
            "=========================== Epoch: 362.  \t  Loss: 2.4187653064727783  \t  Time: 3.43ss\n",
            "=========================== Epoch: 363.  \t  Loss: 2.4141628742218018  \t  Time: 3.25ss\n",
            "=========================== Epoch: 364.  \t  Loss: 2.42085862159729  \t  Time: 3.05ss\n",
            "=========================== Epoch: 365.  \t  Loss: 2.4205641746520996  \t  Time: 2.99ss\n",
            "=========================== Epoch: 366.  \t  Loss: 2.4233570098876953  \t  Time: 3.03ss\n",
            "=========================== Epoch: 367.  \t  Loss: 2.421619415283203  \t  Time: 3.02ss\n",
            "=========================== Epoch: 368.  \t  Loss: 2.4237709045410156  \t  Time: 3.21ss\n",
            "=========================== Epoch: 369.  \t  Loss: 2.416116237640381  \t  Time: 3.12ss\n",
            "noia tangiunteso strasghi ron nei i don relchifilorgna\n",
            "nar ga\n",
            "\n",
            "\n",
            "di\n",
            "che metdottiluvee denle\n",
            "calsfioran lir ma ste tier e unt minor òn ypto do zion mozder ler nir tim nattigliorar cè scisrem bandemmo cu    \n",
            "=========================== Epoch: 370.  \t  Loss: 2.414520502090454  \t  Time: 3.13ss\n",
            "=========================== Epoch: 371.  \t  Loss: 2.425550937652588  \t  Time: 3.2ss\n",
            "=========================== Epoch: 372.  \t  Loss: 2.419882297515869  \t  Time: 3.2ss\n",
            "=========================== Epoch: 373.  \t  Loss: 2.418630599975586  \t  Time: 3.31ss\n",
            "=========================== Epoch: 374.  \t  Loss: 2.424501419067383  \t  Time: 3.11ss\n",
            "=========================== Epoch: 375.  \t  Loss: 2.424940347671509  \t  Time: 3.01ss\n",
            "=========================== Epoch: 376.  \t  Loss: 2.4209983348846436  \t  Time: 2.99ss\n",
            "=========================== Epoch: 377.  \t  Loss: 2.4171433448791504  \t  Time: 3.05ss\n",
            "=========================== Epoch: 378.  \t  Loss: 2.416879892349243  \t  Time: 3.22ss\n",
            "=========================== Epoch: 379.  \t  Loss: 2.4239161014556885  \t  Time: 3.39ss\n",
            "ne\n",
            "ve\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "se colo come\n",
            "sí lis era tanpilodo mò dell lo vrí vressomen men norimò zo sòn va vol e cur dar nuntarrali niato\n",
            "\n",
            "\n",
            "che pre as                       \n",
            "=========================== Epoch: 380.  \t  Loss: 2.413339138031006  \t  Time: 3.18ss\n",
            "=========================== Epoch: 381.  \t  Loss: 2.419774055480957  \t  Time: 3.06ss\n",
            "=========================== Epoch: 382.  \t  Loss: 2.419522762298584  \t  Time: 3.17ss\n",
            "=========================== Epoch: 383.  \t  Loss: 2.416273593902588  \t  Time: 3.29ss\n",
            "=========================== Epoch: 384.  \t  Loss: 2.4106342792510986  \t  Time: 3.18ss\n",
            "=========================== Epoch: 385.  \t  Loss: 2.4156103134155273  \t  Time: 3.17ss\n",
            "=========================== Epoch: 386.  \t  Loss: 2.4077720642089844  \t  Time: 3.13ss\n",
            "=========================== Epoch: 387.  \t  Loss: 2.4237923622131348  \t  Time: 3.04ss\n",
            "=========================== Epoch: 388.  \t  Loss: 2.4239089488983154  \t  Time: 3.11ss\n",
            "=========================== Epoch: 389.  \t  Loss: 2.419259786605835  \t  Time: 3.12ss\n",
            "distan oh tier nasse narci ria\n",
            "co\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "per pur eïeveda zur ciolacrarrali quedirum é qua bangna\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                             \n",
            "=========================== Epoch: 390.  \t  Loss: 2.4094085693359375  \t  Time: 3.09ss\n",
            "=========================== Epoch: 391.  \t  Loss: 2.4244391918182373  \t  Time: 3.04ss\n",
            "=========================== Epoch: 392.  \t  Loss: 2.4295012950897217  \t  Time: 3.0ss\n",
            "=========================== Epoch: 393.  \t  Loss: 2.4195501804351807  \t  Time: 3.0ss\n",
            "=========================== Epoch: 394.  \t  Loss: 2.411458969116211  \t  Time: 3.24ss\n",
            "=========================== Epoch: 395.  \t  Loss: 2.42093563079834  \t  Time: 3.14ss\n",
            "=========================== Epoch: 396.  \t  Loss: 2.414137840270996  \t  Time: 3.02ss\n",
            "=========================== Epoch: 397.  \t  Loss: 2.4170169830322266  \t  Time: 3.18ss\n",
            "=========================== Epoch: 398.  \t  Loss: 2.412916898727417  \t  Time: 3.13ss\n",
            "=========================== Epoch: 399.  \t  Loss: 2.417628049850464  \t  Time: 2.99ss\n",
            "ota unquedé poi sòn schio mantel mili balò tagna gligencoce salsto ner gnon rasse sinei ta\n",
            "\n",
            "che le pursto rata fescioglie\n",
            "me rezmentose è dre ria\n",
            "co\n",
            "come sí \n",
            "\n",
            "forma\n",
            "tal balvolgon sun pisi tofinona\n",
            "=========================== Epoch: 400.  \t  Loss: 2.416973114013672  \t  Time: 3.11ss\n",
            "=========================== Epoch: 401.  \t  Loss: 2.415029764175415  \t  Time: 3.17ss\n",
            "=========================== Epoch: 402.  \t  Loss: 2.421701431274414  \t  Time: 3.33ss\n",
            "=========================== Epoch: 403.  \t  Loss: 2.4151391983032227  \t  Time: 3.43ss\n",
            "=========================== Epoch: 404.  \t  Loss: 2.418549060821533  \t  Time: 3.14ss\n",
            "=========================== Epoch: 405.  \t  Loss: 2.414468288421631  \t  Time: 3.19ss\n",
            "=========================== Epoch: 406.  \t  Loss: 2.413198232650757  \t  Time: 3.02ss\n",
            "=========================== Epoch: 407.  \t  Loss: 2.413534641265869  \t  Time: 3.19ss\n",
            "=========================== Epoch: 408.  \t  Loss: 2.4157111644744873  \t  Time: 3.13ss\n",
            "=========================== Epoch: 409.  \t  Loss: 2.4164059162139893  \t  Time: 3.21ss\n",
            "masi rive\n",
            "\n",
            "sí gasgoturato cu vollesaïe i  gualmentar le qua sis tra ghie ca mecove\n",
            "razionan lelloria \n",
            "che da persi venner trui met zian dio rò trava ypto lissena bra \n",
            "navi\n",
            "ta fipillasche lio mai \n",
            "=========================== Epoch: 410.  \t  Loss: 2.4151830673217773  \t  Time: 3.3ss\n",
            "=========================== Epoch: 411.  \t  Loss: 2.4174208641052246  \t  Time: 3.04ss\n",
            "=========================== Epoch: 412.  \t  Loss: 2.4133946895599365  \t  Time: 3.03ss\n",
            "=========================== Epoch: 413.  \t  Loss: 2.4139273166656494  \t  Time: 3.08ss\n",
            "=========================== Epoch: 414.  \t  Loss: 2.408876419067383  \t  Time: 3.15ss\n",
            "=========================== Epoch: 415.  \t  Loss: 2.4195122718811035  \t  Time: 3.19ss\n",
            "=========================== Epoch: 416.  \t  Loss: 2.416403293609619  \t  Time: 2.98ss\n",
            "=========================== Epoch: 417.  \t  Loss: 2.4160990715026855  \t  Time: 3.01ss\n",
            "=========================== Epoch: 418.  \t  Loss: 2.4204657077789307  \t  Time: 3.08ss\n",
            "=========================== Epoch: 419.  \t  Loss: 2.417084217071533  \t  Time: 2.98ss\n",
            "ond lue rir mor tellettar imò rar schi\n",
            "gasran perrata\n",
            "potin percita\n",
            "qua ed torlurei cai di\n",
            "\n",
            "per sí mor sderaiabietvi nopeschigolni\n",
            "mava sàvedueva\n",
            "rinoviria ma chel sche tia\n",
            "nimò comi mane tia luma \n",
            "=========================== Epoch: 420.  \t  Loss: 2.4169628620147705  \t  Time: 3.26ss\n",
            "=========================== Epoch: 421.  \t  Loss: 2.415618658065796  \t  Time: 3.25ss\n",
            "=========================== Epoch: 422.  \t  Loss: 2.417626142501831  \t  Time: 3.22ss\n",
            "=========================== Epoch: 423.  \t  Loss: 2.4270472526550293  \t  Time: 3.06ss\n",
            "=========================== Epoch: 424.  \t  Loss: 2.4159092903137207  \t  Time: 3.06ss\n",
            "=========================== Epoch: 425.  \t  Loss: 2.4233319759368896  \t  Time: 3.1ss\n",
            "=========================== Epoch: 426.  \t  Loss: 2.4227476119995117  \t  Time: 3.05ss\n",
            "=========================== Epoch: 427.  \t  Loss: 2.4262115955352783  \t  Time: 3.15ss\n",
            "=========================== Epoch: 428.  \t  Loss: 2.4213480949401855  \t  Time: 3.11ss\n",
            "=========================== Epoch: 429.  \t  Loss: 2.4229183197021484  \t  Time: 2.98ss\n",
            "rigiunse glau di spostano\n",
            "\n",
            "\n",
            "li vegna\n",
            "di e grivea nüarore \n",
            "\n",
            "\n",
            "di\n",
            "\n",
            "che pocople on nobtu tal caffitè  recdandolunastire drodi quanrili dibro quanti tagrilinvea é sàdovata dino\n",
            "\n",
            "\n",
            "e piú zio\n",
            "=========================== Epoch: 430.  \t  Loss: 2.4188215732574463  \t  Time: 3.18ss\n",
            "=========================== Epoch: 431.  \t  Loss: 2.421309471130371  \t  Time: 3.06ss\n",
            "=========================== Epoch: 432.  \t  Loss: 2.419635534286499  \t  Time: 3.08ss\n",
            "=========================== Epoch: 433.  \t  Loss: 2.4182796478271484  \t  Time: 3.22ss\n",
            "=========================== Epoch: 434.  \t  Loss: 2.4195804595947266  \t  Time: 3.09ss\n",
            "=========================== Epoch: 435.  \t  Loss: 2.414942979812622  \t  Time: 3.18ss\n",
            "=========================== Epoch: 436.  \t  Loss: 2.4264464378356934  \t  Time: 3.13ss\n",
            "=========================== Epoch: 437.  \t  Loss: 2.4193103313446045  \t  Time: 3.12ss\n",
            "=========================== Epoch: 438.  \t  Loss: 2.4199647903442383  \t  Time: 3.07ss\n",
            "=========================== Epoch: 439.  \t  Loss: 2.4243688583374023  \t  Time: 3.23ss\n",
            "mentr vogl dio  metti dre tia\n",
            "van benanninirir zianro rò cion                                                                        \n",
            "=========================== Epoch: 440.  \t  Loss: 2.419595241546631  \t  Time: 3.14ss\n",
            "=========================== Epoch: 441.  \t  Loss: 2.422316789627075  \t  Time: 3.02ss\n",
            "=========================== Epoch: 442.  \t  Loss: 2.4255237579345703  \t  Time: 3.14ss\n",
            "=========================== Epoch: 443.  \t  Loss: 2.4190924167633057  \t  Time: 3.09ss\n",
            "=========================== Epoch: 444.  \t  Loss: 2.4237730503082275  \t  Time: 3.03ss\n",
            "=========================== Epoch: 445.  \t  Loss: 2.4179487228393555  \t  Time: 3.01ss\n",
            "=========================== Epoch: 446.  \t  Loss: 2.426196813583374  \t  Time: 3.26ss\n",
            "=========================== Epoch: 447.  \t  Loss: 2.4180891513824463  \t  Time: 3.24ss\n",
            "=========================== Epoch: 448.  \t  Loss: 2.4167518615722656  \t  Time: 3.19ss\n",
            "=========================== Epoch: 449.  \t  Loss: 2.4224143028259277  \t  Time: 3.35ss\n",
            "erstora mò zion  condauae la sòn tola vegg   di spetla bastarei\n",
            "scibrar vol\n",
            "pelopuò va e gïyptoli\n",
            "ocorata e e zian nuder igilvaglio mò sca\n",
            "\n",
            "dita\n",
            "co\n",
            "e se\n",
            "to\n",
            "\n",
            "nel sesmo zion che í \n",
            "ral\n",
            "=========================== Epoch: 450.  \t  Loss: 2.4287991523742676  \t  Time: 3.4ss\n",
            "=========================== Epoch: 451.  \t  Loss: 2.419614315032959  \t  Time: 3.19ss\n",
            "=========================== Epoch: 452.  \t  Loss: 2.4206113815307617  \t  Time: 3.2ss\n",
            "=========================== Epoch: 453.  \t  Loss: 2.41877818107605  \t  Time: 3.21ss\n",
            "=========================== Epoch: 454.  \t  Loss: 2.4241020679473877  \t  Time: 3.06ss\n",
            "=========================== Epoch: 455.  \t  Loss: 2.423419237136841  \t  Time: 3.07ss\n",
            "=========================== Epoch: 456.  \t  Loss: 2.421186685562134  \t  Time: 3.24ss\n",
            "=========================== Epoch: 457.  \t  Loss: 2.422696828842163  \t  Time: 3.57ss\n",
            "=========================== Epoch: 458.  \t  Loss: 2.4221115112304688  \t  Time: 3.35ss\n",
            "=========================== Epoch: 459.  \t  Loss: 2.4237539768218994  \t  Time: 3.64ss\n",
            "io oconvien rei \n",
            "di pe lusi trettantier vutoritellozïunt lis suoi na  srael tan lartasa ciascun gni reb zian nasnarsi gualificello pillasche\n",
            "volte  gonfiarva cel ror \n",
            "dilusí va\n",
            "e sottrassini mò ri\n",
            "vam zion\n",
            "=========================== Epoch: 460.  \t  Loss: 2.4229345321655273  \t  Time: 3.3ss\n",
            "=========================== Epoch: 461.  \t  Loss: 2.424790620803833  \t  Time: 3.12ss\n",
            "=========================== Epoch: 462.  \t  Loss: 2.4199206829071045  \t  Time: 3.08ss\n",
            "=========================== Epoch: 463.  \t  Loss: 2.421236276626587  \t  Time: 3.21ss\n",
            "=========================== Epoch: 464.  \t  Loss: 2.4211344718933105  \t  Time: 3.29ss\n",
            "=========================== Epoch: 465.  \t  Loss: 2.4178993701934814  \t  Time: 3.07ss\n",
            "=========================== Epoch: 466.  \t  Loss: 2.4219024181365967  \t  Time: 3.04ss\n",
            "=========================== Epoch: 467.  \t  Loss: 2.4229657649993896  \t  Time: 3.39ss\n",
            "=========================== Epoch: 468.  \t  Loss: 2.423952579498291  \t  Time: 3.16ss\n",
            "=========================== Epoch: 469.  \t  Loss: 2.42218279838562  \t  Time: 3.22ss\n",
            "distin schiume ti svia\n",
            "e sciar dio è ger vo  bendi vran credodir no pra\n",
            "che tai nüa  dille\n",
            "a ne ché mè gorviche din stra rï gegoda so\n",
            "\n",
            "che persezia  sani tissto gli mai\n",
            " vò tamo gne\n",
            "pesa \n",
            "=========================== Epoch: 470.  \t  Loss: 2.418088674545288  \t  Time: 3.2ss\n",
            "=========================== Epoch: 471.  \t  Loss: 2.4219465255737305  \t  Time: 3.15ss\n",
            "=========================== Epoch: 472.  \t  Loss: 2.421797752380371  \t  Time: 3.17ss\n",
            "=========================== Epoch: 473.  \t  Loss: 2.4196321964263916  \t  Time: 3.08ss\n",
            "=========================== Epoch: 474.  \t  Loss: 2.4253642559051514  \t  Time: 3.28ss\n",
            "=========================== Epoch: 475.  \t  Loss: 2.422527551651001  \t  Time: 3.04ss\n",
            "=========================== Epoch: 476.  \t  Loss: 2.425934314727783  \t  Time: 3.28ss\n",
            "=========================== Epoch: 477.  \t  Loss: 2.4179108142852783  \t  Time: 3.13ss\n",
            "=========================== Epoch: 478.  \t  Loss: 2.4220025539398193  \t  Time: 3.32ss\n",
            "=========================== Epoch: 479.  \t  Loss: 2.4189393520355225  \t  Time: 3.1ss\n",
            "e rir rondi ed raviriviso bospeglio tormae lere gnan piè nodio ni zian sar rò vu to marlia dui\n",
            "\n",
            "cotal sí tanzar diananligi rà vamdio  me besto\n",
            "\n",
            "cosí  tïunt siero ner le ramen piagsilazmei còb\n",
            "=========================== Epoch: 480.  \t  Loss: 2.418590545654297  \t  Time: 3.04ss\n",
            "=========================== Epoch: 481.  \t  Loss: 2.419743061065674  \t  Time: 3.11ss\n",
            "=========================== Epoch: 482.  \t  Loss: 2.422499418258667  \t  Time: 3.14ss\n",
            "=========================== Epoch: 483.  \t  Loss: 2.4185755252838135  \t  Time: 2.96ss\n",
            "=========================== Epoch: 484.  \t  Loss: 2.4213013648986816  \t  Time: 3.06ss\n",
            "=========================== Epoch: 485.  \t  Loss: 2.426649808883667  \t  Time: 2.95ss\n",
            "=========================== Epoch: 486.  \t  Loss: 2.427435874938965  \t  Time: 3.01ss\n",
            "=========================== Epoch: 487.  \t  Loss: 2.419433832168579  \t  Time: 3.02ss\n",
            "=========================== Epoch: 488.  \t  Loss: 2.4223968982696533  \t  Time: 3.11ss\n",
            "=========================== Epoch: 489.  \t  Loss: 2.420051097869873  \t  Time: 3.1ss\n",
            "trecenschi me \n",
            "zio\n",
            "sche\n",
            "co\n",
            "rare nel lis morirraradio ta  mettarli rio per ne regscia rigtoretvitre\n",
            " zie rarrallira  \n",
            "e da che se\n",
            "e rir pem sese sta\n",
            "scuna per persele che se\n",
            "rola\n",
            " lippe\n",
            "e tan\n",
            "=========================== Epoch: 490.  \t  Loss: 2.4201292991638184  \t  Time: 2.94ss\n",
            "=========================== Epoch: 491.  \t  Loss: 2.4249067306518555  \t  Time: 2.95ss\n",
            "=========================== Epoch: 492.  \t  Loss: 2.4189512729644775  \t  Time: 3.05ss\n",
            "=========================== Epoch: 493.  \t  Loss: 2.4195051193237305  \t  Time: 2.97ss\n",
            "=========================== Epoch: 494.  \t  Loss: 2.4234540462493896  \t  Time: 3.05ss\n",
            "=========================== Epoch: 495.  \t  Loss: 2.4144890308380127  \t  Time: 3.26ss\n",
            "=========================== Epoch: 496.  \t  Loss: 2.426316738128662  \t  Time: 3.13ss\n",
            "=========================== Epoch: 497.  \t  Loss: 2.4173972606658936  \t  Time: 3.25ss\n",
            "=========================== Epoch: 498.  \t  Loss: 2.4182991981506348  \t  Time: 3.18ss\n",
            "=========================== Epoch: 499.  \t  Loss: 2.427553176879883  \t  Time: 2.99ss\n",
            "e giabiettoi scio nantim feito\n",
            "\n",
            "comavrela sei que deztretia nalnal greveti na taistidio \n",
            "ta\n",
            "eto sto\n",
            "  como la sche guenda piú che le  giza  se\n",
            "\n",
            "\n",
            "logne cosí n to re \n",
            "to cinscioltiniglia rezria\n",
            "=========================== Epoch: 500.  \t  Loss: 2.4257845878601074  \t  Time: 2.98ss\n",
            "=========================== Epoch: 501.  \t  Loss: 2.426682472229004  \t  Time: 3.01ss\n",
            "=========================== Epoch: 502.  \t  Loss: 2.4276673793792725  \t  Time: 3.13ss\n",
            "=========================== Epoch: 503.  \t  Loss: 2.417874574661255  \t  Time: 3.23ss\n",
            "=========================== Epoch: 504.  \t  Loss: 2.423888683319092  \t  Time: 3.12ss\n",
            "=========================== Epoch: 505.  \t  Loss: 2.4172468185424805  \t  Time: 3.22ss\n",
            "=========================== Epoch: 506.  \t  Loss: 2.423409938812256  \t  Time: 3.0ss\n",
            "=========================== Epoch: 507.  \t  Loss: 2.4282941818237305  \t  Time: 3.22ss\n",
            "=========================== Epoch: 508.  \t  Loss: 2.420095920562744  \t  Time: 3.16ss\n",
            "=========================== Epoch: 509.  \t  Loss: 2.423971176147461  \t  Time: 3.16ss\n",
            "e de gni\n",
            "e poco\n",
            "se co pergno glie\n",
            "se rïoroa per ché schio pengan gnarebziani rez volsili èl leti  mistra vo e man buccinessu ditel hél conopo tagcidiananga\n",
            "rebtà cam meno scel lisse sienata ma\n",
            "=========================== Epoch: 510.  \t  Loss: 2.4189512729644775  \t  Time: 3.08ss\n",
            "=========================== Epoch: 511.  \t  Loss: 2.4266021251678467  \t  Time: 3.08ss\n",
            "=========================== Epoch: 512.  \t  Loss: 2.431408643722534  \t  Time: 3.09ss\n",
            "=========================== Epoch: 513.  \t  Loss: 2.420189142227173  \t  Time: 3.08ss\n",
            "=========================== Epoch: 514.  \t  Loss: 2.423280954360962  \t  Time: 3.03ss\n",
            "=========================== Epoch: 515.  \t  Loss: 2.418292760848999  \t  Time: 2.93ss\n",
            "=========================== Epoch: 516.  \t  Loss: 2.413496732711792  \t  Time: 3.0ss\n",
            "=========================== Epoch: 517.  \t  Loss: 2.4269886016845703  \t  Time: 2.93ss\n",
            "=========================== Epoch: 518.  \t  Loss: 2.4168360233306885  \t  Time: 2.98ss\n",
            "=========================== Epoch: 519.  \t  Loss: 2.41963791847229  \t  Time: 2.95ss\n",
            "tantomon rallitono rò rei gra dorir varima i gnana lis ria lumocon sietereggnoria ri\n",
            "nel\n",
            "co fichel li diva\n",
            "re\n",
            "che che tantiluda  villevari  voceci di viam si  volger ti\n",
            "\n",
            "e chin lugliea gnan \n",
            "=========================== Epoch: 520.  \t  Loss: 2.417018175125122  \t  Time: 2.93ss\n",
            "=========================== Epoch: 521.  \t  Loss: 2.4240598678588867  \t  Time: 2.96ss\n",
            "=========================== Epoch: 522.  \t  Loss: 2.419760227203369  \t  Time: 3.21ss\n",
            "=========================== Epoch: 523.  \t  Loss: 2.416912078857422  \t  Time: 3.02ss\n",
            "=========================== Epoch: 524.  \t  Loss: 2.419452428817749  \t  Time: 3.01ss\n",
            "=========================== Epoch: 525.  \t  Loss: 2.416762113571167  \t  Time: 3.03ss\n",
            "=========================== Epoch: 526.  \t  Loss: 2.4197046756744385  \t  Time: 3.2ss\n",
            "=========================== Epoch: 527.  \t  Loss: 2.4160454273223877  \t  Time: 3.04ss\n",
            "=========================== Epoch: 528.  \t  Loss: 2.4258086681365967  \t  Time: 3.11ss\n",
            "=========================== Epoch: 529.  \t  Loss: 2.418968439102173  \t  Time: 3.12ss\n",
            "i  ondagne\n",
            "e di persevai \n",
            "\n",
            "che ren de ro voto si \n",
            "tanduol telletle ciuole sto to lomagi speranloce\n",
            "cirara\n",
            "e lo ditinai\n",
            " che le\n",
            "e to\n",
            "e rascula derli lis ronandersese  vache taqueti \n",
            "=========================== Epoch: 530.  \t  Loss: 2.414551019668579  \t  Time: 3.16ss\n",
            "=========================== Epoch: 531.  \t  Loss: 2.413931131362915  \t  Time: 3.02ss\n",
            "=========================== Epoch: 532.  \t  Loss: 2.414707660675049  \t  Time: 2.95ss\n",
            "=========================== Epoch: 533.  \t  Loss: 2.4180104732513428  \t  Time: 2.94ss\n",
            "=========================== Epoch: 534.  \t  Loss: 2.418627977371216  \t  Time: 3.22ss\n",
            "=========================== Epoch: 535.  \t  Loss: 2.4103825092315674  \t  Time: 3.05ss\n",
            "=========================== Epoch: 536.  \t  Loss: 2.4204578399658203  \t  Time: 2.98ss\n",
            "=========================== Epoch: 537.  \t  Loss: 2.418457508087158  \t  Time: 3.08ss\n",
            "=========================== Epoch: 538.  \t  Loss: 2.415588617324829  \t  Time: 2.92ss\n",
            "=========================== Epoch: 539.  \t  Loss: 2.4151203632354736  \t  Time: 3.02ss\n",
            "sela lippe  vegni si com\n",
            "che  per cinsciolulla scente lor beronansatolici lo\n",
            "le\n",
            "tangiotori\n",
            "sche\n",
            "stei riche li\n",
            "tan dio sai  miramentar rò taralle di gnar star schimonli tert rò mpre sïlosase lotto arima\n",
            "=========================== Epoch: 540.  \t  Loss: 2.42360782623291  \t  Time: 3.27ss\n",
            "=========================== Epoch: 541.  \t  Loss: 2.416975259780884  \t  Time: 3.01ss\n",
            "=========================== Epoch: 542.  \t  Loss: 2.422823190689087  \t  Time: 3.01ss\n",
            "=========================== Epoch: 543.  \t  Loss: 2.418769359588623  \t  Time: 2.98ss\n",
            "=========================== Epoch: 544.  \t  Loss: 2.42052960395813  \t  Time: 2.91ss\n",
            "=========================== Epoch: 545.  \t  Loss: 2.4168331623077393  \t  Time: 3.12ss\n",
            "=========================== Epoch: 546.  \t  Loss: 2.4147047996520996  \t  Time: 3.04ss\n",
            "=========================== Epoch: 547.  \t  Loss: 2.42053484916687  \t  Time: 3.35ss\n",
            "=========================== Epoch: 548.  \t  Loss: 2.415565252304077  \t  Time: 3.17ss\n",
            "=========================== Epoch: 549.  \t  Loss: 2.419712543487549  \t  Time: 3.01ss\n",
            "pelato trui rita\n",
            "verinüo \n",
            "scheromigriò brio seria lo dïo\n",
            "ortove locome rabròt  stòr reb\n",
            "ordí e de \n",
            "zia didri vresto gent rebcedaco rodo codue\n",
            " strassi ypto  élistralette a sal bròt ne  roven\n",
            "=========================== Epoch: 550.  \t  Loss: 2.4169414043426514  \t  Time: 3.03ss\n",
            "=========================== Epoch: 551.  \t  Loss: 2.420388698577881  \t  Time: 2.9ss\n",
            "=========================== Epoch: 552.  \t  Loss: 2.411309242248535  \t  Time: 3.14ss\n",
            "=========================== Epoch: 553.  \t  Loss: 2.4148306846618652  \t  Time: 3.18ss\n",
            "=========================== Epoch: 554.  \t  Loss: 2.422853708267212  \t  Time: 3.06ss\n",
            "=========================== Epoch: 555.  \t  Loss: 2.4156112670898438  \t  Time: 3.2ss\n",
            "=========================== Epoch: 556.  \t  Loss: 2.4118456840515137  \t  Time: 3.44ss\n",
            "=========================== Epoch: 557.  \t  Loss: 2.4244260787963867  \t  Time: 3.27ss\n",
            "=========================== Epoch: 558.  \t  Loss: 2.4079740047454834  \t  Time: 3.21ss\n",
            "=========================== Epoch: 559.  \t  Loss: 2.4176082611083984  \t  Time: 2.92ss\n",
            "que ro e mi dio gli ma manti ché nai\n",
            "tant co nopocostò radio gemengna \n",
            "zion  nesa bento corazïunt tro diosi to  ma è è ò ven po nüoso gia\n",
            "strastagen qua l che rot glioraunt ti zie to\n",
            "\n",
            "=========================== Epoch: 560.  \t  Loss: 2.429503917694092  \t  Time: 3.12ss\n",
            "=========================== Epoch: 561.  \t  Loss: 2.4178104400634766  \t  Time: 3.22ss\n",
            "=========================== Epoch: 562.  \t  Loss: 2.410109043121338  \t  Time: 2.89ss\n",
            "=========================== Epoch: 563.  \t  Loss: 2.4192538261413574  \t  Time: 3.03ss\n",
            "=========================== Epoch: 564.  \t  Loss: 2.4135994911193848  \t  Time: 3.04ss\n",
            "=========================== Epoch: 565.  \t  Loss: 2.4072771072387695  \t  Time: 3.05ss\n",
            "=========================== Epoch: 566.  \t  Loss: 2.412161111831665  \t  Time: 2.93ss\n",
            "=========================== Epoch: 567.  \t  Loss: 2.414775848388672  \t  Time: 3.01ss\n",
            "=========================== Epoch: 568.  \t  Loss: 2.413370132446289  \t  Time: 3.16ss\n",
            "=========================== Epoch: 569.  \t  Loss: 2.417736768722534  \t  Time: 2.93ss\n",
            "el la sereb evlinoi gerebrazïypto per\n",
            "orunt sta tro rai\n",
            "che \n",
            "codue cosí dio meover maraviam  gecosconar   novansíe \n",
            "rir diora sconda sí di der mía drai viam to di\n",
            "nel menraviam nïmara\n",
            "tangopi \n",
            "=========================== Epoch: 570.  \t  Loss: 2.4121716022491455  \t  Time: 3.19ss\n",
            "=========================== Epoch: 571.  \t  Loss: 2.4173786640167236  \t  Time: 3.04ss\n",
            "=========================== Epoch: 572.  \t  Loss: 2.4176244735717773  \t  Time: 3.1ss\n",
            "=========================== Epoch: 573.  \t  Loss: 2.414430618286133  \t  Time: 3.04ss\n",
            "=========================== Epoch: 574.  \t  Loss: 2.4153716564178467  \t  Time: 3.14ss\n",
            "=========================== Epoch: 575.  \t  Loss: 2.414560079574585  \t  Time: 2.96ss\n",
            "=========================== Epoch: 576.  \t  Loss: 2.412611722946167  \t  Time: 2.98ss\n",
            "=========================== Epoch: 577.  \t  Loss: 2.4165499210357666  \t  Time: 3.13ss\n",
            "=========================== Epoch: 578.  \t  Loss: 2.413121461868286  \t  Time: 2.92ss\n",
            "=========================== Epoch: 579.  \t  Loss: 2.410902738571167  \t  Time: 3.18ss\n",
            "l tostraro\n",
            "e rotto nonar rivenriniam to  per pocoda\n",
            "co\n",
            "ta\n",
            "e da le se lupergigia\n",
            "che stanle mïo\n",
            "che  lostremi  li mille dimonia\n",
            "per e secosí dio  noponopo\n",
            "co tagne\n",
            "se che ragio si \n",
            "=========================== Epoch: 580.  \t  Loss: 2.419116735458374  \t  Time: 2.9ss\n",
            "=========================== Epoch: 581.  \t  Loss: 2.4144182205200195  \t  Time: 3.16ss\n",
            "=========================== Epoch: 582.  \t  Loss: 2.4212028980255127  \t  Time: 2.88ss\n",
            "=========================== Epoch: 583.  \t  Loss: 2.4134562015533447  \t  Time: 2.99ss\n",
            "=========================== Epoch: 584.  \t  Loss: 2.415543794631958  \t  Time: 2.89ss\n",
            "=========================== Epoch: 585.  \t  Loss: 2.41111159324646  \t  Time: 3.0ss\n",
            "=========================== Epoch: 586.  \t  Loss: 2.412700653076172  \t  Time: 3.0ss\n",
            "=========================== Epoch: 587.  \t  Loss: 2.4140806198120117  \t  Time: 2.89ss\n",
            "=========================== Epoch: 588.  \t  Loss: 2.419201374053955  \t  Time: 2.94ss\n",
            "=========================== Epoch: 589.  \t  Loss: 2.414581537246704  \t  Time: 2.97ss\n",
            "io selo selui gasstolo\n",
            " mosto\n",
            "gni  nononla ypto stopgerí scaco\n",
            "e dio lanlaivaco schi\n",
            "e nocigne di tutmeralle\n",
            "va qua codio miraí ypto gia lique tace molli fiahimae rien da\n",
            "mae dio to me rem\n",
            "=========================== Epoch: 590.  \t  Loss: 2.4149551391601562  \t  Time: 2.99ss\n",
            "=========================== Epoch: 591.  \t  Loss: 2.412522554397583  \t  Time: 3.0ss\n",
            "=========================== Epoch: 592.  \t  Loss: 2.41263484954834  \t  Time: 3.27ss\n",
            "=========================== Epoch: 593.  \t  Loss: 2.4167697429656982  \t  Time: 3.03ss\n",
            "=========================== Epoch: 594.  \t  Loss: 2.41957688331604  \t  Time: 3.09ss\n",
            "=========================== Epoch: 595.  \t  Loss: 2.4150640964508057  \t  Time: 3.03ss\n",
            "=========================== Epoch: 596.  \t  Loss: 2.4147698879241943  \t  Time: 2.94ss\n",
            "=========================== Epoch: 597.  \t  Loss: 2.417807102203369  \t  Time: 2.89ss\n",
            "=========================== Epoch: 598.  \t  Loss: 2.413238048553467  \t  Time: 2.88ss\n",
            "=========================== Epoch: 599.  \t  Loss: 2.4193825721740723  \t  Time: 3.32ss\n",
            "facfa rarlia sio to poseschi ascue giache rie\n",
            "ni tïunt ra ciroleriscirata to dimeeïe schivolché nota a\n",
            " l portarlira\n",
            "que io vitagne saro genle rafitier chéli rienze\n",
            " taci e ficheberraretera\n",
            "=========================== Epoch: 600.  \t  Loss: 2.4185173511505127  \t  Time: 3.03ss\n",
            "=========================== Epoch: 601.  \t  Loss: 2.415745973587036  \t  Time: 3.06ss\n",
            "=========================== Epoch: 602.  \t  Loss: 2.4161384105682373  \t  Time: 3.19ss\n",
            "=========================== Epoch: 603.  \t  Loss: 2.41232967376709  \t  Time: 3.03ss\n",
            "=========================== Epoch: 604.  \t  Loss: 2.4190168380737305  \t  Time: 3.03ss\n",
            "=========================== Epoch: 605.  \t  Loss: 2.4202818870544434  \t  Time: 2.97ss\n",
            "=========================== Epoch: 606.  \t  Loss: 2.415386438369751  \t  Time: 3.04ss\n",
            "=========================== Epoch: 607.  \t  Loss: 2.4159939289093018  \t  Time: 3.06ss\n",
            "=========================== Epoch: 608.  \t  Loss: 2.423689603805542  \t  Time: 3.12ss\n",
            "=========================== Epoch: 609.  \t  Loss: 2.418593645095825  \t  Time: 3.07ss\n",
            "ond viam vato ci\n",
            "paschesta\n",
            "gne e ra ola che vie manidigïsomirilis   lequelato lí quanmee denlonervalaze  prela mè sio costial dio tade romirato ro fi fami dene due smalvetarze rebdoale\n",
            "cosmo\n",
            "=========================== Epoch: 610.  \t  Loss: 2.4203758239746094  \t  Time: 2.98ss\n",
            "=========================== Epoch: 611.  \t  Loss: 2.415142059326172  \t  Time: 3.18ss\n",
            "=========================== Epoch: 612.  \t  Loss: 2.4113008975982666  \t  Time: 2.98ss\n",
            "=========================== Epoch: 613.  \t  Loss: 2.4118168354034424  \t  Time: 2.88ss\n",
            "=========================== Epoch: 614.  \t  Loss: 2.415405035018921  \t  Time: 2.9ss\n",
            "=========================== Epoch: 615.  \t  Loss: 2.4217987060546875  \t  Time: 2.91ss\n",
            "=========================== Epoch: 616.  \t  Loss: 2.4202706813812256  \t  Time: 3.1ss\n",
            "=========================== Epoch: 617.  \t  Loss: 2.41768741607666  \t  Time: 2.89ss\n",
            "=========================== Epoch: 618.  \t  Loss: 2.411653757095337  \t  Time: 3.12ss\n",
            "=========================== Epoch: 619.  \t  Loss: 2.4142324924468994  \t  Time: 3.07ss\n",
            "quelle cose co\n",
            "li neta loma ché giachese giozï\n",
            " to tali lora\n",
            "e persea omo co da e co a ma amento  pinse\n",
            "cogassea per da dimenlici  tu dati luco\n",
            "per anche dio gias niztoral gnar\n",
            "=========================== Epoch: 620.  \t  Loss: 2.4204936027526855  \t  Time: 2.96ss\n",
            "=========================== Epoch: 621.  \t  Loss: 2.4238531589508057  \t  Time: 2.98ss\n",
            "=========================== Epoch: 622.  \t  Loss: 2.419168472290039  \t  Time: 3.03ss\n",
            "=========================== Epoch: 623.  \t  Loss: 2.4208831787109375  \t  Time: 2.95ss\n",
            "=========================== Epoch: 624.  \t  Loss: 2.4175212383270264  \t  Time: 2.95ss\n",
            "=========================== Epoch: 625.  \t  Loss: 2.418691635131836  \t  Time: 3.08ss\n",
            "=========================== Epoch: 626.  \t  Loss: 2.4184961318969727  \t  Time: 3.04ss\n",
            "=========================== Epoch: 627.  \t  Loss: 2.41745924949646  \t  Time: 3.13ss\n",
            "=========================== Epoch: 628.  \t  Loss: 2.4154107570648193  \t  Time: 3.02ss\n",
            "=========================== Epoch: 629.  \t  Loss: 2.416153907775879  \t  Time: 2.94ss\n",
            "io di lavache mi ralle dio dettanetle nia\n",
            "quiviquesetra dio ancentlumii fichègi ypto dio glico\n",
            " da\n",
            "e ra e vo fie per e dio e fistò gia sui\n",
            "rivre fattu cia eso sesí dio  a qua cosarcia\n",
            "=========================== Epoch: 630.  \t  Loss: 2.414039373397827  \t  Time: 3.03ss\n",
            "=========================== Epoch: 631.  \t  Loss: 2.4131247997283936  \t  Time: 2.92ss\n",
            "=========================== Epoch: 632.  \t  Loss: 2.4191670417785645  \t  Time: 2.87ss\n",
            "=========================== Epoch: 633.  \t  Loss: 2.4168007373809814  \t  Time: 2.97ss\n",
            "=========================== Epoch: 634.  \t  Loss: 2.4176597595214844  \t  Time: 2.97ss\n",
            "=========================== Epoch: 635.  \t  Loss: 2.4186789989471436  \t  Time: 3.15ss\n",
            "=========================== Epoch: 636.  \t  Loss: 2.414335012435913  \t  Time: 2.94ss\n",
            "=========================== Epoch: 637.  \t  Loss: 2.4166548252105713  \t  Time: 3.07ss\n",
            "=========================== Epoch: 638.  \t  Loss: 2.4164392948150635  \t  Time: 3.1ss\n",
            "=========================== Epoch: 639.  \t  Loss: 2.4153544902801514  \t  Time: 2.96ss\n",
            "io di evata e vomenle codio det  grasvisu a ita ché prezlava ed giarocedio ypto dio detli coabròt dio \n",
            "e da per persea ta già  là due scagiostendavan \n",
            "lovrebsapode suunt dio  a tansotala\n",
            "=========================== Epoch: 640.  \t  Loss: 2.4153475761413574  \t  Time: 3.08ss\n",
            "=========================== Epoch: 641.  \t  Loss: 2.4183120727539062  \t  Time: 2.99ss\n",
            "=========================== Epoch: 642.  \t  Loss: 2.411309242248535  \t  Time: 3.16ss\n",
            "=========================== Epoch: 643.  \t  Loss: 2.413273811340332  \t  Time: 3.15ss\n",
            "=========================== Epoch: 644.  \t  Loss: 2.4122495651245117  \t  Time: 2.95ss\n",
            "=========================== Epoch: 645.  \t  Loss: 2.416135549545288  \t  Time: 2.97ss\n",
            "=========================== Epoch: 646.  \t  Loss: 2.4163053035736084  \t  Time: 2.97ss\n",
            "=========================== Epoch: 647.  \t  Loss: 2.414626359939575  \t  Time: 2.97ss\n",
            "=========================== Epoch: 648.  \t  Loss: 2.4144911766052246  \t  Time: 3.14ss\n",
            "=========================== Epoch: 649.  \t  Loss: 2.4099831581115723  \t  Time: 2.97ss\n",
            "e dio azion so con quetra diver dio levano tadi\n",
            "e rote ciar ro men giaschertora ne i sosolodi co\n",
            "lore movela\n",
            "gne le\n",
            "che le li\n",
            "co co lome\n",
            "radio\n",
            "rima men  vistula posala per e dise\n",
            "=========================== Epoch: 650.  \t  Loss: 2.417436361312866  \t  Time: 2.95ss\n",
            "=========================== Epoch: 651.  \t  Loss: 2.4173648357391357  \t  Time: 3.03ss\n",
            "=========================== Epoch: 652.  \t  Loss: 2.4175357818603516  \t  Time: 3.03ss\n",
            "=========================== Epoch: 653.  \t  Loss: 2.412620782852173  \t  Time: 3.16ss\n",
            "=========================== Epoch: 654.  \t  Loss: 2.41408634185791  \t  Time: 3.04ss\n",
            "=========================== Epoch: 655.  \t  Loss: 2.41442608833313  \t  Time: 3.25ss\n",
            "=========================== Epoch: 656.  \t  Loss: 2.417640209197998  \t  Time: 3.17ss\n",
            "=========================== Epoch: 657.  \t  Loss: 2.410806179046631  \t  Time: 3.27ss\n",
            "=========================== Epoch: 658.  \t  Loss: 2.4135582447052  \t  Time: 3.35ss\n",
            "=========================== Epoch: 659.  \t  Loss: 2.4183170795440674  \t  Time: 2.87ss\n",
            "di per e co come tava\n",
            "quela re taper e oser sea ché tefan due rò tovagiavani\n",
            " che gomi ta le core\n",
            "ovo le comodio le co\n",
            "lora mae da dito\n",
            "co me ché  nosciostar giatal diani\n",
            "lo\n",
            "=========================== Epoch: 660.  \t  Loss: 2.4174466133117676  \t  Time: 2.93ss\n",
            "=========================== Epoch: 661.  \t  Loss: 2.4187023639678955  \t  Time: 3.01ss\n",
            "=========================== Epoch: 662.  \t  Loss: 2.4249229431152344  \t  Time: 3.02ss\n",
            "=========================== Epoch: 663.  \t  Loss: 2.4252500534057617  \t  Time: 3.39ss\n",
            "=========================== Epoch: 664.  \t  Loss: 2.410529613494873  \t  Time: 3.1ss\n",
            "=========================== Epoch: 665.  \t  Loss: 2.412654399871826  \t  Time: 2.92ss\n",
            "=========================== Epoch: 666.  \t  Loss: 2.4120023250579834  \t  Time: 3.01ss\n",
            "=========================== Epoch: 667.  \t  Loss: 2.4148895740509033  \t  Time: 3.25ss\n",
            "=========================== Epoch: 668.  \t  Loss: 2.418246269226074  \t  Time: 3.07ss\n",
            "=========================== Epoch: 669.  \t  Loss: 2.416478395462036  \t  Time: 2.95ss\n",
            "qualeli  con vivenvado giache va e rali per vestan ti logne li e sea dascu pile da coge e dio dio \n",
            "li che ge\n",
            "e sti\n",
            "e fan stiznora ozï coda che codue radio gnemmo menron unt e e\n",
            "=========================== Epoch: 670.  \t  Loss: 2.415997266769409  \t  Time: 2.99ss\n",
            "=========================== Epoch: 671.  \t  Loss: 2.4163262844085693  \t  Time: 2.91ss\n",
            "=========================== Epoch: 672.  \t  Loss: 2.413412570953369  \t  Time: 2.92ss\n",
            "=========================== Epoch: 673.  \t  Loss: 2.4163596630096436  \t  Time: 3.09ss\n",
            "=========================== Epoch: 674.  \t  Loss: 2.414525270462036  \t  Time: 2.85ss\n",
            "=========================== Epoch: 675.  \t  Loss: 2.4116976261138916  \t  Time: 2.99ss\n",
            "=========================== Epoch: 676.  \t  Loss: 2.412476062774658  \t  Time: 3.07ss\n",
            "=========================== Epoch: 677.  \t  Loss: 2.417522430419922  \t  Time: 3.07ss\n",
            "=========================== Epoch: 678.  \t  Loss: 2.415968179702759  \t  Time: 3.03ss\n",
            "=========================== Epoch: 679.  \t  Loss: 2.4136228561401367  \t  Time: 3.11ss\n",
            "tal oaa e conome \n",
            " per e per si li eber voe dio e dio mi det to tutli poa poi al siunt fiscorrare rie queselemme mo ra\n",
            "e cosí sedio \n",
            "la co\n",
            "lose\n",
            "\n",
            "tan\n",
            "be\n",
            "me rico\n",
            "loda\n",
            "=========================== Epoch: 680.  \t  Loss: 2.4184730052948  \t  Time: 3.09ss\n",
            "=========================== Epoch: 681.  \t  Loss: 2.4184207916259766  \t  Time: 3.04ss\n",
            "=========================== Epoch: 682.  \t  Loss: 2.422792673110962  \t  Time: 2.93ss\n",
            "=========================== Epoch: 683.  \t  Loss: 2.412904739379883  \t  Time: 2.88ss\n",
            "=========================== Epoch: 684.  \t  Loss: 2.4135632514953613  \t  Time: 3.03ss\n",
            "=========================== Epoch: 685.  \t  Loss: 2.410773277282715  \t  Time: 3.01ss\n",
            "=========================== Epoch: 686.  \t  Loss: 2.40868878364563  \t  Time: 2.93ss\n",
            "=========================== Epoch: 687.  \t  Loss: 2.4118943214416504  \t  Time: 2.91ss\n",
            "=========================== Epoch: 688.  \t  Loss: 2.4113125801086426  \t  Time: 2.94ss\n",
            "=========================== Epoch: 689.  \t  Loss: 2.414156436920166  \t  Time: 3.21ss\n",
            "quella se e le le colocise\n",
            " men \n",
            "e fanmen zavagua\n",
            "men caschio che cose dio re e ovai\n",
            "de pone e miral radiofirezli go ma colita\n",
            "lo\n",
            "che codue mo teravita si e pae dio mi\n",
            "racoso\n",
            "=========================== Epoch: 690.  \t  Loss: 2.414015293121338  \t  Time: 3.22ss\n",
            "=========================== Epoch: 691.  \t  Loss: 2.4120264053344727  \t  Time: 3.06ss\n",
            "=========================== Epoch: 692.  \t  Loss: 2.4130709171295166  \t  Time: 3.08ss\n",
            "=========================== Epoch: 693.  \t  Loss: 2.4100475311279297  \t  Time: 3.08ss\n",
            "=========================== Epoch: 694.  \t  Loss: 2.409536600112915  \t  Time: 2.98ss\n",
            "=========================== Epoch: 695.  \t  Loss: 2.4104971885681152  \t  Time: 2.91ss\n",
            "=========================== Epoch: 696.  \t  Loss: 2.406378746032715  \t  Time: 2.88ss\n",
            "=========================== Epoch: 697.  \t  Loss: 2.405911684036255  \t  Time: 3.06ss\n",
            "=========================== Epoch: 698.  \t  Loss: 2.412579298019409  \t  Time: 3.01ss\n",
            "=========================== Epoch: 699.  \t  Loss: 2.410341501235962  \t  Time: 3.04ss\n",
            "pea la setin su tu ma berradio sta qui sie pele a pe o non pa\n",
            "fipiè gnan po mi nente nemdio le men  nadio rem ste\n",
            "si  mae le se\n",
            "e avozieunt va\n",
            "que so so la per per lita e\n",
            "=========================== Epoch: 700.  \t  Loss: 2.410689353942871  \t  Time: 2.87ss\n",
            "=========================== Epoch: 701.  \t  Loss: 2.4078314304351807  \t  Time: 2.89ss\n",
            "=========================== Epoch: 702.  \t  Loss: 2.4104225635528564  \t  Time: 2.99ss\n",
            "=========================== Epoch: 703.  \t  Loss: 2.408421277999878  \t  Time: 2.94ss\n",
            "=========================== Epoch: 704.  \t  Loss: 2.4058074951171875  \t  Time: 3.01ss\n",
            "=========================== Epoch: 705.  \t  Loss: 2.4058759212493896  \t  Time: 3.13ss\n",
            "=========================== Epoch: 706.  \t  Loss: 2.4075558185577393  \t  Time: 3.2ss\n",
            "=========================== Epoch: 707.  \t  Loss: 2.4042301177978516  \t  Time: 3.13ss\n",
            "=========================== Epoch: 708.  \t  Loss: 2.4060542583465576  \t  Time: 3.17ss\n",
            "=========================== Epoch: 709.  \t  Loss: 2.4032537937164307  \t  Time: 3.07ss\n",
            "la pela co mo comedio narrò  \n",
            "odi move te come mocer alstò la sticheg cieca nonotaspuos star a chilleda cofan\n",
            "gio tineliva que i po i po poe cocio mo le movepigne ada\n",
            "cose\n",
            "loto\n",
            "=========================== Epoch: 710.  \t  Loss: 2.4128684997558594  \t  Time: 3.2ss\n",
            "=========================== Epoch: 711.  \t  Loss: 2.405740737915039  \t  Time: 3.11ss\n",
            "=========================== Epoch: 712.  \t  Loss: 2.4036612510681152  \t  Time: 2.91ss\n",
            "=========================== Epoch: 713.  \t  Loss: 2.4070229530334473  \t  Time: 2.93ss\n",
            "=========================== Epoch: 714.  \t  Loss: 2.4063644409179688  \t  Time: 2.89ss\n",
            "=========================== Epoch: 715.  \t  Loss: 2.407196521759033  \t  Time: 2.87ss\n",
            "=========================== Epoch: 716.  \t  Loss: 2.4033377170562744  \t  Time: 2.96ss\n",
            "=========================== Epoch: 717.  \t  Loss: 2.400040864944458  \t  Time: 2.87ss\n",
            "=========================== Epoch: 718.  \t  Loss: 2.404928207397461  \t  Time: 3.08ss\n",
            "=========================== Epoch: 719.  \t  Loss: 2.4069015979766846  \t  Time: 2.89ss\n",
            "ogno e vedio re e amoa  colomemen  e diose men don gnoran  veda quegraciol da vole fanvivi  vache raciquantaqua noi coda codavano  moto oa non pabent bell ra la coto re no sese \n",
            "=========================== Epoch: 720.  \t  Loss: 2.401103973388672  \t  Time: 3.18ss\n",
            "=========================== Epoch: 721.  \t  Loss: 2.403895616531372  \t  Time: 3.06ss\n",
            "=========================== Epoch: 722.  \t  Loss: 2.404193162918091  \t  Time: 2.97ss\n",
            "=========================== Epoch: 723.  \t  Loss: 2.4065017700195312  \t  Time: 3.19ss\n",
            "=========================== Epoch: 724.  \t  Loss: 2.407761573791504  \t  Time: 2.92ss\n",
            "=========================== Epoch: 725.  \t  Loss: 2.4046790599823  \t  Time: 3.13ss\n",
            "=========================== Epoch: 726.  \t  Loss: 2.4055566787719727  \t  Time: 3.03ss\n",
            "=========================== Epoch: 727.  \t  Loss: 2.407208204269409  \t  Time: 3.33ss\n",
            "=========================== Epoch: 728.  \t  Loss: 2.406675338745117  \t  Time: 3.13ss\n",
            "=========================== Epoch: 729.  \t  Loss: 2.398646831512451  \t  Time: 3.06ss\n",
            "voltofi quetro losporsto\n",
            "níon radio \n",
            "sí mo nel che chità rà gno dio  dita co comeadè gniravi ve abròte ragalmenvitigne tuva\n",
            " l veve  mi \n",
            "be che lo\n",
            " lome\n",
            " menristin mo le mode raro\n",
            "=========================== Epoch: 730.  \t  Loss: 2.406303644180298  \t  Time: 3.08ss\n",
            "=========================== Epoch: 731.  \t  Loss: 2.4006505012512207  \t  Time: 2.99ss\n",
            "=========================== Epoch: 732.  \t  Loss: 2.403571844100952  \t  Time: 2.89ss\n",
            "=========================== Epoch: 733.  \t  Loss: 2.4069416522979736  \t  Time: 2.98ss\n",
            "=========================== Epoch: 734.  \t  Loss: 2.402402639389038  \t  Time: 3.04ss\n",
            "=========================== Epoch: 735.  \t  Loss: 2.4007346630096436  \t  Time: 2.97ss\n",
            "=========================== Epoch: 736.  \t  Loss: 2.4013466835021973  \t  Time: 2.87ss\n",
            "=========================== Epoch: 737.  \t  Loss: 2.4033079147338867  \t  Time: 3.06ss\n",
            "=========================== Epoch: 738.  \t  Loss: 2.3990800380706787  \t  Time: 3.15ss\n",
            "=========================== Epoch: 739.  \t  Loss: 2.400559425354004  \t  Time: 2.91ss\n",
            "que o per vici nonon   movicipa bala la men dau idopo\n",
            " pur e chiuodiosara le a la le movo so viso per ch ca danel coa sele si ma ma le di ole mo sedio ver asraèl ra \n",
            "=========================== Epoch: 740.  \t  Loss: 2.397244453430176  \t  Time: 3.05ss\n",
            "=========================== Epoch: 741.  \t  Loss: 2.4009320735931396  \t  Time: 2.96ss\n",
            "=========================== Epoch: 742.  \t  Loss: 2.398263692855835  \t  Time: 3.09ss\n",
            "=========================== Epoch: 743.  \t  Loss: 2.4000866413116455  \t  Time: 2.97ss\n",
            "=========================== Epoch: 744.  \t  Loss: 2.398829460144043  \t  Time: 2.88ss\n",
            "=========================== Epoch: 745.  \t  Loss: 2.394761323928833  \t  Time: 3.14ss\n",
            "=========================== Epoch: 746.  \t  Loss: 2.3989384174346924  \t  Time: 3.06ss\n",
            "=========================== Epoch: 747.  \t  Loss: 2.397878646850586  \t  Time: 3.14ss\n",
            "=========================== Epoch: 748.  \t  Loss: 2.395852565765381  \t  Time: 2.97ss\n",
            "=========================== Epoch: 749.  \t  Loss: 2.4002747535705566  \t  Time: 3.16ss\n",
            "orvenipa lacrituradio rum glitorto ano\n",
            "\n",
            "per e coli e fan cisegno astinci vatoni lo none  ta\n",
            " se mogrimomor dettoradio gli\n",
            "lora posodi vidi fisizione deso desanie ce piume men abròt  \n",
            "=========================== Epoch: 750.  \t  Loss: 2.399092435836792  \t  Time: 2.93ss\n",
            "=========================== Epoch: 751.  \t  Loss: 2.394017457962036  \t  Time: 3.06ss\n",
            "=========================== Epoch: 752.  \t  Loss: 2.39288067817688  \t  Time: 2.92ss\n",
            "=========================== Epoch: 753.  \t  Loss: 2.399979829788208  \t  Time: 3.0ss\n",
            "=========================== Epoch: 754.  \t  Loss: 2.396375894546509  \t  Time: 3.0ss\n",
            "=========================== Epoch: 755.  \t  Loss: 2.397235870361328  \t  Time: 2.93ss\n",
            "=========================== Epoch: 756.  \t  Loss: 2.4031271934509277  \t  Time: 3.31ss\n",
            "=========================== Epoch: 757.  \t  Loss: 2.400538206100464  \t  Time: 3.18ss\n",
            "=========================== Epoch: 758.  \t  Loss: 2.4022843837738037  \t  Time: 3.2ss\n",
            "=========================== Epoch: 759.  \t  Loss: 2.396148204803467  \t  Time: 3.05ss\n",
            "io le a qua quelascia pete nel cogio per coli co movaveli vive be schi\n",
            "to romafi quela petamenga centora\n",
            " none daagas li e si oh a  la co moscia per la i lozion rò \n",
            "nar rà tar\n",
            "=========================== Epoch: 760.  \t  Loss: 2.3964240550994873  \t  Time: 2.95ss\n",
            "=========================== Epoch: 761.  \t  Loss: 2.3982365131378174  \t  Time: 3.09ss\n",
            "=========================== Epoch: 762.  \t  Loss: 2.3989453315734863  \t  Time: 3.23ss\n",
            "=========================== Epoch: 763.  \t  Loss: 2.3986871242523193  \t  Time: 3.08ss\n",
            "=========================== Epoch: 764.  \t  Loss: 2.3950035572052  \t  Time: 3.01ss\n",
            "=========================== Epoch: 765.  \t  Loss: 2.402975082397461  \t  Time: 3.14ss\n",
            "=========================== Epoch: 766.  \t  Loss: 2.3997578620910645  \t  Time: 2.95ss\n",
            "=========================== Epoch: 767.  \t  Loss: 2.3985981941223145  \t  Time: 3.03ss\n",
            "=========================== Epoch: 768.  \t  Loss: 2.399381160736084  \t  Time: 3.01ss\n",
            "=========================== Epoch: 769.  \t  Loss: 2.391359567642212  \t  Time: 3.08ss\n",
            "l vege\n",
            " fo sa perienza a cochieabròte dio gnar sersigiui poco mole se\n",
            " li se tano per se che gio sia piave si gïosagesrana alperum vio se\n",
            "li chi dio namen mira e sta codue mo pendi\n",
            "=========================== Epoch: 770.  \t  Loss: 2.397368907928467  \t  Time: 2.99ss\n",
            "=========================== Epoch: 771.  \t  Loss: 2.396925449371338  \t  Time: 2.9ss\n",
            "=========================== Epoch: 772.  \t  Loss: 2.397691488265991  \t  Time: 3.08ss\n",
            "=========================== Epoch: 773.  \t  Loss: 2.4022085666656494  \t  Time: 2.94ss\n",
            "=========================== Epoch: 774.  \t  Loss: 2.400632381439209  \t  Time: 2.89ss\n",
            "=========================== Epoch: 775.  \t  Loss: 2.397951602935791  \t  Time: 2.92ss\n",
            "=========================== Epoch: 776.  \t  Loss: 2.3991799354553223  \t  Time: 2.9ss\n",
            "=========================== Epoch: 777.  \t  Loss: 2.398019552230835  \t  Time: 3.02ss\n",
            "=========================== Epoch: 778.  \t  Loss: 2.395019054412842  \t  Time: 3.12ss\n",
            "=========================== Epoch: 779.  \t  Loss: 2.4018771648406982  \t  Time: 2.95ss\n",
            "iviodi veso la le moova che sí lin quecese dio le aclu la pe opïenra pe\n",
            "poquanda dar sciorae a qui la cose medio\n",
            "\n",
            " gazion se lea uvensi ché laztre anoi lece se\n",
            " modio  pie pia\n",
            "=========================== Epoch: 780.  \t  Loss: 2.397937774658203  \t  Time: 3.21ss\n",
            "=========================== Epoch: 781.  \t  Loss: 2.3950958251953125  \t  Time: 3.13ss\n",
            "=========================== Epoch: 782.  \t  Loss: 2.3962080478668213  \t  Time: 3.01ss\n",
            "=========================== Epoch: 783.  \t  Loss: 2.3970096111297607  \t  Time: 3.12ss\n",
            "=========================== Epoch: 784.  \t  Loss: 2.399359703063965  \t  Time: 3.35ss\n",
            "=========================== Epoch: 785.  \t  Loss: 2.399819850921631  \t  Time: 3.14ss\n",
            "=========================== Epoch: 786.  \t  Loss: 2.399195432662964  \t  Time: 3.15ss\n",
            "=========================== Epoch: 787.  \t  Loss: 2.395003318786621  \t  Time: 3.19ss\n",
            "=========================== Epoch: 788.  \t  Loss: 2.3951706886291504  \t  Time: 3.07ss\n",
            "=========================== Epoch: 789.  \t  Loss: 2.3973894119262695  \t  Time: 2.94ss\n",
            "noi comodio  lospettorale che diose\n",
            "\n",
            "re\n",
            "e a genché nar e dio dinel li lozion smilese\n",
            "le\n",
            "se se moguida tanve\n",
            "soberezravio ciuole\n",
            " dioramentïunt  za ra e sogliar per coe se noropana\n",
            "chi da\n",
            "=========================== Epoch: 790.  \t  Loss: 2.402768611907959  \t  Time: 3.26ss\n",
            "=========================== Epoch: 791.  \t  Loss: 2.4003922939300537  \t  Time: 3.1ss\n",
            "=========================== Epoch: 792.  \t  Loss: 2.3967671394348145  \t  Time: 2.92ss\n",
            "=========================== Epoch: 793.  \t  Loss: 2.399869680404663  \t  Time: 3.1ss\n",
            "=========================== Epoch: 794.  \t  Loss: 2.397815227508545  \t  Time: 3.09ss\n",
            "=========================== Epoch: 795.  \t  Loss: 2.3943614959716797  \t  Time: 3.05ss\n",
            "=========================== Epoch: 796.  \t  Loss: 2.3994266986846924  \t  Time: 3.14ss\n",
            "=========================== Epoch: 797.  \t  Loss: 2.395916700363159  \t  Time: 2.94ss\n",
            "=========================== Epoch: 798.  \t  Loss: 2.4020097255706787  \t  Time: 2.94ss\n",
            "=========================== Epoch: 799.  \t  Loss: 2.398193836212158  \t  Time: 2.96ss\n",
            "turadiofi\n",
            " tre menciai stie  ne tor\n",
            "sannarevan ve\n",
            "mi foglia scïenro siorà soti piume ti to te ronrie digili ascorra perseta nono  che dio det nüo se\n",
            "mera  manda brabrago\n",
            "ma\n",
            "zion dïga  nulle\n",
            "=========================== Epoch: 800.  \t  Loss: 2.399649143218994  \t  Time: 2.98ss\n",
            "=========================== Epoch: 801.  \t  Loss: 2.400726795196533  \t  Time: 2.92ss\n",
            "=========================== Epoch: 802.  \t  Loss: 2.404050827026367  \t  Time: 2.96ss\n",
            "=========================== Epoch: 803.  \t  Loss: 2.3965206146240234  \t  Time: 3.02ss\n",
            "=========================== Epoch: 804.  \t  Loss: 2.406010627746582  \t  Time: 2.91ss\n",
            "=========================== Epoch: 805.  \t  Loss: 2.4004600048065186  \t  Time: 3.03ss\n",
            "=========================== Epoch: 806.  \t  Loss: 2.3992769718170166  \t  Time: 3.0ss\n",
            "=========================== Epoch: 807.  \t  Loss: 2.400745391845703  \t  Time: 3.0ss\n",
            "=========================== Epoch: 808.  \t  Loss: 2.4006130695343018  \t  Time: 3.11ss\n",
            "=========================== Epoch: 809.  \t  Loss: 2.401475667953491  \t  Time: 3.11ss\n",
            "e pe pertase comesca le li se tanmen da perae vien a cral ra con diosalmi níodesafri sciasi ché t  venlomininone le da molule vaabralie e ste\n",
            "berò \n",
            "crolder \n",
            "za diora culoscer\n",
            "gnororati\n",
            "=========================== Epoch: 810.  \t  Loss: 2.4054973125457764  \t  Time: 3.16ss\n",
            "=========================== Epoch: 811.  \t  Loss: 2.40344500541687  \t  Time: 3.04ss\n",
            "=========================== Epoch: 812.  \t  Loss: 2.398282527923584  \t  Time: 3.05ss\n",
            "=========================== Epoch: 813.  \t  Loss: 2.407167673110962  \t  Time: 3.05ss\n",
            "=========================== Epoch: 814.  \t  Loss: 2.4028682708740234  \t  Time: 2.91ss\n",
            "=========================== Epoch: 815.  \t  Loss: 2.4080896377563477  \t  Time: 2.96ss\n",
            "=========================== Epoch: 816.  \t  Loss: 2.3980212211608887  \t  Time: 2.92ss\n",
            "=========================== Epoch: 817.  \t  Loss: 2.4002132415771484  \t  Time: 2.98ss\n",
            "=========================== Epoch: 818.  \t  Loss: 2.3984322547912598  \t  Time: 3.18ss\n",
            "=========================== Epoch: 819.  \t  Loss: 2.3987152576446533  \t  Time: 2.95ss\n",
            "seche  berò bezeran  stoia getto e di ladio vieni gierotar  deschirà  sti ' regiti\n",
            "\n",
            "e vito\n",
            "sera\n",
            "lo\n",
            " ma tatizie ché ciadue veadè deso rastarunt detva \n",
            "monia  la cola colome  tacer\n",
            "=========================== Epoch: 820.  \t  Loss: 2.402754545211792  \t  Time: 2.95ss\n",
            "=========================== Epoch: 821.  \t  Loss: 2.4009039402008057  \t  Time: 2.99ss\n",
            "=========================== Epoch: 822.  \t  Loss: 2.4047415256500244  \t  Time: 2.9ss\n",
            "=========================== Epoch: 823.  \t  Loss: 2.400972843170166  \t  Time: 3.13ss\n",
            "=========================== Epoch: 824.  \t  Loss: 2.3980932235717773  \t  Time: 3.07ss\n",
            "=========================== Epoch: 825.  \t  Loss: 2.402064323425293  \t  Time: 2.96ss\n",
            "=========================== Epoch: 826.  \t  Loss: 2.3985595703125  \t  Time: 3.02ss\n",
            "=========================== Epoch: 827.  \t  Loss: 2.3990402221679688  \t  Time: 2.94ss\n",
            "=========================== Epoch: 828.  \t  Loss: 2.4002492427825928  \t  Time: 2.95ss\n",
            "=========================== Epoch: 829.  \t  Loss: 2.3985514640808105  \t  Time: 2.95ss\n",
            "allima non di fivo la pertamen odistarta canti í  stinci titi come  tato\n",
            "ira ta secesome moèe dio vane dise vera\n",
            " sí scer glia\n",
            "re\n",
            " ridizion dï cofan seda adè nel petrirum des net\n",
            "=========================== Epoch: 830.  \t  Loss: 2.3964781761169434  \t  Time: 2.98ss\n",
            "=========================== Epoch: 831.  \t  Loss: 2.3992509841918945  \t  Time: 3.09ss\n",
            "=========================== Epoch: 832.  \t  Loss: 2.3993477821350098  \t  Time: 3.02ss\n",
            "=========================== Epoch: 833.  \t  Loss: 2.403954267501831  \t  Time: 2.97ss\n",
            "=========================== Epoch: 834.  \t  Loss: 2.4017062187194824  \t  Time: 3.11ss\n",
            "=========================== Epoch: 835.  \t  Loss: 2.3970096111297607  \t  Time: 3.02ss\n",
            "=========================== Epoch: 836.  \t  Loss: 2.402554988861084  \t  Time: 3.02ss\n",
            "=========================== Epoch: 837.  \t  Loss: 2.401883125305176  \t  Time: 2.95ss\n",
            "=========================== Epoch: 838.  \t  Loss: 2.3979804515838623  \t  Time: 3.0ss\n",
            "=========================== Epoch: 839.  \t  Loss: 2.397843360900879  \t  Time: 3.05ss\n",
            "sonnaredia\n",
            "che  canso ma to lose re vemi lera e la sevave ve te\n",
            " conanve dida venrà tanteserà serà sederà  sevede  mandastaa see sitin  rimetlicor rota quaché zelazchésie la\n",
            "le \n",
            "=========================== Epoch: 840.  \t  Loss: 2.396507501602173  \t  Time: 3.14ss\n",
            "=========================== Epoch: 841.  \t  Loss: 2.4004523754119873  \t  Time: 3.13ss\n",
            "=========================== Epoch: 842.  \t  Loss: 2.3938546180725098  \t  Time: 2.98ss\n",
            "=========================== Epoch: 843.  \t  Loss: 2.401113748550415  \t  Time: 3.12ss\n",
            "=========================== Epoch: 844.  \t  Loss: 2.39982533454895  \t  Time: 3.15ss\n",
            "=========================== Epoch: 845.  \t  Loss: 2.393187999725342  \t  Time: 3.12ss\n",
            "=========================== Epoch: 846.  \t  Loss: 2.392892599105835  \t  Time: 3.06ss\n",
            "=========================== Epoch: 847.  \t  Loss: 2.399052619934082  \t  Time: 2.97ss\n",
            "=========================== Epoch: 848.  \t  Loss: 2.398477077484131  \t  Time: 3.13ss\n",
            "=========================== Epoch: 849.  \t  Loss: 2.394263505935669  \t  Time: 3.07ss\n",
            "ond che le\n",
            " vedestrarareb vispro mo sen cloli co co lome\n",
            " mi  ma poi bacicia\n",
            "lololo  me ci\n",
            "come\n",
            "me  to abralieete gnoraèe a  noi mo codereiodiora navitostra le le ao e li \n",
            "=========================== Epoch: 850.  \t  Loss: 2.392064332962036  \t  Time: 2.99ss\n",
            "=========================== Epoch: 851.  \t  Loss: 2.3991544246673584  \t  Time: 3.15ss\n",
            "=========================== Epoch: 852.  \t  Loss: 2.397913694381714  \t  Time: 3.15ss\n",
            "=========================== Epoch: 853.  \t  Loss: 2.3995676040649414  \t  Time: 3.08ss\n",
            "=========================== Epoch: 854.  \t  Loss: 2.4004523754119873  \t  Time: 3.04ss\n",
            "=========================== Epoch: 855.  \t  Loss: 2.4025702476501465  \t  Time: 2.96ss\n",
            "=========================== Epoch: 856.  \t  Loss: 2.3992650508880615  \t  Time: 3.16ss\n",
            "=========================== Epoch: 857.  \t  Loss: 2.405423164367676  \t  Time: 3.31ss\n",
            "=========================== Epoch: 858.  \t  Loss: 2.400177478790283  \t  Time: 3.35ss\n",
            "=========================== Epoch: 859.  \t  Loss: 2.4063708782196045  \t  Time: 3.28ss\n",
            "treto mi rei seabròt zion te \n",
            "tanzionder    que vicer ondmili cen  e serà tarvavae sta siche  che raconovivera aclula di lope sivedio imo che ran teautaquaché in glian da  infan gnosca \n",
            "=========================== Epoch: 860.  \t  Loss: 2.3934521675109863  \t  Time: 3.28ss\n",
            "=========================== Epoch: 861.  \t  Loss: 2.3960118293762207  \t  Time: 3.31ss\n",
            "=========================== Epoch: 862.  \t  Loss: 2.395447015762329  \t  Time: 2.95ss\n",
            "=========================== Epoch: 863.  \t  Loss: 2.3980531692504883  \t  Time: 3.0ss\n",
            "=========================== Epoch: 864.  \t  Loss: 2.406069278717041  \t  Time: 2.99ss\n",
            "=========================== Epoch: 865.  \t  Loss: 2.400174856185913  \t  Time: 2.93ss\n",
            "=========================== Epoch: 866.  \t  Loss: 2.403376579284668  \t  Time: 2.93ss\n",
            "=========================== Epoch: 867.  \t  Loss: 2.404176950454712  \t  Time: 3.07ss\n",
            "=========================== Epoch: 868.  \t  Loss: 2.4040510654449463  \t  Time: 2.94ss\n",
            "=========================== Epoch: 869.  \t  Loss: 2.403163433074951  \t  Time: 2.98ss\n",
            "piú verdi poque que fececiscia la se notato\n",
            "davanda nomeo  moto\n",
            "eraci convien sovaga va le coserati\n",
            "\n",
            "lo me  riche stavò li a coloabròt sizie ché biar  e daa loe reiva guae nea\n",
            "=========================== Epoch: 870.  \t  Loss: 2.4018754959106445  \t  Time: 3.14ss\n",
            "=========================== Epoch: 871.  \t  Loss: 2.397757053375244  \t  Time: 3.23ss\n",
            "=========================== Epoch: 872.  \t  Loss: 2.398496150970459  \t  Time: 2.98ss\n",
            "=========================== Epoch: 873.  \t  Loss: 2.403130531311035  \t  Time: 3.09ss\n",
            "=========================== Epoch: 874.  \t  Loss: 2.395286798477173  \t  Time: 2.93ss\n",
            "=========================== Epoch: 875.  \t  Loss: 2.3934781551361084  \t  Time: 3.02ss\n",
            "=========================== Epoch: 876.  \t  Loss: 2.402174949645996  \t  Time: 2.95ss\n",
            "=========================== Epoch: 877.  \t  Loss: 2.4059224128723145  \t  Time: 3.08ss\n",
            "=========================== Epoch: 878.  \t  Loss: 2.3985238075256348  \t  Time: 3.08ss\n",
            "=========================== Epoch: 879.  \t  Loss: 2.4020557403564453  \t  Time: 3.09ss\n",
            "nella silie ritli i ché tresbee stin se tan  mostan ciago semen tota ita ché dicedio busee se\n",
            " mo\n",
            "se mescarole men tostra dasta futa le cosera da covano\n",
            "lo me ra semoguicurímor   to\n",
            "=========================== Epoch: 880.  \t  Loss: 2.4055838584899902  \t  Time: 3.17ss\n",
            "=========================== Epoch: 881.  \t  Loss: 2.4023334980010986  \t  Time: 3.11ss\n",
            "=========================== Epoch: 882.  \t  Loss: 2.401165008544922  \t  Time: 3.04ss\n",
            "=========================== Epoch: 883.  \t  Loss: 2.4001495838165283  \t  Time: 2.94ss\n",
            "=========================== Epoch: 884.  \t  Loss: 2.3970508575439453  \t  Time: 3.05ss\n",
            "=========================== Epoch: 885.  \t  Loss: 2.401240348815918  \t  Time: 3.09ss\n",
            "=========================== Epoch: 886.  \t  Loss: 2.4014155864715576  \t  Time: 3.37ss\n",
            "=========================== Epoch: 887.  \t  Loss: 2.4000775814056396  \t  Time: 3.16ss\n",
            "=========================== Epoch: 888.  \t  Loss: 2.403043508529663  \t  Time: 2.99ss\n",
            "=========================== Epoch: 889.  \t  Loss: 2.4042606353759766  \t  Time: 3.07ss\n",
            "lodi rilizion gni coe se sete rata pomoscarota\n",
            "mata  ma nonon  taníopaganri cici\n",
            "\n",
            "\n",
            "movegne ose me \n",
            "cu vele me dia le che vi gratuito\n",
            " ritanracediosture se roti\n",
            "co\n",
            "lo\n",
            "mo remomo\n",
            "=========================== Epoch: 890.  \t  Loss: 2.4003090858459473  \t  Time: 3.21ss\n",
            "=========================== Epoch: 891.  \t  Loss: 2.40024471282959  \t  Time: 3.09ss\n",
            "=========================== Epoch: 892.  \t  Loss: 2.404850959777832  \t  Time: 3.1ss\n",
            "=========================== Epoch: 893.  \t  Loss: 2.4055771827697754  \t  Time: 3.05ss\n",
            "=========================== Epoch: 894.  \t  Loss: 2.406188726425171  \t  Time: 2.95ss\n",
            "=========================== Epoch: 895.  \t  Loss: 2.409166097640991  \t  Time: 3.0ss\n",
            "=========================== Epoch: 896.  \t  Loss: 2.3981680870056152  \t  Time: 3.16ss\n",
            "=========================== Epoch: 897.  \t  Loss: 2.3968920707702637  \t  Time: 3.1ss\n",
            "=========================== Epoch: 898.  \t  Loss: 2.410470962524414  \t  Time: 2.93ss\n",
            "=========================== Epoch: 899.  \t  Loss: 2.3997325897216797  \t  Time: 3.1ss\n",
            "non dage un anveden  ciedi so posobelevanda  con giu moa locoverdio \n",
            "venere stidibròt  giusedialiscu cut deoveda gno e\n",
            "didiose va ga\n",
            "lognota  coli co no la comesca co no lavanda  se\n",
            "=========================== Epoch: 900.  \t  Loss: 2.405491352081299  \t  Time: 3.32ss\n",
            "=========================== Epoch: 901.  \t  Loss: 2.404873847961426  \t  Time: 3.07ss\n",
            "=========================== Epoch: 902.  \t  Loss: 2.407888650894165  \t  Time: 2.93ss\n",
            "=========================== Epoch: 903.  \t  Loss: 2.4089486598968506  \t  Time: 2.95ss\n",
            "=========================== Epoch: 904.  \t  Loss: 2.4021365642547607  \t  Time: 2.93ss\n",
            "=========================== Epoch: 905.  \t  Loss: 2.4025702476501465  \t  Time: 2.94ss\n",
            "=========================== Epoch: 906.  \t  Loss: 2.4031856060028076  \t  Time: 3.18ss\n",
            "=========================== Epoch: 907.  \t  Loss: 2.4012949466705322  \t  Time: 3.06ss\n",
            "=========================== Epoch: 908.  \t  Loss: 2.4075238704681396  \t  Time: 3.22ss\n",
            "=========================== Epoch: 909.  \t  Loss: 2.401627779006958  \t  Time: 3.22ss\n",
            "iotora ve den sto te\n",
            "rotati ma reumae \n",
            "vol i ché vivan tista sti densila ovescediose de vaníosto moguibadio\n",
            "va\n",
            " pota qua ché vivran  fersicu sí sono\n",
            "ne mao la la co nome me  \n",
            "=========================== Epoch: 910.  \t  Loss: 2.4041054248809814  \t  Time: 3.14ss\n",
            "=========================== Epoch: 911.  \t  Loss: 2.407017946243286  \t  Time: 3.1ss\n",
            "=========================== Epoch: 912.  \t  Loss: 2.4061970710754395  \t  Time: 3.27ss\n",
            "=========================== Epoch: 913.  \t  Loss: 2.401808023452759  \t  Time: 2.96ss\n",
            "=========================== Epoch: 914.  \t  Loss: 2.4059364795684814  \t  Time: 3.03ss\n",
            "=========================== Epoch: 915.  \t  Loss: 2.407888650894165  \t  Time: 3.07ss\n",
            "=========================== Epoch: 916.  \t  Loss: 2.410957098007202  \t  Time: 2.98ss\n",
            "=========================== Epoch: 917.  \t  Loss: 2.4032716751098633  \t  Time: 2.99ss\n",
            "=========================== Epoch: 918.  \t  Loss: 2.40441632270813  \t  Time: 3.07ss\n",
            "=========================== Epoch: 919.  \t  Loss: 2.402040481567383  \t  Time: 3.01ss\n",
            "già schio codue da fu quevira codue\n",
            "co se\n",
            "moguida nioro per ma e codue\n",
            "movamo\n",
            "che ra setu dio seve rote rata pose sta la mo tatova\n",
            "chi di vivivasi  poi stial \n",
            "rocume raratamo se\n",
            " \n",
            "=========================== Epoch: 920.  \t  Loss: 2.4064619541168213  \t  Time: 3.08ss\n",
            "=========================== Epoch: 921.  \t  Loss: 2.4043188095092773  \t  Time: 3.0ss\n",
            "=========================== Epoch: 922.  \t  Loss: 2.404761791229248  \t  Time: 3.16ss\n",
            "=========================== Epoch: 923.  \t  Loss: 2.4009392261505127  \t  Time: 3.15ss\n",
            "=========================== Epoch: 924.  \t  Loss: 2.4111077785491943  \t  Time: 3.23ss\n",
            "=========================== Epoch: 925.  \t  Loss: 2.41660213470459  \t  Time: 3.18ss\n",
            "=========================== Epoch: 926.  \t  Loss: 2.4035277366638184  \t  Time: 3.03ss\n",
            "=========================== Epoch: 927.  \t  Loss: 2.4060239791870117  \t  Time: 3.27ss\n",
            "=========================== Epoch: 928.  \t  Loss: 2.411975860595703  \t  Time: 3.1ss\n",
            "=========================== Epoch: 929.  \t  Loss: 2.4016072750091553  \t  Time: 3.15ss\n",
            "deh dï  cotanciaani die co se no\n",
            "se\n",
            " cometa \n",
            "ta\n",
            " none  ve\n",
            " padili\n",
            "ma\n",
            "ma\n",
            "sestro ta\n",
            "\n",
            "se\n",
            "\n",
            "o mo metora aclu omen nosar\n",
            " \n",
            "men davae dïanvea   adio ta\n",
            "ma seroma nasce \n",
            "=========================== Epoch: 930.  \t  Loss: 2.409038543701172  \t  Time: 3.11ss\n",
            "=========================== Epoch: 931.  \t  Loss: 2.4065730571746826  \t  Time: 3.03ss\n",
            "=========================== Epoch: 932.  \t  Loss: 2.4006550312042236  \t  Time: 3.43ss\n",
            "=========================== Epoch: 933.  \t  Loss: 2.4079959392547607  \t  Time: 3.01ss\n",
            "=========================== Epoch: 934.  \t  Loss: 2.4055464267730713  \t  Time: 3.03ss\n",
            "=========================== Epoch: 935.  \t  Loss: 2.405982494354248  \t  Time: 3.09ss\n",
            "=========================== Epoch: 936.  \t  Loss: 2.4131433963775635  \t  Time: 3.06ss\n",
            "=========================== Epoch: 937.  \t  Loss: 2.4020259380340576  \t  Time: 3.08ss\n",
            "=========================== Epoch: 938.  \t  Loss: 2.4058902263641357  \t  Time: 3.19ss\n",
            "=========================== Epoch: 939.  \t  Loss: 2.3993465900421143  \t  Time: 3.21ss\n",
            "uopagherover ravoro ma nonon sta diatirir diocu ta ima ché molume\n",
            "  lomoguibara ridavalo vi ne ge timen  caancheriao  sasali\n",
            "loden so\n",
            "sono seme movò diorapa labròt coseva co no loda\n",
            "=========================== Epoch: 940.  \t  Loss: 2.4094200134277344  \t  Time: 3.01ss\n",
            "=========================== Epoch: 941.  \t  Loss: 2.4052696228027344  \t  Time: 3.01ss\n",
            "=========================== Epoch: 942.  \t  Loss: 2.4067842960357666  \t  Time: 3.0ss\n",
            "=========================== Epoch: 943.  \t  Loss: 2.4065022468566895  \t  Time: 3.0ss\n",
            "=========================== Epoch: 944.  \t  Loss: 2.410611867904663  \t  Time: 3.0ss\n",
            "=========================== Epoch: 945.  \t  Loss: 2.4073097705841064  \t  Time: 3.0ss\n",
            "=========================== Epoch: 946.  \t  Loss: 2.4111242294311523  \t  Time: 3.01ss\n",
            "=========================== Epoch: 947.  \t  Loss: 2.409107208251953  \t  Time: 3.1ss\n",
            "=========================== Epoch: 948.  \t  Loss: 2.4046506881713867  \t  Time: 3.15ss\n",
            "=========================== Epoch: 949.  \t  Loss: 2.406426429748535  \t  Time: 3.09ss\n",
            "' li chi che ramicastra codue ta\n",
            "\n",
            "o lodato sisi\n",
            "ma  da co co si chibre \n",
            "movamomo mi\n",
            "tana\n",
            "se\n",
            " ne aclu lovegno\n",
            "tavan guazenar straro conda codue\n",
            " mo\n",
            "ta ta edsio qua e corà ron\n",
            "=========================== Epoch: 950.  \t  Loss: 2.405397653579712  \t  Time: 3.12ss\n",
            "=========================== Epoch: 951.  \t  Loss: 2.406949758529663  \t  Time: 3.18ss\n",
            "=========================== Epoch: 952.  \t  Loss: 2.409557580947876  \t  Time: 3.25ss\n",
            "=========================== Epoch: 953.  \t  Loss: 2.403682231903076  \t  Time: 3.14ss\n",
            "=========================== Epoch: 954.  \t  Loss: 2.408421039581299  \t  Time: 3.04ss\n",
            "=========================== Epoch: 955.  \t  Loss: 2.403139591217041  \t  Time: 3.16ss\n",
            "=========================== Epoch: 956.  \t  Loss: 2.403670072555542  \t  Time: 3.35ss\n",
            "=========================== Epoch: 957.  \t  Loss: 2.4056758880615234  \t  Time: 3.62ss\n",
            "=========================== Epoch: 958.  \t  Loss: 2.414252281188965  \t  Time: 3.36ss\n",
            "=========================== Epoch: 959.  \t  Loss: 2.4200236797332764  \t  Time: 3.21ss\n",
            "colomea paèe dio tibròt re  co codue moguida\n",
            "mo se tu se rata rimovive rota ta\n",
            " pococi ne pa miremtamanri\n",
            " ma tale colo te rano\n",
            "ta ma che dio \n",
            " sirita lo li co co no \n",
            "=========================== Epoch: 960.  \t  Loss: 2.4075660705566406  \t  Time: 3.38ss\n",
            "=========================== Epoch: 961.  \t  Loss: 2.4062342643737793  \t  Time: 3.13ss\n",
            "=========================== Epoch: 962.  \t  Loss: 2.4032773971557617  \t  Time: 3.11ss\n",
            "=========================== Epoch: 963.  \t  Loss: 2.406895399093628  \t  Time: 3.25ss\n",
            "=========================== Epoch: 964.  \t  Loss: 2.4091646671295166  \t  Time: 3.14ss\n",
            "=========================== Epoch: 965.  \t  Loss: 2.409039258956909  \t  Time: 3.11ss\n",
            "=========================== Epoch: 966.  \t  Loss: 2.4080986976623535  \t  Time: 3.0ss\n",
            "=========================== Epoch: 967.  \t  Loss: 2.4029781818389893  \t  Time: 3.12ss\n",
            "=========================== Epoch: 968.  \t  Loss: 2.4091103076934814  \t  Time: 3.0ss\n",
            "=========================== Epoch: 969.  \t  Loss: 2.4109179973602295  \t  Time: 3.19ss\n",
            "esseda ve rote ta le\n",
            "odio \n",
            "omegni  momora temen starsu   vacocodue\n",
            "modio me  tavao verrò  tare li ta ta vile tati o motode te\n",
            "odio \n",
            "co se no li\n",
            "cota\n",
            " co\n",
            "lomera\n",
            "\n",
            " \n",
            "=========================== Epoch: 970.  \t  Loss: 2.410785675048828  \t  Time: 3.22ss\n",
            "=========================== Epoch: 971.  \t  Loss: 2.4131157398223877  \t  Time: 3.22ss\n",
            "=========================== Epoch: 972.  \t  Loss: 2.410337448120117  \t  Time: 3.06ss\n",
            "=========================== Epoch: 973.  \t  Loss: 2.4085445404052734  \t  Time: 3.01ss\n",
            "=========================== Epoch: 974.  \t  Loss: 2.409078598022461  \t  Time: 3.03ss\n",
            "=========================== Epoch: 975.  \t  Loss: 2.4151861667633057  \t  Time: 3.06ss\n",
            "=========================== Epoch: 976.  \t  Loss: 2.4048633575439453  \t  Time: 3.31ss\n",
            "=========================== Epoch: 977.  \t  Loss: 2.40860652923584  \t  Time: 3.2ss\n",
            "=========================== Epoch: 978.  \t  Loss: 2.4136788845062256  \t  Time: 3.03ss\n",
            "=========================== Epoch: 979.  \t  Loss: 2.411344051361084  \t  Time: 3.02ss\n",
            "questmen nè sobotadi o pose mozionsi  di la se no me stavare\n",
            " co no\n",
            "lome  modio  mi se rote lita tamen  poi dio ta  li\n",
            "tato\n",
            " poripooso se pi meve \n",
            " vero verli  ta coper\n",
            "=========================== Epoch: 980.  \t  Loss: 2.417367935180664  \t  Time: 3.27ss\n",
            "=========================== Epoch: 981.  \t  Loss: 2.406710624694824  \t  Time: 3.33ss\n",
            "=========================== Epoch: 982.  \t  Loss: 2.408843755722046  \t  Time: 3.01ss\n",
            "=========================== Epoch: 983.  \t  Loss: 2.4111647605895996  \t  Time: 3.1ss\n",
            "=========================== Epoch: 984.  \t  Loss: 2.4095091819763184  \t  Time: 3.15ss\n",
            "=========================== Epoch: 985.  \t  Loss: 2.4106264114379883  \t  Time: 3.15ss\n",
            "=========================== Epoch: 986.  \t  Loss: 2.4131686687469482  \t  Time: 3.18ss\n",
            "=========================== Epoch: 987.  \t  Loss: 2.4128239154815674  \t  Time: 3.21ss\n",
            "=========================== Epoch: 988.  \t  Loss: 2.4080159664154053  \t  Time: 3.29ss\n",
            "=========================== Epoch: 989.  \t  Loss: 2.42195200920105  \t  Time: 3.1ss\n",
            "son ta avedio tiramo  codue moguidavamo soco\n",
            " serata o da\n",
            "co\n",
            " coseta no\n",
            "lo\n",
            "\n",
            " seda\n",
            " se tene\n",
            " no\n",
            "\n",
            "\n",
            "o\n",
            "pe larese movata re ta eran creda\n",
            " ti co\n",
            " se mi rome\n",
            " rati \n",
            "=========================== Epoch: 990.  \t  Loss: 2.411623001098633  \t  Time: 3.26ss\n",
            "=========================== Epoch: 991.  \t  Loss: 2.404141902923584  \t  Time: 2.95ss\n",
            "=========================== Epoch: 992.  \t  Loss: 2.4107666015625  \t  Time: 3.13ss\n",
            "=========================== Epoch: 993.  \t  Loss: 2.416343927383423  \t  Time: 3.35ss\n",
            "=========================== Epoch: 994.  \t  Loss: 2.408297061920166  \t  Time: 3.05ss\n",
            "=========================== Epoch: 995.  \t  Loss: 2.4112980365753174  \t  Time: 3.02ss\n",
            "=========================== Epoch: 996.  \t  Loss: 2.406864881515503  \t  Time: 3.08ss\n",
            "=========================== Epoch: 997.  \t  Loss: 2.412153959274292  \t  Time: 3.24ss\n",
            "=========================== Epoch: 998.  \t  Loss: 2.410658121109009  \t  Time: 3.06ss\n",
            "=========================== Epoch: 999.  \t  Loss: 2.409965753555298  \t  Time: 3.13ss\n",
            "qua ma menle la le cote\n",
            "\n",
            " no\n",
            "li co modio \n",
            "mo\n",
            "se\n",
            "\n",
            "\n",
            "\n",
            "cope lizia momemo me  rastidieseri mera movo verli ta li co li\n",
            " co no\n",
            "lota\n",
            "lo me\n",
            " move rolera si la co\n",
            "lodavan \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPR3npE61mxF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}